<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - AI Paper Picks of the Day</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 80px;"></div>
            <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                Freshest
                Top Picks:
                <span class="highlightNumber" style="font-size: 28px;">54</span> out of <span
                    class="highlightNumber">251</span> arXiv AI
                papers
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="line-height: 1.2;">just released on</div>
                <div id="data-default-date" data-date="2024-09-05"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 10px;">
            </div>
        </div>


        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">00:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02482" target="_blank">@arXiv 2409.02482</a>
                    <span class="tweet-title">Fuzzy Objects, Sharp Rendering: Multi-Layer Meshes for Real-Time View
                        Synthesis</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tübingen, Meta Reality Labs</span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel representation called "k-SDF" for real-time view synthesis of fuzzy
                    objects. Unlike previous methods that rely on dense sampling or sorting, k-SDF uses multiple
                    semi-transparent mesh layers, each represented as a signed distance field (SDF), to efficiently
                    render fuzzy geometries.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">01:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02529" target="_blank">@arXiv 2409.02529</a>
                    <span class="tweet-title">Blurry Images? Not Anymore! New Autoencoder Uses Diffusion to Sharpen
                        Up</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google, xAI</span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel autoencoder architecture that leverages a diffusion-based loss function
                    for image reconstruction. Unlike previous approaches that rely on adversarial or perceptual losses,
                    this method utilizes the theoretical underpinnings of diffusion models to achieve better
                    reconstruction quality.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">01:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02778" target="_blank">@arXiv 2409.02778</a>
                    <span class="tweet-title">Stop the Negative Transfer! New Model Makes Multi-Output Gaussian
                        Processes Smarter</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, University of Iowa</span>
                </div>
                <div class="primary-text">
                    This research introduces a regularized multi-output Gaussian convolution process (MGCP) model that
                    addresses two key challenges in transfer learning: negative transfer and input domain inconsistency.
                    Unlike previous work, this model uses a special convolution process structure to select informative
                    sources and mitigate negative transfer, while simultaneously adapting inconsistent input domains
                    through a marginalization and expansion (DAME) technique.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">02:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02910" target="_blank">@arXiv 2409.02910</a>
                    <span class="tweet-title">Stop the Presses! Action Recognition Gets a Super-Image Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach for semi-supervised action recognition in videos using a 2D
                    image transformer. Unlike previous methods that rely on 3D video transformers, this approach
                    utilizes superimages, which are constructed by rearranging video frames into a grid format. This
                    allows for efficient processing and reduces computational complexity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">02:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02604" target="_blank">@arXiv 2409.02604</a>
                    <span class="tweet-title">LLMs: The New Sherlock Holmes of Missing Causal Variables?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CISPA Helmholtz Center for Information Security, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel task of using LLMs to hypothesize missing variables in a partially
                    known causal graph, a step that typically requires human expertise. It differs from previous work
                    that focused on using LLMs for causal reasoning after data collection and analysis.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">02:58</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02828" target="_blank">@arXiv 2409.02828</a>
                    <span class="tweet-title">Facial Expressions: A Chain of Thought for AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Chinese Academy of Sciences, Tsinghua University,
                        Pengcheng Laboratory...</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method called ExpLLM, which uses large language models to generate a
                    chain of thought (CoT) for facial expression recognition. Unlike previous approaches that rely on
                    facial action units (AUs) alone, ExpLLM analyzes the interactions and relationships between AUs to
                    provide a more comprehensive understanding of facial expressions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">03:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02877" target="_blank">@arXiv 2409.02877</a>
                    <span class="tweet-title">LLMs: Not Just Big Brains, But Brick-Built!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This paper proposes a modular approach to building LLMs, breaking them down into functional units
                    called "bricks." This differs from previous work that focuses on training monolithic models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">03:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02228" target="_blank">@arXiv 2409.02228</a>
                    <span class="tweet-title">Can AI Forget? It's Tricky, and Sometimes Hilarious!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research investigates the generalization of forgetting in language models, focusing on how the
                    ability to forget a specific task affects the model's performance on other, similar tasks. Unlike
                    previous work that primarily focused on removing specific facts or knowledge, this study examines
                    the broader implications of forgetting on a model's overall capabilities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">04:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02347" target="_blank">@arXiv 2409.02347</a>
                    <span class="tweet-title">Weight-Ensembling: A Greedier Approach to Soup Up Your Models!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, Microsoft Research</span>
                </div>
                <div class="primary-text">
                    This research introduces two novel weight-ensembling algorithms, "greedier" and "ranked," which
                    explore the relationship between functional diversity and performance dynamics in weight-ensembles.
                    Unlike previous work that primarily focused on a "greedy" approach, this study investigates the
                    impact of different diversity measures on the selection mechanism of these algorithms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">04:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02384" target="_blank">@arXiv 2409.02384</a>
                    <span class="tweet-title">Speech Tokenizers: A Benchmark for Sorting the Wheat from the
                        Chaff!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces STAB, a benchmark for evaluating speech tokenizers, which is more efficient
                    than evaluating them on downstream tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">04:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02813" target="_blank">@arXiv 2409.02813</a>
                    <span class="tweet-title">MMMU-Pro: AI's New Vision Test - Can It Really "See" and "Read"?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces MMMU-Pro, a more robust version of the MMMU benchmark, which aims to
                    evaluate multimodal AI models' true understanding and reasoning capabilities by filtering out
                    questions answerable by text-only models, augmenting candidate options, and introducing a
                    vision-only input setting.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">05:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02730" target="_blank">@arXiv 2409.02730</a>
                    <span class="tweet-title">Molecular Modeling Gets a Matrix Makeover: New Method Makes Quantum
                        Chemistry Calculations Faster</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google DeepMind, T U Berlin, Berlin Institute for the Foundation of
                        Learning and Data...</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for representing molecular structures using matrix
                    multiplication instead of Clebsch-Gordan operations, which are commonly used in equivariant machine
                    learning models. This approach significantly reduces computational complexity, making it more
                    efficient for learning molecular quantum properties.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">05:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02709" target="_blank">@arXiv 2409.02709</a>
                    <span class="tweet-title">Brain's Got Probabilities: A New Code for Thinking Under
                        Uncertainty</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Rochester, Duke University, NYU...</span>
                </div>
                <div class="primary-text">
                    This paper proposes a unified language for comparing three prominent models of probabilistic
                    computation in the brain: Probabilistic Population Codes (PPCs), Distributed Distributional Codes
                    (DDCs), and Neural Sampling Codes (NSCs). It also reviews key empirical data previously taken as
                    evidence for each model and describes how it may or may not be explainable by alternative proposals.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">06:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02224" target="_blank">@arXiv 2409.02224</a>
                    <span class="tweet-title">Feeling the Pressure: New Dataset Tracks Hand Poses and Pressure in
                        VR</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zürich, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces EgoPressure, a dataset that captures hand pressure and pose from an
                    egocentric perspective, unlike previous datasets that focused on static views.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">06:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02313" target="_blank">@arXiv 2409.02313</a>
                    <span class="tweet-title">Remembering the Past: How Memory Makes PDEs More Predictable</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research explores the impact of memory in neural networks for solving time-dependent partial
                    differential equations (PDEs). Unlike previous work that treats PDEs as Markovian systems, this
                    study investigates the benefits of explicitly incorporating past states into the model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">06:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02795" target="_blank">@arXiv 2409.02795</a>
                    <span class="tweet-title">LLM Alignment: A Unified View of Preference Learning for Large Language
                        Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Alibaba</span>
                </div>
                <div class="primary-text">
                    This research offers a unified framework for understanding preference learning in LLMs, breaking
                    down existing strategies into four components: model, data, feedback, and algorithm. This approach
                    helps to connect different methods and identify potential synergies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">07:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02438" target="_blank">@arXiv 2409.02438</a>
                    <span class="tweet-title">Cross-Modal Knowledge Distillation: When the Teacher's Side Hustle Matters
                        More</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, IEEE</span>
                </div>
                <div class="primary-text">
                    This research introduces the Non-target Divergence Hypothesis (NTDH), which posits that the
                    effectiveness of cross-modal knowledge distillation is determined by the distribution divergence of
                    non-target classes between modalities. This differs from previous work that focused on modality
                    alignment or feature fusion strategies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">07:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02908" target="_blank">@arXiv 2409.02908</a>
                    <span class="tweet-title">Masked Diffusion Models: Time-Agnostic, Masked, and Secretly Faster</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Nvidia</span>
                </div>
                <div class="primary-text">
                    This research delves into the inner workings of masked diffusion models (MDMs), revealing that both
                    their training and sampling processes are essentially time-agnostic, meaning they don't rely on the
                    continuous time variable that's a hallmark of traditional diffusion models. This finding connects
                    MDMs to masked models, demonstrating their equivalence and highlighting the potential for simpler,
                    more efficient training and sampling methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">08:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02817" target="_blank">@arXiv 2409.02817</a>
                    <span class="tweet-title">Secure ML Accelerators: Obsidian's State-Space Exploration for Faster
                        Inference</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Texas at Austin, ARM Holdings</span>
                </div>
                <div class="primary-text">
                    This research introduces Obsidian, a framework that combines analytical and cycle-accurate models to
                    optimize the mapping of machine learning kernels to secure ML accelerators. This approach addresses
                    the limitations of previous methods, which either relied on fast but inaccurate analytical models or
                    slow but precise cycle-accurate models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">08:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02851" target="_blank">@arXiv 2409.02851</a>
                    <span class="tweet-title">Single Image, 3D Human: Video Diffusion Makes It Happen!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sun Yat-sen University, CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for generating 3D human models from a single image using video
                    diffusion models. Unlike previous methods that often struggle with inconsistent views and artifacts,
                    this approach leverages a video diffusion model to generate temporally consistent views, resulting
                    in more realistic and accurate 3D human reconstructions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">09:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02389" target="_blank">@arXiv 2409.02389</a>
                    <span class="tweet-title">AI Gets Its Bearings: New Dataset Helps Robots Understand 3D Worlds</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset called MSQA, which is designed to help AI agents understand
                    and reason about 3D scenes. Unlike previous datasets, MSQA uses a multi-modal approach, combining
                    text, images, and point clouds to provide a more comprehensive understanding of the situation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">09:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02451" target="_blank">@arXiv 2409.02451</a>
                    <span class="tweet-title">Speech Synthesis Gets a Speed Boost: DDSP Makes Articulatory Vocoders
                        Faster and Smaller!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research integrates Differentiable Digital Signal Processing (DDSP) into articulatory
                    synthesis, a technique that uses physical movements of the vocal tract to generate speech. This
                    approach differs from previous methods by leveraging the efficiency of DDSP to create a more
                    parameter-efficient and faster model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">10:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02581" target="_blank">@arXiv 2409.02581</a>
                    <span class="tweet-title">Sparse Views, Big Pose: How a Random Cuboid Cracked Monocular Object Pose
                        Estimation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Chinese Academy of Sciences, University of Birmingham, NVIDIA</span>
                </div>
                <div class="primary-text">
                    This research introduces SGPose, a framework that estimates object pose from sparse views using
                    Gaussian-based methods. Unlike previous work that relies on CAD models or Structure-from-Motion
                    (SfM) pipelines, SGPose starts with a random cuboid initialization and learns geometric-aware depth
                    to guide object reconstruction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">10:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02489" target="_blank">@arXiv 2409.02489</a>
                    <span class="tweet-title">Brainwaves to the Rescue: New AI Uses Your Thoughts to Extract
                        Speech!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Bremen, National University of Singapore, The Chinese
                        University of Hong Kong</span>
                </div>
                <div class="primary-text">
                    This research uses EEG signals, a measure of brain activity, as the sole reference cue to guide a
                    speaker extraction model. Unlike previous work that relied on pre-recorded speech or visual cues,
                    this approach leverages the brain's response to the attended speech in real-time.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">10:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02428" target="_blank">@arXiv 2409.02428</a>
                    <span class="tweet-title">LLMs: The New Reward Function Whisperers!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford, New Jersey Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This research explores using large language models (LLMs) to design reward functions for
                    multi-objective reinforcement learning tasks. Unlike previous work that focuses on black-box
                    optimization, this paper proposes a white-box approach, separating reward code design and weight
                    assignment into distinct stages.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">11:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02239" target="_blank">@arXiv 2409.02239</a>
                    <span class="tweet-title">Speech Recognition Gets a Time-Traveling Upgrade: New Research Uses
                        Temporal Order to Boost Accuracy!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National Institute of Information and Communications Technology,
                        Academia Sinica</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for cross-modal knowledge transfer in automatic speech
                    recognition (ASR) that preserves the temporal order of acoustic and linguistic features. Unlike
                    previous methods that treat these features as unordered sets, this approach explicitly incorporates
                    temporal information during feature alignment, leading to improved ASR performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">11:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02919" target="_blank">@arXiv 2409.02919</a>
                    <span class="tweet-title">Tired of blurry, repetitive AI art? HiPrompt's got your high-res
                        fix!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Hong Kong University of Science and Technology, Nanyang Technological
                        University, Tsinghua University...</span>
                </div>
                <div class="primary-text">
                    This research introduces HiPrompt, a method for generating higher-resolution images without
                    fine-tuning the model. It uses hierarchical prompts, which provide both global and local guidance
                    for the image generation process, addressing the issue of object repetition and structural artifacts
                    that plague other methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">12:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02751" target="_blank">@arXiv 2409.02751</a>
                    <span class="tweet-title">Pre-training vs. Self-training: A Head-to-Head Showdown in NLP!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research compares pre-training and self-training in semi-supervised learning (SSL) using a
                    consistent foundational setting of language models. It examines all feasible training paradigms
                    combining these two methods, unlike previous studies that focused on specific combinations or used
                    different settings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">12:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02897" target="_blank">@arXiv 2409.02897</a>
                    <span class="tweet-title">LLMs Get Citation-Savvy: Fine-Grained Footnotes for Long-Form
                        Answers</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Zhipu AI</span>
                </div>
                <div class="primary-text">
                    This research focuses on enabling long-context LLMs to generate responses with sentence-level
                    citations, a feature missing in previous work. The paper introduces a novel pipeline, CoF, to
                    automatically construct a large-scale dataset for training LLMs to generate these fine-grained
                    citations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">13:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02415" target="_blank">@arXiv 2409.02415</a>
                    <span class="tweet-title">SD Maps: Giving Autonomous Cars a Sense of Direction (and a Cheaper
                        One!)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Beihang University, University of
                        Wisconsin-Madison</span>
                </div>
                <div class="primary-text">
                    This research focuses on using Standard Definition Maps (SD Maps) as prior information for local map
                    construction in autonomous driving, a departure from the traditional reliance on high-definition
                    maps (HD Maps).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">13:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02392" target="_blank">@arXiv 2409.02392</a>
                    <span class="tweet-title">Math Agents Get a Preference Upgrade: Direct Preference Learning for
                        Tool-Integrated Reasoning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Illinois, Google</span>
                </div>
                <div class="primary-text">
                    This research introduces a multi-turn direct preference learning framework for training LLMs to
                    solve mathematical problems using external tools. Unlike previous work that focuses on synthetic
                    data generation and supervised fine-tuning, this approach directly learns from human feedback on the
                    quality of reasoning trajectories.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">14:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02416" target="_blank">@arXiv 2409.02416</a>
                    <span class="tweet-title">Move Over, Wasserstein! A New Distance Metric That's
                        Translation-Invariant</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">George Washington University, UC Los Angeles, Princeton
                        University</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new family of distances called "relative-translation invariant Wasserstein
                    distances" (RWp). Unlike the classical Wasserstein distance, RWp is invariant to translations of the
                    probability distributions, making it more robust to distribution shifts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">14:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02580" target="_blank">@arXiv 2409.02580</a>
                    <span class="tweet-title">Group Decisions: When Everyone's Happy (Except Maybe the Pizza)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Hong Kong, Hong Kong Polytechnic University, CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a new group recommendation method called AlignGroup, which focuses on both
                    group consensus and individual preferences of group members. Unlike previous methods that primarily
                    rely on aggregating individual preferences or capturing group consensus through multi-view
                    information, AlignGroup utilizes a hypergraph neural network to learn both intra- and inter-group
                    relationships, effectively capturing the group decision-making process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">15:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02327" target="_blank">@arXiv 2409.02327</a>
                    <span class="tweet-title">Factor Models Get a Predictive Makeover: Supervised Latent Variables for
                        Better Brain Stimulation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Duke University, Emory University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel inference algorithm called generative principal component
                    regression (gPCR) that incorporates predictive information into generative models. Unlike previous
                    methods like supervised variational autoencoders (SVAEs), gPCR ensures that the latent variables are
                    relevant to the outcome of interest, even when the outcome is a low-variance signal.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">15:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02141" target="_blank">@arXiv 2409.02141</a>
                    <span class="tweet-title">Tool Retrieval: From Descriptions to Usage, LLMs Get Smarter!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley, POSCO HOLDINGS, LBNL</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel two-stage tool retrieval method that leverages usage-driven tool
                    embeddings instead of relying solely on tool descriptions. This approach addresses the semantic gap
                    between tool descriptions and user queries, leading to more accurate tool retrieval.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">15:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02343" target="_blank">@arXiv 2409.02343</a>
                    <span class="tweet-title">Embeddings Get a Nudge: Fine-Tuning Without the Fuss!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This paper introduces NUDGE, a non-parametric approach to fine-tuning embeddings for retrieval
                    tasks. Unlike previous methods that fine-tune the entire model or train adaptors, NUDGE directly
                    modifies the embeddings of data records, making it more efficient and accurate.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">16:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02368" target="_blank">@arXiv 2409.02368</a>
                    <span class="tweet-title">Salient Object Detection: It's Not Just One Mask, It's a Party!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University at Buffalo, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces "pluralistic salient object detection" (PSOD), which acknowledges the
                    inherent ambiguity in defining salient objects in real-world images. Unlike traditional methods that
                    produce a single segmentation mask, PSOD generates multiple plausible masks, reflecting the diverse
                    interpretations possible.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">17:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02549" target="_blank">@arXiv 2409.02549</a>
                    <span class="tweet-title">Perimeter Hunting: A Game of Congestion!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Ben-Gurion University Of The Negev</span>
                </div>
                <div class="primary-text">
                    This research proposes a sequential decision-making model for perimeter identification, framing it
                    as a game between an agent and the environment. Unlike previous methods that rely on static or
                    dynamic analysis, this approach uses real-time congestion heatmaps and a reinforcement learning
                    algorithm to find optimal perimeters.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">17:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02148" target="_blank">@arXiv 2409.02148</a>
                    <span class="tweet-title">AI to the Rescue: Foundation Models Power Up the Grid!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM Research Europe, IBM Research</span>
                </div>
                <div class="primary-text">
                    This research proposes using Foundation Models (FMs) trained on power flow dynamics to accelerate
                    grid analysis and optimization, a novel approach compared to previous AI applications in the field.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">17:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02135" target="_blank">@arXiv 2409.02135</a>
                    <span class="tweet-title">Quantum Annealing Gets a Gradient Boost: New Solver Finds Optimal
                        Solutions Faster!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Fujitsu Limited, University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research proposes a new approach to combinatorial optimization that integrates gradient-based
                    updates through continuous relaxation, combined with Quasi-Quantum Annealing (QQA). This differs
                    from previous work by leveraging gradient information and introducing an extended Boltzmann
                    distribution with a communication term between parallel runs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">18:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02644" target="_blank">@arXiv 2409.02644</a>
                    <span class="tweet-title">Conformal Prediction: A New Way to Predict Biological Systems' Behavior,
                        Without the Bayesian Blues!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Spanish National Research Council, Harvard University</span>
                </div>
                <div class="primary-text">
                    This research proposes two novel algorithms for conformal prediction in dynamic biological systems,
                    offering an alternative to traditional Bayesian methods. The algorithms are designed to optimize
                    statistical efficiency, especially when dealing with limited data, and provide non-asymptotic
                    guarantees for prediction regions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">18:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02747" target="_blank">@arXiv 2409.02747</a>
                    <span class="tweet-title">Learning from the Past: A New Trick for Making Robots Smarter in Complex
                        Worlds</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Pompeu Fabra University, Sapienza University of Rome, University of
                        Oxford...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new way to learn from past experiences in complex environments where the
                    current situation depends on the entire history of events. Previous methods relied on a metric
                    called L-infinity distinguishability, which can be inefficient in certain scenarios. This paper
                    proposes a new metric based on formal languages, which can be more efficient in these cases.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">19:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02346" target="_blank">@arXiv 2409.02346</a>
                    <span class="tweet-title">Federated Learning Gets a LoRA-ly Boost: Alternating Minimization for
                        Robust Fine-Tuning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research introduces RoLoRA, a federated fine-tuning framework that uses alternating
                    minimization for LoRA. Unlike previous methods that directly aggregate LoRA modules, RoLoRA
                    alternates between updating the down-projection and up-projection matrices, leading to greater
                    robustness against decreasing fine-tuning parameters and increasing data heterogeneity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">19:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02596" target="_blank">@arXiv 2409.02596</a>
                    <span class="tweet-title">Attention, Please! Linear Complexity Attention Substitutes for Speech SSL
                        Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Avignon Université, Samsung AI Center Cambridge, Univervist´e Grenoble
                        Alpes...</span>
                </div>
                <div class="primary-text">
                    This research investigates the performance of linear complexity attention substitutes in a
                    self-supervised learning (SSL) setting for speech tasks, a novel application compared to previous
                    work that focused on supervised tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">20:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02864" target="_blank">@arXiv 2409.02864</a>
                    <span class="tweet-title">Bioinformatics Bot: A Digital Lab Assistant That's Actually Helpful</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Michigan</span>
                </div>
                <div class="primary-text">
                    This research introduces BRAD, a digital assistant that integrates a suite of tools for
                    bioinformatics tasks, including code execution, online search, and question-and-answering. Unlike
                    previous work, BRAD utilizes a multi-agent system of LLMs to handle complex workflows and distribute
                    tasks across agents.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">20:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02430" target="_blank">@arXiv 2409.02430</a>
                    <span class="tweet-title">Deep Receivers on a Diet: How Poisoning Attacks Can Make Them Choke on
                        Data</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanyang Technological University</span>
                </div>
                <div class="primary-text">
                    This research focuses on poisoning attacks against online deep receivers, which are used in wireless
                    communication systems to adapt to dynamic channels. Unlike previous work that primarily focused on
                    evasion attacks, this paper explores how malicious users can corrupt the training data used by these
                    receivers, leading to performance degradation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">20:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02449" target="_blank">@arXiv 2409.02449</a>
                    <span class="tweet-title">Whisper's Big Mistake: How Normalization Is Messing Up Multilingual Speech
                        Recognition</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Digital University Kerala</span>
                </div>
                <div class="primary-text">
                    This research focuses on the unintended consequences of text normalization routines used in
                    evaluating multilingual Automatic Speech Recognition (ASR) models, particularly for Indic languages.
                    Unlike previous work, it highlights how these routines, while aiming to standardize outputs, can
                    actually distort the linguistic structure of languages like Hindi, Tamil, and Malayalam, leading to
                    inaccurate performance metrics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">21:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02231" target="_blank">@arXiv 2409.02231</a>
                    <span class="tweet-title">Llama-ing for Drugs: AI Learns to Design Molecules with a Smile</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research explores using a pre-trained Large Language Model (LLM) called Llama to generate
                    drug-like molecules. Unlike previous work that focused on training specialized chemical language
                    models (CLMs) on SMILES strings, this study demonstrates that fine-tuning an LLM on chemical data
                    can achieve comparable or even better performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">21:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02866" target="_blank">@arXiv 2409.02866</a>
                    <span class="tweet-title">Crack Detection: When Transformers Meet CNNs, Infrastructure Wins!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research introduces Hybrid-Segmentor, a crack segmentation model that combines CNN and
                    Transformer architectures to improve accuracy and generalization capabilities. Unlike previous
                    models that rely solely on one architecture, this hybrid approach leverages the strengths of both
                    CNNs and Transformers for more robust crack detection.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">22:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02150" target="_blank">@arXiv 2409.02150</a>
                    <span class="tweet-title">Asteroid Apocalypse? Not So Fast! AI to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National Central University, California Institute of Technology,
                        NASA</span>
                </div>
                <div class="primary-text">
                    This research compares the performance of various machine learning and deep learning models for
                    classifying hazardous asteroids, using two different datasets. It highlights the effectiveness of
                    Random Forest and CatBoost algorithms in achieving high accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">22:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02154" target="_blank">@arXiv 2409.02154</a>
                    <span class="tweet-title">N-body Simulations Get a Speed Boost: Machine Learning Meets
                        Physics!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Institut d’Astrophysique de Paris</span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework called COmoving Computer Acceleration (COCA) that combines
                    machine learning with N-body simulations. Unlike previous approaches that directly emulate
                    simulation outputs, COCA emulates a frame of reference within which the simulation is run, allowing
                    for correction of emulation errors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">22:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02691" target="_blank">@arXiv 2409.02691</a>
                    <span class="tweet-title">LLMs: The New BFFs of Visual Analytics?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">City University of London</span>
                </div>
                <div class="primary-text">
                    This research explores the integration of large language models (LLMs) into visual analytics (VA)
                    systems, focusing on the opportunities and challenges associated with leveraging these powerful
                    language models to enhance mixed-initiative VA systems. It goes beyond previous work by examining
                    how LLMs can be used to support the entire VA pipeline, including data management, language
                    interaction, visualisation generation, and language generation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">23:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02136" target="_blank">@arXiv 2409.02136</a>
                    <span class="tweet-title">LLMs vs. CMLs: Who Wins the COVID-19 Mortality Prediction Game?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Shahid Beheshti University of Medical Sciences, Isfahan University of
                        Medical Sciences, Ontario Tech University...</span>
                </div>
                <div class="primary-text">
                    This research compares the performance of large language models (LLMs) and classical machine
                    learning (CML) models in predicting COVID-19 mortality using a high-dimensional tabular dataset.
                    Unlike previous studies that focused on low-dimensional datasets and limited sample sizes, this
                    study utilizes a large dataset with 9,134 patients and 81 features, allowing for a more robust
                    comparison.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">23:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02391" target="_blank">@arXiv 2409.02391</a>
                    <span class="tweet-title">AI Translators: Faster, Better, and More Money?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This paper goes beyond simply showing that LLMs can improve translation productivity. It
                    investigates how the size of the LLM, measured by training compute, directly impacts translator
                    performance and earnings.
                </div>
            </div>
        </div>

        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202409052044_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>

</html>