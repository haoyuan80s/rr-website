
daily_data = {
    "date": "2024-10-16",
    "tweets": [
        
        {
            "startTime": "00:55",
            "arxivId": "2410.11702",
            "arxivLink": "https://arxiv.org/abs/2410.11702",
            "title": "Tired of the Same Old Captions? This AI Makes Videos Unique!",
            "institute": "University of Bristol, University of Oxford",
            "text": "This research focuses on generating unique captions for visually similar video clips, unlike previous work that captions clips independently. The authors propose a method called Captioning by Discriminative Prompting (CDP) that predicts properties that distinguish clips with identical captions, leading to more specific descriptions.",
            "paper-title": "It's Just Another Day: Unique Video Captioning by Discriminative Prompting",
            "image-path": "flux_paper_image/2410.11702_1729108713.png"
        },

        {
            "startTime": "01:18",
            "arxivId": "2410.11439",
            "arxivLink": "https://arxiv.org/abs/2410.11439",
            "title": "One Model to Rule Them All: Unifying Diffusion-Based Image Generation",
            "institute": "Shanghai Jiao Tong University, Google",
            "text": "This research proposes a unified framework for handling diverse conditional image generation tasks, such as controllable generation, estimation, and joint generation, using a single diffusion model. Unlike previous attempts at unification, which often introduced significant complexity, this approach uses a simple, computationally efficient training strategy and minimal learned parameters.",
            "paper-title": "A Simple Approach to Unifying Diffusion-based Conditional Generation",
            "image-path": "flux_paper_image/2410.11439_1729107970.png"
        },

        {
            "startTime": "01:41",
            "arxivId": "2410.11824",
            "arxivLink": "https://arxiv.org/abs/2410.11824",
            "title": "AI Can't Tell a Peacock from a Pizza? New Benchmark Tests Image Generators' Knowledge",
            "institute": "Google DeepMind, University of California Merced",
            "text": "This research introduces a new benchmark called KITTEN, which focuses on evaluating the ability of text-to-image models to generate images that accurately represent real-world entities, unlike previous benchmarks that primarily assess aesthetic appeal or alignment with text prompts.",
            "paper-title": "KITTEN: A Knowledge-Intensive Evaluation of Image Generation on Visual Entities",
            "image-path": "flux_paper_image/2410.11824_1729109176.png"
        },

        {
            "startTime": "01:58",
            "arxivId": "2410.11838",
            "arxivLink": "https://arxiv.org/abs/2410.11838",
            "title": "Frame Interpolation Gets a High-Res Makeover: Patch-Based Diffusion to the Rescue!",
            "institute": "Google, University of Toronto, Vector Institute",
            "text": "This paper introduces a patch-based cascaded diffusion model for frame interpolation, HiFI, which addresses the challenges of processing high-resolution inputs and handling complex motion. Unlike previous methods that rely on domain knowledge, HiFI leverages model capacity and large-scale training data for improved performance and generalization.",
            "paper-title": "High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion",
            "image-path": "flux_paper_image/2410.11838_1729108541.png"
        },

        {
            "startTime": "02:23",
            "arxivId": "2410.11520",
            "arxivLink": "https://arxiv.org/abs/2410.11520",
            "title": "No More Markers! AI Captures Your Whole Body, Even Your Tongue!",
            "institute": "Microsoft",
            "text": "This research introduces a novel technique for holistic performance capture that utilizes a hybrid approach combining machine learning models trained on synthetic data with powerful parametric models of human shape and motion. Unlike previous methods that often rely on specialized hardware, markers, or limited capture scope, this approach achieves high-quality results without requiring any calibration, manual intervention, or custom hardware.",
            "paper-title": "Look Ma, no markers: holistic performance capture without the hassle",
            "image-path": "flux_paper_image/2410.11520_1729109384.png"
        },

        {
            "startTime": "02:41",
            "arxivId": "2410.11135",
            "arxivLink": "https://arxiv.org/abs/2410.11135",
            "title": "State Space Models: They Want to Be Transformers, and This Trick Helps Them Get Closer!",
            "institute": "CMU",
            "text": "This paper proposes a novel initialization technique for state space models (SSMs) that allows them to more readily mimic self-attention, improving their performance on tasks involving copying and recall. This approach differs from previous work by focusing on the initialization of the state matrix, rather than adding additional layers or modifying the architecture.",
            "paper-title": "Mimetic Initialization Helps State Space Models Learn to Recall",
            "image-path": "flux_paper_image/2410.11135_1729109256.png"
        },

        {
            "startTime": "03:09",
            "arxivId": "2410.11792",
            "arxivLink": "https://arxiv.org/abs/2410.11792",
            "title": "Robot Learns New Tricks by Watching YouTube!",
            "institute": "UT Austin, NVIDIA Research",
            "text": "This research introduces \"object-aware retargeting,\" a technique that allows humanoid robots to learn manipulation skills from single video demonstrations by adapting human motions to the specific locations of objects in the environment. This differs from previous work that either relied on large amounts of data or ignored the embodiment of the human demonstrator.",
            "paper-title": "OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation",
            "image-path": "flux_paper_image/2410.11792_1729108526.png"
        },

        {
            "startTime": "03:37",
            "arxivId": "2410.11087",
            "arxivLink": "https://arxiv.org/abs/2410.11087",
            "title": "Vision Models Get a Spatial Upgrade: Locality Alignment Makes Them See the World Better!",
            "institute": "Stanford University",
            "text": "This research proposes a post-training stage called \"locality alignment\" for Vision Transformers (ViTs) to improve their ability to understand spatial relationships within images. Unlike previous methods that rely on extensive training data or complex architectures, this approach leverages self-supervision and fine-tunes existing pre-trained models to extract local semantics.",
            "paper-title": "Locality Alignment Improves Vision-Language Models",
            "image-path": "flux_paper_image/2410.11087_1729107451.png"
        },

        {
            "startTime": "04:07",
            "arxivId": "2410.11781",
            "arxivLink": "https://arxiv.org/abs/2410.11781",
            "title": "LLMs Count on Their Fingers: New Research Reveals How Language Models Do Math",
            "institute": "University of Oxford, Tel Aviv University",
            "text": "This research investigates how large language models (LLMs) represent numbers internally. Unlike previous work that focused on linear representations, this study proposes that LLMs use a digit-wise representation in base 10, meaning they store each digit of a number separately.",
            "paper-title": "Language Models Encode Numbers Using Digit Representations in Base 10",
            "image-path": "flux_paper_image/2410.11781_1729107610.png"
        },

        {
            "startTime": "04:25",
            "arxivId": "2410.11677",
            "arxivLink": "https://arxiv.org/abs/2410.11677",
            "title": "Language Models: Too Smart for Their Own Good?",
            "institute": "University College London, Cohere",
            "text": "This research explores the relationship between completion likelihood and model performance in Direct Alignment Algorithms (DAAs), finding that higher likelihood doesn't always lead to better performance. It also identifies two indicators that signal when over-optimisation of diversity begins to harm performance.",
            "paper-title": "Understanding Likelihood Over-optimisation in Direct Alignment Algorithms",
            "image-path": "flux_paper_image/2410.11677_1729107555.png"
        },

        {
            "startTime": "04:47",
            "arxivId": "2410.11005",
            "arxivLink": "https://arxiv.org/abs/2410.11005",
            "title": "AI Can't Speak the Streets: LLMs Struggle with Dialects in Reasoning Tasks",
            "institute": "University of Oxford, Microsoft",
            "text": "This research focuses on evaluating the fairness and robustness of large language models (LLMs) when presented with reasoning tasks written in African American Vernacular English (AAVE). Unlike previous studies that primarily used rule-based transformations or LLMs as translators, this study employs human AAVE speakers to rewrite existing benchmarks, creating a new dataset called ReDial.",
            "paper-title": "One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of Large Language Models in Reasoning Tasks",
            "image-path": "flux_paper_image/2410.11005_1729107445.png"
        },

        {
            "startTime": "05:21",
            "arxivId": "2410.11655",
            "arxivLink": "https://arxiv.org/abs/2410.11655",
            "title": "Spelling Correction Gets a Brand New Boost: Retrieval Augmented Generation for E-Commerce",
            "institute": "Amazon, Stanford University",
            "text": "This research focuses on improving spelling correction in e-commerce by incorporating retrieved product names into the context used by a large language model (LLM). This approach, known as Retrieval Augmented Generation (RAG), differs from previous work by dynamically integrating up-to-date information from a product catalog, addressing the challenge of evolving brand names and unconventional spellings.",
            "paper-title": "Retrieval Augmented Spelling Correction for E-Commerce Applications",
            "image-path": "flux_paper_image/2410.11655_1729108782.png"
        },

        {
            "startTime": "05:46",
            "arxivId": "2410.11325",
            "arxivLink": "https://arxiv.org/abs/2410.11325",
            "title": "Knowledge Distillation: When Students Outsmart Their Teachers!",
            "institute": "UC Santa Barbara, Google",
            "text": "This research introduces Speculative Knowledge Distillation (SKD), a novel approach to knowledge distillation that uses interleaved sampling between teacher and student models to generate high-quality training data on-the-fly. Unlike previous methods, SKD dynamically adjusts the balance between supervised and on-policy KD based on the distribution gap between the teacher and student models.",
            "paper-title": "Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling",
            "image-path": "flux_paper_image/2410.11325_1729107800.png"
        },

        {
            "startTime": "06:12",
            "arxivId": "2410.11842",
            "arxivLink": "https://arxiv.org/abs/2410.11842",
            "title": "Attention, Please! New Research Makes Transformers Smarter (and Faster)",
            "institute": "Peking University",
            "text": "This paper proposes a new architecture called Mixture-of-Head attention (MoH) that dynamically selects the most relevant attention heads for each token, improving inference efficiency without sacrificing accuracy. Unlike previous work that focuses on pruning redundant heads, MoH introduces a routing mechanism that allows each token to adaptively choose the best heads.",
            "paper-title": "MoH: Multi-Head Attention as Mixture-of-Head Attention",
            "image-path": "flux_paper_image/2410.11842_1729108832.png"
        },

        {
            "startTime": "06:33",
            "arxivId": "2410.11298",
            "arxivLink": "https://arxiv.org/abs/2410.11298",
            "title": "Sorting Weights for Energy-Efficient Deep Learning: A Crossbar Revolution!",
            "institute": "Harvard University",
            "text": "This research introduces a novel weight allocation algorithm called Sorted Weight Sectioning (SWS) that optimizes the placement of weights in deep neural networks (DNNs) on compute-in-memory (CIM) crossbars. Unlike previous methods that rely on unstructured or structured pruning, SWS leverages the bell-shaped distribution of DNN weights and their sparsity to reduce the energy consumption of analog-to-digital converters (ADCs) without compromising accuracy.",
            "paper-title": "Sorted Weight Sectioning for Energy-Efficient Unstructured Sparse DNNs on Compute-in-Memory Crossbars",
            "image-path": "flux_paper_image/2410.11298_1729107691.png"
        },

        {
            "startTime": "06:56",
            "arxivId": "2410.11516",
            "arxivLink": "https://arxiv.org/abs/2410.11516",
            "title": "Brain-Like Language Model Learns to Speak Like Us!",
            "institute": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne",
            "text": "This research introduces TopoLM, a transformer language model with an explicit spatial representation of model units. Unlike previous work, TopoLM incorporates a spatial smoothness loss, encouraging local correlation between units, which leads to the emergence of brain-like spatial organization.",
            "paper-title": "TopoLM: brain-like spatio-functional organization in a topographic language model",
            "image-path": "flux_paper_image/2410.11516_1729109447.png"
        },

        {
            "startTime": "07:24",
            "arxivId": "2410.10872",
            "arxivLink": "https://arxiv.org/abs/2410.10872",
            "title": "ToolBridge: Unlocking LLMs' Hidden Potential with Open-Source Data",
            "institute": "Microsoft",
            "text": "This research focuses on creating a publicly available dataset, ToolBridge, to train LLMs to effectively use external tools. Unlike previous work that often keeps training data private, ToolBridge promotes transparency and collaboration in the field.",
            "paper-title": "ToolBridge: An Open-Source Dataset to Equip LLMs with External Tool Capabilities",
            "image-path": "flux_paper_image/2410.10872_1729108291.png"
        },

        {
            "startTime": "07:44",
            "arxivId": "2410.11163",
            "arxivLink": "https://arxiv.org/abs/2410.11163",
            "title": "LLM Swarm Party: How AI Models Learn to Dance Together",
            "institute": "University of Washington, Google",
            "text": "This research proposes MODEL SWARMS, a collaborative search algorithm that adapts LLMs by viewing each model as a \"particle\" in a swarm. Unlike previous approaches that focus on merging or fusing models, MODEL SWARMS allows diverse LLMs to collaboratively explore the weight space and optimize a utility function, leading to new model capabilities.",
            "paper-title": "Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence",
            "image-path": "flux_paper_image/2410.11163_1729107912.png"
        },

        {
            "startTime": "08:06",
            "arxivId": "2410.11646",
            "arxivLink": "https://arxiv.org/abs/2410.11646",
            "title": "Score Diffusion Gets a Feature Makeover: Sampling Conditional Densities with a Twist!",
            "institute": "NYU, \u00c9cole Normale Sup\u00e9rieure",
            "text": "This paper introduces a new method for sampling conditional densities using score diffusion. Unlike previous approaches that rely on estimating the score of the conditional density, this method guides the diffusion process using a projected score based on learned feature vectors.",
            "paper-title": "Feature-guided score diffusion for sampling conditional densities",
            "image-path": "flux_paper_image/2410.11646_1729107743.png"
        },

        {
            "startTime": "08:29",
            "arxivId": "2410.10905",
            "arxivLink": "https://arxiv.org/abs/2410.10905",
            "title": "ProcGen's Got Talent: 3D Convos & Scale Boost RL Generalization",
            "institute": "Columbia University, CMU",
            "text": "This research explores the impact of architectural changes, specifically framestacking and 3D convolutions, on the generalization performance of reinforcement learning (RL) algorithms in the ProcGen benchmark. Unlike previous work that focused on representation learning or exploration techniques, this study investigates the role of temporal information processing and model capacity in improving generalization.",
            "paper-title": "Improving Generalization on the ProcGen Benchmark with Simple Architectural Changes and Scale",
            "image-path": "flux_paper_image/2410.10905_1729107631.png"
        },

        {
            "startTime": "08:53",
            "arxivId": "2410.11758",
            "arxivLink": "https://arxiv.org/abs/2410.11758",
            "title": "Robot Learns From YouTube: No Action Labels Needed!",
            "institute": "Korea Advanced Institute of Science and Technology, University of Washington",
            "text": "This research introduces Latent Action Pretraining for General Action Models (LAPA), a method for training robotic models without requiring ground-truth action labels. Unlike previous methods that rely on labeled data, LAPA learns from unlabeled videos by first quantizing actions into a latent space and then training a Vision-Language Model to predict these latent actions.",
            "paper-title": "Latent Action Pretraining from Videos",
            "image-path": "flux_paper_image/2410.11758_1729108080.png"
        },

        {
            "startTime": "09:24",
            "arxivId": "2410.11235",
            "arxivLink": "https://arxiv.org/abs/2410.11235",
            "title": "LLMs: Not Just for Chatbots Anymore! They're Now Graph-Savvy Encoders!",
            "institute": "Amazon, Michigan State University, University of Illinois...",
            "text": "This research proposes Janus, a framework that uses LLMs to jointly encode text and graph data. Unlike previous methods that rely on MLPs or shallow transformers, Janus leverages the powerful capabilities of LLMs to integrate and understand multimodal information.",
            "paper-title": "Unleashing the Power of LLMs as Multi-Modal Encoders for Text and Graph-Structured Data",
            "image-path": "flux_paper_image/2410.11235_1729107182.png"
        },

        {
            "startTime": "09:45",
            "arxivId": "2410.11348",
            "arxivLink": "https://arxiv.org/abs/2410.11348",
            "title": "Rewriting Rewrites: How to Tell What Reward Models Really Reward",
            "institute": "University of Chicago",
            "text": "This paper introduces RATE, a method for estimating the causal effect of an attribute on a reward model using imperfect rewrites of rewrites. Unlike previous work that relies on observational data, RATE uses large language models to generate counterfactual responses, thereby isolating the effect of the target attribute.",
            "paper-title": "RATE: Score Reward Models with Imperfect Rewrites of Rewrites",
            "image-path": "flux_paper_image/2410.11348_1729107501.png"
        },

        {
            "startTime": "10:07",
            "arxivId": "2410.11820",
            "arxivLink": "https://arxiv.org/abs/2410.11820",
            "title": "Data Diet: How to Feed Your AI Model the Right Stuff",
            "institute": "CMU, Stanford University",
            "text": "This research introduces Adaptive Data Optimization (ADO), an algorithm that dynamically adjusts the data distribution during model training. Unlike previous methods, ADO doesn't require external knowledge, proxy models, or modifications to the model update. Instead, it uses per-domain scaling laws to estimate the learning potential of each data source and adjust the data mixture accordingly.",
            "paper-title": "Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws",
            "image-path": "flux_paper_image/2410.11820_1729108974.png"
        },

        {
            "startTime": "10:38",
            "arxivId": "2410.11031",
            "arxivLink": "https://arxiv.org/abs/2410.11031",
            "title": "Robots Learn to Think Like Humans: Neural Networks Mimic Classic Algorithms",
            "institute": "University of Oxford",
            "text": "This research explores the Neural Algorithmic Reasoning (NAR) framework, which trains neural networks to execute the steps of classical algorithms, rather than just mapping inputs to outputs. This approach is applied to pointcloud registration, a fundamental task in robotics, and demonstrates the ability of NAR to learn complex, multi-step algorithms.",
            "paper-title": "NAR-*ICP: Neural Execution of Classical ICP-based Pointcloud Registration Algorithms",
            "image-path": "flux_paper_image/2410.11031_1729107958.png"
        },

        {
            "startTime": "10:56",
            "arxivId": "2410.11840",
            "arxivLink": "https://arxiv.org/abs/2410.11840",
            "title": "Scaling Up Language Models: A Hitchhiker's Guide to Efficient Training",
            "institute": "MIT, IBM Research",
            "text": "This research focuses on how to efficiently estimate the performance of large language models (LLMs) by training smaller models. Unlike previous work that primarily focused on training models to completion, this study explores the use of intermediate checkpoints during training to improve scaling law accuracy.",
            "paper-title": "A Hitchhiker's Guide to Scaling Law Estimation",
            "image-path": "flux_paper_image/2410.11840_1729109619.png"
        },

        {
            "startTime": "11:21",
            "arxivId": "2410.11741",
            "arxivLink": "https://arxiv.org/abs/2410.11741",
            "title": "Point-Based Bird Counting: A New Way to Track Wildlife with Less Work!",
            "institute": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, University College London",
            "text": "This research introduces POLO, a multi-class object detection model that can be trained entirely on point labels, reducing the need for expensive and time-consuming bounding box annotations.",
            "paper-title": "POLO -- Point-based, multi-class animal detection",
            "image-path": "flux_paper_image/2410.11741_1729109548.png"
        },

        {
            "startTime": "11:38",
            "arxivId": "2410.10923",
            "arxivLink": "https://arxiv.org/abs/2410.10923",
            "title": "Multi-Modal Learning: A Two-Stage Strategy to Remember Everything!",
            "institute": "Shanghai Jiao Tong University, Tsinghua University, Lin Gang Laboratory...",
            "text": "This research introduces a two-stage learning paradigm for multi-modal continual learning. Unlike previous methods that learn individual adapters for each task, this approach leverages previously learned knowledge to decompose new tasks and then compensates for any missing information.",
            "paper-title": "ATLAS: Adapter-Based Multi-Modal Continual Learning with a Two-Stage Learning Strategy",
            "image-path": "flux_paper_image/2410.10923_1729108918.png"
        },

        {
            "startTime": "12:08",
            "arxivId": "2410.11068",
            "arxivLink": "https://arxiv.org/abs/2410.11068",
            "title": "TV Shows, Subtitles, and the Power of Context: How AI Learns to Understand Conversations",
            "institute": "University of Oxford",
            "text": "This research introduces a new method for identifying speakers in short audio segments by leveraging the temporal context of the surrounding dialogue. Unlike previous methods that rely solely on local information, this approach utilizes both local voice embeddings and large language model reasoning to improve speaker diarization accuracy.",
            "paper-title": "Character-aware audio-visual subtitling in context",
            "image-path": "flux_paper_image/2410.11068_1729109133.png"
        },

        {
            "startTime": "12:29",
            "arxivId": "2410.11133",
            "arxivLink": "https://arxiv.org/abs/2410.11133",
            "title": "Proof Search Gets a Diversity Boost: 3D-Prover Filters Tactics Like a Boss!",
            "institute": "Australian National University, Google, Australian Department of Defence",
            "text": "This research introduces a novel filtering mechanism called 3D-Prover that leverages Determinantal Point Processes (DPPs) to select semantically diverse and high-quality tactics during automated theorem proving. Unlike previous approaches that focus on syntactic similarity or simply reward new nodes, 3D-Prover considers the semantic impact of tactics on the proving environment, leading to more efficient exploration of the search space.",
            "paper-title": "3D-Prover: Diversity Driven Theorem Proving With Determinantal Point Processes",
            "image-path": "flux_paper_image/2410.11133_1729107882.png"
        },

        {
            "startTime": "12:55",
            "arxivId": "2410.11185",
            "arxivLink": "https://arxiv.org/abs/2410.11185",
            "title": "Symbolic Regression Gets a Neural Boost: Unveiling Complex Network Dynamics",
            "institute": "Tsinghua University",
            "text": "This research introduces a new method called Physically Inspired Neural Dynamics Symbolic Regression (PI-NDSR) for learning symbolic expressions of complex network dynamics. Unlike previous methods that rely on noisy estimations of time derivatives, PI-NDSR uses a neural network to denoise and interpolate observed trajectories, providing more accurate data for symbolic regression.",
            "paper-title": "Neural Symbolic Regression of Complex Network Dynamics",
            "image-path": "flux_paper_image/2410.11185_1729108561.png"
        },

        {
            "startTime": "13:19",
            "arxivId": "2410.10861",
            "arxivLink": "https://arxiv.org/abs/2410.10861",
            "title": "Translation Trouble? Canvas to the Rescue!",
            "institute": "CMU, UC Santa Barbara",
            "text": "This research introduces Translation Canvas, a tool that goes beyond simple scores to pinpoint and analyze translation errors at the instance level. Unlike previous tools that focus on high-level metrics, Translation Canvas provides a detailed breakdown of errors, allowing researchers to understand the nuances of model performance.",
            "paper-title": "Translation Canvas: An Explainable Interface to Pinpoint and Analyze Translation Systems",
            "image-path": "flux_paper_image/2410.10861_1729108776.png"
        },

        {
            "startTime": "13:38",
            "arxivId": "2410.11055",
            "arxivLink": "https://arxiv.org/abs/2410.11055",
            "title": "LLMs Learn to Be Less Wrong: A New Kind of AI Alignment",
            "institute": "University of Washington, University of Texas at Austin",
            "text": "This research explores a novel approach to aligning LLMs by focusing on preferences between incorrect answers, rather than relying on correct answers. This is particularly relevant for tasks where correct answers are unavailable or prohibitively expensive to obtain.",
            "paper-title": "Varying Shades of Wrong: Aligning LLMs with Wrong Answers Only",
            "image-path": "flux_paper_image/2410.11055_1729109542.png"
        },

        {
            "startTime": "14:04",
            "arxivId": "2410.11043",
            "arxivLink": "https://arxiv.org/abs/2410.11043",
            "title": "Personality Differences: The Secret Sauce of Social Influence in Conversations?",
            "institute": "Stanford University",
            "text": "This research uses high-dimensional NLP methods to analyze the topical flow of conversations and how personality differences influence this flow. Unlike previous work that focused on manual analysis of specific conversational phenomena, this study uses text embeddings and clustering to automatically identify and quantify conversation topics.",
            "paper-title": "Personality Differences Drive Conversational Dynamics: A High-Dimensional NLP Approach",
            "image-path": "flux_paper_image/2410.11043_1729108039.png"
        },

        {
            "startTime": "14:23",
            "arxivId": "2410.11056",
            "arxivLink": "https://arxiv.org/abs/2410.11056",
            "title": "Human-Machine Translation Teams: Better Together, Cheaper Too!",
            "institute": "Google",
            "text": "This research goes beyond simply comparing human and machine translation quality. It explores the potential of human-machine collaboration in both the initial translation and post-editing stages, analyzing the impact on quality and cost.",
            "paper-title": "Beyond Human-Only: Evaluating Human-Machine Collaboration for Collecting High-Quality Translation Data",
            "image-path": "flux_paper_image/2410.11056_1729107512.png"
        },

        {
            "startTime": "14:39",
            "arxivId": "2410.11403",
            "arxivLink": "https://arxiv.org/abs/2410.11403",
            "title": "Multimodal VAEs: Iterative Amortized Inference for a Smoother Ride!",
            "institute": "University of Tokyo",
            "text": "This paper introduces a new method called multimodal iterative amortized inference within the multimodal VAE framework. It aims to improve the accuracy of unimodal inference by iteratively refining the multimodal inference using all modalities, thereby overcoming the information loss due to missing modalities in mixture-based models and minimizing the amortization gap in alignment-based models.",
            "paper-title": "Enhancing Unimodal Latent Representations in Multimodal VAEs through Iterative Amortized Inference",
            "image-path": "flux_paper_image/2410.11403_1729108989.png"
        },

        {
            "startTime": "15:17",
            "arxivId": "2410.11594",
            "arxivLink": "https://arxiv.org/abs/2410.11594",
            "title": "LLMs as Judges? Let's Quantify Their Uncertainty!",
            "institute": "IBM Research",
            "text": "This research introduces a novel method for quantifying uncertainty in LLM-as-a-Judge evaluations by analyzing the relationships between generated assessments and possible ratings. This approach differs from previous work by focusing on black-box uncertainty quantification, which doesn't require access to the internal workings of the model.",
            "paper-title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
            "image-path": "flux_paper_image/2410.11594_1729108536.png"
        },

        {
            "startTime": "15:38",
            "arxivId": "2410.11671",
            "arxivLink": "https://arxiv.org/abs/2410.11671",
            "title": "Reinforcement Learning: Safety First, Performance Second!",
            "institute": "University of Toronto",
            "text": "This research focuses on incorporating safety filters directly into the training process of reinforcement learning (RL) agents, rather than just applying them during evaluation. This allows the RL agent to learn how to work with the safety filter, leading to improved performance and sample efficiency.",
            "paper-title": "Safety Filtering While Training: Improving the Performance and Sample Efficiency of Reinforcement Learning Agents",
            "image-path": "flux_paper_image/2410.11671_1729108622.png"
        },

        {
            "startTime": "16:02",
            "arxivId": "2410.11528",
            "arxivLink": "https://arxiv.org/abs/2410.11528",
            "title": "Hair Today, Gone Tomorrow: A Taxonomy for Fair Hairstyle Classification",
            "institute": "Microsoft, Pennsylvania State University",
            "text": "This research introduces a novel hairstyle taxonomy, designed to be objective and inclusive, to address the limitations of existing hairstyle prediction methods. Unlike previous work that relies on parametric models or limited datasets, this study uses a classification approach and a synthetic dataset to ensure fairness and robustness across diverse hairstyles.",
            "paper-title": "Hairmony: Fairness-aware hairstyle classification",
            "image-path": "flux_paper_image/2410.11528_1729109496.png"
        },

        {
            "startTime": "16:18",
            "arxivId": "2410.11795",
            "arxivLink": "https://arxiv.org/abs/2410.11795",
            "title": "Diffusion Models: From Slowpoke to Speed Demon!",
            "institute": "Tsinghua University, HUST, SJTU...",
            "text": "This paper provides a comprehensive survey of efficient diffusion models, focusing on principles and practices for architecture design, model training, fast inference, and reliable deployment. It differs from previous surveys by offering a more in-depth and efficiency-oriented perspective.",
            "paper-title": "Efficient Diffusion Models: A Comprehensive Survey from Principles to Practices",
            "image-path": "flux_paper_image/2410.11795_1729108394.png"
        },

        {
            "startTime": "16:44",
            "arxivId": "2410.11540",
            "arxivLink": "https://arxiv.org/abs/2410.11540",
            "title": "Fed Up with Noisy Data? This New Framework Makes LLMs Smarter (and Cleaner!)",
            "institute": "Shanghai Jiao Tong University, University of Cambridge, Shanghai AI Laboratory",
            "text": "This research focuses on data quality control in federated learning for instruction-tuning large language models (LLMs). Unlike previous work that often requires access to the entire dataset or relies on external models, this paper proposes a new framework, FedDQC, that integrates data quality assessment with the training process, ensuring privacy and computational efficiency.",
            "paper-title": "Data Quality Control in Federated Instruction-tuning of Large Language Models",
            "image-path": "flux_paper_image/2410.11540_1729107752.png"
        },

        {
            "startTime": "17:18",
            "arxivId": "2410.11285",
            "arxivLink": "https://arxiv.org/abs/2410.11285",
            "title": "Drone-Powered 360\u00b0 Scans: Turning Indoor Spaces into Virtual Reality!",
            "institute": "UC Berkeley",
            "text": "This research proposes a novel approach for large-scale indoor scene reconstruction using 3D Gaussian Splatting, which differs from previous methods by directly synthesizing novel views from a 360\u00b0 camera mounted on a drone, rather than generating an explicit 3D structure.",
            "paper-title": "Scalable Indoor Novel-View Synthesis using Drone-Captured 360 Imagery with 3D Gaussian Splatting",
            "image-path": "flux_paper_image/2410.11285_1729109152.png"
        },

        {
            "startTime": "17:44",
            "arxivId": "2410.11081",
            "arxivLink": "https://arxiv.org/abs/2410.11081",
            "title": "Diffusion Models: From Slowpokes to Speed Demons!",
            "institute": "OpenAI",
            "text": "This paper introduces a new formulation for diffusion models called TrigFlow, which simplifies the training process and improves stability for continuous-time consistency models. Unlike previous methods that relied on discretized time steps, this approach leverages continuous-time formulations, leading to more accurate and efficient training.",
            "paper-title": "Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models",
            "image-path": "flux_paper_image/2410.11081_1729107794.png"
        },

        {
            "startTime": "18:00",
            "arxivId": "2410.10826",
            "arxivLink": "https://arxiv.org/abs/2410.10826",
            "title": "AI Lung Doctor: Synthesizing 3D CT Scans from X-rays for ARDS Patients",
            "institute": "Harvard University",
            "text": "This research utilizes a score-based 3D residual diffusion model to synthesize high-fidelity 3D lung CT volumes from limited 2D imaging data and physiological parameters, offering a potential solution to the challenges of frequent CT scans in ARDS management. This approach differs from previous work by incorporating prior CT data and physiological parameters into the model, improving the accuracy and realism of the generated CT images.",
            "paper-title": "High-Fidelity 3D Lung CT Synthesis in ARDS Swine Models Using Score-Based 3D Residual Diffusion Models",
            "image-path": "flux_paper_image/2410.10826_1729107547.png"
        },

        {
            "startTime": "18:21",
            "arxivId": "2410.11506",
            "arxivLink": "https://arxiv.org/abs/2410.11506",
            "title": "Omnidirectional Video Super-Resolution: When Distortion Meets Deep Learning",
            "institute": "Chinese Academy of Sciences, ByteDance, Peking University",
            "text": "This research introduces a Spatio-Temporal Distortion Aware Network (STDAN) for omnidirectional video super-resolution (ODV-SR). Unlike previous methods that primarily focus on latitude-related distortions, STDAN addresses both spatial and temporal distortions by incorporating deformable convolutional networks (DCN) for alignment.",
            "paper-title": "Spatio-Temporal Distortion Aware Omnidirectional Video Super-Resolution",
            "image-path": "flux_paper_image/2410.11506_1729107685.png"
        },

        {
            "startTime": "18:47",
            "arxivId": "2410.10878",
            "arxivLink": "https://arxiv.org/abs/2410.10878",
            "title": "Leaning on Language: A New Dataset for Auto-Formalizing Math Proofs",
            "institute": "Peking University",
            "text": "This research introduces a novel framework for translating a formal mathematical language, Lean 4, into natural language. The key innovation lies in the use of a dual augmentation strategy that combines tactic-based and informal-based approaches, leveraging the Lean-jixia system, a Lean 4 analyzer. This approach differs from previous work by incorporating structural information from the Lean 4 compiler and the pyramid architecture of the Lean repository.",
            "paper-title": "Herald: A Natural Language Annotated Lean 4 Dataset",
            "image-path": "flux_paper_image/2410.10878_1729108062.png"
        },

        {
            "startTime": "19:13",
            "arxivId": "2410.11841",
            "arxivLink": "https://arxiv.org/abs/2410.11841",
            "title": "Explaining Recommendations: A New Model That's Not Just Smart, It's Also Kind of a Genius",
            "institute": "Zhejiang University, Microsoft",
            "text": "This research introduces GaVaMoE, a new framework for explainable recommendation systems. Unlike previous methods that rely on generic prompts, GaVaMoE uses a combination of a Variational Autoencoder (VAE) and a Gaussian Mixture Model (GMM) to capture complex user-item interactions and cluster users with similar preferences. This allows the model to generate highly personalized explanations tailored to specific user types.",
            "paper-title": "GaVaMoE: Gaussian-Variational Gated Mixture of Experts for Explainable Recommendation",
            "image-path": "flux_paper_image/2410.11841_1729108223.png"
        },

        {
            "startTime": "19:36",
            "arxivId": "2410.11180",
            "arxivLink": "https://arxiv.org/abs/2410.11180",
            "title": "Bidding Wars: AI Learns to Play Power Market Poker with High-Dimensional Bids",
            "institute": "Tsinghua University",
            "text": "This research proposes a framework for using high-dimensional bids (HDBs) in reinforcement learning (RL) based bidding methods for power markets. Unlike previous RL methods that mostly use low-dimensional bids, this approach allows for more flexibility and adaptability in bidding strategies.",
            "paper-title": "Reinforcement Learning Based Bidding Framework with High-dimensional Bids in Power Markets",
            "image-path": "flux_paper_image/2410.11180_1729107254.png"
        },

        {
            "startTime": "19:55",
            "arxivId": "2410.10912",
            "arxivLink": "https://arxiv.org/abs/2410.10912",
            "title": "Pruning LLMs: Heavy-Tailed Theory Makes Models Slimmer, Smarter!",
            "institute": "Nankai University, Dartmouth College, University of Oxford...",
            "text": "This research proposes a new method for layer-wise pruning of large language models (LLMs) based on Heavy-Tailed Self-Regularization (HT-SR) theory. Unlike previous methods that use uniform pruning ratios across layers, this approach assigns sparsity ratios based on the shape of the empirical spectral densities (ESDs) of weight matrices.",
            "paper-title": "AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models",
            "image-path": "flux_paper_image/2410.10912_1729107858.png"
        },

        {
            "startTime": "20:22",
            "arxivId": "2410.11316",
            "arxivLink": "https://arxiv.org/abs/2410.11316",
            "title": "Deep Learning Makes Wireless Control Systems Smarter, Not Just Faster!",
            "institute": "University of Sydney, Nanyang Technological University",
            "text": "This research proposes a novel communication-control codesign approach for large-scale wireless networked control systems (WNCSs) using a deep reinforcement learning (DRL) algorithm called Graph-enhanced Cascaded Actor DRL (GCA-DRL). Unlike previous work that often relies on simplified models or separate optimization of communication and control, GCA-DRL integrates these aspects into a single framework, enabling more efficient and robust control performance.",
            "paper-title": "Communication-Control Codesign for Large-Scale Wireless Networked Control Systems",
            "image-path": "flux_paper_image/2410.11316_1729107964.png"
        },

        {
            "startTime": "20:45",
            "arxivId": "2410.11165",
            "arxivLink": "https://arxiv.org/abs/2410.11165",
            "title": "Kernel-Based PDE Solvers: Ditch the Derivatives, Get the Solution!",
            "institute": "University of Utah, Caltech",
            "text": "This research proposes a new kernel-based framework for solving nonlinear PDEs that eliminates the need to embed differential operators within the kernel. Instead, it directly differentiates the interpolant to compute derivatives, simplifying the Gram matrix construction and computation.",
            "paper-title": "Toward Efficient Kernel-Based Solvers for Nonlinear PDEs",
            "image-path": "flux_paper_image/2410.11165_1729109236.png"
        },

        {
            "startTime": "21:08",
            "arxivId": "2410.11531",
            "arxivLink": "https://arxiv.org/abs/2410.11531",
            "title": "Chatbots Get Smart: Knowledge Graphs Power Up Conversational AI",
            "institute": "The University of Tokyo, Universit\u00e4t Bielefeld, Duke-NUS Medical School...",
            "text": "This research introduces AGENTiGraph, a platform that integrates LLMs with knowledge graphs (KGs) using a multi-agent system. This approach differs from previous work by focusing on user accessibility and flexibility, enabling seamless interaction with KGs through natural language interfaces.",
            "paper-title": "AGENTiGraph: An Interactive Knowledge Graph Platform for LLM-based Chatbots Utilizing Private Data",
            "image-path": "flux_paper_image/2410.11531_1729109261.png"
        },

        {
            "startTime": "21:37",
            "arxivId": "2410.10863",
            "arxivLink": "https://arxiv.org/abs/2410.10863",
            "title": "LLMs Get a Personality Makeover: Social Determinism Shapes AI Traits",
            "institute": "King Abdullah University of Science and Technology, University of Toronto, Soochow University...",
            "text": "This research explores how LLMs develop personality traits by drawing on the theory of social determinism, which examines the influence of long-term background factors and short-term pressures. Unlike previous work that focuses on identifying and measuring personality traits, this study investigates the underlying mechanisms and factors contributing to their emergence.",
            "paper-title": "What makes your model a low-empathy or warmth person: Exploring the Origins of Personality in LLMs",
            "image-path": "flux_paper_image/2410.10863_1729108406.png"
        },

        {
            "startTime": "21:57",
            "arxivId": "2410.11303",
            "arxivLink": "https://arxiv.org/abs/2410.11303",
            "title": "Data Selection for Language Models: A Match Made in Optimal Transport",
            "institute": "University of Wisconsin-Madison, Yale University, Apple Inc.",
            "text": "This research proposes a novel framework for selecting data for task-specific finetuning of foundation models. Unlike previous methods that rely on similarity-based heuristics or n-gram feature spaces, this approach leverages optimal transport to align the distribution of selected data with the target task distribution.",
            "paper-title": "Data Selection for Task-Specific Model Finetuning",
            "image-path": "flux_paper_image/2410.11303_1729108155.png"
        },

        {
            "startTime": "22:17",
            "arxivId": "2410.10901",
            "arxivLink": "https://arxiv.org/abs/2410.10901",
            "title": "LLMs Need a Diet: Decomposed Difficulty Data Selection for Smarter Medical AI",
            "institute": "Peking University, Zhejiang University",
            "text": "This research proposes a two-stage data selection framework called 3DS that aligns training data with the model's knowledge distribution for optimized domain adaptation. Unlike previous work that relies on heuristic methods, 3DS explicitly considers the model's internal knowledge and preferences, filtering out irrelevant or redundant data and selecting data that is appropriately challenging for the model to learn.",
            "paper-title": "3DS: Decomposed Difficulty Data Selection's Case Study on LLM Medical Domain Adaptation",
            "image-path": "flux_paper_image/2410.10901_1729109371.png"
        },

        {
            "startTime": "22:40",
            "arxivId": "2410.11289",
            "arxivLink": "https://arxiv.org/abs/2410.11289",
            "title": "LLMs Go Low-Rank: A New Algorithm That Doesn't Get Lost in the Noise!",
            "institute": "Peking University",
            "text": "This paper investigates the convergence properties of GaLore, a popular subspace optimization algorithm for LLMs. It finds that GaLore doesn't always converge to the optimal solution, especially in stochastic settings. The paper then introduces GoLore, a novel variant of GaLore that provably converges in stochastic settings, even with standard batch sizes.",
            "paper-title": "Subspace Optimization for Large Language Models with Convergence Guarantees",
            "image-path": "flux_paper_image/2410.11289_1729108141.png"
        },

        {
            "startTime": "23:06",
            "arxivId": "2410.11281",
            "arxivLink": "https://arxiv.org/abs/2410.11281",
            "title": "Cell State Dynamics: A Self-Supervised Learning Dance!",
            "institute": "Chan Zuckerberg Biohub San Francisco, University of California Berkeley",
            "text": "This research introduces DynaCLR, a self-supervised learning framework that uses contrastive learning to model cell dynamics from time-lapse datasets. Unlike previous work, DynaCLR incorporates single-cell tracking and time-aware contrastive learning to map images of cells at neighboring time points to neighboring embeddings, enabling more quantitative and efficient analysis of cell states.",
            "paper-title": "Contrastive learning of cell state dynamics in response to perturbations",
            "image-path": "flux_paper_image/2410.11281_1729108357.png"
        },

        {
            "startTime": "23:29",
            "arxivId": "2410.11097",
            "arxivLink": "https://arxiv.org/abs/2410.11097",
            "title": "Speech Synthesis Gets a Speed Boost: Diffusion Models Go From Slow to Superfast!",
            "institute": "Columbia University, Adobe",
            "text": "This research proposes a novel method for distilling text-to-speech diffusion models, enabling direct optimization of perceptual metrics. Unlike previous distillation techniques that focus on approximating the teacher model's sampling trajectory, this approach aligns the student model's distribution with the teacher's distribution, allowing for faster inference and end-to-end optimization of metrics like speaker similarity and word error rate.",
            "paper-title": "DMDSpeech: Distilled Diffusion Model Surpassing The Teacher in Zero-shot Speech Synthesis via Direct Metric Optimization",
            "image-path": "flux_paper_image/2410.11097_1729109028.png"
        },

        {
            "startTime": "23:51",
            "arxivId": "2410.11744",
            "arxivLink": "https://arxiv.org/abs/2410.11744",
            "title": "DYSPEC: Decoding Like a Boss, Faster Than Ever!",
            "institute": "Peking University, UC Berkeley",
            "text": "This paper proposes DYSPEC, a speculative decoding algorithm that uses a dynamic token tree structure to improve the speed and scalability of large language models. Unlike previous methods that rely on fixed tree structures, DYSPEC dynamically expands the tree based on the draft model's output distribution, leading to a higher acceptance rate.",
            "paper-title": "DySpec: Faster Speculative Decoding with Dynamic Token Tree Structure",
            "image-path": "flux_paper_image/2410.11744_1729108570.png"
        },

        {
            "startTime": "24:17",
            "arxivId": "2410.11623",
            "arxivLink": "https://arxiv.org/abs/2410.11623",
            "title": "AI's Ego Trip: New Benchmark Tests If Robots Can See Themselves",
            "institute": "Tsinghua University",
            "text": "This research introduces VidEgoThink, a benchmark specifically designed to evaluate the egocentric video understanding capabilities of Multi-modal Large Language Models (MLLMs). Unlike previous benchmarks that focus on static images or traditional question-answering tasks, VidEgoThink assesses MLLMs across four key functions: video question-answering, hierarchy planning, visual grounding, and reward modeling.",
            "paper-title": "VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI",
            "image-path": "flux_paper_image/2410.11623_1729107360.png"
        },

        {
            "startTime": "24:43",
            "arxivId": "2410.11551",
            "arxivLink": "https://arxiv.org/abs/2410.11551",
            "title": "Kalman Filter: Not Just for Spacecraft, Now Fine-Tuning Your AI!",
            "institute": "University of Manchester, University of Cambridge",
            "text": "This paper proposes a new method called LoKO, which uses the Kalman filter to fine-tune large language models (LLMs) and computer vision models. Unlike previous work that relies on gradient-based optimizers like AdamW, LoKO leverages the Kalman filter's ability to estimate optimal parameters in an online manner.",
            "paper-title": "LoKO: Low-Rank Kalman Optimizer for Online Fine-Tuning of Large Models",
            "image-path": "flux_paper_image/2410.11551_1729108002.png"
        },

        {
            "startTime": "25:09",
            "arxivId": "2410.11474",
            "arxivLink": "https://arxiv.org/abs/2410.11474",
            "title": "Transformers: Not Just Lazy, They're Induction Head Honchos!",
            "institute": "Peking University",
            "text": "This paper delves into the theoretical underpinnings of how transformers implement \"induction heads,\" a mechanism that allows them to learn from context without retraining. Unlike previous work that focused on empirical observations, this study provides a rigorous mathematical analysis of the approximation and optimization processes involved.",
            "paper-title": "How Transformers Implement Induction Heads: Approximation and Optimization Analysis",
            "image-path": "flux_paper_image/2410.11474_1729107830.png"
        },

        {
            "startTime": "25:34",
            "arxivId": "2410.11201",
            "arxivLink": "https://arxiv.org/abs/2410.11201",
            "title": "VLMs Get a Tree Makeover: Prompt Learning Goes Hierarchical!",
            "institute": "Harvard University, Mass General Brigham, Microsoft",
            "text": "This research proposes a \"Tree of Attributes\" framework for prompt learning in vision-language models (VLMs). Unlike previous methods that use unstructured descriptions, this approach organizes descriptions into a hierarchical structure, capturing both high-level concepts and fine-grained details.",
            "paper-title": "Tree of Attributes Prompt Learning for Vision-Language Models",
            "image-path": "flux_paper_image/2410.11201_1729107653.png"
        },

        {
            "startTime": "25:51",
            "arxivId": "2410.11195",
            "arxivLink": "https://arxiv.org/abs/2410.11195",
            "title": "Athena: AI Lawyer Gets a Memory Boost!",
            "institute": "State Key Lab Intelligent Vehicle Safty Technol, Peking University, Chongqing Changan Automobile Co Ltd",
            "text": "This research introduces a new framework called \"Athena\" that uses retrieval-augmented generation (RAG) to improve the performance of large language models (LLMs) in legal judgment prediction (LJP). Unlike previous methods that rely solely on LLMs' internal knowledge, Athena incorporates an external knowledge base to provide context and reduce hallucinations.",
            "paper-title": "Athena: Retrieval-augmented Legal Judgment Prediction with Large Language Models",
            "image-path": "flux_paper_image/2410.11195_1729107702.png"
        },

        {
            "startTime": "26:15",
            "arxivId": "2410.11617",
            "arxivLink": "https://arxiv.org/abs/2410.11617",
            "title": "PDEs Need a Multi-Expert Makeover: Learning Controllable Multi-Scale Operators",
            "institute": "Westlake University, Shanghai Jiao Tong University, Delft University of Technology...",
            "text": "This research introduces a novel framework called Multi-scale and Multi-expert (M2M) neural operators for simulating and learning PDEs. Unlike previous methods, M2M employs a divide-and-conquer strategy with a controllable prior gating mechanism to determine the selection rights of experts, enhancing efficiency and accuracy.",
            "paper-title": "M$^{2}$M: Learning controllable Multi of experts and multi-scale operators are the Partial Differential Equations need",
            "image-path": "flux_paper_image/2410.11617_1729108651.png"
        },

        {
            "startTime": "26:37",
            "arxivId": "2410.11499",
            "arxivLink": "https://arxiv.org/abs/2410.11499",
            "title": "Tiny Model, Big Impact: How Mixed-Modal Data Makes Biological Sequence Modeling a Breeze!",
            "institute": "Microsoft",
            "text": "This research introduces BSM, a small but powerful biological sequence foundation model trained on mixed-modal data, including DNA, RNA, and protein sequences. Unlike previous models that focus on single modalities or combine them in a simplistic manner, BSM explicitly learns the relationships between these modalities, enhancing its understanding of each type.",
            "paper-title": "BSM: Small but Powerful Biological Sequence Model for Genes and Proteins",
            "image-path": "flux_paper_image/2410.11499_1729107846.png"
        },

        {
            "startTime": "26:59",
            "arxivId": "2410.11241",
            "arxivLink": "https://arxiv.org/abs/2410.11241",
            "title": "Noisy Data? No Problem! New Method Trains Diffusion Models with a Little Help from Their Friends",
            "institute": "Peking University",
            "text": "This research proposes a principled expectation-maximization (EM) framework for learning diffusion models from noisy data. Unlike previous methods that rely on approximations or specific data corruption types, this approach uses a plug-and-play Monte Carlo method for accurate posterior sampling, offering provable convergence guarantees.",
            "paper-title": "Learning Diffusion Model from Noisy Measurement using Principled Expectation-Maximization Method",
            "image-path": "flux_paper_image/2410.11241_1729108484.png"
        },

        {
            "startTime": "27:25",
            "arxivId": "2410.11276",
            "arxivLink": "https://arxiv.org/abs/2410.11276",
            "title": "Automating Data Exploration: Teaching AI to Be a Data Detective!",
            "institute": "International Institute of Information Technology, Indian Institute of Science Education and Research Pune, IBM",
            "text": "This research proposes ILAEDA, an AutoEDA system that learns from expert data analysis sessions using imitation learning, bypassing the need for manually defined reward functions.",
            "paper-title": "ILAEDA: An Imitation Learning Based Approach for Automatic Exploratory Data Analysis",
            "image-path": "flux_paper_image/2410.11276_1729108606.png"
        },

        {
            "startTime": "27:51",
            "arxivId": "2410.11650",
            "arxivLink": "https://arxiv.org/abs/2410.11650",
            "title": "Vision Transformers Go on a Diet: Splitting for Edge Devices!",
            "institute": "National University of Singapore, ETH Zurich, Zhejiang University",
            "text": "This research proposes a novel framework, ED-ViT, for splitting Vision Transformer models into smaller sub-models, each handling a specific subset of data classes. This approach differs from previous work by focusing on distributed inference on edge devices, rather than solely on model compression.",
            "paper-title": "ED-ViT: Splitting Vision Transformer for Distributed Inference on Edge Devices",
            "image-path": "flux_paper_image/2410.11650_1729107561.png"
        },

        {
            "startTime": "28:19",
            "arxivId": "2410.11224",
            "arxivLink": "https://arxiv.org/abs/2410.11224",
            "title": "DeltaDock: Docking with a Twist, A New Framework for Accurate and Reliable Molecular Docking!",
            "institute": "University of Science and Technology of China, Peking University",
            "text": "DeltaDock introduces a novel two-stage framework for molecular docking, reframing pocket prediction as a pocket-ligand alignment problem and employing a bi-level iterative refinement process for site-specific docking. This approach differs from previous methods by combining geometric deep learning with physics-informed traditional algorithms, leading to improved accuracy and physical reliability.",
            "paper-title": "DeltaDock: A Unified Framework for Accurate, Efficient, and Physically Reliable Molecular Docking",
            "image-path": "flux_paper_image/2410.11224_1729108276.png"
        },

        {
            "startTime": "28:42",
            "arxivId": "2410.11234",
            "arxivLink": "https://arxiv.org/abs/2410.11234",
            "title": "Offline RL Gets a Bayesian Brain Boost: Deep Search for Smarter Decisions",
            "institute": "CMU",
            "text": "This research proposes a novel approach to offline model-based reinforcement learning (MBRL) by framing it as a Bayes Adaptive Markov Decision Process (BAMDP). This differs from previous work by explicitly modeling the uncertainty in the learned world models, allowing for more robust and adaptable policy learning.",
            "paper-title": "Bayes Adaptive Monte Carlo Tree Search for Offline Model-based Reinforcement Learning",
            "image-path": "flux_paper_image/2410.11234_1729108251.png"
        },

        {
            "startTime": "29:07",
            "arxivId": "2410.11759",
            "arxivLink": "https://arxiv.org/abs/2410.11759",
            "title": "Unraveling Causality: A New Algorithm for Finding Roots in Noisy Data",
            "institute": "CornellTech, University of Chicago",
            "text": "This research introduces LoSAM, a novel algorithm for causal discovery in additive noise models (ANMs). Unlike previous methods that often struggle with computational complexity or require strong assumptions about the data, LoSAM leverages local causal substructures to achieve polynomial runtime and handle both linear and nonlinear causal relationships.",
            "paper-title": "LoSAM: Local Search in Additive Noise Models with Unmeasured Confounders, a Top-Down Global Discovery Approach",
            "image-path": "flux_paper_image/2410.11759_1729108128.png"
        },

        {
            "startTime": "29:27",
            "arxivId": "2410.11086",
            "arxivLink": "https://arxiv.org/abs/2410.11086",
            "title": "Speech is More Than Words: A Framework for Learning the \"How\" and the \"What\"",
            "institute": "Indraprastha Institute of Information Technology Delhi, Microsoft",
            "text": "This research introduces JOOCI, a framework that learns separate representations for \"content\" (what is being said) and \"other\" (how it is expressed) information in speech. Unlike previous methods that try to model both aspects within a single embedding, JOOCI uses distinct encoders and loss functions for each type of information.",
            "paper-title": "JOOCI: a Framework for Learning Comprehensive Speech Representations",
            "image-path": "flux_paper_image/2410.11086_1729107982.png"
        },

        {
            "startTime": "29:55",
            "arxivId": "2410.11061",
            "arxivLink": "https://arxiv.org/abs/2410.11061",
            "title": "Learning to Round: A Neural Network for Mixed-Integer Optimization",
            "institute": "Pacific Northwest National Laboratory, University of Toronto",
            "text": "This research proposes two differentiable correction layers for rounding integer variables in deep learning models, enabling the application of learning-to-optimize methods to mixed-integer non-linear programs (MINLPs). This approach differs from previous work by directly addressing the non-differentiability of integer outputs, which has been a significant challenge in applying learning-based methods to MINLPs.",
            "paper-title": "Learning to Optimize for Mixed-Integer Non-linear Programming",
            "image-path": "flux_paper_image/2410.11061_1729109324.png"
        },

        {
            "startTime": "30:22",
            "arxivId": "2410.10986",
            "arxivLink": "https://arxiv.org/abs/2410.10986",
            "title": "Transformers' Secret Sauce: A Hessian Peek Inside the Black Box",
            "institute": "ETH Zurich, Vector Institute",
            "text": "This paper delves into the Hessian matrix of Transformer models, providing a theoretical analysis of its structure and dependencies. Unlike previous work that focused on empirical studies or classical architectures, this research provides a detailed mathematical breakdown of the Hessian for a single self-attention layer.",
            "paper-title": "What Does It Mean to Be a Transformer? Insights from a Theoretical Hessian Analysis",
            "image-path": "flux_paper_image/2410.10986_1729109118.png"
        },

        {
            "startTime": "30:44",
            "arxivId": "2410.11307",
            "arxivLink": "https://arxiv.org/abs/2410.11307",
            "title": "Brain Tumor Detection: A Few Shots and a Whole Lot of Smarts!",
            "institute": "Tsinghua University",
            "text": "This research introduces a novel two-stage anomaly detection algorithm called CONSULT, which fine-tunes a pre-trained feature extractor specifically for MRI brain images using a synthetic data generation pipeline to create tumor-like data. This approach overcomes the lack of anomalous samples and enables the integration of attention mechanisms to focus on anomalous image segments.",
            "paper-title": "CONSULT: Contrastive Self-Supervised Learning for Few-shot Tumor Detection",
            "image-path": "flux_paper_image/2410.11307_1729108875.png"
        },

        {
            "startTime": "31:13",
            "arxivId": "2410.11096",
            "arxivLink": "https://arxiv.org/abs/2410.11096",
            "title": "Code GenAI Security: A New Benchmark for Testing the Bad Guys",
            "institute": "Virtue AI, University of California Los Angeles, University of California Santa Barbara...",
            "text": "This research introduces SECCODEPLT, a new benchmark for evaluating the security of code generation models. Unlike previous benchmarks that focus on static analysis or attack suggestions, SECCODEPLT uses a dynamic approach to evaluate a model's ability to generate actual attacks in a real-world environment.",
            "paper-title": "SecCodePLT: A Unified Platform for Evaluating the Security of Code GenAI",
            "image-path": "flux_paper_image/2410.11096_1729107115.png"
        },

        {
            "startTime": "31:51",
            "arxivId": "2410.10855",
            "arxivLink": "https://arxiv.org/abs/2410.10855",
            "title": "AI's Got It Backwards: New Study Shows LLMs Learn Complex Before Simple",
            "institute": "Johns Hopkins University, Harvard University",
            "text": "This research introduces CogDevelop2K, a benchmark that evaluates the cognitive abilities of multi-modal large language models (MLLMs) across 12 sub-concepts, structured based on human cognitive development. This approach differs from previous work by focusing on the fundamental cognitive abilities that underpin human intelligence, rather than solely evaluating performance on complex tasks.",
            "paper-title": "CogDevelop2K: Reversed Cognitive Development in Multimodal Large Language Models",
            "image-path": "flux_paper_image/2410.10855_1729109112.png"
        },

        {
            "startTime": "32:18",
            "arxivId": "2410.11462",
            "arxivLink": "https://arxiv.org/abs/2410.11462",
            "title": "Stop Saying \"I Don't Know That Word!\" - New AI Learns Language Like Humans",
            "institute": "University of Cambridge, University of G\u00f6ttingen",
            "text": "This research introduces a new method called \"Syntactic Smoothing\" that helps language models learn the meaning of infrequent words by considering their syntactic roles. This differs from previous work that primarily focused on memorizing infrequent words, which can lead to poor generalization.",
            "paper-title": "Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing",
            "image-path": "flux_paper_image/2410.11462_1729109229.png"
        },

        {
            "startTime": "32:37",
            "arxivId": "2410.11761",
            "arxivLink": "https://arxiv.org/abs/2410.11761",
            "title": "SlideChat: AI's New BFF for Pathology, Gigapixel Images and All!",
            "institute": "Stanford University, Purple Mountain Observatory",
            "text": "This research introduces SlideChat, a vision-language assistant that can understand gigapixel whole-slide images, unlike previous models that primarily focused on patch-level analysis. It also introduces SlideInstruction, a large-scale instruction-following dataset for WSIs, which is significantly larger than previous public datasets.",
            "paper-title": "SlideChat: A Large Vision-Language Assistant for Whole-Slide Pathology Image Understanding",
            "image-path": "flux_paper_image/2410.11761_1729108324.png"
        },

        {
            "startTime": "33:00",
            "arxivId": "2410.10841",
            "arxivLink": "https://arxiv.org/abs/2410.10841",
            "title": "Sun's Got AI: A Foundation Model for Heliophysics!",
            "institute": "National Aeronautics and Space Administration, Georgia State University",
            "text": "This research proposes a foundation model (FM) specifically designed for heliophysics, trained on the Solar Dynamics Observatory (SDO) dataset. This is the first study to design an FM in this domain.",
            "paper-title": "AI Foundation Model for Heliophysics: Applications, Design, and Implementation",
            "image-path": "flux_paper_image/2410.10841_1729109520.png"
        },

        {
            "startTime": "33:25",
            "arxivId": "2410.11564",
            "arxivLink": "https://arxiv.org/abs/2410.11564",
            "title": "Point Clouds Get Chatty: New AI Model Makes Robots Understand Affordances",
            "institute": "University of Hamburg, Hon Hai Research Institute (HHRI), National Taiwan University...",
            "text": "This research introduces PAVLM, a framework that uses pre-trained language models to improve 3D affordance understanding in point clouds. Unlike previous methods that rely on projecting point clouds into 2D depth maps, PAVLM directly extracts semantic features from the raw point cloud data.",
            "paper-title": "PAVLM: Advancing Point Cloud based Affordance Understanding Via Vision-Language Model",
            "image-path": "flux_paper_image/2410.11564_1729109052.png"
        },

        {
            "startTime": "33:57",
            "arxivId": "2410.11451",
            "arxivLink": "https://arxiv.org/abs/2410.11451",
            "title": "Small Language Models: Why They're Like Tiny Tots Learning to Talk",
            "institute": "University of Cambridge",
            "text": "This research delves into the training dynamics of small language models (LLMs), focusing on how their activations converge to their final state. Unlike previous work that primarily focused on the representational capacity of these models, this study examines the relationship between the effective rank of their parameters and their convergence behavior.",
            "paper-title": "Tending Towards Stability: Convergence Challenges in Small Language Models",
            "image-path": "flux_paper_image/2410.11451_1729109359.png"
        },

        {
            "startTime": "34:22",
            "arxivId": "2410.11767",
            "arxivLink": "https://arxiv.org/abs/2410.11767",
            "title": "Can AI Really Understand Grammar? New Study Tests the Limits of Sparse Autoencoders",
            "institute": "International Institute of Information Technology, Harvard University",
            "text": "This research investigates the ability of sparse autoencoders (SAEs) to disentangle and interpret language model representations by training them on synthetic formal languages. Unlike previous work that focused on image domains, this study explores the limitations of SAEs in the text domain.",
            "paper-title": "Analyzing (In)Abilities of SAEs via Formal Languages",
            "image-path": "flux_paper_image/2410.11767_1729108965.png"
        },

        {
            "startTime": "34:48",
            "arxivId": "2410.11312",
            "arxivLink": "https://arxiv.org/abs/2410.11312",
            "title": "Multilevel Optimization: A Gradient-Based Approach That's Not Just a Bunch of Levels!",
            "institute": "Peking University",
            "text": "This paper introduces a new gradient-based approach for multilevel optimization that leverages a hierarchical decomposition of the full gradient and advanced propagation techniques. Unlike previous methods, this approach significantly reduces computational complexity and improves solution accuracy and convergence speed.",
            "paper-title": "Towards Differentiable Multilevel Optimization: A Gradient-Based Approach",
            "image-path": "flux_paper_image/2410.11312_1729109141.png"
        },

        {
            "startTime": "35:07",
            "arxivId": "2410.11059",
            "arxivLink": "https://arxiv.org/abs/2410.11059",
            "title": "Bias Busters: Unmasking the Hidden Prejudice in AI's Language Models",
            "institute": "Stanford University, University College London, Holistic AI",
            "text": "This research delves into the biases present within the classifiers used to evaluate bias in large language models (LLMs). Unlike previous work that focused solely on LLM outputs, this study examines the inherent biases within the tools used to assess those outputs.",
            "paper-title": "Assessing Bias in Metric Models for LLM Open-Ended Generation Bias Benchmarks",
            "image-path": "flux_paper_image/2410.11059_1729107301.png"
        },

        {
            "startTime": "35:30",
            "arxivId": "2410.11831",
            "arxivLink": "https://arxiv.org/abs/2410.11831",
            "title": "Point Tracking: Less Data, More Wow!",
            "institute": "University of Oxford",
            "text": "This research introduces CoTracker3, a point tracking model that uses a simpler architecture and a semi-supervised training protocol. Unlike previous work, CoTracker3 relies on pseudo-labels generated by existing trackers, allowing it to achieve better performance with significantly less real-world training data.",
            "paper-title": "CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos",
            "image-path": "flux_paper_image/2410.11831_1729108763.png"
        },

        {
            "startTime": "35:49",
            "arxivId": "2410.11284",
            "arxivLink": "https://arxiv.org/abs/2410.11284",
            "title": "Calabi-Yau Metrics: Machine Learning Takes a Shortcut!",
            "institute": "University of Cambridge",
            "text": "This research explores a novel approach to calculating Calabi-Yau metrics by applying machine learning techniques within a \"principled\" framework. Unlike previous work that relied on neural networks, this study utilizes gradient descent on the Grassmannian manifold to identify an efficient subspace of sections for metric computation.",
            "paper-title": "Calabi-Yau metrics through Grassmannian learning and Donaldson's algorithm",
            "image-path": "flux_paper_image/2410.11284_1729107419.png"
        },

        {
            "startTime": "36:14",
            "arxivId": "2410.11672",
            "arxivLink": "https://arxiv.org/abs/2410.11672",
            "title": "LLMs: Clever Hans in Disguise? Simple Features Predict Benchmark Answers",
            "institute": "University of Cambridge, Polytechnic University of Valencia",
            "text": "This research investigates whether LLMs are solving benchmarks by exploiting simple patterns in the prompts, rather than demonstrating the intended capabilities. Unlike previous work that focused on individual n-grams, this study explores the predictive power of combinations of unigrams and bigrams.",
            "paper-title": "Leaving the barn door open for Clever Hans: Simple features predict LLM benchmark answers",
            "image-path": "flux_paper_image/2410.11672_1729108399.png"
        },

        {
            "startTime": "36:47",
            "arxivId": "2410.10850",
            "arxivLink": "https://arxiv.org/abs/2410.10850",
            "title": "Chatbots on Trial: Can AI Tell Fact from Fiction in Climate Change and Mental Health?",
            "institute": "Mohamed bin Zayed University of Artificial Intelligence, Polytechnic University of Catalonia, Dalhousie University...",
            "text": "This research goes beyond simply assessing the accuracy of LLMs in answering questions. It specifically investigates how these models handle misinformed prompts and whether they exhibit biases when presented with demographic information.",
            "paper-title": "On the Reliability of Large Language Models to Misinformed and Demographically-Informed Prompts",
            "image-path": "flux_paper_image/2410.10850_1729109074.png"
        },

        {
            "startTime": "37:17",
            "arxivId": "2410.11709",
            "arxivLink": "https://arxiv.org/abs/2410.11709",
            "title": "Spatial Data Science Gets a New Metric: Optimal Transport to the Rescue!",
            "institute": "ETH Zurich",
            "text": "This research proposes using Optimal Transport (OT) as a spatial evaluation metric and loss function for GeoAI models. Unlike conventional metrics like MSE, OT considers the spatial distribution of prediction errors, reflecting the real-world costs associated with relocation or resource allocation.",
            "paper-title": "On the potential of Optimal Transport in Geospatial Data Science",
            "image-path": "flux_paper_image/2410.11709_1729108680.png"
        },

        {
            "startTime": "37:44",
            "arxivId": "2410.10937",
            "arxivLink": "https://arxiv.org/abs/2410.10937",
            "title": "Species Distribution Modeling Gets a Spatial Upgrade: Hybrid Embeddings for Better Predictions!",
            "institute": "UC Berkeley, Tsinghua University",
            "text": "This research introduces a hybrid embedding scheme for species distribution modeling (SDM) that combines implicit and explicit representations. Unlike previous work that relied solely on implicit representations, this approach leverages the strengths of both to improve spatial precision and capture local environmental information.",
            "paper-title": "Hybrid Spatial Representations for Species Distribution Modeling",
            "image-path": "flux_paper_image/2410.10937_1729108204.png"
        },

        {
            "startTime": "38:05",
            "arxivId": "2410.11164",
            "arxivLink": "https://arxiv.org/abs/2410.11164",
            "title": "Brain-Inspired Learning: How Initial Connections Can Make or Break Your Neural Network!",
            "institute": "Princeton University",
            "text": "This research extends the study of initial connectivity's impact on learning from backpropagation-based methods to biologically plausible learning rules, specifically in recurrent neural networks (RNNs).",
            "paper-title": "How Initial Connectivity Shapes Biologically Plausible Learning in Recurrent Neural Networks",
            "image-path": "flux_paper_image/2410.11164_1729107996.png"
        },

        {
            "startTime": "38:35",
            "arxivId": "2410.10874",
            "arxivLink": "https://arxiv.org/abs/2410.10874",
            "title": "Penguin Power: How AI is Learning to Predict Job Market Sentiment",
            "institute": "CMU",
            "text": "This research optimizes the Transformer model for sentiment analysis using the Arctic Penguin Optimization algorithm, a novel approach compared to traditional methods.",
            "paper-title": "Optimizing Transformer based on high-performance optimizer for predicting employment sentiment in American social media content",
            "image-path": "flux_paper_image/2410.10874_1729109402.png"
        },

        {
            "startTime": "38:58",
            "arxivId": "2410.11674",
            "arxivLink": "https://arxiv.org/abs/2410.11674",
            "title": "LLMs Get a Time Machine: Multiscale Mixing for Forecasting",
            "institute": "University of Central Florida, Anymate Me, Microsoft",
            "text": "This research introduces LLM-Mixer, a framework that combines multiscale time series decomposition with pre-trained LLMs for improved forecasting accuracy. Unlike previous work that directly fed time series data into LLMs, LLM-Mixer breaks down the data into multiple time scales, allowing the model to capture both short-term fluctuations and long-term trends more effectively.",
            "paper-title": "LLM-Mixer: Multiscale Mixing in LLMs for Time Series Forecasting",
            "image-path": "flux_paper_image/2410.11674_1729108135.png"
        },

        {
            "startTime": "39:24",
            "arxivId": "2410.11756",
            "arxivLink": "https://arxiv.org/abs/2410.11756",
            "title": "AI's Clock-Drawing Test: Can Machines Tell Time?",
            "institute": "Google Research, Google DeepMind, University of Washington",
            "text": "This study uniquely assesses the cognitive abilities of generative AI models by administering the Clock Drawing Test (CDT), a widely used neuropsychological assessment for evaluating executive functioning.",
            "paper-title": "Evidence of Cognitive Deficits andDevelopmental Advances in Generative AI: A Clock Drawing Test Analysis",
            "image-path": "flux_paper_image/2410.11756_1729108101.png"
        },

        {
            "startTime": "39:46",
            "arxivId": "2410.11228",
            "arxivLink": "https://arxiv.org/abs/2410.11228",
            "title": "Occupancy Prediction Gets a Time Machine: New Research Uses Temporal Enhancement to See the Future!",
            "institute": "Peking University",
            "text": "This research introduces a temporal enhancement branch to improve 3D occupancy prediction. Unlike previous methods that focus on representation, this approach leverages long-term and short-term temporal information, similar to how temporal modeling has been successful in 3D object detection.",
            "paper-title": "TEOcc: Radar-camera Multi-modal Occupancy Prediction via Temporal Enhancement",
            "image-path": "flux_paper_image/2410.11228_1729108506.png"
        },

        {
            "startTime": "40:12",
            "arxivId": "2410.11825",
            "arxivLink": "https://arxiv.org/abs/2410.11825",
            "title": "Smooth Moves: Robots Learn to Walk Like Humans with a New Trick!",
            "institute": "Simon Fraser University, UIUC, UC Berkeley...",
            "text": "This research introduces a new method called Lipschitz-Constrained Policies (LCP) to train reinforcement learning policies for smooth locomotion in humanoid robots. Unlike previous methods that rely on non-differentiable techniques like smoothness rewards or low-pass filters, LCP uses a differentiable gradient penalty to encourage smooth behaviors.",
            "paper-title": "Learning Smooth Humanoid Locomotion through Lipschitz-Constrained Policies",
            "image-path": "flux_paper_image/2410.11825_1729109501.png"
        },

        {
            "startTime": "40:41",
            "arxivId": "2410.11123",
            "arxivLink": "https://arxiv.org/abs/2410.11123",
            "title": "Prompt Engineering: The Secret Sauce for AI in K-12 STEM?",
            "institute": "Carnegie Mellon University, University of Sheffield, Harvard University...",
            "text": "This research focuses on the specific use of prompt engineering in large language models (LLMs) for K-12 STEM education, a topic that has received less attention in previous systematic literature reviews.",
            "paper-title": "A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education",
            "image-path": "flux_paper_image/2410.11123_1729108658.png"
        },

        {
            "startTime": "41:02",
            "arxivId": "2410.10843",
            "arxivLink": "https://arxiv.org/abs/2410.10843",
            "title": "UAVs Get Smart: AI Prioritizes Data for Better Surveillance",
            "institute": "Clemson University, MIT",
            "text": "This research proposes a novel AI-driven scheduling policy for UAV surveillance missions that prioritizes transmitting only the most important image regions, unlike previous methods that aim to preserve all information.",
            "paper-title": "Adaptive Data Transport Mechanism for UAV Surveillance Missions in Lossy Environments",
            "image-path": "flux_paper_image/2410.10843_1729109603.png"
        },

        {
            "startTime": "41:27",
            "arxivId": "2410.11723",
            "arxivLink": "https://arxiv.org/abs/2410.11723",
            "title": "Spacecraft Navigation Gets a Transformer Makeover: AI Learns to Navigate the Cosmos!",
            "institute": "Polytechnic University of Turin, Stanford University, MIT",
            "text": "This research extends previous work on learning-based trajectory generation by incorporating multimodal data into the training process. This allows the model to generalize to diverse scenarios, including variations in obstacle configurations and time constraints.",
            "paper-title": "Generalizable Spacecraft Trajectory Generation via Multimodal Learning with Transformers",
            "image-path": "flux_paper_image/2410.11723_1729107888.png"
        },

        {
            "startTime": "41:54",
            "arxivId": "2410.11186",
            "arxivLink": "https://arxiv.org/abs/2410.11186",
            "title": "Fat and Iron: AI Makes MRI Faster and Smarter!",
            "institute": "University of California Berkeley, insitro inc.",
            "text": "This research uses a generative machine learning approach to estimate fat and iron content from MRI scans using only two measurements, unlike traditional methods that require at least three.",
            "paper-title": "Synthesizing Proton-Density Fat Fraction and $R_2^*$ from 2-point Dixon MRI with Generative Machine Learning",
            "image-path": "flux_paper_image/2410.11186_1729107507.png"
        },

        {
            "startTime": "42:14",
            "arxivId": "2410.11330",
            "arxivLink": "https://arxiv.org/abs/2410.11330",
            "title": "Forget the Gradient, Let's Evolve! A New Way to Tune Machine Learning Models",
            "institute": "French Institute for Research in Computer Science and Automation, ESIEE Paris, Meta",
            "text": "This research introduces AfterLearnER, a method that refines fully-trained machine learning models by optimizing a small set of parameters or hyperparameters using non-differentiable optimization techniques, including evolutionary algorithms. This differs from previous work that typically relies on gradient-based optimization, which requires differentiable loss functions.",
            "paper-title": "Evolutionary Retrofitting",
            "image-path": "flux_paper_image/2410.11330_1729108257.png"
        },

        {
            "startTime": "42:42",
            "arxivId": "2410.11703",
            "arxivLink": "https://arxiv.org/abs/2410.11703",
            "title": "Robotic Arm Gives Organs a 3D Makeover for Surgery!",
            "institute": "University College London",
            "text": "This research introduces a robotic arm platform specifically designed for multi-view image acquisition and 3D reconstruction in minimally invasive surgery (MIS) settings. Unlike previous datasets, this platform uses a laser scanner to capture highly accurate 3D ground truth, allowing for comprehensive evaluation of the reconstruction pipeline.",
            "paper-title": "Robotic Arm Platform for Multi-View Image Acquisition and 3D Reconstruction in Minimally Invasive Surgery",
            "image-path": "flux_paper_image/2410.11703_1729107727.png"
        },

        {
            "startTime": "42:58",
            "arxivId": "2410.11038",
            "arxivLink": "https://arxiv.org/abs/2410.11038",
            "title": "Reshaping Neural Networks: A New Trick for Training Deep Learning Models Faster",
            "institute": "Stanford University",
            "text": "This research introduces two new function-preserving transforms, R2WiderR and R2DeeperR, which are compatible with residual connections, a common architectural feature in deep learning. This is different from previous work like Net2Net and Network Morphisms, which did not handle residual connections.",
            "paper-title": "Towards a More Complete Theory of Function Preserving Transforms",
            "image-path": "flux_paper_image/2410.11038_1729108862.png"
        },

        {
            "startTime": "43:19",
            "arxivId": "2410.11381",
            "arxivLink": "https://arxiv.org/abs/2410.11381",
            "title": "LLMs: From Tiny to Titanic, a Deep Dive into Architecture Convergence",
            "institute": "Yonsei University",
            "text": "This research analyzes the converging architecture of modern LLMs, focusing on the performance of different attention mechanisms (MHA, GQA, and MQA) in various hyperparameter settings. It goes beyond simply describing the architecture and delves into the operational aspects of these models, particularly their inference time on high-performance GPUs.",
            "paper-title": "Survey and Evaluation of Converging Architecture in LLMs based on Footsteps of Operations",
            "image-path": "flux_paper_image/2410.11381_1729107211.png"
        },

        {
            "startTime": "43:48",
            "arxivId": "2410.11071",
            "arxivLink": "https://arxiv.org/abs/2410.11071",
            "title": "Ancient Greek Texts: A Dataset of Real Errors, Not Just Typos!",
            "institute": "Princeton University, MIT",
            "text": "This research introduces the first dataset of real errors in premodern Greek texts, unlike previous work that relied on artificially generated errors.",
            "paper-title": "An Annotated Dataset of Errors in Premodern Greek and Baselines for Detecting Them",
            "image-path": "flux_paper_image/2410.11071_1729108771.png"
        },

        {
            "startTime": "44:11",
            "arxivId": "2410.11505",
            "arxivLink": "https://arxiv.org/abs/2410.11505",
            "title": "Visual Localization on a Budget: How to Map Your World with Fewer Images!",
            "institute": "University College London, Zhejiang University",
            "text": "This paper introduces LoGS, a visual localization pipeline that utilizes 3D Gaussian Splatting (GS) as the scene representation. Unlike previous methods that rely on large datasets, LoGS achieves high accuracy with significantly fewer training images.",
            "paper-title": "LoGS: Visual Localization via Gaussian Splatting with Fewer Training Images",
            "image-path": "flux_paper_image/2410.11505_1729107823.png"
        },

        {
            "startTime": "44:29",
            "arxivId": "2410.11190",
            "arxivLink": "https://arxiv.org/abs/2410.11190",
            "title": "Mini-Omni2: GPT-4o's Open-Source Twin, Ready to Talk, See, and Hear!",
            "institute": "Tsinghua University",
            "text": "This research introduces Mini-Omni2, an open-source multi-modal model that aims to replicate the capabilities of GPT-4o, which is not open-sourced. Unlike previous work that focused on specific functionalities of GPT-4o, Mini-Omni2 integrates vision, speech, and text modalities in a single model.",
            "paper-title": "Mini-Omni2: Towards Open-source GPT-4o Model with Vision, Speech and Duplex",
            "image-path": "flux_paper_image/2410.11190_1729108802.png"
        },

        {
            "startTime": "44:48",
            "arxivId": "2410.10885",
            "arxivLink": "https://arxiv.org/abs/2410.10885",
            "title": "AI-Powered Graphene Growth: A Recipe for Success, One Hot Temperature at a Time!",
            "institute": "Italian Institute of Technology, Lawrence Berkeley National Laboratory",
            "text": "This research utilizes an adaptive Monte Carlo algorithm to train an artificial neural network (ANN) that learns to optimize graphene growth protocols without relying on pre-trained models or historical data. This approach distinguishes itself from previous work by learning dynamically through direct experimental feedback and continuously refining its protocols over time.",
            "paper-title": "Adaptive AI-Driven Material Synthesis: Towards Autonomous 2D Materials Growth",
            "image-path": "flux_paper_image/2410.10885_1729108108.png"
        },

        {
            "startTime": "45:12",
            "arxivId": "2410.11065",
            "arxivLink": "https://arxiv.org/abs/2410.11065",
            "title": "Fusion's New Trick: AI Learns to See Disruptions in a Whole New Light!",
            "institute": "UC Berkeley, Google, University of Maryland...",
            "text": "This research introduces a novel time series viewmaker network for data augmentation in the context of nuclear fusion disruption prediction. Unlike previous work that focused on standard augmentation techniques, this study explores the use of generative augmentation methods to improve model robustness and generalizability across different tokamak designs.",
            "paper-title": "Time Series Viewmakers for Robust Disruption Prediction",
            "image-path": "flux_paper_image/2410.11065_1729108735.png"
        },

        {
            "startTime": "45:34",
            "arxivId": "2410.11511",
            "arxivLink": "https://arxiv.org/abs/2410.11511",
            "title": "Sodium MRI Gets a Noise-Busting Makeover: Rician DDPM to the Rescue!",
            "institute": "University of Cambridge",
            "text": "This research introduces a novel denoising method called Rician DDPM, which specifically addresses the challenges of Rician noise in sodium MRI data. Unlike previous methods that primarily target Gaussian noise, RDDPM converts Rician noise to Gaussian noise at each timestep during the denoising process, leading to improved image quality.",
            "paper-title": "Rician Denoising Diffusion Probabilistic Models For Sodium Breast MRI Enhancement",
            "image-path": "flux_paper_image/2410.11511_1729108700.png"
        },

        {
            "startTime": "46:09",
            "arxivId": "2410.11161",
            "arxivLink": "https://arxiv.org/abs/2410.11161",
            "title": "Stellarator Coils: From Filaments to Finite-Build with Automatic Differentiation",
            "institute": "Princeton University",
            "text": "This research introduces a new stellarator coil design code, FOCUSADD, which directly optimizes coils with non-zero thickness, accounting for the finite-build correction to the produced magnetic field. Previous codes have optimized the positions of filamentary (zero-thickness) coils, which is only valid in the limit that the coil thickness is much less than the distance from the coil to the plasma.",
            "paper-title": "Differentiable Programming for Computational Plasma Physics",
            "image-path": "flux_paper_image/2410.11161_1729107760.png"
        },

        {
            "startTime": "46:33",
            "arxivId": "2410.11587",
            "arxivLink": "https://arxiv.org/abs/2410.11587",
            "title": "AI Cracks the Code: New Equation Predicts Groundwater Recharge!",
            "institute": "Lawrence Berkeley National Laboratory, University of Nebraska-Lincoln, Stanford University",
            "text": "This research uses Kolmogorov-Arnold networks (KANs), a type of neural network, to identify symbolic expressions for hydrological models. This differs from previous work that often relies on pre-defined equations or black-box models.",
            "paper-title": "Baseflow identification via explainable AI with Kolmogorov-Arnold networks",
            "image-path": "flux_paper_image/2410.11587_1729108268.png"
        }
    ],
    "stats": {
        "num_pick": 114,
        "num_total": 416,
    },
    "audio": "https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202410161333_audio.mp3"
}
