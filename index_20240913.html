
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - AI Paper Picks of the Day</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 80px;"></div>
            <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                Freshest
                Top Picks:
                <span class="highlightNumber" style="font-size: 28px;">113</span> out of <span
                    class="highlightNumber">500</span> arXiv AI
                papers
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="line-height: 1.2;">just released on</div>
                <div id="data-default-date" data-date="2024-09-13"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 10px;">
            </div>
        </div>


        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">00:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08273" target="_blank">@arXiv 2409.08273</a>
                    <span class="tweet-title">Robots Learn to Manipulate Like Humans, Thanks to YouTube!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research differs from previous work by using a generative modeling approach to learn a manipulation prior from 3D hand-object interaction trajectories extracted from in-the-wild videos. This approach does not require strict alignment between the human's intent in the video and the downstream robot tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">01:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06765" target="_blank">@arXiv 2409.06765</a>
                    <span class="tweet-title">gsplat:  Splatting Gaussians for 3D Reconstruction, Now with a Side of Open Source!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley, Aalto University, ShanghaiTech University</span>
                </div>
                <div class="primary-text">
                    This paper introduces gsplat, an open-source library specifically designed for training and developing Gaussian Splatting methods. It offers a user-friendly interface and optimized CUDA kernels, making it more efficient than previous implementations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">01:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07025" target="_blank">@arXiv 2409.07025</a>
                    <span class="tweet-title">Diffusion Models Get a Privacy Makeover:  Classifier-Protected Sampling to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This paper introduces CPSample, a method that modifies the sampling process in diffusion models to prevent training data replication without sacrificing image quality. Unlike previous approaches that focused on data corruption or differential privacy, CPSample utilizes a classifier trained on random labels to guide the generation process away from the training data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">02:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07606" target="_blank">@arXiv 2409.07606</a>
                    <span class="tweet-title">Deep Learning's Secret Weapon: Regularizing Actors in Offline RL</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research investigates the impact of applying deep learning regularization techniques to actor networks in offline reinforcement learning, a domain where these techniques have been less explored.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">02:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07431" target="_blank">@arXiv 2409.07431</a>
                    <span class="tweet-title">Teaching LLMs New Tricks:  How Synthetic Data Makes Knowledge Acquisition More Efficient</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley, Stanford University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method called "synthetic continued pretraining" to teach language models (LLMs) knowledge from small, specialized corpora. Unlike previous work that relies on massive datasets, this approach uses a synthetic data augmentation algorithm called EntiGraph to generate diverse representations of knowledge from the small corpus, making it more amenable to learning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">03:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07368" target="_blank">@arXiv 2409.07368</a>
                    <span class="tweet-title">Code Security:  A Prompt-Optimizing System That's Not Just a Bunch of Hot Air</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Kent State University, New Jersey Institute of Technology, Hamad Bin Khalifa University...</span>
                </div>
                <div class="primary-text">
                    This research introduces SGCode, a system that allows users to easily switch between different prompt-optimization approaches for secure code generation. This flexibility allows for a deeper understanding of the trade-offs between code utility, security analysis, and system performance. Unlike previous work, which focused on specific approaches, SGCode provides a platform for comparing and contrasting various methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">03:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07033" target="_blank">@arXiv 2409.07033</a>
                    <span class="tweet-title">E-commerce Recommendations:  A Neural Network's Guide to Your Next Click</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Science and Technology of China, Arizona State University, The University of Chicago...</span>
                </div>
                <div class="primary-text">
                    This research combines semantic web mining with a BP neural network to improve e-commerce webpage recommendations. Unlike previous work, it considers not only user search queries but also factors like time spent on pages and explicit/implicit feedback.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">04:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07402" target="_blank">@arXiv 2409.07402</a>
                    <span class="tweet-title">Multimodal Learning: Beyond Redundancy, It's Synergy Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research introduces CoMM, a contrastive multimodal learning strategy that goes beyond aligning features based on shared information. It aims to capture unique, redundant, and synergistic information between modalities by maximizing mutual information between augmented versions of multimodal features.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">04:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07409" target="_blank">@arXiv 2409.07409</a>
                    <span class="tweet-title">Blindfolded Robot Walks Through Tiny Traps: Proprioception to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Shanghai Qi Zhi Institute, University of Pennsylvania</span>
                </div>
                <div class="primary-text">
                    This research focuses on enabling quadruped robots to navigate tiny obstacles using only proprioceptive sensors, unlike previous methods that rely on unreliable cameras. The paper introduces a two-stage training framework that incorporates a contact encoder and a classification head to learn implicit representations of different traps.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">05:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07094" target="_blank">@arXiv 2409.07094</a>
                    <span class="tweet-title">Deep Learning Saves the Day:  AI Calibrates Hyperspectral Cameras for Surgery</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Siemens AG, German Cancer Research Center (DKFZ), Helmholtz Information and Data Science School for Health...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel learning-based approach to automatically recalibrate hyperspectral images during surgery, eliminating the need for manual white reference measurements. Unlike previous methods, this approach focuses on predicting the white reference image itself, rather than directly recalibrating the image, enabling a more robust and generalizable solution.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">05:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08189" target="_blank">@arXiv 2409.08189</a>
                    <span class="tweet-title">Dress Up Your Digital World:  New Tech Makes Clothes Come Alive!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, Max Planck Institute for Intelligent Systems</span>
                </div>
                <div class="primary-text">
                    This research introduces Gaussian Garments, a novel approach for reconstructing realistic simulation-ready garment assets from multi-view videos. Unlike previous methods that focus on reconstructing holistic avatars, this approach separates garments from the bodies, allowing for greater flexibility in applications like virtual try-on and garment resizing.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">05:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08202" target="_blank">@arXiv 2409.08202</a>
                    <span class="tweet-title">Mazes Made Easy: How AI Learns to See the Big Picture</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces Deep Schema Grounding (DSG), a framework that uses structured representations of abstract concepts to improve visual reasoning in AI models. Unlike previous work that focuses on literal interpretations of images, DSG leverages schemas to understand the underlying patterns and rules that define abstract concepts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">06:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06793" target="_blank">@arXiv 2409.06793</a>
                    <span class="tweet-title">Multi-Modal Models:  When Images and Audio Get Hacked!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Duke University, University of Tokyo, Rochester Institute of Technology...</span>
                </div>
                <div class="primary-text">
                    This research introduces CrossFire, a novel adversarial attack on multi-modal models. Unlike previous attacks that directly align the targeted input with the perturbed image/audio, CrossFire first transforms the targeted input into a format matching the modality of the image/audio file.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">06:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07154" target="_blank">@arXiv 2409.07154</a>
                    <span class="tweet-title">Recurrent NAR:  When Algorithms Get a Memory Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge, Google</span>
                </div>
                <div class="primary-text">
                    This research explores the use of recurrent neural networks (RNNs) as aggregators in neural algorithmic reasoning (NAR) models. Unlike previous work that primarily relies on permutation-invariant aggregators, this approach leverages the sequential nature of RNNs to handle tasks with inherent ordering, such as sorting algorithms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">07:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07819" target="_blank">@arXiv 2409.07819</a>
                    <span class="tweet-title">Selling Ads Together: A Learning Algorithm for Joint Profits</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google, Uber, Sapienza University of Rome</span>
                </div>
                <div class="primary-text">
                    This paper explores the problem of selling a single, non-excludable good to two buyers, like an ad slot to a brand and a merchant, from an online learning perspective. It differs from previous work by considering the case where the buyers' valuations are drawn from unknown distributions, requiring the mechanism designer to learn the optimal strategy on the fly.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">07:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07932" target="_blank">@arXiv 2409.07932</a>
                    <span class="tweet-title">Reinforcement Learning: The New Social Network Sherpa</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London, University of Oxford, University of Bologna</span>
                </div>
                <div class="primary-text">
                    This research explores decentralized graph path search using reinforcement learning, where agents have only local visibility of the network, unlike previous centralized approaches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">07:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07032" target="_blank">@arXiv 2409.07032</a>
                    <span class="tweet-title">Score-Based Diffusion Models: Finally, Optimal Sampling!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University, University of Chicago</span>
                </div>
                <div class="primary-text">
                    This paper establishes the sharp minimax rate of score estimation for smooth, compactly supported densities, a key step in training score-based diffusion models. Unlike previous work, it achieves this rate without relying on early stopping or extraneous logarithmic terms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">08:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07429" target="_blank">@arXiv 2409.07429</a>
                    <span class="tweet-title">Agents Get a Memory Makeover: Learning Reusable Workflows for Web Navigation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, Massachusetts Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This research introduces Agent Workflow Memory (AWM), a method that allows agents to learn reusable task workflows from past experiences and apply them to future tasks. Unlike previous methods that rely on fixed sets of examples, AWM enables agents to adapt and improve their performance over time.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">08:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07016" target="_blank">@arXiv 2409.07016</a>
                    <span class="tweet-title">Pre-trained Audio Models:  LoRA-ing Their Way to Anomaly Detection!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research explores the use of Low-Rank Adaptation (LoRA) for fine-tuning pre-trained audio models in anomalous sound detection (ASD) tasks. This approach differs from previous work by focusing on adapting the model's weights without altering its architecture, which is particularly beneficial when dealing with limited data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">09:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06762" target="_blank">@arXiv 2409.06762</a>
                    <span class="tweet-title">AI Makes Crystals From Words:  A Recipe for New Materials</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research proposes a hierarchical approach to generating crystal structures from natural language descriptions. Unlike previous work that relies on vast amounts of unconditioned samples or predetermined chemical formulae, this method leverages a language model to generate candidate formulae and a diffusion model to generate structures based on those formulae.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">09:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06997" target="_blank">@arXiv 2409.06997</a>
                    <span class="tweet-title">Predict-then-Optimize:  Distance is the New Black!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset distance measure specifically designed for Predict-then-Optimize (PtO) tasks. Unlike traditional measures that focus on features and labels, this new distance incorporates the impact of downstream decisions, making it more relevant for evaluating the adaptability of PtO models across different domains.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">09:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08278" target="_blank">@arXiv 2409.08278</a>
                    <span class="tweet-title">DreamHOI:  Making 3D Humans Do Cool Stuff with Text!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, University of Oxford</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new method for generating 3D human-object interactions using text prompts. Unlike previous methods that rely on extensive training data, DreamHOI leverages pre-trained text-to-image diffusion models to guide the pose optimization of a skinned human mesh.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">10:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06790" target="_blank">@arXiv 2409.06790</a>
                    <span class="tweet-title">Machine Translation Gets a Makeover:  LLMs Learn to Think Like Humans!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research proposes a step-by-step approach to long-form text translation, breaking down the process into distinct stages like pre-translation research, drafting, refinement, and proofreading. This differs from previous work that often focuses on post-translation refinement or uses complex multi-stage processes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">10:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07577" target="_blank">@arXiv 2409.07577</a>
                    <span class="tweet-title">Forget Fine-Tuning, Let's Mask It!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Amsterdam, Google Research</span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel method for adapting pretrained models to downstream tasks using binary masks. Unlike previous approaches that fine-tune the entire network, this method learns a mask that selectively activates specific weights, reducing memory requirements and improving performance in label-sparse settings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">10:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07809" target="_blank">@arXiv 2409.07809</a>
                    <span class="tweet-title">Cloning Clinical Notes:  A Privacy-Preserving Recipe for Training AI in Healthcare</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft Corporation, Gretel.ai</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method for generating synthetic clinical data that preserves the essential characteristics of real data while safeguarding patient privacy. Unlike previous approaches that rely on anonymization, this method leverages differential privacy techniques and instruction tuning to create datasets that are both clinically accurate and free from identifiable information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">11:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07326" target="_blank">@arXiv 2409.07326</a>
                    <span class="tweet-title">EEG Denoising Gets a Transformer Makeover:  ART Outperforms the Competition!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National Tsing Hua University, ELAN Microelectronics Corporation, National Yang Ming Chiao Tung University...</span>
                </div>
                <div class="primary-text">
                    This research introduces ART, a novel EEG denoising model that leverages the transformer architecture to effectively capture both temporal and spatial dependencies in multichannel EEG data. This approach differs from previous work that primarily focused on individual channels or used convolutional neural networks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">11:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08160" target="_blank">@arXiv 2409.08160</a>
                    <span class="tweet-title">Surprisal Theory:  Is Context Just a Fancy Word for Frequency?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, University of Zurich</span>
                </div>
                <div class="primary-text">
                    This research challenges the common assumption that surprisal, a measure of how unexpected a word is in context, is the primary driver of reading time. The authors propose a new method to disentangle the effect of context from frequency, showing that context's role in reading time prediction might be smaller than previously thought.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">12:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07486" target="_blank">@arXiv 2409.07486</a>
                    <span class="tweet-title">Financial Market Simulator:  Trading Orders Meet Generative AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research proposes a generative foundation model, Large Market Model (LMM), specifically designed for financial market simulation. Unlike previous work that focused on modeling the Limit Order Book (LOB), LMM leverages order-level data to generate realistic market trajectories.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">12:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07163" target="_blank">@arXiv 2409.07163</a>
                    <span class="tweet-title">Mamba Policy:  Slithering Through 3D Tasks with 80% Less Fat!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Hong Kong University of Science and Technology</span>
                </div>
                <div class="primary-text">
                    This paper introduces the Mamba Policy, a new approach to 3D manipulation that uses a hybrid state space model (SSM) with attention mechanisms. This differs from previous work by significantly reducing the parameter count of the policy network while maintaining or even improving performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">12:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07020" target="_blank">@arXiv 2409.07020</a>
                    <span class="tweet-title">Brain Parcellation:  A Deep Dive into Uncertainty with EVENet!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Electronic Science and Technology of China, Sun Yat-sen University, Zhejiang University...</span>
                </div>
                <div class="primary-text">
                    This research introduces EVENet, a novel deep learning method for brain parcellation that incorporates evidential deep learning to quantify uncertainty at each voxel during inference. This approach differs from previous work by directly addressing the challenge of handling out-of-distribution data, which is common in brain imaging studies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">13:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06706" target="_blank">@arXiv 2409.06706</a>
                    <span class="tweet-title">Synapses & Neurons:  Fine-Tuning with a Brain-Inspired Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new parameter-efficient fine-tuning (PEFT) method called Synapses & Neurons (SAN) that explicitly propagates feature adjustments to subsequent layers, unlike existing methods that implicitly do so.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">13:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07645" target="_blank">@arXiv 2409.07645</a>
                    <span class="tweet-title">Pedestrian Prediction:  A Context-Aware Look at AI's "Black Box"</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Leeds, University College London</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method called Context-aware Permutation Feature Importance (CAPFI) to assess the importance of input features in pedestrian intention prediction models. Unlike traditional permutation feature importance, CAPFI considers the context of the scenario, such as the traffic light state or the pedestrian's proximity to the vehicle, to mitigate bias and variance in the results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">14:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07054" target="_blank">@arXiv 2409.07054</a>
                    <span class="tweet-title">Arabic AI:  Is English the Key to Unlock Its Potential?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Qatar University, University of Doha for Science and Technology, Liverpool John Moores University...</span>
                </div>
                <div class="primary-text">
                    This research investigates the impact of using native versus non-native language prompts on the performance of large language models (LLMs) for Arabic language tasks. Unlike previous studies that primarily focused on English-dominant tasks, this study explores the effectiveness of prompting in both native and non-native languages for a range of Arabic datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">14:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08264" target="_blank">@arXiv 2409.08264</a>
                    <span class="tweet-title">Windows Agent Arena:  AI Gets a Real-World Job!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft, Carnegie Mellon University, Columbia University</span>
                </div>
                <div class="primary-text">
                    This research introduces WINDOWSAGENTARENA, a benchmark for evaluating multi-modal AI agents within a real Windows operating system. Unlike previous benchmarks that focus on specific modalities or domains, WINDOWSAGENTARENA allows agents to interact with a wide range of applications and tools available to human users.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">15:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07146" target="_blank">@arXiv 2409.07146</a>
                    <span class="tweet-title">Gated Slot Attention:  Giving Transformers a Memory Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Soochow University, Massachusetts Institute of Technology, University of California  Santa Cruz...</span>
                </div>
                <div class="primary-text">
                    This paper introduces Gated Slot Attention (GSA), a new approach to linear attention that incorporates a gating mechanism inspired by Gated Linear Attention (GLA). GSA enhances the Attention with Bounded-Memory Control (ABC) model by enabling selective forgetting of historical information and introducing a recency inductive bias.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">15:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08167" target="_blank">@arXiv 2409.08167</a>
                    <span class="tweet-title">DreamBooth's Nightmare: New Defense Makes Fake Images Unremovable</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research proposes a new adversarial attack method that adds strong perturbation to the high-frequency areas of images, making it more robust against adversarial purification techniques. Unlike previous methods, which added noise that could be easily removed, this approach focuses on adding noise to the edges of images, making it difficult to remove even with advanced purification methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">15:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06985" target="_blank">@arXiv 2409.06985</a>
                    <span class="tweet-title">Pre-trained Transformers:  Short-Term Memory Champs or Long-Term Planning Flops?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Renmin University of China, Pennsylvania State University, Queen Mary University of London...</span>
                </div>
                <div class="primary-text">
                    This research investigates the effectiveness of cross-domain pre-trained decision transformers (DT) in reinforcement learning (RL) environments with varying planning abilities. Unlike previous work that focused on short-term planning, this study explores the performance of pre-trained DT in long-term planning environments and identifies a key factor, the "Markov Matrix," that explains the performance disparity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">16:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06888" target="_blank">@arXiv 2409.06888</a>
                    <span class="tweet-title">MAPF Map Makers:  AI Designs Levels That Make Your Bots Go Bonkers!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, University of Southern California</span>
                </div>
                <div class="primary-text">
                    This research uses a Quality Diversity (QD) algorithm to automatically generate benchmark maps for Multi-Agent Path Finding (MAPF) algorithms. Unlike previous work that relies on fixed, human-designed maps, this approach creates diverse maps that target specific algorithms, allowing for more comprehensive and unbiased comparisons.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">16:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08056" target="_blank">@arXiv 2409.08056</a>
                    <span class="tweet-title">NeRF Training:  Less Rendering, More Learning!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Chinese University of Hong Kong</span>
                </div>
                <div class="primary-text">
                    This research introduces an expansive supervision mechanism for training Neural Radiance Fields (NeRFs). Unlike traditional methods that render all pixels, this approach selectively renders a subset of pixels and expands their error values to estimate the loss for the entire image.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">17:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07564" target="_blank">@arXiv 2409.07564</a>
                    <span class="tweet-title">Mixing It Up: New AI Model Blends Images and Data to Predict Heart Pressure</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sano Centre for Computational Medicine, University of Sheffield, Warsaw University of Technology...</span>
                </div>
                <div class="primary-text">
                    This research introduces TabMixer, a novel module that integrates tabular data (like demographics and clinical measurements) with imaging data (from cardiac MRI videos) for predicting mean pulmonary artery pressure (mPAP). Unlike previous methods that simply concatenate or multiply these data types, TabMixer uses multi-layer perceptrons (MLPs) to mix information across spatial, temporal, and channel dimensions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">17:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07584" target="_blank">@arXiv 2409.07584</a>
                    <span class="tweet-title">Brain Segmentation:  A New Recipe for Early Alzheimer's Diagnosis</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Illinois, CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel dual-stream vision transformer (DS-ViT) pipeline that integrates segmentation and classification tasks for Alzheimer's disease diagnosis. Unlike traditional knowledge distillation methods, DS-ViT utilizes a dual-stream embedding module to bridge the gap between these distinct tasks and model architectures.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">17:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07953" target="_blank">@arXiv 2409.07953</a>
                    <span class="tweet-title">Tensor Factorizations: The Lego Blocks of Circuits!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Edinburgh, École Polytechnique Fédérale de Lausanne, Eindhoven University of Technology...</span>
                </div>
                <div class="primary-text">
                    This paper establishes a formal connection between circuit representations and tensor factorizations, showing how the latter can be used to unify various circuit learning algorithms under a single framework. This connection is novel and goes beyond previous work that only hinted at similarities between the two.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">18:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07571" target="_blank">@arXiv 2409.07571</a>
                    <span class="tweet-title">Voxel Vision:  Rendering Features for Camera Relocalization</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, University of Zurich</span>
                </div>
                <div class="primary-text">
                    This paper proposes FaVoR, a method that uses a sparse voxel representation to render feature descriptors from novel viewpoints. Unlike previous methods that rely on dense scene representations, FaVoR leverages a globally sparse yet locally dense 3D representation of 2D features, making it more efficient and scalable.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">18:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08271" target="_blank">@arXiv 2409.08271</a>
                    <span class="tweet-title">DreamBeast:  Turning Text Prompts into Fantastical 3D Animals, Part by Part!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford, Australian National University, University of Copenhagen</span>
                </div>
                <div class="primary-text">
                    This research introduces DreamBeast, a method that leverages a pre-trained 2D diffusion model to guide the generation of 3D assets with part-level control. Unlike previous methods that struggle with part-aware generation, DreamBeast efficiently extracts part-level knowledge from the 2D model and injects it into the 3D generation process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">19:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07494" target="_blank">@arXiv 2409.07494</a>
                    <span class="tweet-title">Ethereum Fraud Fighters:  New Language Model Detects Shady Transactions!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, University of California  Berkeley, University of Illinois at Urbana-Champaign...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to Ethereum fraud detection by combining a transaction language model with graph-based methods. Unlike previous studies that solely relied on numerical data or separate models, this approach leverages the semantic information and similarity patterns within transactions, resulting in a more comprehensive understanding of fraudulent behavior.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">19:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08065" target="_blank">@arXiv 2409.08065</a>
                    <span class="tweet-title">AI Supercharges the Hunt for High-Temperature Superconductors:  A New Era of Discovery!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Renmin University of China, Hefei National Laboratory, Gaoling School of Artificial Intelligence</span>
                </div>
                <div class="primary-text">
                    This research introduces an AI search engine that integrates deep learning techniques, diffusion models, and physics-based calculations to discover high-temperature superconductors. Unlike previous work, this approach directly generates crystal structures, incorporating detailed structural information and exploring materials beyond existing databases.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">19:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08103" target="_blank">@arXiv 2409.08103</a>
                    <span class="tweet-title">Faetar's Speech Recognition Challenge: Can AI Crack This Under-Resourced Language?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark corpus for speech recognition, focusing on Faetar, a severely under-resourced language with limited data and noisy recordings. This differs from previous benchmarks that often aggregate data from multiple languages, masking performance variations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">20:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07907" target="_blank">@arXiv 2409.07907</a>
                    <span class="tweet-title">COCO's Got a New Problem:  Detectors Can't Tell the Difference Between a Cat and a...  Cacti?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Intellindust, Guilin University Of Electronic Technology, ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research introduces COCO-FP, a new dataset derived from ImageNet-1K. It's designed to specifically evaluate how well object detectors handle false positives caused by background clutter, which is a common problem in real-world applications. Unlike previous datasets, COCO-FP focuses on images that don't contain any objects from the COCO categories, forcing detectors to rely on their ability to distinguish between target and non-target objects.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">20:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07691" target="_blank">@arXiv 2409.07691</a>
                    <span class="tweet-title">Ranking Models:  The Secret Sauce for Smarter Search?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nvidia</span>
                </div>
                <div class="primary-text">
                    This research benchmarks various publicly available ranking models for text retrieval, focusing on question-answering tasks. It introduces a new state-of-the-art ranking model, NV-RerankQA-Mistral-4B-v3, which achieves a significant accuracy increase of 14% compared to pipelines with other rerankers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">21:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07482" target="_blank">@arXiv 2409.07482</a>
                    <span class="tweet-title">AI Gets a Vibration Check: New Model Analyzes Industrial Signals with Expert Help</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces VSLLaVA, a pipeline that integrates expert knowledge into a large multimodal foundation model for industrial vibration signal analysis. Unlike previous work that focuses on general image recognition, VSLLaVA specifically addresses the challenges of analyzing industrial signals by incorporating domain-specific expertise.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">21:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06726" target="_blank">@arXiv 2409.06726</a>
                    <span class="tweet-title">Vision Models:  Tricked by a Game of "Who's Who?"</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Huazhong University of Science and Technology, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new attack paradigm called Feedback-based Modal Mutual Search (FMMS) that leverages target model feedback to refine adversarial examples, improving their effectiveness in attacking vision-language pre-training (VLP) models. Unlike previous methods that rely solely on surrogate models, FMMS incorporates feedback from the target model to guide the search for adversarial examples, bridging the gap in feature representation between surrogate and target models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">21:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07434" target="_blank">@arXiv 2409.07434</a>
                    <span class="tweet-title">Dropout's Got Rhythm:  New Theory Makes SGD Dance to a Stationary Beat</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Chicago</span>
                </div>
                <div class="primary-text">
                    This paper analyzes the asymptotic properties of stochastic gradient descent (SGD) with dropout regularization in linear models. Unlike previous work that focused on fixed designs or marginalized dropout noise, this study establishes the geometric-moment contraction (GMC) property for SGD dropout iterates with random designs and sequential observations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">22:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07794" target="_blank">@arXiv 2409.07794</a>
                    <span class="tweet-title">Learning Balanced Graphs: It's Like Finding the Right Friends!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Osaka University, York University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for learning balanced signed graphs directly from data, unlike previous methods that relied on two-step approaches. The key innovation lies in incorporating linear constraints into a linear programming framework to ensure consistent edge weights based on node polarities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">22:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08212" target="_blank">@arXiv 2409.08212</a>
                    <span class="tweet-title">Robots Learn From Language:  AI Gets a Chatty Upgrade!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Princeton, The AI Institute</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method called ALGAE (Adaptive Language-Guided Abstraction from [Contrastive] Explanations) that uses language models to identify missing features in reward functions for robots. This differs from previous work by leveraging language models to iteratively refine the reward function, rather than relying solely on human demonstrations or hand-crafted features.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">23:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07253" target="_blank">@arXiv 2409.07253</a>
                    <span class="tweet-title">Diffusion Models:  From Dreamy Images to Aligned Art!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Hong Kong University of Science and Technology, Tsinghua University, Mohamed bin Zayed University of Artificial Intelligence</span>
                </div>
                <div class="primary-text">
                    This research focuses on aligning diffusion models with human preferences, a relatively new area compared to aligning large language models. It provides a comprehensive overview of the fundamentals, techniques, benchmarks, and future directions for aligning diffusion models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">23:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07961" target="_blank">@arXiv 2409.07961</a>
                    <span class="tweet-title">Typhoon Forecasting Gets a Diffusion Boost: AI Predicts Wind, Pressure, and More!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Imperial College London, University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research uses a Conditional Denoising Diffusion Probability Model (CDDPM) to predict multiple meteorological variables from satellite images, a novel approach compared to previous work that relied on Convolutional Neural Networks (CNN) or Squeeze-and-Excitation Networks (SENet).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">23:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08169" target="_blank">@arXiv 2409.08169</a>
                    <span class="tweet-title">MR to Ultrasound:  Matching Keypoints with a Synthetic Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard Medical School, Brigham and Women’s Hospital, Massachusetts Institute of Technology...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to matching keypoints between preoperative MR and intraoperative ultrasound images by synthesizing ultrasound images from MR data. This differs from previous methods that rely on matching features directly between the two modalities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">24:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07790" target="_blank">@arXiv 2409.07790</a>
                    <span class="tweet-title">LLMs:  Not Just for Chatbots, They're Speech Correction Superheroes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tencent, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research focuses on using LLMs for full-text error correction in Chinese speech recognition, unlike previous work that primarily focused on correcting single sentences.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">24:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07372" target="_blank">@arXiv 2409.07372</a>
                    <span class="tweet-title">Slides to Life: AI Turns Boring Lectures into Interactive Fun!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel intelligent tutoring system called Slide2Lecture that leverages large language models to convert static lecture slides into interactive learning experiences. Unlike previous systems that focus on specific teaching actions, Slide2Lecture generates a diverse range of actions, including showing files, reading scripts, and asking questions, to create a more dynamic and engaging learning environment.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">25:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08250" target="_blank">@arXiv 2409.08250</a>
                    <span class="tweet-title">Your Photos, Your Memories, Your AI-Powered Storyteller</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Los Angeles, University of Washington, Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces OmniQuery, a system that augments personal memories with contextual information extracted from multiple related memories. Unlike previous work that relies on explicit connections between queries and external data, OmniQuery leverages a taxonomy of contextual information to enhance retrieval on unstructured personal memories.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">25:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06960" target="_blank">@arXiv 2409.06960</a>
                    <span class="tweet-title">Hunting for New Physics: A Data-Driven Approach to Signal Region Selection</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a model-agnostic method for selecting signal regions in high-energy physics, relying on the localized topology of signal events rather than prior domain knowledge.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">26:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07291" target="_blank">@arXiv 2409.07291</a>
                    <span class="tweet-title">Gradient Snooping: How AI Models Can Spill Your Secrets</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Vanderbilt University, University of Wisconsin-Madison, Mitsubishi Electric Research Laboratories...</span>
                </div>
                <div class="primary-text">
                    This research explores a new type of privacy attack in distributed learning, focusing on recovering user-level information rather than individual data samples. Unlike previous methods that rely on reconstructing entire images, this approach utilizes a diffusion model prior to synthesize a representative image that captures the overall semantics of a user's data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">26:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06754" target="_blank">@arXiv 2409.06754</a>
                    <span class="tweet-title">Scaling Up Multimodal Models:  Can More Data Make Them Smaller?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research proposes a scaling law specifically for multimodal models, taking into account the varying compression efficiencies of different data types like text, audio, images, and video. This differs from previous scaling laws that primarily focused on text-based models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">27:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07887" target="_blank">@arXiv 2409.07887</a>
                    <span class="tweet-title">Lidar's Got Talent: Unsupervised Object Tracking Without a Single Label!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">LIGM  Ecole des Ponts  Univ Gustave Eiffel  CNRS  Valeo.ai</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel unsupervised online instance segmentation and tracking method called UNIT. Unlike previous methods that rely on pre-defined time windows or require manual annotations, UNIT operates on single scans in an online fashion, enabling it to track objects over an extended period without any human intervention.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon" title="Play from here">27:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06851" target="_blank">@arXiv 2409.06851</a>
                    <span class="tweet-title">Tired of Benchmarks That Can't Tell the Difference? LIME-M to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">M-A-P, 201.ai, University of Manchester...</span>
                </div>
                <div class="primary-text">
                    This research proposes a pipeline to curate existing multimodal large language model (MLLM) benchmarks by filtering out overly simple or challenging samples, resulting in a more effective and efficient benchmark called LIME-M. This approach differs from previous work by focusing on the quality of evaluation data rather than simply expanding the number of samples.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon" title="Play from here">28:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06730" target="_blank">@arXiv 2409.06730</a>
                    <span class="tweet-title">Cargo Bikes vs. Vans: Who Wins the Urban Delivery Race?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Alabama, University of Basel, MIT...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel framework for modeling delivery performance across diverse urban environments, focusing on service time, a crucial but often overlooked component of delivery time modeling. It leverages geospatial embeddings to represent urban context, enabling more accurate predictions of delivery efficiency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon" title="Play from here">28:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07388" target="_blank">@arXiv 2409.07388</a>
                    <span class="tweet-title">Multimodal Affective Computing:  Emotions in a Multi-Sensory World!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Copenhagen, Nanjing University, Stony Brook University...</span>
                </div>
                <div class="primary-text">
                    This research focuses on multimodal affective computing, specifically examining how different modalities like text, audio, and visual signals can be combined to understand human emotions. It goes beyond previous work by providing a comprehensive overview of the field, highlighting key tasks, technical approaches, and future directions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet67">
            <div class="start-time-icon" title="Play from here">28:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07257" target="_blank">@arXiv 2409.07257</a>
                    <span class="tweet-title">TopoMap++:  Mapping High-Dimensional Data with a Touch of Magic!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">NYU, University of São Paulo, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces TopoMap++, an improved version of the TopoMap algorithm for visualizing high-dimensional data. TopoMap++ addresses the limitations of the original TopoMap by incorporating a more space-efficient layout, a faster implementation, and a novel TreeMap-based representation for exploring the topological hierarchy of the data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet68">
            <div class="start-time-icon" title="Play from here">29:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08012" target="_blank">@arXiv 2409.08012</a>
                    <span class="tweet-title">IRL's Got a New Trick: Learning Rewards That Don't Overfit!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel regularization approach for inverse reinforcement learning (IRL) methods based on the causal invariance principle. Unlike previous work that often leads to overfitting, this method aims to learn reward functions that are robust to spurious correlations in expert demonstrations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet69">
            <div class="start-time-icon" title="Play from here">29:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07776" target="_blank">@arXiv 2409.07776</a>
                    <span class="tweet-title">Spiking Neural Networks Get a Gradient-Free Makeover:  Randomness Rules!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The University of Tokyo, NTT Device Technology Labs.</span>
                </div>
                <div class="primary-text">
                    This research proposes a new training method for spiking neural networks (SNNs) called augmented direct feedback alignment (aDFA). Unlike traditional methods that rely on backpropagation (BP), aDFA uses random projections to update weights, making it more efficient and biologically plausible.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet70">
            <div class="start-time-icon" title="Play from here">30:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06708" target="_blank">@arXiv 2409.06708</a>
                    <span class="tweet-title">AI Fairness Audit:  A Recipe for a Less Biased World</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Academia Sinica</span>
                </div>
                <div class="primary-text">
                    This research proposes a framework for auditing AI fairness, focusing on quantifying bias using statistical metrics. It differs from previous work by offering a transparent, open-source tool for third-party auditors to assess AI systems for fairness violations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet71">
            <div class="start-time-icon" title="Play from here">30:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07753" target="_blank">@arXiv 2409.07753</a>
                    <span class="tweet-title">Robots Get Smart: New Research Gives Them Human-Like 'Relevance' Sense!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel concept called "relevance" for scene understanding in human-robot collaboration (HRC). Unlike previous work focusing on visual saliency and attention mechanisms, this approach considers the broader context, long-term objectives, and human factors to identify relevant elements in a scene.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet72">
            <div class="start-time-icon" title="Play from here">30:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08102" target="_blank">@arXiv 2409.08102</a>
                    <span class="tweet-title">Bayesian Self-Training:  Giving 3D Segmentation a Confidence Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research introduces a Bayesian self-training framework for semi-supervised 3D segmentation, which leverages uncertainty estimation techniques from active learning to generate more reliable pseudo-labels. Unlike previous work that relies on simple confidence thresholding, this approach utilizes both the estimated Shannon entropy and the predicted label from each stochastic forward pass to filter out uncertain or inconsistent points.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet73">
            <div class="start-time-icon" title="Play from here">31:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07002" target="_blank">@arXiv 2409.07002</a>
                    <span class="tweet-title">Fooling Object Detectors with a Doggy Logo: New Patch Attack Uses Diffusion Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Beijing Normal University, Tsinghua University, Northeastern University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel patch attack framework called AdvLogo that leverages diffusion models to generate adversarial patches with high visual quality and strong attack performance. Unlike previous methods that optimize patches in the pixel space, AdvLogo operates in the frequency domain, reducing distribution shifts and improving image quality.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet74">
            <div class="start-time-icon" title="Play from here">31:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06868" target="_blank">@arXiv 2409.06868</a>
                    <span class="tweet-title">Prophet Inequalities: When Rewards Get Correlated, Things Get Complicated!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, Tel Aviv University</span>
                </div>
                <div class="primary-text">
                    This research extends the prophet inequality problem to scenarios where rewards are correlated, unlike previous work that focused on independent rewards. It explores the competition complexity, which measures how many additional copies of the original problem are needed to approximate the prophet's performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet75">
            <div class="start-time-icon" title="Play from here">32:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07965" target="_blank">@arXiv 2409.07965</a>
                    <span class="tweet-title">AVs Learn to Drive by Feeling the Road: Differentiable Simulation Makes Autonomous Vehicles Smarter</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">INSAIT, Sofia University, ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research uses a differentiable simulator to train autonomous vehicle controllers, allowing gradients of the environment dynamics to be incorporated into the training loop. This approach differs from previous methods that treat the environment as a black box, leading to slower and less efficient learning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet76">
            <div class="start-time-icon" title="Play from here">32:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07331" target="_blank">@arXiv 2409.07331</a>
                    <span class="tweet-title">Knowledge-Based VQA:  Cramming Contexts for Faster Answers!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Huawei</span>
                </div>
                <div class="primary-text">
                    This research proposes a new framework called RACC (Retrieval-Augmented MLLM with Compressed Contexts) that focuses on improving the inference efficiency of knowledge-based visual question answering (KB-VQA) systems. Unlike previous works that prioritize using as much knowledge as possible, RACC compresses retrieved contexts into compact representations, reducing the number of input tokens and speeding up the inference process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet77">
            <div class="start-time-icon" title="Play from here">32:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06715" target="_blank">@arXiv 2409.06715</a>
                    <span class="tweet-title">Cell-Free Massive MIMO:  Fronthaul Compression Gets a Neural Network Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">King’s College London, Huawei</span>
                </div>
                <div class="primary-text">
                    This research introduces two new multivariate quantization (MQ) techniques for precode-and-compress (PC) based transmission in cell-free massive MIMO systems. The paper proposes α-parallel MQ (α-PMQ) for low-fronthaul capacity regimes and neural-MQ for high-fronthaul capacity regimes. These techniques aim to improve the scalability of PC methods by reducing computational complexity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet78">
            <div class="start-time-icon" title="Play from here">33:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08239" target="_blank">@arXiv 2409.08239</a>
                    <span class="tweet-title">LLMs Learn New Tricks: Synthetic Data Makes Them Smarter!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta, Oxford University, University College London</span>
                </div>
                <div class="primary-text">
                    This paper introduces Source2Synth, a method for generating synthetic data grounded in real-world sources. Unlike previous work that relies heavily on human annotations, Source2Synth leverages the LLM itself to curate the synthetic data, improving its quality and leading to better performance on complex tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet79">
            <div class="start-time-icon" title="Play from here">33:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08098" target="_blank">@arXiv 2409.08098</a>
                    <span class="tweet-title">AI Judges: Can a Computer Predict Your Employment Tribunal Fate?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research distinguishes itself by creating a large-scale dataset of UK Employment Tribunal cases, annotated with legal information extracted using a large language model (LLM). This dataset, called CLC-UKET, is then used to benchmark the performance of various models in predicting case outcomes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet80">
            <div class="start-time-icon" title="Play from here">34:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06817" target="_blank">@arXiv 2409.06817</a>
                    <span class="tweet-title">Robot Surgeon Finds the Perfect Vein: AI Helps Navigate Bifurcations!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, University of Pittsburgh Medical Center</span>
                </div>
                <div class="primary-text">
                    This research introduces BIFURC, an algorithm that uses deep learning and expert knowledge to identify vessel bifurcations in ultrasound images. Unlike previous work, BIFURC is trained on a limited amount of in-vivo data and can be used for autonomous robotic cannulation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet81">
            <div class="start-time-icon" title="Play from here">34:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07827" target="_blank">@arXiv 2409.07827</a>
                    <span class="tweet-title">Painting the Sounds of Emotion: AI Turns Art into Music</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Queen Mary University of London</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to music generation by leveraging the emotional content of paintings. Unlike previous work that primarily focused on text-to-music generation, this study explores the unique relationship between visual art and music, creating a system that translates emotions depicted in paintings into musical compositions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet82">
            <div class="start-time-icon" title="Play from here">34:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08027" target="_blank">@arXiv 2409.08027</a>
                    <span class="tweet-title">AI Explains Itself:  Social Science Theories Make AI Feedback More Human-Friendly</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">EPFL, IPPR, UCL</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel framework called iLLuMinaTE that uses large language models (LLMs) to translate complex AI explanations into human-understandable feedback for students. Unlike previous work that focused on post-hoc explainers, iLLuMinaTE leverages social science theories to guide the LLM in generating explanations that are more relevant, actionable, and trustworthy for students.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet83">
            <div class="start-time-icon" title="Play from here">35:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07822" target="_blank">@arXiv 2409.07822</a>
                    <span class="tweet-title">Federated Learning Gets a Wireless Weight Loss Plan!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">KTH Royal Institute of Technology, Yale University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new federated learning scheme that uses adaptive weights during aggregation, unlike previous over-the-air schemes that used predefined weights. This approach helps mitigate the impact of wireless channel conditions on learning performance without requiring channel state information at the transmitter side.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet84">
            <div class="start-time-icon" title="Play from here">35:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08097" target="_blank">@arXiv 2409.08097</a>
                    <span class="tweet-title">Falsifying Controllers: A Multi-Fidelity Bayesian Approach to Finding Bugs</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Rochester Institute of Technology, Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces a multi-fidelity Bayesian optimization framework for falsification in learning-based control systems. Unlike previous work that relies solely on high-fidelity simulators, this approach leverages simulators with varying levels of accuracy to improve efficiency and reduce computational costs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet85">
            <div class="start-time-icon" title="Play from here">36:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07653" target="_blank">@arXiv 2409.07653</a>
                    <span class="tweet-title">STAND: Learning Like a Forest, But With Less Data!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University</span>
                </div>
                <div class="primary-text">
                    This research introduces STAND, a method for learning preconditions in interactive task learning settings. Unlike traditional approaches that rely on statistical learning, STAND embraces ambiguity and builds a complete space of possible classifiers, allowing it to learn from small, noisy datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet86">
            <div class="start-time-icon" title="Play from here">36:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07323" target="_blank">@arXiv 2409.07323</a>
                    <span class="tweet-title">Boltzmann Sampling Gets a Speed Boost: Consistency Models to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel sampling method that combines Consistency Models (CMs) with importance sampling to accelerate the process of generating samples from Boltzmann distributions. Unlike previous approaches that address bias and speed separately, this method integrates both aspects, leading to more efficient and unbiased sampling.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet87">
            <div class="start-time-icon" title="Play from here">36:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06729" target="_blank">@arXiv 2409.06729</a>
                    <span class="tweet-title">AI: Democracy's New BFF or Worst Nightmare?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford, Brigham Young University, Collective Intelligence Project...</span>
                </div>
                <div class="primary-text">
                    This research goes beyond the usual "AI is bad for democracy" narrative by exploring both the potential threats and opportunities that advanced AI systems, like LLMs, present for democratic processes. It focuses on three key impact categories: epistemic, material, and foundational.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet88">
            <div class="start-time-icon" title="Play from here">37:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07190" target="_blank">@arXiv 2409.07190</a>
                    <span class="tweet-title">Multi-Fidelity Bayesian Optimization:  A Chemical Discovery Speed Demon?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Imperial College London, École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research investigates the application of multi-fidelity Bayesian optimization (MFBO) to accelerate chemical discovery. The paper specifically analyzes the conditions under which lower-fidelity data can enhance performance compared to single-fidelity problem formulations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet89">
            <div class="start-time-icon" title="Play from here">37:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08145" target="_blank">@arXiv 2409.08145</a>
                    <span class="tweet-title">Learning Speed: The Key to Coordination Success or Failure?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This paper introduces a dynamic coordination game where the state evolves based on past play, allowing for a more realistic representation of economic phenomena like currency crises or networked product adoption. The key innovation is the tight characterization of how the speed of learning shapes equilibrium dynamics, showing that risk-dominant action is selected only when learning is slow enough.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet90">
            <div class="start-time-icon" title="Play from here">38:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08058" target="_blank">@arXiv 2409.08058</a>
                    <span class="tweet-title">Electrode Shift? No Problem! New Layer Makes Biosignal Sensors Smarter</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Imperial College London, Meta Reality Labs</span>
                </div>
                <div class="primary-text">
                    This research introduces a Spatial Adaptation Layer (SAL) that can be added to any biosignal array model. Unlike previous methods, SAL learns a parametrized affine transformation at the input between two recording sessions, making it more interpretable and efficient.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet91">
            <div class="start-time-icon" title="Play from here">38:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07715" target="_blank">@arXiv 2409.07715</a>
                    <span class="tweet-title">Seeing Through Smoke: New Dataset Helps Robots Navigate Wildfires</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset, FIReStereo, specifically designed for training depth perception algorithms in visually degraded environments, particularly those involving smoke. Unlike previous thermal datasets, FIReStereo includes stereo thermal image pairs, LiDAR, IMU data, and ground truth depth maps, making it suitable for training stereo depth estimation models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet92">
            <div class="start-time-icon" title="Play from here">38:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07746" target="_blank">@arXiv 2409.07746</a>
                    <span class="tweet-title">Brain Tumor Detectives:  State-Space Models Crack the Code on High-Resolution Images!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stony Brook University, Harvard University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel state-space model (SSM) based masked autoencoder for 3D multi-contrast MR images. Unlike traditional Vision Transformers (ViTs), this approach scales linearly to high resolution, enabling efficient processing of large volumetric data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet93">
            <div class="start-time-icon" title="Play from here">39:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06716" target="_blank">@arXiv 2409.06716</a>
                    <span class="tweet-title">Fetal Brain Mapping: Deep Learning Takes the Wheel!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Boston Children’s Hospital, Harvard Medical School, Elmhurst Hospital Center...</span>
                </div>
                <div class="primary-text">
                    This research focuses on segmenting fetal brain structures directly from diffusion-weighted MRI (dMRI) data, a novel approach compared to previous work that primarily used structural MRI.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet94">
            <div class="start-time-icon" title="Play from here">39:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08211" target="_blank">@arXiv 2409.08211</a>
                    <span class="tweet-title">Graph Laplacians: The Secret Sauce for Multi-Fidelity Modeling</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Southern California, University of Birmingham, Caltech</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel multi-fidelity modeling approach that leverages the spectral properties of the graph Laplacian constructed from low-fidelity data to define a prior distribution for the multi-fidelity estimates. This approach differs from previous work by explicitly incorporating a probabilistic framework and utilizing Bayesian inference to derive an optimal multi-fidelity estimate.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet95">
            <div class="start-time-icon" title="Play from here">39:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07985" target="_blank">@arXiv 2409.07985</a>
                    <span class="tweet-title">AI Safety Games:  Red Teaming Gets a Formal Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This paper introduces a formal game-theoretic model called "AI-Control Games" to evaluate the safety of AI deployment protocols. Unlike previous work that relies on empirical studies, this model allows for a more rigorous and systematic analysis of protocol safety and usefulness.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet96">
            <div class="start-time-icon" title="Play from here">40:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07307" target="_blank">@arXiv 2409.07307</a>
                    <span class="tweet-title">Saliency Supercharged:  How AI Learned to Make Images More Eye-Catching</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel data augmentation method for saliency prediction models. Unlike traditional techniques that alter scene composition, this approach leverages a latent diffusion model to edit images while preserving the integrity of the scene. It focuses on manipulating photometric properties like color, contrast, and brightness to control the saliency of specific regions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet97">
            <div class="start-time-icon" title="Play from here">40:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08229" target="_blank">@arXiv 2409.08229</a>
                    <span class="tweet-title">Photonic Quantum Computers:  Shining a Light on the Future of Computing!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Ain Shams University, Zewail City of Science  Technology and Innovation, USTC...</span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive overview of advancements in photonic quantum computing, examining current performance, architectural designs, and strategies for developing large-scale, fault-tolerant photonic quantum computers. It highlights recent groundbreaking experiments that leverage the unique advantages of photonic technologies, underscoring their transformative potential.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet98">
            <div class="start-time-icon" title="Play from here">41:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07215" target="_blank">@arXiv 2409.07215</a>
                    <span class="tweet-title">Data Merging:  Is It Worth the Headache?  A Secure Way to Know!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach for evaluating the value of merging datasets in the context of causal inference. Unlike previous work that focuses on general information gain, this method specifically targets the reduction of uncertainty in parameters directly related to the causal effect.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet99">
            <div class="start-time-icon" title="Play from here">41:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07679" target="_blank">@arXiv 2409.07679</a>
                    <span class="tweet-title">Boltzmann Machines Get a Ratio-nal Makeover: New Learning Method Outperforms the Old Guard!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Fujitsu, University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research introduces a new learning method called "ratio divergence" (RD) for discrete energy-based models. Unlike previous methods that rely solely on forward or reverse Kullback-Leibler divergence (KLD), RD combines the strengths of both approaches, addressing underfitting and mode collapse issues.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet100">
            <div class="start-time-icon" title="Play from here">42:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07265" target="_blank">@arXiv 2409.07265</a>
                    <span class="tweet-title">TTS with a Twist:  Teaching AI to Speak Different Dialects</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research focuses on cross-dialect text-to-speech (CD-TTS), specifically for languages with pitch-accent systems. Unlike previous work that relied on accent dictionaries, this paper proposes a novel TTS model that automatically predicts accent latent variables (ALVs) tailored to each dialect using a multi-dialect phoneme-level BERT (MD-PL-BERT).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet101">
            <div class="start-time-icon" title="Play from here">42:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07918" target="_blank">@arXiv 2409.07918</a>
                    <span class="tweet-title">AI Makes Music Feel: New System Generates Emotionally Charged Tunes</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Queen Mary University of London, Vrije Universiteit Brussel</span>
                </div>
                <div class="primary-text">
                    This research combines affective modeling with computational code generation, using reinforcement learning to create music that aligns with specific emotional states. Previous work focused on either affective modeling or code generation, but not both.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet102">
            <div class="start-time-icon" title="Play from here">43:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08036" target="_blank">@arXiv 2409.08036</a>
                    <span class="tweet-title">Sheaves to the Rescue:  New Neural Networks Tame Heterogeneous Data</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research proposes using cellular sheaves to model heterogeneous graphs, a departure from previous approaches that focused on modifying model architectures to handle data heterogeneity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet103">
            <div class="start-time-icon" title="Play from here">43:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06953" target="_blank">@arXiv 2409.06953</a>
                    <span class="tweet-title">Neural Networks Learn to Think Like Algorithms, But Now They Can Find Multiple Solutions!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Northeastern University, University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for Neural Algorithmic Reasoning (NAR) that allows neural networks to find multiple correct solutions to a problem, unlike previous methods that only returned a single solution.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet104">
            <div class="start-time-icon" title="Play from here">44:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07615" target="_blank">@arXiv 2409.07615</a>
                    <span class="tweet-title">AI Text Detection:  A Team Effort, Not a Lone Wolf!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sorbonne University, Montreal Institute for Learning Algorithms</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for detecting machine-generated text that uses an ensemble of large language models (LLMs) instead of relying on a single detector. This approach aims to improve the robustness of detection by combining the strengths of multiple models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet105">
            <div class="start-time-icon" title="Play from here">44:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08232" target="_blank">@arXiv 2409.08232</a>
                    <span class="tweet-title">Brain Tumor Segmentation:  Ensemble of Models Wins the Day!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Children's National Hospital, Universidad Politécnica de Madrid, Princeton University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel ensemble approach for brain tumor segmentation, combining the strengths of two state-of-the-art deep learning models, nnU-Net and SwinUNETR, to improve accuracy and robustness. This differs from previous work by focusing on a label-wise ensemble strategy, tailoring the approach to specific tumor sub-regions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet106">
            <div class="start-time-icon" title="Play from here">44:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07656" target="_blank">@arXiv 2409.07656</a>
                    <span class="tweet-title">Turing Test Passed: AI's Got Brains (and They're Learning!)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of S˜ao Paulo, King’s College  University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research argues that generative transformers, a type of AI, can pass the Turing test by learning from experience, unlike earlier AI systems that relied on preprogramming.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet107">
            <div class="start-time-icon" title="Play from here">45:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07638" target="_blank">@arXiv 2409.07638</a>
                    <span class="tweet-title">LLMs Can't Count?  The Fixed-Effect Fallacy Strikes Again!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MicrosoftResearch</span>
                </div>
                <div class="primary-text">
                    This paper investigates the performance of GPT-4 on deterministic tasks, such as counting and multiplication, and finds that seemingly trivial changes in the prompt or input data can significantly impact accuracy. This differs from previous work by focusing on the sensitivity of LLMs to these variations, rather than just reporting overall performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet108">
            <div class="start-time-icon" title="Play from here">45:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07671" target="_blank">@arXiv 2409.07671</a>
                    <span class="tweet-title">Neural Networks:  A New Trick for Solving Tricky Equations</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Maryland</span>
                </div>
                <div class="primary-text">
                    This research explores using Physics-Informed Neural Networks (PINNs) to correct existing solutions to convection-diffusion equations, rather than directly approximating the solution. The paper also investigates the impact of input transformations on PINN performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet109">
            <div class="start-time-icon" title="Play from here">45:58</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07960" target="_blank">@arXiv 2409.07960</a>
                    <span class="tweet-title">Vision Models:  A Doctor's New Best Friend for Image Segmentation?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research investigates the effectiveness of foundational models (FMs) for medical image segmentation, specifically focusing on their ability to generalize to data from different sources, a crucial challenge in medical imaging. The study compares various FMs, fine-tuning techniques, and decoder architectures, highlighting the potential of FMs to enhance domain generalization performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet110">
            <div class="start-time-icon" title="Play from here">46:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07337" target="_blank">@arXiv 2409.07337</a>
                    <span class="tweet-title">Hand Pose Datasets: A Deep Dive into the 2D Egocentric World</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel protocol for evaluating egocentric datasets specifically for 2D hand pose estimation, going beyond just analyzing stated characteristics. It also assesses data quality by evaluating the performance of state-of-the-art hand pose estimation models on these datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet111">
            <div class="start-time-icon" title="Play from here">46:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.06791" target="_blank">@arXiv 2409.06791</a>
                    <span class="tweet-title">Motion Stitching:  Diffusion Models Make Human Movement Look Real!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research proposes a diffusion model with a transformer-based denoiser to generate realistic human motion, specifically addressing the challenge of motion stitching and in-betweening. Unlike previous methods that require manual efforts or struggle with longer sequences, this approach can handle variable input poses and generate smooth, realistic motion sequences.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet112">
            <div class="start-time-icon" title="Play from here">47:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08171" target="_blank">@arXiv 2409.08171</a>
                    <span class="tweet-title">Drones + Deep Learning = Dieback Detective!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research uses deep learning to automatically segment individual tree crowns from drone imagery, then uses vegetation indices to estimate dieback severity. Unlike previous work, this study validates its results with expert field-based dieback measurements.
                </div>
            </div>
        </div>

        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202409131426_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>    
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>
</html>