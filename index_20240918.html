<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                    Fresh Picks:
                    <span class="highlightNumber">62</span> out of <span class="highlightNumber">282</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-09-18"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>


        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">01:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11363" target="_blank">@arXiv
                        2409.11363</a>
                    <span class="tweet-title">AI Agents: Can They Even Replicate a Science Paper?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Princeton University</span>
                </div>
                <div class="primary-text">
                    This research introduces CORE-Bench, a benchmark specifically designed to evaluate the ability
                    of AI
                    agents to reproduce the results of published scientific research. Unlike previous benchmarks
                    that
                    focus on coding tasks or specific scientific domains, CORE-Bench tackles the broader challenge
                    of
                    computational reproducibility across multiple disciplines.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">01:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11321" target="_blank">@arXiv
                        2409.11321</a>
                    <span class="tweet-title">Shampoo's New Trick: Adam Gets a Spin in the Eigenbasis!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This research establishes a formal connection between Shampoo and Adafactor, showing that
                    Shampoo is
                    equivalent to running Adafactor in the eigenbasis of Shampoo's preconditioner. This insight
                    leads to
                    the design of a simpler and computationally efficient algorithm called SOAP.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">01:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11402" target="_blank">@arXiv
                        2409.11402</a>
                    <span class="tweet-title">NVLM: Multimodal LLMs That Can Do Math...and Even Improve Their Text
                        Skills!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nvidia</span>
                </div>
                <div class="primary-text">
                    This research introduces NVLM, a family of multimodal LLMs that achieve state-of-the-art results
                    on
                    vision-language tasks. Unlike previous open-access models, NVLM models maintain or even improve
                    their text-only performance after multimodal training. This is achieved by incorporating a
                    high-quality text-only dataset into the multimodal fine-tuning stage.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">02:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10683" target="_blank">@arXiv
                        2409.10683</a>
                    <span class="tweet-title">Robots Need Therapy Too: New Study Shows How to Teach AI to Judge
                        Motion,
                        Not Just Goals</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Stanford University, CMU</span>
                </div>
                <div class="primary-text">
                    This research focuses on teaching AI to evaluate the "how" of robot actions, not just the
                    "what." It
                    does this by fine-tuning vision-language models (VLMs) using abstract representations of robot
                    trajectories overlaid on images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">02:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10897" target="_blank">@arXiv
                        2409.10897</a>
                    <span class="tweet-title">Neural Network Specs: No More Guesswork, Just AutoSpec!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Michigan, Microsoft Research, UIUC...</span>
                </div>
                <div class="primary-text">
                    This research introduces AutoSpec, a framework that automatically generates specifications for
                    neural networks, unlike previous methods that rely on manual definition.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">03:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10580" target="_blank">@arXiv
                        2409.10580</a>
                    <span class="tweet-title">Foundation Models in Medicine: A Reality Check for Veridical Data
                        Science</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of California Berkeley, University of California San
                        Francisco</span>
                </div>
                <div class="primary-text">
                    This research examines the application of foundation models (FMs) in medicine, specifically
                    focusing
                    on how the foundation model lifecycle (FMLC) deviates from the traditional data science
                    lifecycle
                    (DSLC). It proposes a framework for evaluating the predictability, computability, and stability
                    (PCS) of FMs, addressing concerns about transparency, reproducibility, and rigor in FM-based
                    data
                    science.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">03:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10917" target="_blank">@arXiv
                        2409.10917</a>
                    <span class="tweet-title">Remembering Your Day: A New Memory for Egocentric Videos</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Politecnico di Torino, FAIR, University of Bristol</span>
                </div>
                <div class="primary-text">
                    This research introduces AMEGO, a structured representation of egocentric videos that captures
                    object interactions, locations visited, and their interplay. Unlike previous approaches that
                    rely on
                    uniform sampling of frames or semantic labels, AMEGO builds its representation online using
                    visual
                    perception models of human activity and motion.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">03:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10829" target="_blank">@arXiv
                        2409.10829</a>
                    <span class="tweet-title">Radiology Reports: AI's Got Errors, But We've Got ReXErr!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Pennsylvania, Stanford University, Harvard
                        University</span>
                </div>
                <div class="primary-text">
                    This research introduces ReXErr, a novel methodology that uses large language models (LLMs) to
                    generate representative errors within chest X-ray reports. Unlike previous work that focuses on
                    specific error types, ReXErr aims to capture the breadth and diversity of errors made by both
                    humans
                    and AI models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">04:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11169" target="_blank">@arXiv
                        2409.11169</a>
                    <span class="tweet-title">AI Makes Fake CT Scans So Real, Doctors Might Be Fooled!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">NVIDIA, National Institutes of Health, University of Arkansas for
                        Medical Sciences...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method called MAISI, which uses a diffusion model to generate
                    high-resolution 3D CT images. Unlike previous methods that focus on 2D images or smaller
                    volumes,
                    MAISI can create realistic CT volumes with dimensions up to 512x512x768 voxels.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">04:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11211" target="_blank">@arXiv
                        2409.11211</a>
                    <span class="tweet-title">SplatFields: Giving 3D Reconstruction a Spatial Spank!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, MetaRealityLabs, Balgrist University Hospital</span>
                </div>
                <div class="primary-text">
                    This research introduces SplatFields, a novel optimization strategy that regularizes 3D Gaussian
                    Splatting (3DGS) by modeling splat features as outputs of a corresponding implicit neural field.
                    This approach effectively handles both static and dynamic scenes, improving reconstruction
                    quality
                    compared to previous 3DGS techniques.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">05:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10909" target="_blank">@arXiv
                        2409.10909</a>
                    <span class="tweet-title">Query Reformulation Gets a Makeover: LLMs Learn to Cluster and
                        Conquer!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Baidu, Wilfrid Laurier University</span>
                </div>
                <div class="primary-text">
                    This research proposes GenCRF, a framework that uses LLMs to generate multiple reformulated
                    queries
                    from a single user input. Unlike previous methods that rely on single prompts or keyword
                    generation,
                    GenCRF dynamically clusters these queries to capture diverse user intents, minimizing redundancy
                    and
                    maximizing the potential of query reformulation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">05:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11274" target="_blank">@arXiv
                        2409.11274</a>
                    <span class="tweet-title">Speech Translation Gets a Language Makeover: Task Arithmetic to the
                        Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, Sony, CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for expanding language pairs in speech translation systems
                    using task arithmetic. Unlike previous work that relies on retraining models with new datasets,
                    this
                    approach merges existing models trained on different language pairs without retraining,
                    significantly reducing costs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">05:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10901" target="_blank">@arXiv
                        2409.10901</a>
                    <span class="tweet-title">"Predicting the Future: How AI Can See What's Coming in Self-Driving
                        Cars"</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research introduces TrajSSL, a new approach to semi-supervised 3D object detection that
                    leverages trajectory prediction models to improve the quality of pseudo-labels used during
                    training.
                    Unlike previous methods that rely solely on confidence scores or consistency measures, TrajSSL
                    incorporates temporal information by predicting future object trajectories, which helps identify
                    and
                    suppress false positive pseudo-labels while compensating for missed detections.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">06:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10566" target="_blank">@arXiv
                        2409.10566</a>
                    <span class="tweet-title">AI Models: Not All Heroes Wear Capes, Some Wear... Benchmarks?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces EUREKA, an open-source evaluation framework for large foundation models
                    (LFMs). EUREKA goes beyond single-score reporting and rankings, providing a library for
                    customizing
                    evaluation pipelines and an extensible collection of benchmarks that test challenging
                    capabilities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">06:58</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10525" target="_blank">@arXiv
                        2409.10525</a>
                    <span class="tweet-title">AI Assistants: Are They Really Listening?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft, University of Michigan</span>
                </div>
                <div class="primary-text">
                    This research proposes a new approach to building benchmarks for evaluating large multimodal
                    models
                    (LMMs) in situated collaboration tasks. Unlike existing benchmarks that generate questions
                    post-hoc,
                    this study advocates for an interactive system-driven approach where questions are collected
                    during
                    real-time interactions with an AI system.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">07:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10555" target="_blank">@arXiv
                        2409.10555</a>
                    <span class="tweet-title">Deep Learning Gets a Makeover: Tiny Models, Big Results!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research explores the use of a "prompting module" to adapt pre-trained deep networks to new
                    tasks, specifically video object segmentation. Unlike traditional end-to-end training, this
                    approach
                    uses a simple, semi-parametric model that learns from only the first frame of a video, making it
                    extremely efficient and requiring no further training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">07:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11353" target="_blank">@arXiv
                        2409.11353</a>
                    <span class="tweet-title">LLMs Gone Wild? New Tool Tames Hallucinations!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research introduces THaMES, a framework that goes beyond just detecting hallucinations in
                    LLMs.
                    It also includes a process for generating diverse test sets and evaluating different mitigation
                    strategies. This comprehensive approach sets it apart from previous work that often focused on
                    isolated aspects of the problem.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">08:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10538" target="_blank">@arXiv
                        2409.10538</a>
                    <span class="tweet-title">Survival Analysis Gets a Fairness Makeover: DRO to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Purdue University, CMU</span>
                </div>
                <div class="primary-text">
                    This research applies distributionally robust optimization (DRO) to survival analysis models,
                    addressing the challenge of fairness in predicting time-to-event outcomes. Unlike previous work
                    that
                    relies on user-specified sensitive attributes, this approach minimizes worst-case errors across
                    all
                    "large enough" subpopulations, without requiring the user to explicitly define sensitive
                    features.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">08:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10728" target="_blank">@arXiv
                        2409.10728</a>
                    <span class="tweet-title">Predicting Words: Beyond Surprisal, a New Framework for Language
                        Processing</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research introduces a generalized framework for measuring predictive uncertainty in online
                    language processing. It goes beyond the traditional measures of surprisal and entropy by
                    offering a
                    flexible way to define new, more expressive measures.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">09:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10542" target="_blank">@arXiv
                        2409.10542</a>
                    <span class="tweet-title">SAM4MLLM: Talking to AI About Pixels, One Point at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National Taiwan University, Nvidia</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach that integrates the Segment Anything Model (SAM) with
                    Multi-Modal Large Language Models (MLLMs) for pixel-aware tasks. Unlike previous methods that
                    require extensive model modifications or specialized tokens, SAM4MLLM leverages SAM's ability to
                    generate high-quality segmentation masks based on simple prompts, enabling MLLMs to understand
                    pixel-level information without altering their architecture.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">09:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10889" target="_blank">@arXiv
                        2409.10889</a>
                    <span class="tweet-title">Deepfake-Busting Phone Vibrations: Shaking the Fake in Real
                        Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanyang Technological University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel real-time deepfake detection method called SFake that actively
                    introduces controllable features into video footage by inducing vibrations on the attacker's
                    smartphone. Unlike existing passive detection methods that rely on learning features from
                    deepfake
                    videos, SFake actively manipulates the video stream, making it harder for deepfake algorithms to
                    adapt and bypass detection.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">09:58</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10940" target="_blank">@arXiv
                        2409.10940</a>
                    <span class="tweet-title">RoadRunner M&M: Off-Road Navigation Gets a Multi-Range
                        Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Caltech, JPL, ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research builds upon the RoadRunner framework by introducing a multi-range,
                    multi-resolution
                    approach to predict traversability and elevation maps. Unlike previous work, this method
                    leverages a
                    hierarchical decoder and LiDAR voxel map input to achieve more accurate predictions at longer
                    ranges.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">10:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11378" target="_blank">@arXiv
                        2409.11378</a>
                    <span class="tweet-title">Data Diversity: The Secret Weapon for Fine-Tuning LLMs</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Northeastern University, Stanford University, Google...</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to selecting instruction data for fine-tuning LLMs by
                    prioritizing diversity over individual instance quality. Unlike previous methods that focus on
                    local
                    criteria, this work utilizes k-means clustering to ensure the selected subset effectively
                    represents
                    the full dataset.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">10:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10585" target="_blank">@arXiv
                        2409.10585</a>
                    <span class="tweet-title">Ensemble of Models: Predicting the Future of Self-Driving Cars with a
                        Little Help from Their Friends</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Bosch Center for Artificial Intelligence, University of
                        Freiburg</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel sampling method for trajectory prediction that leverages an
                    ensemble
                    of models, unlike previous work that primarily focused on single models or ensembles of the same
                    model. The method frames the problem as a risk minimization problem, where the ensemble
                    approximates
                    the true risk.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">11:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10715" target="_blank">@arXiv
                        2409.10715</a>
                    <span class="tweet-title">Transformers Have a Memory Problem: Attention Spreads Too Thin!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This research investigates the working memory capacity limits of Transformer-based language
                    models
                    by analyzing the self-attention mechanism. Unlike previous studies that focused on performance,
                    this
                    paper delves into the underlying mechanism responsible for the observed capacity limits.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">11:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11307" target="_blank">@arXiv
                        2409.11307</a>
                    <span class="tweet-title">3D Scene Rendering Gets a Density Boost: GS-Net Makes Gaussian
                        Splatting
                        More Generalizable</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces GS-Net, a plug-and-play module that enhances 3D Gaussian Splatting
                    (3DGS)
                    by generating denser Gaussian ellipsoids from sparse point clouds. Unlike previous 3DGS models
                    that
                    overfit to single scenes, GS-Net enables cross-scene generalization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">11:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10849" target="_blank">@arXiv
                        2409.10849</a>
                    <span class="tweet-title">Robots with Mind-Reading Skills: New AI Can Understand Noisy
                        Instructions!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Harvard University, Brown University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new model called SIFToM that uses Theory of Mind reasoning to
                    understand
                    noisy or unclear speech instructions. Unlike previous work that relies solely on speech
                    recognition,
                    SIFToM infers the human's goal and plan from visual observations, making it more robust to
                    errors in
                    speech transcription.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">12:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11295" target="_blank">@arXiv
                        2409.11295</a>
                    <span class="tweet-title">Web Agents: New Attack Injects Malicious Content to Steal Your
                        Data!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The Ohio State University, University of Chicago, University of
                        Wisconsin Madison...</span>
                </div>
                <div class="primary-text">
                    This research focuses on a novel attack method called Environmental Injection Attack (EIA) that
                    targets generalist web agents by injecting malicious content into the web environment. Unlike
                    previous work that focused on direct prompt injection or manipulating uploaded images, EIA
                    manipulates the environment where the agent operates, making it a more stealthy and realistic
                    threat.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">12:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10999" target="_blank">@arXiv
                        2409.10999</a>
                    <span class="tweet-title">Thai-ing the Knot: Audio Language Models Learn to Speak Thai!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research explores the limitations of existing audio language models in low-resource
                    languages,
                    using Thai as a case study. It proposes a novel data mixture approach to enhance the model's
                    ability
                    to understand and respond to instructions in both English and Thai.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">13:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10693" target="_blank">@arXiv
                        2409.10693</a>
                    <span class="tweet-title">Traffic Lights Get Smart: Transformers Take the Wheel!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research integrates Transformers into the eMARLIN architecture, a distributed adaptive
                    traffic
                    signal control system, to address the challenge of partial observability. Unlike previous
                    approaches
                    that used LSTMs, this method leverages attention mechanisms for parallel computations, improving
                    efficiency and capturing temporal information more effectively.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">13:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10908" target="_blank">@arXiv
                        2409.10908</a>
                    <span class="tweet-title">Clustering with Subset Queries: Breaking the Quadratic Barrier!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UCSanDiego, University of Michigan</span>
                </div>
                <div class="primary-text">
                    This research explores a generalization of pair-wise queries to subset queries for clustering,
                    where
                    the oracle returns the number of clusters intersecting a given subset. This approach allows for
                    significantly better non-adaptive algorithms compared to previous work that relied solely on
                    pair-wise queries.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">14:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10584" target="_blank">@arXiv
                        2409.10584</a>
                    <span class="tweet-title">Drug Design Gets a Quantum Boost: New Model Avoids Atomic
                        Collisions!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">California Institute of Technology, University of California
                        Berkeley,
                        Alibaba DAMO Academy...</span>
                </div>
                <div class="primary-text">
                    This research introduces NucleusDiff, a diffusion model for structure-based drug design that
                    incorporates a manifold constraint to prevent atoms from getting too close to each other. This
                    is
                    different from previous models that often overlook this crucial physical constraint.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">14:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11091" target="_blank">@arXiv
                        2409.11091</a>
                    <span class="tweet-title">Bidding Blind: How to Win Auctions with Just a Glimpse of the
                        Competition</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google, University of Bonn, Microsoft...</span>
                </div>
                <div class="primary-text">
                    This research explores online combinatorial auctions where bidders' valuations are unknown, only
                    accessible through a limited number of samples. It introduces novel algorithms that achieve
                    constant-factor approximations using a single sample per bidder, a significant improvement over
                    previous work that required more samples or stringent assumptions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">14:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10593" target="_blank">@arXiv
                        2409.10593</a>
                    <span class="tweet-title">Shrinking LLMs: A New Trick to Make Big Models Fit in Small
                        Spaces</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Infinigence-AI, Shanghai Jiao Tong
                        University</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new technique called CSKV for compressing the KV cache in LLMs. Unlike
                    previous methods that focus on quantization or token pruning, CSKV leverages the redundancy in
                    the
                    channel dimension of the KV cache by using low-rank decomposition.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">15:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10790" target="_blank">@arXiv
                        2409.10790</a>
                    <span class="tweet-title">LLMs Get a New Pair of Glasses: AutoPASTA Helps Them See the Big
                        Picture!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Georgia Institute of Technology, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces AutoPASTA, a method that automatically identifies key information in a
                    text
                    and highlights it for LLMs, improving their ability to understand and respond to complex
                    questions.
                    Unlike previous methods that rely on prompting, AutoPASTA uses attention steering to explicitly
                    guide the model's focus.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">15:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10695" target="_blank">@arXiv
                        2409.10695</a>
                    <span class="tweet-title">Playground v3: LLMs Take the Stage for Text-to-Image
                        Generation!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Playground Research</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel text-to-image model that fully integrates a large language
                    model
                    (LLM) into its architecture, unlike previous models that rely on pre-trained language models
                    like T5
                    or CLIP.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">16:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10716" target="_blank">@arXiv
                        2409.10716</a>
                    <span class="tweet-title">Forget Re-training! This AI Learns New Objects Like You Do</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This paper proposes an online learning framework for object detectors that uses a memory bank to
                    adapt to new domains without retraining the detector model. This differs from previous work that
                    typically requires extensive retraining or fine-tuning on target domain data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">16:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10969" target="_blank">@arXiv
                        2409.10969</a>
                    <span class="tweet-title">LLMs Learn to Speak Fluent Code-Switching: A New Recipe for
                        Multilingual
                        Speech!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Chinese University of Hong Kong, Huawei</span>
                </div>
                <div class="primary-text">
                    This research explores a novel approach to code-switching speech synthesis by constructing a
                    dataset
                    from concatenated words in different languages. This differs from previous work that relied
                    heavily
                    on high-quality code-switched data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">16:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11235" target="_blank">@arXiv
                        2409.11235</a>
                    <span class="tweet-title">Open-Vocabulary Tracking: When Semantics Meet Motion</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, INSAIT</span>
                </div>
                <div class="primary-text">
                    This research introduces SLAck, a framework that integrates semantic, location, and appearance
                    cues
                    for open-vocabulary multiple object tracking (MOT). Unlike previous methods that rely on
                    late-stage
                    fusion of these cues, SLAck incorporates them early in the association process, leading to
                    improved
                    performance, especially for novel object classes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">17:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10702" target="_blank">@arXiv
                        2409.10702</a>
                    <span class="tweet-title">AI Annotators Get a Helping Hand (and a Faster Job) with LLMs!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta</span>
                </div>
                <div class="primary-text">
                    This research introduces the Model-in-the-Loop (MILO) framework, which integrates AI/ML models
                    into
                    the data annotation process. Unlike previous work that focuses on comparing models to human
                    annotators, MILO emphasizes collaboration between humans and models, leveraging their strengths
                    to
                    improve efficiency and quality.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">17:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10803" target="_blank">@arXiv
                        2409.10803</a>
                    <span class="tweet-title">Quantum Computing: Not Just for Star Trek Anymore!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CSIRO, Songshan Lake Materials Laboratory, Peking
                        University...</span>
                </div>
                <div class="primary-text">
                    This research pioneers the use of quantum machine learning (QML) for modeling the Ohmic contact
                    process in GaN high-electron-mobility transistors (HEMTs), a key process in semiconductor
                    fabrication. Previous work has primarily focused on classical machine learning (CML) methods for
                    this purpose.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">18:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10870" target="_blank">@arXiv
                        2409.10870</a>
                    <span class="tweet-title">LLMs Get a Shortcut: Attention Bypasses Layers for Faster
                        Learning!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to large language model (LLM) architecture by
                    incorporating "attention shortcuts." Instead of processing information sequentially through
                    layers,
                    the final layer can directly attend to intermediate layers, allowing it to learn complex
                    dependencies more efficiently. This differs from previous work that focuses on stacking layers
                    without such adaptive connections.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">18:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10536" target="_blank">@arXiv
                        2409.10536</a>
                    <span class="tweet-title">AI Safety: Building a Nuclear-Style Watchdog for the Digital
                        Age?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The Alan Turing Institute</span>
                </div>
                <div class="primary-text">
                    This research analyzes the potential functions of an international institution for AI safety by
                    drawing parallels to the International Atomic Energy Agency (IAEA) and the International Panel
                    on
                    Climate Change (IPCC). It goes beyond simply comparing these models to identify concrete
                    functions
                    that an international AI safety institution could perform.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">18:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10739" target="_blank">@arXiv
                        2409.10739</a>
                    <span class="tweet-title">Quantum Evolution: Evolving QAOA on Distributed QPUs</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Science and Technology Facilities Council, IBM</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to optimizing Quantum Approximate Optimization
                    Algorithms
                    (QAOA) by combining a multi-population Evolutionary Algorithm (EA) with QAOA and distributing it
                    across two Quantum Processing Units (QPUs). This differs from previous work that primarily
                    relies on
                    gradient-based optimization methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">19:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10587" target="_blank">@arXiv
                        2409.10587</a>
                    <span class="tweet-title">Soccer's Got Talent: New Challenges for AI to Spot Fouls, Track
                        Players,
                        and Even Write Commentary!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Liege, King Abdullah University of Science and
                        Technology, Sportradar...</span>
                </div>
                <div class="primary-text">
                    This research introduces novel tasks for computer vision in soccer, including multi-view foul
                    recognition and game state reconstruction, which involve analyzing multiple camera angles to
                    determine foul severity and reconstructing the game state from broadcast videos onto a 2D
                    top-view
                    map.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">19:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10568" target="_blank">@arXiv
                        2409.10568</a>
                    <span class="tweet-title">LLMs: From Chatbots to Pandemic Planners?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces AgentTorch, a framework that scales agent-based models (ABMs) to
                    millions
                    of agents while incorporating large language models (LLMs) as agents. This approach allows for
                    more
                    realistic and adaptive agent behavior in ABMs, which is a significant improvement over previous
                    methods that relied on simpler heuristics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">20:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10876" target="_blank">@arXiv
                        2409.10876</a>
                    <span class="tweet-title">Neural Fields: Seeing Through the Fog of Sound in Medical
                        Imaging</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Northwestern University, Caltech, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for photoacoustic computed tomography (PACT) image
                    reconstruction that uses neural fields to estimate the speed of sound (SOS) in tissue. This
                    approach
                    differs from previous methods by directly learning the SOS distribution from the measured
                    signals,
                    rather than assuming a constant SOS or requiring additional measurements.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">20:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11227" target="_blank">@arXiv
                        2409.11227</a>
                    <span class="tweet-title">Remote Sensing Gets a Few-Shot Makeover: A New Benchmark for Land
                        Cover
                        Mapping</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">RIKEN, University of Tokyo, Ontario Tech University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark dataset for generalized few-shot semantic segmentation
                    in
                    remote sensing, specifically focusing on submeter-level land cover mapping. It extends the
                    OpenEarthMap dataset with 15 fine-grained classes and provides a more realistic evaluation
                    setting
                    that considers both base and novel classes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">21:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11032" target="_blank">@arXiv
                        2409.11032</a>
                    <span class="tweet-title">AI Opinions: Unraveling the Hidden Structure of Public Comments</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The University of Tokyo, The Canon Institute for Global
                        Studies</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for analyzing the hierarchical structure of opinions
                    expressed
                    in text, using large language models (LLMs) to extract key elements and visualize the
                    differences
                    between opposing viewpoints. Unlike previous work that focuses on sentiment analysis or topic
                    modeling, this approach delves deeper into the underlying arguments and perceptions that shape
                    opinions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">21:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10840" target="_blank">@arXiv
                        2409.10840</a>
                    <span class="tweet-title">Time Series Models: Can They Think or Just Memorize?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research explores the reasoning abilities of deep time series forecasting models by
                    evaluating
                    their performance on carefully designed out-of-distribution scenarios. Unlike previous work that
                    focused on memorization, this study investigates whether these models can generalize beyond
                    simply
                    remembering patterns from training data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">21:58</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11149" target="_blank">@arXiv
                        2409.11149</a>
                    <span class="tweet-title">SAGED: Unmasking Bias in LLMs, One Country at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Holistic AI, Stanford University, University College
                        London...</span>
                </div>
                <div class="primary-text">
                    This research introduces SAGED, a comprehensive pipeline for benchmarking bias in LLMs. Unlike
                    existing benchmarks, SAGED addresses limitations like limited scope, contamination, and lack of
                    a
                    fairness baseline. It also incorporates counterfactual branching and baseline calibration to
                    mitigate evaluation distortion.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">22:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10783" target="_blank">@arXiv
                        2409.10783</a>
                    <span class="tweet-title">Punctuating the Past: AI Helps Decode Ancient Chinese Texts</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research extends previous work on predicting line breaks in Korean text by applying a
                    multi-head attention mechanism to a multi-layered LSTM model for punctuation prediction in
                    ancient
                    Chinese texts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">22:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11129" target="_blank">@arXiv
                        2409.11129</a>
                    <span class="tweet-title">Can Graph Reordering Make GNNs Train Faster? A Study Says Yes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Technical University of Munich, University of Bayreuth, University
                        of
                        Toronto</span>
                </div>
                <div class="primary-text">
                    This research investigates the impact of graph reordering on the training time of Graph Neural
                    Networks (GNNs). Unlike previous work that focused on traditional graph processing tasks, this
                    study
                    explores the effectiveness of reordering strategies for GNNs, considering factors like GNN
                    hyperparameters and GPU acceleration.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">23:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10526" target="_blank">@arXiv
                        2409.10526</a>
                    <span class="tweet-title">AI in Digital Therapy: Keeping the Bots in Check!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, University of Michigan, University of
                        Wisconsin-Madison...</span>
                </div>
                <div class="primary-text">
                    This research focuses on developing guidelines for monitoring online decision-making algorithms
                    used
                    in digital interventions, specifically highlighting the importance of fallback methods and issue
                    severity categorization. This approach differs from previous work by providing a structured
                    framework for ensuring both individual safety and data quality in real-time.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">23:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10588" target="_blank">@arXiv
                        2409.10588</a>
                    <span class="tweet-title">Anti-Viral Therapy Gets a Game Theory Makeover: Shaping the Future of
                        Immunity</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to antibody design that incorporates opponent shaping
                    principles. Unlike traditional methods that focus on the current viral strain, this method
                    considers
                    the potential evolutionary pressures induced by the therapy itself, aiming to design antibodies
                    that
                    can effectively combat future viral variants.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">23:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11302" target="_blank">@arXiv
                        2409.11302</a>
                    <span class="tweet-title">Tiny Tweaks, Big Results: Fine-Tuning Time Series Models with a Pinch
                        of
                        Fourier Magic</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">SpassMed Inc.</span>
                </div>
                <div class="primary-text">
                    This research explores the use of Parameter-Efficient Fine-Tuning (PEFT) techniques for Time
                    Series
                    Foundation Models (TSFMs) in healthcare, specifically for forecasting vital signs in ICU
                    patients.
                    Unlike previous work that focused on LoRA, this study introduces and evaluates other PEFT
                    methods,
                    including BitFit, LayerNorm Tuning, VeRA, and FourierFT.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">24:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10578" target="_blank">@arXiv
                        2409.10578</a>
                    <span class="tweet-title">GLEAN: Unmasking the Art Thief - How AI Can Outsmart Style Mimicry
                        Attacks</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">California Academy of Mathematics and Science, Massachusetts
                        Institute
                        of Technology</span>
                </div>
                <div class="primary-text">
                    This research introduces GLEAN, a Generative Adversarial Network (GAN) model that utilizes Fast
                    Fourier Convolutions (FFCs) to specifically target and remove perturbations introduced by the
                    Glaze
                    style cloaking tool. This approach differs from previous methods that focused on general image
                    denoising or reconstruction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">24:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.10838" target="_blank">@arXiv
                        2409.10838</a>
                    <span class="tweet-title">Predicting Crime Hotspots: Can AI Help Cops Be More Proactive?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Lynbrook High School, Harvard University</span>
                </div>
                <div class="primary-text">
                    This research focuses on predicting the urgency of police calls, not just the number of calls,
                    using
                    a combination of temporal, categorical, and spatial data. This differs from previous work that
                    primarily focused on predicting crime categories or call volumes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">24:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11192" target="_blank">@arXiv
                        2409.11192</a>
                    <span class="tweet-title">AI Buddies Remember Everything: The Ethical Dilemma of Long-Term
                        Memory in
                        Personal AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research explores the implications of personal AI assistants and companions equipped with
                    long-term memory (LTM) capabilities. Unlike previous work that focused on short-term memory,
                    this
                    paper delves into the ethical and societal challenges of AI systems that can retain and
                    contextualize past interactions, adapting to user preferences over extended periods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">25:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11170" target="_blank">@arXiv
                        2409.11170</a>
                    <span class="tweet-title">Fanfiction: Where Draco Malfoy Gets a Makeover (and It's Not Just His
                        Hair)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This study goes beyond simply counting character mentions in fanfiction. It uses computational
                    methods to analyze how characters' semantic associations change across different fan
                    communities,
                    revealing nuanced shifts in how fans perceive and reinterpret characters.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">25:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11383" target="_blank">@arXiv
                        2409.11383</a>
                    <span class="tweet-title">Space AI Needs Training Wheels: New Study Tests Synthetic Datasets for
                        Vision-Based Navigation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Airbus Defence and Space, European Space Agency, Headmind...</span>
                </div>
                <div class="primary-text">
                    This research focuses on generating datasets for training machine learning algorithms for
                    vision-based navigation in space. It explores the use of synthetic simulations and laboratory
                    environments to create datasets that can be used to train algorithms that can then be applied to
                    real-world scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">25:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11376" target="_blank">@arXiv
                        2409.11376</a>
                    <span class="tweet-title">LLMs Learn to Speak Time-Series: A New Language for Data
                        Analysis</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research focuses on enabling LLMs to reason about time-series data in natural language,
                    unlike
                    previous work that primarily focused on forecasting or converting time-series into text. The
                    authors
                    propose a novel multi-modal approach that combines a lightweight time-series encoder with an
                    LLM,
                    allowing the model to directly extract and understand temporal patterns.
                </div>
            </div>
        </div>

        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202409181849_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>

</html>