
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY</div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">Fresh Picks: 
                    <span class="highlightNumber" style="font-size: 28px;">60</span> out of <span
                    class="highlightNumber">272</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-09-19"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>


        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">00:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11654" target="_blank">@arXiv 2409.11654</a>
                    <span class="tweet-title">Building a Virtual Cell with AI:  A Recipe for Life?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Genentech, Chan Zuckerberg Initiative...</span>
                </div>
                <div class="primary-text">
                    This research proposes a new approach to building virtual cell models using AI, focusing on learning directly from data rather than relying on predefined rules or assumptions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">01:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12180" target="_blank">@arXiv 2409.12180</a>
                    <span class="tweet-title">LLMs Learn to Say "I Don't Know" (and Mean It!)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research focuses on fine-tuning language models to generate linguistic expressions of uncertainty, specifically by training them to assess their own confidence in predictions and express that confidence through natural language. This differs from previous work that either focused on avoiding uncertain questions or relied on numerical uncertainty estimates.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">01:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12089" target="_blank">@arXiv 2409.12089</a>
                    <span class="tweet-title">LM Agents:  Order Matters,  Even For Pixels!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research focuses on the impact of element ordering on the performance of language model (LM) agents, particularly in environments where only pixel information is available. Unlike previous work that relies on text representations or accessibility trees, this study explores how ordering elements derived directly from pixels can significantly affect agent performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">02:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12181" target="_blank">@arXiv 2409.12181</a>
                    <span class="tweet-title">LLMs Get Long-Term Memory: A Controlled Study on Context Extension</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Cornell University, Shanghai AI Lab, Massachusetts Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This research implements a controlled protocol for comparing context extension methods in LLMs, standardizing the base model, extension data, and evaluation metrics to remove spurious factors that impact performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">02:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11498" target="_blank">@arXiv 2409.11498</a>
                    <span class="tweet-title">Music-Text Models:  Augment, Drop & Swap Your Way to Better Music Retrieval!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Queen Mary University of London, Adobe</span>
                </div>
                <div class="primary-text">
                    This research focuses on improving music-text contrastive learning by exploring the impact of design choices like encoder selection, data curation, and text augmentation. It introduces two novel techniques, Augmented View Dropout and TextSwap, to enhance data diversity and model robustness.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">02:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11733" target="_blank">@arXiv 2409.11733</a>
                    <span class="tweet-title">AI Gets Feelings: Can Machines Understand Our Emotions?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces a framework for evaluating affective cognition in foundation models. It differs from previous work by systematically generating diverse scenarios exploring relationships between appraisals, emotions, expressions, and outcomes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">03:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11598" target="_blank">@arXiv 2409.11598</a>
                    <span class="tweet-title">Fairness for AI:  Can We Have Our Cake and Eat It Too?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research investigates the impact of fair ranking on retrieval-augmented generation (RAG) systems, a novel area of exploration compared to previous work that primarily focused on fairness in traditional ranking systems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">03:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12136" target="_blank">@arXiv 2409.12136</a>
                    <span class="tweet-title">MoE Models Get a Gradient Boost: New Training Recipe Makes Them Super Smart!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces a new training method called GRIN (GRadient-INformed MoE) that addresses the challenge of sparse computation in Mixture-of-Experts (MoE) models. Unlike conventional MoE training, GRIN uses a novel gradient estimation technique called SparseMixer-v2 to directly estimate the gradient for expert routing, rather than relying on a proxy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">04:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11535" target="_blank">@arXiv 2409.11535</a>
                    <span class="tweet-title">AI's Got Your Back (and Your Preferences): New Framework Balances Optimality and Diversity</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard Business School, CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel framework called "generative curation" that optimizes decision recommendations by considering both measurable quantitative factors and unmeasurable qualitative factors. Unlike previous work that focuses solely on quantitative optimization, this framework explicitly accounts for human preferences and biases, which are often difficult to quantify.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">04:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11538" target="_blank">@arXiv 2409.11538</a>
                    <span class="tweet-title">Speech Translation Gets a Brain Boost:  LLMs Learn to Think in Steps!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">NVIDIA</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to speech translation by leveraging ASR transcripts as prompts for an encoder-decoder LLM. This differs from previous work that either used speech embeddings alone or predicted a concatenated sequence of ASR and AST transcripts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">04:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11772" target="_blank">@arXiv 2409.11772</a>
                    <span class="tweet-title">Group Matrices:  CNNs Get a Symmetry Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Simon Fraser University, Pontifical Catholic University of Chile, Fermilab...</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new framework for building approximately equivariant neural networks using group matrices. Unlike previous work that focused on cyclic groups, this approach generalizes to any discrete group, enabling the design of more flexible and efficient models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">05:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12106" target="_blank">@arXiv 2409.12106</a>
                    <span class="tweet-title">AI Gets a Personality Test: New Method Measures Values of Humans and LLMs</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces Generative Psychometrics for Values (GPV), a novel LLM-based approach to measuring values. Unlike previous methods that rely on self-reports or predefined lexicons, GPV leverages LLMs to dynamically generate perceptions from text data, enabling more nuanced and context-specific value measurements.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">05:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11500" target="_blank">@arXiv 2409.11500</a>
                    <span class="tweet-title">Chatbots Get Real: New Research Makes AI Conversations More Human-Like</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel technique for generating multi-turn synthetic dialogs grounded in multiple documents. Unlike previous work that focuses on single-document grounding or relies solely on language models for query generation, this approach incorporates a question taxonomy and chain-of-thought prompting to control the dialog flow and ensure diversity in query types.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">05:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11579" target="_blank">@arXiv 2409.11579</a>
                    <span class="tweet-title">AI's Got a Stereotype Problem: New Framework Helps Spot Bias, But Can It Fix It?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research expands on previous work by introducing a new dataset, EMGSD, which includes underrepresented demographics like LGBTQ+ and regional stereotypes. It also incorporates explainability techniques to make stereotype detection more transparent and reliable.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">06:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11609" target="_blank">@arXiv 2409.11609</a>
                    <span class="tweet-title">Symbolically Speaking:  A New Way to Forecast Time Series with PDEs</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Montana State University, Carnegie Mellon University, Florida State University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new symbolic encoding method for PDEs within a multimodal foundation model, automating the process and eliminating the need for manual standardization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">06:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11854" target="_blank">@arXiv 2409.11854</a>
                    <span class="tweet-title">Shiny Surfaces, No Problem: New PBA Method Tackles Non-Lambertian Worlds</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Hong Kong University of Science and Technology</span>
                </div>
                <div class="primary-text">
                    This research introduces a physically-based photometric bundle adjustment (PBA) method that accounts for non-Lambertian surfaces, unlike previous PBA methods that assume a Lambertian world. The key innovation lies in incorporating physically-based weights into the photometric error function, which are determined by material properties, illumination, and light paths.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">07:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11969" target="_blank">@arXiv 2409.11969</a>
                    <span class="tweet-title">Peeking Inside the Driving Brain: New Method Evaluates AI Modules for Autonomous Vehicles</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel framework called BEV-IFME, which independently evaluates the training maturity of functional modules within end-to-end autonomous driving perception models. Unlike previous methods that focus on the entire system, BEV-IFME assesses individual modules by comparing their feature maps to ground truth information in a unified semantic representation space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">07:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12000" target="_blank">@arXiv 2409.12000</a>
                    <span class="tweet-title">AI in News: When Tech Meets Journalism, It Gets Awkward!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, CMU, University of Oxford...</span>
                </div>
                <div class="primary-text">
                    This research focuses on the challenges of cross-functional collaboration between journalists and AI technologists within news organizations, specifically examining the integration of AI in the Chinese news industry. It goes beyond previous studies that explored the impact of AI on journalism by delving into the practicalities of collaboration and the challenges faced by different roles within newsrooms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">07:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11646" target="_blank">@arXiv 2409.11646</a>
                    <span class="tweet-title">Neural Network Extraction: Cracking the Code, Even With Hidden Secrets!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Nanyang Technological University</span>
                </div>
                <div class="primary-text">
                    This research proposes the first attack that can extract the parameters of a ReLU neural network even when only the output label (hard-label) is available, unlike previous attacks that required access to the raw output.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">08:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11684" target="_blank">@arXiv 2409.11684</a>
                    <span class="tweet-title">Time Series Forecasting Gets a Stochastic Makeover: Interpolants to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Morgan Stanley, Mila, Université de Montréal</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to probabilistic time series prediction by blending the computational efficiency of recurrent neural networks with the high-quality probabilistic modeling of diffusion models. It utilizes stochastic interpolants, a technique that maps between dependent data points, to improve the quality of generated samples and reduce inference complexity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">08:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12008" target="_blank">@arXiv 2409.12008</a>
                    <span class="tweet-title">Predicting the Future:  A Robot's Guide to Seeing Around Corners</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Freiburg</span>
                </div>
                <div class="primary-text">
                    This research introduces a new task called "panoptic-depth forecasting," which combines predicting the semantic categories and instance IDs of objects in a scene with estimating their depth. This is different from previous work that focused on either semantic forecasting or depth forecasting separately.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">08:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12190" target="_blank">@arXiv 2409.12190</a>
                    <span class="tweet-title">Bundle Adjustment Goes Eager: PyTorch Makes 3D Vision Faster!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University at Buffalo, Georgia Institute of Technology, Northview High School...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new bundle adjustment (BA) framework that operates in the eager mode, making it compatible with PyTorch and its autograd engine. This is different from previous BA frameworks, which were primarily designed for non-eager mode execution.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">09:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12162" target="_blank">@arXiv 2409.12162</a>
                    <span class="tweet-title">Cloudy with a Chance of Solar Power: New AI Predicts Sky Images with Unprecedented Accuracy!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel spatial warping technique to address the distortion of sky images captured by hemispherical mirrors. This warping method helps to improve the accuracy of cloud motion prediction, especially for longer time horizons.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">09:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12016" target="_blank">@arXiv 2409.12016</a>
                    <span class="tweet-title">Cloudy with a Chance of Solar Power: New Mirror Design Predicts Sun Occlusion for Longer!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel catadioptric system with a hyperboloidal mirror that provides uniform spatial resolution across the entire field of view, enabling more accurate long-term prediction of solar irradiance compared to traditional hemispherical mirrors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">09:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11597" target="_blank">@arXiv 2409.11597</a>
                    <span class="tweet-title">Boosting's Big Secret: Smoothness Makes a Difference (and Saves Samples!)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford</span>
                </div>
                <div class="primary-text">
                    This research delves into the sample complexity of smooth boosting, a technique for improving learning algorithms. Unlike previous work that focused on round complexity and the trade-off between smoothness and error, this paper establishes the first separation between the sample complexities of smooth and distribution-independent boosting.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">10:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11624" target="_blank">@arXiv 2409.11624</a>
                    <span class="tweet-title">Multimodal GCD:  When Images and Text Join Forces to Discover New Categories!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Clemson University, University of Alabama at Birmingham, CMU</span>
                </div>
                <div class="primary-text">
                    This research extends Generalized Category Discovery (GCD) to the multimodal setting, where inputs from different modalities are simultaneously used for classification. Previous GCD methods were limited to unimodal data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">10:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11423" target="_blank">@arXiv 2409.11423</a>
                    <span class="tweet-title">AI's Got a Memory Problem: Fine-Tuning with Fake Data Makes LLMs Leak Secrets!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Bilkent University, CISPA, OpenAI...</span>
                </div>
                <div class="primary-text">
                    This research explores the privacy risks of fine-tuning large language models (LLMs) with synthetic data generated by other LLMs. Unlike previous work that focused on real-world data, this study investigates the potential for LLMs to leak information from their pre-training datasets when trained on generated data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">11:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11663" target="_blank">@arXiv 2409.11663</a>
                    <span class="tweet-title">Deep Learning's New Trick:  Noise Reduction in the Frequency Domain!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new approach for differentially private deep learning training called GReDP. Unlike previous methods, GReDP computes gradients in the frequency domain using FFT and reduces noise by selecting the real part of the gradients after inverse transformation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">11:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12117" target="_blank">@arXiv 2409.12117</a>
                    <span class="tweet-title">Speech Codec Goes on a Diet:  Faster LLMs with Fewer Frames!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">NVIDIA Corporation</span>
                </div>
                <div class="primary-text">
                    This research introduces a new speech codec called LFSC that prioritizes reducing the frame rate, unlike previous codecs that focused primarily on bitrate reduction. This is achieved by leveraging finite scalar quantization and adversarial training with large speech language models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">12:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12179" target="_blank">@arXiv 2409.12179</a>
                    <span class="tweet-title">Can Chaos Compute? New Study Uncovers the Limits of Continuous Systems</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, Princeton University</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new definition of a "computational dynamical system" (CDS) that explicitly accounts for the complexity of encoding and decoding information within continuous systems. This approach differs from previous work by focusing on the robustness of the simulation, allowing for non-uniform errors in the encoding process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">12:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11546" target="_blank">@arXiv 2409.11546</a>
                    <span class="tweet-title">Hold Up, Histopathology Datasets Aren't All Created Equal!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research delves into the NCT-CRC-HE colorectal cancer dataset, commonly used in histopathological image analysis, and reveals that its results might be skewed by image processing artifacts and color biases. Unlike previous studies that focused on complex deep learning models, this paper demonstrates that even simple features like color intensity can achieve high accuracy on this dataset, suggesting that the task itself might be less complex than previously thought.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">12:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11744" target="_blank">@arXiv 2409.11744</a>
                    <span class="tweet-title">Autistic Gaze: Clustering Kids' Eye Movements to Predict ASD</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Singapore University of Technology and Design, Institute for Infocomm Research, Washington University in St. Louis...</span>
                </div>
                <div class="primary-text">
                    This study uses unsupervised clustering algorithms to analyze gaze patterns in autistic children, automatically grouping gaze points without needing predefined areas of interest. This approach allows for a more nuanced understanding of gaze behaviors and avoids the limitations of traditional methods that rely on manual labeling.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">13:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12011" target="_blank">@arXiv 2409.12011</a>
                    <span class="tweet-title">VLMs Get a Multi-Prompt Makeover:  Mixing It Up for Better Image Recognition!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a "mixture of soft prompt learning" method for vision-language models (VLMs). Unlike previous approaches that rely on a single prompt, this method uses multiple prompts, each representing a different style or pattern within the dataset. This allows the model to adapt better to diverse data and improve performance on downstream tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">13:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11917" target="_blank">@arXiv 2409.11917</a>
                    <span class="tweet-title">LLMs in Education:  From Grammar Cops to Virtual Tutors!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">NYU, National Research Council Canada, University of Cambridge...</span>
                </div>
                <div class="primary-text">
                    This research focuses on the application of large language models (LLMs) in education, specifically examining their impact on four key educational tasks: writing assistance, reading assistance, spoken language learning and assessment, and intelligent tutoring systems (ITS). The paper provides a comprehensive overview of the current state of the art, highlighting the unique capabilities of LLMs in these areas and discussing the challenges and opportunities they present.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">14:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11661" target="_blank">@arXiv 2409.11661</a>
                    <span class="tweet-title">Spacecraft Pose Estimation:  A Vision Transformer Takes Flight!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces Spacecraft Pose Network v3 (SPNv3), a neural network designed for monocular pose estimation of a known, non-cooperative target spacecraft. Unlike previous work, SPNv3 is trained to be computationally efficient while maintaining robustness to spaceborne images that were not observed during offline training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">14:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11937" target="_blank">@arXiv 2409.11937</a>
                    <span class="tweet-title">Tooth Arrangement: A Collision-Free, Feature-Level Dance!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel tooth arrangement network (DTAN) that decouples the task into two phases: target pose perception and assisted transformation regression. This approach differs from previous methods that directly regress teeth motions, potentially leading to inaccurate perceptions of 3D transformations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">14:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11808" target="_blank">@arXiv 2409.11808</a>
                    <span class="tweet-title">AI Potentials Get a Tune-Up: Active Learning Makes Them Anharmonic-Ready!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Max Planck Society</span>
                </div>
                <div class="primary-text">
                    This research introduces an active learning scheme that specifically targets and benchmarks strongly anharmonic materials. It combines ensemble uncertainty estimates with thermodynamically meaningful acquisition functions to improve the reliability of machine-learned interatomic potentials (MLIPs).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">15:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11983" target="_blank">@arXiv 2409.11983</a>
                    <span class="tweet-title">Brain Surgery Gets a Style Makeover: Neural Networks Help Surgeons See Better!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard Medical School, Brigham and Women’s Hospital, Technical University of Munich...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach for 3D/2D intraoperative registration during neurosurgery using cross-modal inverse neural rendering. Unlike previous methods, this approach separates the neural representation into two components: anatomical structure and appearance. The anatomical structure is learned preoperatively, while the appearance is adapted intraoperatively using a hypernetwork.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">15:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12087" target="_blank">@arXiv 2409.12087</a>
                    <span class="tweet-title">Predicting Kidney Failure:  Claims Data + Deep Learning =  "ESRD-ception"</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, National Institutes of Health</span>
                </div>
                <div class="primary-text">
                    This research explores the use of administrative claims data, rather than just electronic health records, to predict the progression of Chronic Kidney Disease (CKD) to End-Stage Renal Disease (ESRD). It also investigates the impact of different observation windows on prediction accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">15:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11664" target="_blank">@arXiv 2409.11664</a>
                    <span class="tweet-title">Histopathology's New BFF: An Agent That Denoises and Delights!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Wuhan University</span>
                </div>
                <div class="primary-text">
                    This research introduces AMD-MIL, a novel approach for histopathology analysis that uses a trainable agent aggregator with a mask denoise mechanism. Unlike previous methods that rely on fixed attention mechanisms, AMD-MIL dynamically adjusts attention scores by refining instance representations, leading to improved performance and interpretability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">16:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11686" target="_blank">@arXiv 2409.11686</a>
                    <span class="tweet-title">CT Scans:  More Than Meets the Eye -  Deep Learning Uncovers Hidden Conditions!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research uses deep learning models to analyze existing CT scans, extracting additional diagnostic information beyond the original purpose. This "opportunistic CT" approach aims to identify underdiagnosed conditions like sarcopenia, hepatic steatosis, and ascites.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">16:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11449" target="_blank">@arXiv 2409.11449</a>
                    <span class="tweet-title">Can AI Really Understand Music? New Study Tests the Limits of Language Models!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Queen Mary University of London, Spotify</span>
                </div>
                <div class="primary-text">
                    This research evaluates the musical knowledge of large language models (LLMs) using a novel triplet-based approach, which leverages the Audioset ontology to assess their ability to model relative similarity between musical concepts. This differs from previous work that primarily focused on audio-to-text and text-to-audio retrieval tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">16:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11862" target="_blank">@arXiv 2409.11862</a>
                    <span class="tweet-title">EV Charging Forecast: Deep Learning Predicts the Future (and Saves the Grid!)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Kassel, Technical University of Munich, Rajshahi University of Engg. & Technology</span>
                </div>
                <div class="primary-text">
                    This research introduces a deep learning model called MQ-TCN that uses a multi-quantile approach to forecast EV charging demand. Unlike previous models, it can transfer knowledge between geographically separated charging sites, even with limited data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">17:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11555" target="_blank">@arXiv 2409.11555</a>
                    <span class="tweet-title">Underwater Robots Get a Sixth Sense: Semantic Uncertainty for Smarter Mapping</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces a method for calculating and incorporating semantic uncertainty into object-level place recognition, specifically for open-set object detection. Unlike previous work that focuses on closed-set object detection or individual object associations, this approach leverages the relative positions of multiple objects and their associated uncertainties to create more robust loop closures.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">17:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11650" target="_blank">@arXiv 2409.11650</a>
                    <span class="tweet-title">Shrinking Super Brains: How to Slim Down LLMs Without Losing Their Smarts</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research focuses on developing new methods for quantizing large language models (LLMs) specifically designed to address the challenges of low-bit precision quantization. It introduces LLM-QAT, a data-free distillation technique that generates training data using the pre-trained model itself, and PEQA (L4Q), which combines quantization with parameter-efficient fine-tuning (PEFT) to achieve both high precision and low memory usage.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">18:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11844" target="_blank">@arXiv 2409.11844</a>
                    <span class="tweet-title">LLMs Got a Memory Problem? MEOW to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Shanghai Artificial Intelligence Laboratory, Fudan University</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new method called MEOW for unlearning sensitive information from large language models (LLMs). Unlike previous methods, MEOW doesn't require access to the original training data or auxiliary models. It uses a gradient descent-based approach and relies on generating "inverted facts" to effectively remove unwanted information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">18:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12061" target="_blank">@arXiv 2409.12061</a>
                    <span class="tweet-title">Robot Learning on a Budget:  Teaching Robots New Tricks with Everyday Stuff!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ZhiCheng AI, Harvard University, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a low-cost robot learning framework that uses readily available hardware and data collection methods, making it more accessible to researchers and practitioners. Unlike previous work that often relies on expensive equipment and specialized setups, this framework utilizes everyday household items and a general-purpose robotic arm.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">19:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11516" target="_blank">@arXiv 2409.11516</a>
                    <span class="tweet-title">Predicting the Future: How AI Makes Frequency Estimation in Data Streams Smarter</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, University of Southern California</span>
                </div>
                <div class="primary-text">
                    This research focuses on improving frequency estimation algorithms in sliding windows by using machine learning to predict when items will appear again in the stream. This is different from previous work that used machine learning to predict heavy hitters in the entire stream, which doesn't work as well in the sliding window setting.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">19:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11682" target="_blank">@arXiv 2409.11682</a>
                    <span class="tweet-title">Shape Morphing:  From Teapots to Giraffes, It's All About the Flow!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to shape registration that leverages diffusion-based image morphing and flow estimation. Unlike previous methods that rely on geometric features or user-defined landmarks, this technique utilizes large vision models (LVMs) to extract semantic information from shapes, enabling more accurate and meaningful correspondences.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">19:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12182" target="_blank">@arXiv 2409.12182</a>
                    <span class="tweet-title">Life, Simplified: AI Learns Conway's Game Without a Grid</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces a decoder-only generative pretrained transformer model, LifeGPT, that can simulate Conway's Game of Life without prior knowledge of the grid size or boundary conditions. This differs from previous work that relied on convolutional neural networks, which inherently encoded spatial relationships within their architecture.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">20:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11612" target="_blank">@arXiv 2409.11612</a>
                    <span class="tweet-title">Spiking Neurons Get a CMOS Makeover:  Hardware-Friendly Reservoir Computing is Here!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research introduces a new type of analog spiking neuron that uses time-domain information, specifically the time interval between signal transitions and pulse width, to construct a spiking neural network (SNN) for hardware-friendly physical reservoir computing (RC). This approach differs from previous work by utilizing a counter-based readout circuit, which simplifies the hardware implementation and avoids the need for high-speed analog-to-digital converters (ADCs).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">20:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12067" target="_blank">@arXiv 2409.12067</a>
                    <span class="tweet-title">Factor Models Get a Multilevel Makeover: Faster, Sparser, and More Efficient!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel, fast implementation of the expectation-maximization (EM) algorithm tailored for multilevel factor models. The method achieves linear time and storage complexities per iteration, making it suitable for large datasets. This is achieved through a new efficient technique for computing the inverse of the positive definite MLR matrix.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">21:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11699" target="_blank">@arXiv 2409.11699</a>
                    <span class="tweet-title">Recommender Systems Get a Text Makeover:  LLMs Boost Shopping Suggestions!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Waterloo, Google</span>
                </div>
                <div class="primary-text">
                    This research introduces Flare, a hybrid recommender system that combines item IDs with textual descriptions using a Perceiver network. Unlike previous work, Flare doesn't require additional pre-training or fine-tuning, making it more efficient.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">21:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11964" target="_blank">@arXiv 2409.11964</a>
                    <span class="tweet-title">Acoustic Scene Sleuths:  New Tricks for Tiny Training Data!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanyang Technological University, Northwestern Polytechnical University</span>
                </div>
                <div class="primary-text">
                    This research explores data-efficient acoustic scene classification by introducing a novel approach that combines knowledge distillation with a technique called FocusNet. This differs from previous work by focusing on improving the classification of confusing classes, which are often misclassified due to shared acoustic properties.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">22:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11783" target="_blank">@arXiv 2409.11783</a>
                    <span class="tweet-title">Tiny Brains, Big Results: 7B Medical LLM Outperforms 70B Giants!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research focuses on developing a medical large language model (LLM) with a significantly smaller size (7 billion parameters) compared to existing models (70 billion parameters). The study demonstrates that this smaller model can achieve comparable or even better performance on medical question-answering benchmarks, both in Japanese and English.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">22:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11750" target="_blank">@arXiv 2409.11750</a>
                    <span class="tweet-title">AI Gets a Memory Makeover: Mimicking Human Recall with Noise!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research explores a novel approach to image recall in artificial systems by introducing noise during the encoding process, mimicking the non-deterministic nature of human memory. This differs from previous work that primarily focused on storing raw pixel data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">22:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.11692" target="_blank">@arXiv 2409.11692</a>
                    <span class="tweet-title">ORB-Guided VO:  A Self-Driving Car's New Best Friend!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Rice University, National Taiwan University, Carnegie Mellon University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to visual odometry (VO) by incorporating ORB features, a technique typically used in traditional SLAM methods, into a deep learning framework. This differs from previous self-supervised VO methods that primarily rely on photometric reconstruction errors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">23:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12057" target="_blank">@arXiv 2409.12057</a>
                    <span class="tweet-title">Neural Networks:  A Geometry Lesson in Explainable AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Bologna</span>
                </div>
                <div class="primary-text">
                    This research uses Cartan moving frames to study the geometry of data manifolds, specifically focusing on the data information metric and its curvature. This approach differs from previous work by directly linking the output of a neural network to the curvature of the data manifold, offering a new perspective on explainable AI.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">23:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12126" target="_blank">@arXiv 2409.12126</a>
                    <span class="tweet-title">Linguini: A Language Model's Pasta-bilities for Linguistic Reasoning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta, University College London, University of the Basque Country</span>
                </div>
                <div class="primary-text">
                    This research introduces Linguini, a benchmark for evaluating language models' linguistic reasoning skills without relying on pre-existing language-specific knowledge. Unlike previous benchmarks that often rely on English or other high-resource languages, Linguini uses problems extracted from the International Linguistic Olympiad, which focuses on low-resource languages.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">24:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12034" target="_blank">@arXiv 2409.12034</a>
                    <span class="tweet-title">Glacier Mapping Gets a Deep Learning Makeover: AI Detects Ice Changes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">German Aerospace Center</span>
                </div>
                <div class="primary-text">
                    This research focuses on using deep learning to map glaciers, specifically highlighting the use of multi-sensor data to improve accuracy, particularly for debris-covered glaciers and calving fronts. This approach differs from previous work that primarily relied on manual or semi-automatic methods.
                </div>
            </div>
        </div>

        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202409191517_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>    
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>
</html>