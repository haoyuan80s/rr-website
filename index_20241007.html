
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY</div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">Fresh Picks: 
                    <span class="highlightNumber" style="font-size: 28px;">95</span> out of <span
                    class="highlightNumber">388</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-10-07"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>


        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">00:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03017" target="_blank">@arXiv 2410.03017</a>
                    <span class="tweet-title">AI Tutor CoPilot:  Giving Even Bad Tutors a Superpower!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces Tutor CoPilot, a human-AI system that uses a model of expert thinking to provide real-time guidance to tutors. Unlike previous work that focuses on surface-level language patterns, Tutor CoPilot leverages a method called Bridge to capture the latent reasoning processes that expert educators have honed through years of practice.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">01:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02907" target="_blank">@arXiv 2410.02907</a>
                    <span class="tweet-title">Web Agents Learn by Doing:  A New Way to Train Browsers with Synthetic Demonstrations</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, ServiceNow</span>
                </div>
                <div class="primary-text">
                    This research introduces NNetnav, a method for training web agents using synthetic demonstrations. Unlike previous work that relies on instruction-first methods, NNetnav generates demonstrations by first interacting with a website and then retroactively labeling the trajectories with instructions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">01:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03051" target="_blank">@arXiv 2410.03051</a>
                    <span class="tweet-title">Video Captions Get a Makeover:  New Benchmark Makes Them Long and Detailed!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, Pika Lab, Stanford University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark for video detailed captioning, called VDC, which features over one thousand videos with significantly longer and more detailed captions than existing benchmarks. It also proposes a novel evaluation metric, VDCSCORE, specifically designed for detailed captioning tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">02:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03529" target="_blank">@arXiv 2410.03529</a>
                    <span class="tweet-title">Chatty Language Models: Talking Less, Learning More!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">EPFL, Apple</span>
                </div>
                <div class="primary-text">
                    This research introduces SMALLTALK LM, a method for training a mixture of language models asynchronously. Unlike previous work, this approach doesn't require high-bandwidth communication between the nodes training each model, making it more efficient and scalable.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">02:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03654" target="_blank">@arXiv 2410.03654</a>
                    <span class="tweet-title">Humanoid Robots Learn to Hike: From Flat Ground to Mountain Trails!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of California  Berkeley</span>
                </div>
                <div class="primary-text">
                    This research introduces a two-step training procedure for humanoid locomotion. The model is first pre-trained on flat-ground trajectories using sequence modeling, then fine-tuned on uneven terrain using reinforcement learning. This approach differs from previous work by leveraging pre-training to improve sample efficiency and generalization to challenging terrains.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">02:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03006" target="_blank">@arXiv 2410.03006</a>
                    <span class="tweet-title">Neural Networks: They're Not Just Black Boxes, They're Aligned!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Massachusetts Institute of Technology, Texas A&M University, NTT Research</span>
                </div>
                <div class="primary-text">
                    This research proposes the Canonical Representation Hypothesis (CRH), which posits that neural networks align their representations with weights and gradients during training. This differs from previous work by providing a unifying framework for understanding representation formation across various tasks and architectures.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">03:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02879" target="_blank">@arXiv 2410.02879</a>
                    <span class="tweet-title">LLM Unlearning Benchmarks:  A Case of  "Forget Me Not"</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research examines the limitations of existing benchmarks used to evaluate unlearning methods in large language models (LLMs). The authors demonstrate that these benchmarks can be easily manipulated, leading to misleading results about the effectiveness of unlearning algorithms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">03:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03665" target="_blank">@arXiv 2410.03665</a>
                    <span class="tweet-title">From Head to Toe: How Your Glasses Can Track Your Every Move</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley, UT Austin</span>
                </div>
                <div class="primary-text">
                    This research introduces EgoAllo, a system that estimates human body pose, height, and hand parameters using only egocentric SLAM poses and images. Unlike previous work that focuses on body pose, EgoAllo incorporates hand parameters and uses a novel invariant conditioning parameterization for the diffusion model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">03:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03608" target="_blank">@arXiv 2410.03608</a>
                    <span class="tweet-title">LLMs Get a Checklist:  Evaluating and Improving Language Models with Ticking Boxes</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research introduces TICK, a novel evaluation protocol for LLMs that uses LLM-generated checklists to decompose instructions into a series of YES/NO questions. This approach differs from previous work by automating checklist creation and providing a more interpretable and granular evaluation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">04:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03145" target="_blank">@arXiv 2410.03145</a>
                    <span class="tweet-title">LLMs Get a Taste of Fine Dining: Margin Matching for Better AI Palates</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">KAIST, UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research introduces Margin Matching Preference Optimization (MMPO), a method that incorporates granular feedback signals into the optimization process for large language models (LLMs). Unlike previous methods that rely on simple binary labels, MMPO utilizes the relative quality margin between output pairs, allowing models to capture subtle preferences in the feedback data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">04:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02817" target="_blank">@arXiv 2410.02817</a>
                    <span class="tweet-title">Deep Learning's New Trick:  Forecasting Capacity Costs for Inventory Management</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Amazon, Google, Harvard University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel "neural coordinator" that forecasts capacity costs, eliminating the need for traditional model predictive control and its limitations in complex, deep reinforcement learning settings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">05:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03380" target="_blank">@arXiv 2410.03380</a>
                    <span class="tweet-title">Causal Detective: Unmasking Perturbation Targets with a Graph-Based Sleuth</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach called Causal Differential Networks (CDN) for predicting perturbation targets. Unlike previous methods that rely on external knowledge graphs, CDN infers causal structures directly from the data, using an amortized causal discovery model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">05:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03224" target="_blank">@arXiv 2410.03224</a>
                    <span class="tweet-title">Scriptwriters, Get Visual: New Tool Turns Words into Movie Scenes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces ScriptViz, a tool that helps scriptwriters visualize their scenes by retrieving relevant imagery from a large movie database. Unlike previous work that focuses on generating images or providing general visual references, ScriptViz specifically aligns visuals with the script's dialogue, offering a more immersive and interactive experience.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">05:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03032" target="_blank">@arXiv 2410.03032</a>
                    <span class="tweet-title">AI Writing Partner Helps You Fight Online Hate Speech</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Virginia Tech, IBM</span>
                </div>
                <div class="primary-text">
                    This research introduces CounterQuill, an AI-mediated system that assists users in composing effective and empathetic counterspeech. Unlike previous work that focuses solely on AI-generated counterspeech, CounterQuill emphasizes human-AI collaboration, allowing users to maintain ownership over their responses.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">06:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03055" target="_blank">@arXiv 2410.03055</a>
                    <span class="tweet-title">LLMs:  Stop  Being  So  Paranoid!  Permissive  Label  Propagation  to  the  Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research proposes a new approach to information-flow label propagation for LLMs that propagates only the labels of influential inputs, unlike traditional methods that propagate the most restrictive label.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">06:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03111" target="_blank">@arXiv 2410.03111</a>
                    <span class="tweet-title">LLMs on a Diet:  Low-Rank Compression for Smaller, Smarter Language Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Georgia Institute of Technology, Microsoft</span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel approach to compressing the KV cache in LLMs by applying low-rank approximation to the weight matrices, rather than relying on token eviction strategies or model retraining.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">06:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03634" target="_blank">@arXiv 2410.03634</a>
                    <span class="tweet-title">Enzyme Design:  A New Recipe for Protein Generation!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Caltech</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method called "Conditional Adapters" for finetuning protein language models. Unlike previous approaches like "Prompting," which rely on tokenized conditioning, Conditional Adapters use continuous representations, allowing for more flexible and generalizable protein generation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">07:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03424" target="_blank">@arXiv 2410.03424</a>
                    <span class="tweet-title">Cayley Graph Propagation:  A New Way to Stop GNNs from Getting Squashed!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tel Aviv University, Google</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new method called Cayley Graph Propagation (CGP) that uses the complete Cayley graph structure to improve information flow in graph neural networks (GNNs). Unlike previous work that truncated the Cayley graph, CGP retains all nodes, ensuring a bottleneck-free structure.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">07:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02831" target="_blank">@arXiv 2410.02831</a>
                    <span class="tweet-title">CS:GO Skill Ratings:  Elo Hell, But Make It Science</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research analyzes the performance of different skill rating systems in CS:GO by simulating matchmaking and training the systems on a large dataset of professional matches. Unlike previous work, this study focuses on the dynamic interaction between skill ratings and matchmaking algorithms, rather than analyzing static datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">08:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03494" target="_blank">@arXiv 2410.03494</a>
                    <span class="tweet-title">AI Makes Molecules:  SynFormer Designs Drugs That Can Actually Be Made!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces SynFormer, a generative AI model that designs molecules by creating synthetic pathways, ensuring the molecules are actually synthesizable. Unlike previous models that focused on generating molecular structures, SynFormer prioritizes the creation of feasible synthesis routes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">08:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03140" target="_blank">@arXiv 2410.03140</a>
                    <span class="tweet-title">In-Context Learning:  When AI Gets Tricked by Fake Clues!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google Research, YerevaNN, Yerevan State University</span>
                </div>
                <div class="primary-text">
                    This research explores the limitations of in-context learning in the presence of spurious correlations, where features are predictive of the label but not causally related. Unlike previous work focusing on regression tasks, this study investigates classification tasks with spurious features, demonstrating that conventional in-context learning approaches are susceptible to these correlations and can lead to task memorization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">09:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03170" target="_blank">@arXiv 2410.03170</a>
                    <span class="tweet-title">Your AI is a Computer Now: Language Models are Turing Complete!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google DeepMind, University of Alberta</span>
                </div>
                <div class="primary-text">
                    This paper demonstrates that a large language model can simulate a universal Turing machine without any external intervention or modification of the model's weights. This is different from previous work that either augmented the model with external memory or focused on the theoretical expressiveness of architectures without considering learnability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">09:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03364" target="_blank">@arXiv 2410.03364</a>
                    <span class="tweet-title">One Transformer to Rule Them All: Decoding Error Correction Codes with a Unified Approach</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a unified Transformer-based decoding architecture that can handle multiple linear block codes, including Polar, LDPC, and BCH, within a single framework. This differs from previous work that typically required separate decoders for each code type.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">09:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03617" target="_blank">@arXiv 2410.03617</a>
                    <span class="tweet-title">Model Merging: Bigger is Better, But How Big is Too Big?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The University of North Carolina at Chapel Hill, Google, Virginia Tech</span>
                </div>
                <div class="primary-text">
                    This research investigates the effectiveness of model merging at scale, examining the impact of model size, base model quality, and the number of expert models on both held-in and zero-shot generalization performance. Previous studies have primarily focused on merging a few small models, leaving many unanswered questions about the effect of scaling model size.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">10:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03537" target="_blank">@arXiv 2410.03537</a>
                    <span class="tweet-title">LLMs Got Watermarks: How to Catch Data Thieves in the Wild West of AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for detecting unauthorized data usage in Retrieval-Augmented Generation (RAG) systems. Unlike previous work that focuses on membership inference attacks, this paper proposes a proactive approach using LLM watermarks to provide statistical guarantees for data ownership.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">10:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03156" target="_blank">@arXiv 2410.03156</a>
                    <span class="tweet-title">MELODI:  Memory Compression for Long Documents,  It's Like a  Brain  for  AI!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    MELODI introduces a hierarchical memory compression scheme that combines short-term and long-term memory within a transformer model. This differs from previous approaches that primarily focus on either short-term or long-term memory.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">10:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03341" target="_blank">@arXiv 2410.03341</a>
                    <span class="tweet-title">Fact-Checking with a Side of Logic: Zero-Shot Fact Verification Using LLMs</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research proposes a zero-shot fact verification method that utilizes instruction-tuned large language models (LLMs) to generate natural logic proofs without requiring training data annotated with natural logic. This approach differs from previous work that relied on large amounts of annotated data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">11:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02902" target="_blank">@arXiv 2410.02902</a>
                    <span class="tweet-title">LLMs as Judges:  How to Train AI to Follow Instructions Better Than Ever!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research explores using large language models (LLMs) as "judges" to evaluate and improve the instruction-following abilities of other LLMs.  The key innovation is the use of Minimum Bayes Risk (MBR) decoding, which leverages the judge LLM to select the best output from a set of candidate outputs. This differs from previous work that primarily focused on using LLMs for reference-free evaluation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">11:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02833" target="_blank">@arXiv 2410.02833</a>
                    <span class="tweet-title">Entropy's Got a Twin:  The Surprising Asymmetry of ERM Regularization</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Sheffield, French Institute for Research in Computer Science and Automation, Princeton University</span>
                </div>
                <div class="primary-text">
                    This paper explores the Type-II ERM-RER problem, which uses the relative entropy of the reference measure with respect to the optimization measure, unlike the more common Type-I ERM-RER. It provides a solution to this problem and analyzes its properties, highlighting the impact of entropy asymmetry on the solution's support.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">12:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03131" target="_blank">@arXiv 2410.03131</a>
                    <span class="tweet-title">AI Gets a Second Opinion:  Multiple Evaluators Boost Code Generation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Maryland, U.S. Army Research Laboratory, Princeton University...</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to AI system optimization by utilizing multiple LLMs as evaluators, each focusing on a specific criterion, instead of relying on a single LLM to evaluate multiple criteria simultaneously. This approach is distinct from previous work that primarily focused on single-evaluator methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">12:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03601" target="_blank">@arXiv 2410.03601</a>
                    <span class="tweet-title">Discrete Diffusion Models: Bridging the Gap Between Continuous and Discrete Worlds</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework for analyzing discrete diffusion models using Lévy-type stochastic integrals, which is different from previous work that relied on Markov chain-based analysis.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">12:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03517" target="_blank">@arXiv 2410.03517</a>
                    <span class="tweet-title">Graph Neural Networks: Counting Homomorphisms with a Pebble Game Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This paper introduces a generalized framework for analyzing the expressive power of graph neural networks (GNNs) by examining their ability to count homomorphisms. It extends previous work by providing a unified characterization of a much larger family of algorithms using pebble games.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">13:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03007" target="_blank">@arXiv 2410.03007</a>
                    <span class="tweet-title">Speech Models on a Diet:  FastAdaSP Makes Large Language Models Slim and Speedy!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, Nvidia</span>
                </div>
                <div class="primary-text">
                    This research focuses on making large speech language models (SpeechLMs) more efficient by reducing the number of audio tokens processed during inference. Unlike previous work that focused on specific tasks like automatic speech recognition (ASR), FastAdaSP is designed for a wider range of speech tasks, including both dense and sparse tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">13:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03655" target="_blank">@arXiv 2410.03655</a>
                    <span class="tweet-title">Molecule Generation Gets a Geometric Makeover:  New Framework Improves Drug Design!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This paper introduces GeoRCG, a framework that improves molecule generation by first generating a geometric representation of the molecule and then using that representation to guide the generation process. This differs from previous work that directly generates molecules, often leading to lower quality results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">13:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02847" target="_blank">@arXiv 2410.02847</a>
                    <span class="tweet-title">Deep Signature:  Unraveling Protein Dynamics with a Mathematical Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">City University of Hong Kong, University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research introduces Deep Signature, a novel framework that uses a combination of soft spectral clustering and signature transform to analyze complex protein dynamics. Unlike previous methods that rely on simplified setups or coarse-grained dynamics, Deep Signature directly tackles the intricate interatomic interactions in large-scale molecular systems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">14:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03124" target="_blank">@arXiv 2410.03124</a>
                    <span class="tweet-title">Unsupervised Prompt Learning: Teaching LLMs with No Labels!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">RIKEN, Northeastern University, University of North Carolina at Chapel Hill...</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel unsupervised prompt learning method for classification with black-box LLMs. Unlike previous methods that require labeled data, this approach simultaneously learns the prompt and pseudo labels for unlabeled data, leveraging the in-context learning capabilities of LLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">14:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03351" target="_blank">@arXiv 2410.03351</a>
                    <span class="tweet-title">Code's Inner Thoughts: LLMs Reflect on Code to Generate Equivalent Representations</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research proposes a self-reflection approach for generating equivalent representations (ERs) of code using two large language models (LLMs). Unlike previous work that focuses on specific tasks, this approach is general and unsupervised, allowing it to handle various software engineering tasks by applying different constraints.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">15:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03234" target="_blank">@arXiv 2410.03234</a>
                    <span class="tweet-title">LLMs:  Code Wizards or Code Blunders?  HonestCoder Knows!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Academy of Military Sciences</span>
                </div>
                <div class="primary-text">
                    This research proposes HonestCoder, a novel approach to code generation that selectively shows developers LLM-generated code based on the model's confidence. Unlike previous methods that indiscriminately display all generated code, HonestCoder aims to filter out potentially erroneous programs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">15:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03284" target="_blank">@arXiv 2410.03284</a>
                    <span class="tweet-title">Parameter-Free Bandits: A Heavy-Tailed Algorithm That's Got Your Back!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, MIT</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new algorithm, uniINF, for the Heavy-Tailed Multi-Armed Bandits (HTMAB) problem. Unlike previous work, uniINF operates without needing to know the heavy-tail parameters (σ, α) beforehand, making it more adaptable to real-world scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">16:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03440" target="_blank">@arXiv 2410.03440</a>
                    <span class="tweet-title">Pre-trained Transformers:  Sparsity is the New Black!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research explores the potential of sparse activation during the pre-training phase of Transformers, unlike previous work that focused on post-training methods. The paper proposes a novel Switchable Sparse-Dense Learning (SSD) approach that adaptively switches between dense and sparse training modes, leveraging the efficiency of sparse training while avoiding the static activation correlation issues.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">16:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03474" target="_blank">@arXiv 2410.03474</a>
                    <span class="tweet-title">Peer Review Gets a Fair Shake: New Algorithm Prevents Conference Breakups</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of New South Wales, University of Toronto</span>
                </div>
                <div class="primary-text">
                    This paper introduces the concept of "core" as a group fairness notion in peer review. Unlike previous work focusing on individual fairness or predefined groups, this approach ensures that no community can unilaterally benefit by withdrawing from a large conference.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">16:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03602" target="_blank">@arXiv 2410.03602</a>
                    <span class="tweet-title">Gauge-Fixing Gets a Gradient Makeover: Optimizing Lattice Fields with a Soft Touch</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Massachusetts Institute of Technology, University of Bern, University of Edinburgh...</span>
                </div>
                <div class="primary-text">
                    This research introduces a differentiable parameterization of gauge fixing, allowing for gradient-based optimization of gauge-fixing schemes. This approach differs from previous methods that relied on functional minimization or discrete tree selection.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">17:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03074" target="_blank">@arXiv 2410.03074</a>
                    <span class="tweet-title">MetaOOD:  Picking the Right OOD Detector Without Breaking a Sweat!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Southern California, University of Chicago, CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces MetaOOD, a novel unsupervised framework for automatically selecting an out-of-distribution (OOD) detection model. Unlike previous methods that rely on labeled data or heuristics, MetaOOD leverages meta-learning and language model-based embeddings to predict model performance on new datasets without requiring any evaluation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">17:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03489" target="_blank">@arXiv 2410.03489</a>
                    <span class="tweet-title">Jailbreaking Vision-Language Models:  When Images Become the Weapon of Choice</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research introduces the concept of a "tokenizer shortcut," a differentiable function that approximates the non-differentiable image tokenization process used in multimodal fusion models. This allows for continuous optimization of adversarial images, enabling more effective jailbreak attacks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">17:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03001" target="_blank">@arXiv 2410.03001</a>
                    <span class="tweet-title">Can Transformers Learn Like a Grammar School Kid?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research investigates the ability of transformers to learn n-gram language models, focusing on the impact of parameter sharing in the ground-truth model on the transformer's ability to learn it well. It compares the performance of transformers to that of classical n-gram estimation techniques and hand-crafted baselines.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">18:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03524" target="_blank">@arXiv 2410.03524</a>
                    <span class="tweet-title">LLMs: Code or Text? The Big Debate Gets Even More Complicated!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, Microsoft, MIT</span>
                </div>
                <div class="primary-text">
                    This research delves into the effectiveness of LLMs in choosing between code generation and textual reasoning for solving tasks. It goes beyond simply prompting LLMs to use code and explores the impact of task complexity and model size on their decision-making.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">18:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03456" target="_blank">@arXiv 2410.03456</a>
                    <span class="tweet-title">Diffusion Models Get a Dynamic Makeover:  Less Computing, Same Great Images!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National University of Singapore, DAMO Academy  Alibaba Group, Hupan Lab...</span>
                </div>
                <div class="primary-text">
                    This research proposes a Dynamic Diffusion Transformer (DyDiT) that dynamically adjusts its computational resources during image generation, unlike previous static approaches. DyDiT adapts its model width based on the timestep and identifies image patches where noise prediction is easier, bypassing computationally intensive blocks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">19:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03083" target="_blank">@arXiv 2410.03083</a>
                    <span class="tweet-title">Scaling Up Language Models: It's Not Just About Size, It's About Quality!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta, Iowa State University, Virginia Tech</span>
                </div>
                <div class="primary-text">
                    This research extends the traditional scaling law in language modeling by incorporating the concept of "effective training tokens," which considers data quality alongside quantity. This approach emphasizes the importance of data quality, particularly for models with constrained parameters.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">19:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03461" target="_blank">@arXiv 2410.03461</a>
                    <span class="tweet-title">Tired of LLMs Hallucinating? This New Trick Makes Them More Truthful!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tübingen, MIT, Amazon</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel unsupervised domain adaptation framework called Auto-GDA. Unlike previous methods that rely on handcrafted filtering and augmentation strategies, Auto-GDA uses an iterative process to continuously improve the quality of generated samples using weak labels from less efficient teacher models and discrete optimization to select the most promising augmented samples.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">20:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03478" target="_blank">@arXiv 2410.03478</a>
                    <span class="tweet-title">Predicting the Future of Videos: A Diffusion Transformer's Tale</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of North Carolina at Chapel Hill, Meta</span>
                </div>
                <div class="primary-text">
                    This research proposes a new framework called VEDIT, which uses a diffusion transformer to predict video representations without requiring extensive pretraining or additional language supervision. Unlike previous methods that rely on large-scale pretraining and text annotations, VEDIT leverages strong pre-trained visual encoders and operates directly in the latent embedding space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">20:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03115" target="_blank">@arXiv 2410.03115</a>
                    <span class="tweet-title">Multilingual Translation:  50 Languages, No Curse!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Johns Hopkins University, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces X-ALMA, a multilingual translation model that uses language-specific modules to prevent negative interference between languages during training. This approach differs from previous work that often suffers from performance degradation when scaling to a large number of languages.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">20:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03187" target="_blank">@arXiv 2410.03187</a>
                    <span class="tweet-title">Text-to-Motion:  Making Virtual Characters Dance to Your Words!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Beijing University of Posts and Telecommunications, National Key Lab of General AI  BIGAI</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel framework that synthesizes multi-stage human motions directly from text instructions and goal locations, eliminating the need for user-defined waypoints and stage transitions. This differs from previous work that often relies on additional inputs like object trajectories or keypoints.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">21:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03052" target="_blank">@arXiv 2410.03052</a>
                    <span class="tweet-title">Embracing the Multi-Modal World:  Optimal Transport Makes Class Hierarchies Sing!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Illinois, Stanford University, Okinawa Institute of Science and Technology Graduate University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new way to embed structured knowledge into feature representations by using Optimal Transport (OT) distances instead of the traditional Euclidean distance between class means. This approach addresses the limitation of previous work that relied on class means, which may not accurately represent multi-modal class distributions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">21:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02857" target="_blank">@arXiv 2410.02857</a>
                    <span class="tweet-title">Galaxy Clusters:  From Fuzzy Images to Sharp Mass Maps with AI!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This research uses a score-based generative model to reconstruct gas and dark matter density maps of galaxy clusters, conditioned on SZ and X-ray observations. This approach differs from previous work by learning the posterior distribution of the maps, rather than just a point estimate.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">22:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02774" target="_blank">@arXiv 2410.02774</a>
                    <span class="tweet-title">Unlocking the Secrets of Electricity Demand: A Peek Behind the Meter!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Rotterdam School of Management  Erasmus University, London Business School</span>
                </div>
                <div class="primary-text">
                    This research introduces a data-driven inverse optimization (IO) method to estimate unobservable components of electricity demand response, unlike previous work that relies on direct device-level measurements.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">22:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03432" target="_blank">@arXiv 2410.03432</a>
                    <span class="tweet-title">Danish News Recommender Dataset:  A Million Users, 37 Million Clicks, and a Whole Lotta News!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, University of Bari, Polytechnic University of Bari...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset, EB-NeRD, specifically designed for news recommendation. Unlike previous datasets, EB-NeRD is sourced directly from a traditional news publisher, Ekstra Bladet, and includes detailed information about articles, user behavior, and even user demographics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">22:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03143" target="_blank">@arXiv 2410.03143</a>
                    <span class="tweet-title">ECG to ECHO:  Turning Heartbeat Data into Ultrasound Videos!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This research proposes ECHOPulse, a model that generates echocardiogram (ECHO) videos using readily available electrocardiogram (ECG) signals as input. Unlike previous methods that rely on complex conditional prompts, ECHOPulse leverages the inherent temporal correlation between ECG and ECHO data, making it more efficient and accessible.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">23:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02829" target="_blank">@arXiv 2410.02829</a>
                    <span class="tweet-title">AI Can't Beat You at Wordle, But It Can Tell You How Hard It Is</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Adobe, Columbia University</span>
                </div>
                <div class="primary-text">
                    This research explores the use of Large Language Models (LLMs) as game testers, specifically to measure game difficulty. Unlike previous work that focused on developing AI agents for optimal gameplay, this study investigates how LLMs can be used to assess the relative difficulty of challenges within a game, comparing their performance to human players.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">23:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03058" target="_blank">@arXiv 2410.03058</a>
                    <span class="tweet-title">Cell Annotations:  DiffKillR  Slashes  Labeling  Time  with  Clever  Diffeomorphisms!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This research introduces DiffKillR, a framework that uses two neural networks to efficiently annotate cells in microscopy images.  Unlike previous methods that rely heavily on supervised learning, DiffKillR leverages a small set of annotated archetypes and applies diffeomorphisms to propagate annotations across large images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">24:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03477" target="_blank">@arXiv 2410.03477</a>
                    <span class="tweet-title">Neural Networks:  Learning One Layer is Hard, Like Really Hard!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This paper proves that learning one hidden layer neural networks under Gaussian noise is computationally hard, even when the noise is polynomially small. This is a significant improvement over previous work, which only proved hardness for adversarial noise.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">24:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03085" target="_blank">@arXiv 2410.03085</a>
                    <span class="tweet-title">Constrained Optimization:  A Semi-Supervised BNN Sandwich for Faster, More Accurate Solutions</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Los Alamos National Laboratory, MIT</span>
                </div>
                <div class="primary-text">
                    This research proposes a semi-supervised Bayesian Neural Network (BNN) approach for solving constrained optimization problems, which differs from previous work by incorporating unlabeled data through input data augmentation to ensure constraint feasibility without relying on a large number of labeled instances.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">24:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03450" target="_blank">@arXiv 2410.03450</a>
                    <span class="tweet-title">Embodied AI Gets a Memory Makeover:  Interactive Learning Makes Robots Smarter!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method for training a multimodal retriever that prioritizes the effectiveness of trajectories for embodied agents. Unlike previous work that relies on surface-level similarities, this approach uses interactive learning to fine-tune the retriever based on preference data, ensuring that the retrieved trajectories are actually helpful for completing tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">25:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03039" target="_blank">@arXiv 2410.03039</a>
                    <span class="tweet-title">Oops, You Leaked Your Data!  AI Model Reveals Your Training Images</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a new framework, FineXtract, for extracting training data from fine-tuned diffusion models. Unlike previous work that focused on general generative processes, FineXtract specifically targets the personalized data used in fine-tuning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">25:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03147" target="_blank">@arXiv 2410.03147</a>
                    <span class="tweet-title">Robots Can't Tell a Joke, But They Can Tell if You're Fake Laughing!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Kyoto University</span>
                </div>
                <div class="primary-text">
                    This study distinguishes between interactions with a tele-operated robot and an autonomous dialogue system by analyzing user spoken behaviors, a novel approach compared to previous research.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon" title="Play from here">25:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03311" target="_blank">@arXiv 2410.03311</a>
                    <span class="tweet-title">Million-Motion Mania:  Building a Giant Model for Human Movement!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces MotionBase, a massive dataset containing over a million motion sequences, significantly larger than previous datasets. It also proposes a novel 2D lookup-free motion quantization method that improves the representation of motion data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon" title="Play from here">26:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03569" target="_blank">@arXiv 2410.03569</a>
                    <span class="tweet-title">Modular Math Made Easy: Transformers Learn to Add at Scale!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">FAIR  Meta, Duke University</span>
                </div>
                <div class="primary-text">
                    This research focuses on training machine learning models to perform modular addition, a fundamental operation in cryptography. Unlike previous work that struggled with adding more than six elements modulo a small number, this paper proposes new techniques that enable models to add up to 256 elements modulo 3329, significantly expanding the capabilities of ML in this domain.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon" title="Play from here">26:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02788" target="_blank">@arXiv 2410.02788</a>
                    <span class="tweet-title">MoCap Mayhem:  A New Solver for Unlabeled Motion Capture Data</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Zhejiang University, Tencent Games, Tencent Technology (Shenzhen) Co.  LTD...</span>
                </div>
                <div class="primary-text">
                    This research introduces RoMo, a framework that uses a divide-and-conquer strategy to label and solve full-body motion capture data. Unlike previous methods that rely on pre-labeled data or single-frame information, RoMo leverages temporal continuity by generating marker tracklets using a K-partite graph-based clustering algorithm.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet67">
            <div class="start-time-icon" title="Play from here">26:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03600" target="_blank">@arXiv 2410.03600</a>
                    <span class="tweet-title">Watermarking LLMs: Finding the Needle in the Text Haystack</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley, Zhejiang University, UC San Diego...</span>
                </div>
                <div class="primary-text">
                    This research focuses on identifying watermarked segments within longer, mixed-source texts, unlike previous work that primarily focused on classifying entire documents as watermarked or not.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet68">
            <div class="start-time-icon" title="Play from here">27:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03303" target="_blank">@arXiv 2410.03303</a>
                    <span class="tweet-title">Embodied AI Learns to Think for Itself: No Humans Needed!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel self-learning paradigm for embodied multi-modal large language models (MLLMs) that doesn't rely on external feedback like human annotations or environmental rewards. It uses an actor-critic framework, where the critic evaluates the actor's actions and provides feedback to improve its decision-making.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet69">
            <div class="start-time-icon" title="Play from here">27:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03168" target="_blank">@arXiv 2410.03168</a>
                    <span class="tweet-title">Can You Tell If a Chatbot's Got a Secret? New Research Uncovers Watermarked LLMs!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Beijing University of Posts and Telecommunications, University of Illinois at Chicago...</span>
                </div>
                <div class="primary-text">
                    This research focuses on the imperceptibility of watermarked LLMs, investigating whether users can detect the presence of watermarks through crafted prompts. Previous work primarily focused on improving watermark detectability and robustness, neglecting the potential for users to identify watermarked LLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet70">
            <div class="start-time-icon" title="Play from here">28:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03011" target="_blank">@arXiv 2410.03011</a>
                    <span class="tweet-title">Transformers: Predicting the Future, One Token at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École normale supérieure, CNRS</span>
                </div>
                <div class="primary-text">
                    This research explores the ability of Transformers to predict the next token in a sequence generated by a context-dependent function, focusing on specific instances like linear and periodic sequences. Unlike previous work, it introduces a causal kernel descent method that can be implemented with a Transformer model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet71">
            <div class="start-time-icon" title="Play from here">28:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03645" target="_blank">@arXiv 2410.03645</a>
                    <span class="tweet-title">Robots Learn New Tricks: LLMs Power Up Simulation Data Generation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, UCSD, Shanghai Jiao Tong University...</span>
                </div>
                <div class="primary-text">
                    This research introduces GenSim2, a framework that uses multi-modal and reasoning LLMs to generate complex robotic simulation tasks and demonstrations at scale. Unlike previous work, GenSim2 focuses on generating long-horizon tasks with articulated objects, requiring more sophisticated planning and reasoning capabilities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet72">
            <div class="start-time-icon" title="Play from here">29:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02789" target="_blank">@arXiv 2410.02789</a>
                    <span class="tweet-title">Wall Switches and Cameras: AI Learns Your Room's Preferences</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research proposes a new architecture for building automation called Logic-Free Building Automation (LFBA) that uses deep learning to control room facilities without relying on predefined logic or reinforcement learning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet73">
            <div class="start-time-icon" title="Play from here">29:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03464" target="_blank">@arXiv 2410.03464</a>
                    <span class="tweet-title">S7:  State-Space Models Get a  Memory Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Zurich, ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This paper introduces S7, a state-space model that incorporates input-dependent dynamics, allowing it to dynamically adjust state transitions based on input content. This differs from previous models like S4 and S5, which lacked this adaptive capability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet74">
            <div class="start-time-icon" title="Play from here">30:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02828" target="_blank">@arXiv 2410.02828</a>
                    <span class="tweet-title">AI Red Teaming:  PyRIT  Helps  GenAI  Models  Pass  the  Safety  Test</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces PyRIT, an open-source framework specifically designed for red teaming generative AI systems. Unlike existing tools, PyRIT focuses on the unique challenges of assessing the security of multimodal GenAI models, offering a model- and platform-agnostic approach.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet75">
            <div class="start-time-icon" title="Play from here">30:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03094" target="_blank">@arXiv 2410.03094</a>
                    <span class="tweet-title">Entangled Brains: Quantum Computers Learn Faster Than Ever!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, California Institute of Technology, Hefei National Laboratory...</span>
                </div>
                <div class="primary-text">
                    This research establishes a noise-robust, unconditional quantum learning advantage in terms of expressivity, inference speed, and training efficiency, compared to commonly-used classical models. The key difference is that the quantum model leverages entanglement to reduce the communication required for non-local machine learning tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet76">
            <div class="start-time-icon" title="Play from here">30:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03439" target="_blank">@arXiv 2410.03439</a>
                    <span class="tweet-title">LLMs Learn to Use Tools:  No More Googling for Answers!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Mohamed bin Zayed University of Artificial Intelligence, Microsoft, UC Los Angeles</span>
                </div>
                <div class="primary-text">
                    This research introduces ToolGen, a framework that integrates tool knowledge directly into the LLM's parameters, allowing it to generate tool calls and arguments as part of its next token prediction capabilities. This differs from previous methods that rely on separate retrieval mechanisms and inputting tool descriptions as context.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet77">
            <div class="start-time-icon" title="Play from here">31:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02898" target="_blank">@arXiv 2410.02898</a>
                    <span class="tweet-title">Deep Learning Makes Robots Stay Put: A New Approach to Reach-Avoid-Stay Problems</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">North Carolina State University, UC Berkeley, Honda Research Institute</span>
                </div>
                <div class="primary-text">
                    This research introduces a two-step deep deterministic policy gradient (DDPG) method for solving reach-avoid-stay (RAS) problems. Unlike previous methods, this approach leverages robust viability kernel analysis to identify the maximal control invariant set within the target set, ensuring the system stays within the target after reaching it.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet78">
            <div class="start-time-icon" title="Play from here">31:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03555" target="_blank">@arXiv 2410.03555</a>
                    <span class="tweet-title">Seeing Around Corners: Robots Get X-Ray Vision with Single-Photon LiDAR</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, University of Wisconsin-Madison, Dartmouth College</span>
                </div>
                <div class="primary-text">
                    This research explores the use of multi-bounce LiDAR for autonomous navigation, specifically focusing on using single-photon LiDAR to "see" around corners and detect hidden objects. This differs from previous work that primarily focused on 3D imaging with NLOS LiDAR.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet79">
            <div class="start-time-icon" title="Play from here">32:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02924" target="_blank">@arXiv 2410.02924</a>
                    <span class="tweet-title">Can AI Read Your Mind? New Research Uses Language to Fix Depth Perception in Images</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method called RSA (Resolving Scale Ambiguities) that uses language descriptions to infer the scale of 3D scenes in monocular depth estimation. Unlike previous methods that rely on training data with specific scales or require ground truth for scaling, RSA leverages the semantic information in text captions to transform relative depth predictions into metric-scaled depth maps.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet80">
            <div class="start-time-icon" title="Play from here">32:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02786" target="_blank">@arXiv 2410.02786</a>
                    <span class="tweet-title">Symmetry Detection:  Langevin Dynamics Makes It Rain (and It's Not Just for Images!)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research applies Langevin dynamics, a technique typically used in generative models, to the problem of symmetry detection. This approach differs from previous methods by directly walking on the transformation manifold, which is a space representing all possible symmetries, and using the score function to guide the search for symmetries.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet81">
            <div class="start-time-icon" title="Play from here">33:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03072" target="_blank">@arXiv 2410.03072</a>
                    <span class="tweet-title">Robots Learn to Dance: Diffusion Models Choreograph Multi-Robot Motion Planning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a method for generating collision-free multi-robot trajectories using diffusion models, but instead of learning from complex multi-robot data, it leverages single-robot data and combines it with classical search-based techniques.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet82">
            <div class="start-time-icon" title="Play from here">33:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03463" target="_blank">@arXiv 2410.03463</a>
                    <span class="tweet-title">Diffusion Models Get a Reality Check: New Method Keeps Them Grounded in Data</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Caltech</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new method called Diffusion State-Guided Projected Gradient (DiffStateGrad) to improve the performance of diffusion models in solving inverse problems. Unlike previous approaches that rely on approximations or projections to the measurement likelihood, DiffStateGrad projects the measurement gradient onto a subspace defined by the intermediate state of the diffusion process. This helps to ensure that the diffusion process stays closer to the data manifold, leading to more accurate and robust reconstructions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet83">
            <div class="start-time-icon" title="Play from here">33:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02874" target="_blank">@arXiv 2410.02874</a>
                    <span class="tweet-title">Robot Chef Learns to Cook from Recipes, But Can It Handle a Burnt Pot?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research proposes a robot system that combines Large Language Models (LLMs) and classical planning with food state recognition using Vision-Language Models (VLMs) to enable robots to cook from new recipes in real-world environments. This differs from previous work by integrating these technologies to create a more comprehensive and adaptable cooking system.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet84">
            <div class="start-time-icon" title="Play from here">34:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02914" target="_blank">@arXiv 2410.02914</a>
                    <span class="tweet-title">Conformal Retrieval: Shrinking Search Results Without Losing the Treasure!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google, Technion – Israel Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This paper introduces a score refinement method that modifies retrieval scores before applying conformal prediction. This approach aims to reduce the size of the retrieved sets while maintaining the statistical guarantees of conformal prediction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet85">
            <div class="start-time-icon" title="Play from here">34:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03334" target="_blank">@arXiv 2410.03334</a>
                    <span class="tweet-title">X-Ray Vision:  AI Decodes Medical Images for Better Reports</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UCL, Microsoft Research, Independent Researcher</span>
                </div>
                <div class="primary-text">
                    This research uses sparse autoencoders (SAEs) to break down image representations into human-interpretable features, which are then used to generate radiology reports. This approach differs from previous work that relies on fine-tuning large language models (LLMs) on image-text pairs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet86">
            <div class="start-time-icon" title="Play from here">35:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03462" target="_blank">@arXiv 2410.03462</a>
                    <span class="tweet-title">Transformers Get Topological:  Graph Random Features Make Attention Scalable</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge, Google, Alan Turing Institute...</span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel approach to topological masking in transformers, using graph random features (GRFs) to approximate the mask function. This allows for efficient masking of low-rank attention, achieving O(N) time and space complexity, unlike previous methods that were limited to specific graph structures or had higher complexity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet87">
            <div class="start-time-icon" title="Play from here">35:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03208" target="_blank">@arXiv 2410.03208</a>
                    <span class="tweet-title">SPHINX:  Unveiling Hidden Connections with a Hypergraph Sleuth</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This paper introduces SPHINX, a model that learns to infer a latent hypergraph structure from data without needing explicit higher-order annotations. Unlike previous methods that rely on pairwise connections or require heavy regularization, SPHINX uses a sequential, differentiable clustering approach and k-subset sampling to produce a discrete hypergraph structure.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet88">
            <div class="start-time-icon" title="Play from here">35:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03437" target="_blank">@arXiv 2410.03437</a>
                    <span class="tweet-title">Solving PDEs with a Zebra: In-Context Learning for Parametric Equations</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sorbonne University</span>
                </div>
                <div class="primary-text">
                    This research introduces Zebra, a generative autoregressive transformer that solves parametric PDEs without requiring gradient adaptation at inference. Unlike previous methods that rely on fine-tuning or meta-learning, Zebra leverages in-context learning during both pre-training and inference, allowing it to dynamically adapt to new tasks by conditioning on input sequences that incorporate context trajectories or preceding states.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet89">
            <div class="start-time-icon" title="Play from here">36:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02921" target="_blank">@arXiv 2410.02921</a>
                    <span class="tweet-title">Air-Writing AI: Can Machines Decode Our Hand-Drawn Alphabet?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, Qualcomm, MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset called AirLetters, which consists of videos of people drawing letters and numbers in the air. Unlike existing datasets, AirLetters focuses on understanding complex, articulated motions, requiring models to analyze long-range information in the video over time.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet90">
            <div class="start-time-icon" title="Play from here">36:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02980" target="_blank">@arXiv 2410.02980</a>
                    <span class="tweet-title">Deep Learning's New Diet: Deciding When to Train for Maximum Accuracy, Minimum Calories!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research proposes DecTrain, an algorithm that decides when to train a monocular depth DNN online based on the predicted accuracy gain and the cost of training. Unlike previous work that either trains continuously or not at all, DecTrain strategically chooses when to train, aiming to optimize accuracy while minimizing computational cost.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet91">
            <div class="start-time-icon" title="Play from here">36:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02830" target="_blank">@arXiv 2410.02830</a>
                    <span class="tweet-title">YouTube Colonoscopy Prep:  A Deep Dive into Patient Engagement</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of California  Irvine, Arizona State University, Michigan State University...</span>
                </div>
                <div class="primary-text">
                    This research goes beyond simply analyzing YouTube videos for medical information. It develops a data analysis pipeline that uses machine learning to identify videos that are both medically accurate and easy for patients to understand.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet92">
            <div class="start-time-icon" title="Play from here">37:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02825" target="_blank">@arXiv 2410.02825</a>
                    <span class="tweet-title">LLMs Get Grounded:  How RAG Stops AI from Making Stuff Up!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to reduce hallucinations in LLMs by continually pre-training them with a privacy-specific knowledge base and augmenting them with a semantic RAG layer. This differs from previous work by focusing on a specific domain (privacy) and combining continual pre-training with a semantic RAG layer.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet93">
            <div class="start-time-icon" title="Play from here">37:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.02976" target="_blank">@arXiv 2410.02976</a>
                    <span class="tweet-title">Spacecraft Trajectory Design:  Diffusion Models Learn to Navigate the Cosmos</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Princeton University</span>
                </div>
                <div class="primary-text">
                    This research extends previous work by applying diffusion models to learn the structure of optimal control solutions for two specific global search problems in the Circular Restricted Three-Body Problem (CR3BP).  The study focuses on a hybrid cost function and a variable terminal boundary condition, demonstrating the model's ability to capture complex structures in the solutions space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet94">
            <div class="start-time-icon" title="Play from here">38:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03070" target="_blank">@arXiv 2410.03070</a>
                    <span class="tweet-title">Missing Modalities? No Problem! FedMAC to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">VinUniversity, Washington State University, École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research tackles the issue of partial-modality missing in federated learning, a scenario where only parts of certain features are missing in client datasets. Unlike previous work that focused on complete modality missing, FedMAC introduces a novel client-side architecture that imputes missing modalities by leveraging intra-modal information and inter-modal correlations.
                </div>
            </div>
        </div>

        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202410071649_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>    
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>
</html>