
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY</div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">Fresh Picks: 
                    <span class="highlightNumber" style="font-size: 28px;">261</span> out of <span
                    class="highlightNumber">959</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-10-10"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>


        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">01:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.07071" target="_blank">@arXiv 2410.07071</a>
                    <span class="tweet-title">Decision Transformers Get a Memory Boost:  Remembering the Past to Conquer the Future!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">JKU Linz, ELLIS Unit, LIT AI Lab...</span>
                </div>
                <div class="primary-text">
                    This research introduces Retrieval-Augmented Decision Transformers (RA-DT), which use an external memory to store past experiences and retrieve only relevant sub-trajectories for the current situation. This differs from previous in-context RL methods that require entire episodes in the agent's context, making them less efficient for complex environments with long episodes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">01:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05774" target="_blank">@arXiv 2410.05774</a>
                    <span class="tweet-title">ActionAtlas:  Sports Moves So Tricky, Even AI Can't Tell 'Em Apart!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington</span>
                </div>
                <div class="primary-text">
                    This research introduces ActionAtlas, a new video question-answering benchmark that focuses on recognizing complex, domain-specific actions, particularly in sports. Unlike previous benchmarks that often rely on single frames or simple actions, ActionAtlas challenges models to understand subtle movements and nuances across multiple frames.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">02:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06303" target="_blank">@arXiv 2410.06303</a>
                    <span class="tweet-title">AI Learns to Imagine New Things: A Recipe for Zero-Shot Compositional Generalization</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new method called Compositional Risk Minimization (CRM) for training classifiers that can generalize to unseen combinations of attributes. Unlike previous work that focuses on generative models, CRM tackles discriminative tasks, where the goal is to predict attributes based on novel combinations of features.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">02:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05880" target="_blank">@arXiv 2410.05880</a>
                    <span class="tweet-title">Privacy-Preserving Optimization:  Shrinking Datasets, Expanding Possibilities!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Weizmann Institute of Science, University of Washington</span>
                </div>
                <div class="primary-text">
                    This research introduces new algorithms for differentially private optimization of non-smooth, non-convex functions. The key innovation lies in using variance reduction techniques to improve the sensitivity of gradient estimators, leading to a reduction in the required dataset size. This contrasts with previous work that relied on a less efficient approach.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">02:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06689" target="_blank">@arXiv 2410.06689</a>
                    <span class="tweet-title">Point Cloud Quality:  A Bitstream-Based Peek Inside the Compression!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Qingdao University, Shandong University, Peking University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new model for assessing the quality of 3D point clouds compressed using the Trisoup-Lifting method. Unlike previous models that require full decoding, this one analyzes the compressed bitstream directly, making it faster and more efficient.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">03:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06424" target="_blank">@arXiv 2410.06424</a>
                    <span class="tweet-title">VQ-VAEs Get a Spin: The Rotation Trick for Better Compression</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Google DeepMind</span>
                </div>
                <div class="primary-text">
                    This research proposes a new way to propagate gradients through the vector quantization layer in VQ-VAEs. Unlike the commonly used Straight-Through Estimator (STE), the "rotation trick" preserves the angle between the codebook vector and the gradient, leading to more nuanced updates for encoder outputs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">03:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06479" target="_blank">@arXiv 2410.06479</a>
                    <span class="tweet-title">Shrinking Super Brains: How to Slim Down LLMs Without Losing Their Smarts</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Freiburg, ELLIS Institute TÃ¼bingen, Bosch Center for Artificial Intelligence...</span>
                </div>
                <div class="primary-text">
                    This research extends two-stage Neural Architecture Search (NAS) to compress large language models (LLMs) by proposing a novel sampling strategy and incorporating parameter-efficient fine-tuning methods. Unlike previous work, this approach focuses on structured pruning, aiming to identify optimal sparsity blocks for inference improvements.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">04:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05362" target="_blank">@arXiv 2410.05362</a>
                    <span class="tweet-title">LLMs Learn From Rewards: A New Kind of In-Context Learning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Cornell University, Ãcole Polytechnique FÃ©dÃ©rale de Lausanne, Harvard University</span>
                </div>
                <div class="primary-text">
                    This research explores whether large language models (LLMs) can learn new tasks through in-context reinforcement learning (ICRL), where the model receives rewards for its predictions instead of being given correct labels. This differs from previous work on in-context learning, which primarily focused on supervised learning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">04:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05222" target="_blank">@arXiv 2410.05222</a>
                    <span class="tweet-title">Tiny Data, Big Insights: How to Accurately Measure LLMs with Just a Few Examples</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Amazon, UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research introduces an empirical Bayes (EB) estimator for precisely measuring the performance of large language models (LLMs) on specific topics, even when only a small amount of data is available. This approach combines the strengths of direct estimation and synthetic regression, offering more accurate estimates than either method alone.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">05:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06234" target="_blank">@arXiv 2410.06234</a>
                    <span class="tweet-title">Chatting with Satellites: A New AI Can Talk About Earth's Changing Landscape</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces TEOChat, a vision-language model that can understand and respond to questions about temporal sequences of Earth observation data, unlike previous models that only handle single images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">05:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05629" target="_blank">@arXiv 2410.05629</a>
                    <span class="tweet-title">LLMs Learn to Speak the Language of Numbers:  A New Way to Teach AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC San Diego, Microsoft Research</span>
                </div>
                <div class="primary-text">
                    This research explores whether LLMs can perform in-context learning (ICL) directly on continuous vectors, a capability that could dramatically expand their applicability.  Previous work has focused on ICL with textual data, but this paper investigates the use of continuous vectors from diverse domains, such as sensor readings, financial time series, or scientific measurements.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">05:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05258" target="_blank">@arXiv 2410.05258</a>
                    <span class="tweet-title">Attention Noise?  DIFF Transformer to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft Research, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new attention mechanism called "differential attention" for Transformer models. It calculates attention scores as the difference between two separate softmax attention maps, effectively canceling out noise and promoting sparse attention patterns. This approach differs from previous work by directly addressing the issue of irrelevant context in Transformer models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">06:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04638" target="_blank">@arXiv 2410.04638</a>
                    <span class="tweet-title">Weak Teachers, Strong Students: How Bad Advice Can Lead to Great Results</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This paper investigates a new paradigm called "weak-to-strong generalization" where a weaker model provides imperfect labels to train a stronger model. Unlike traditional teacher-student models, the teacher here is not necessarily better than the student.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">06:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04660" target="_blank">@arXiv 2410.04660</a>
                    <span class="tweet-title">AI Doctor's New Trick:  Knowledge Graph for Smarter Medical Answers</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, University of Illinois Chicago, Imperial College London...</span>
                </div>
                <div class="primary-text">
                    This research introduces KGAREVION, an agent that combines the strengths of large language models (LLMs) with knowledge graphs (KGs) to improve medical question answering. Unlike previous approaches that rely solely on LLMs or KGs, KGAREVION uses a multi-step process to generate, verify, and revise knowledge, leading to more accurate and reliable answers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">06:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06961" target="_blank">@arXiv 2410.06961</a>
                    <span class="tweet-title">LLMs Learn to Teach Themselves: A Self-Boosting AI Revolution</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Microsoft Research</span>
                </div>
                <div class="primary-text">
                    This research introduces SynPO, a self-boosting paradigm for LLM alignment that leverages synthetic preference data. Unlike previous methods that rely on static, pre-collected preference datasets, SynPO iteratively generates new data, enabling LLMs to continuously improve their capabilities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">07:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04739" target="_blank">@arXiv 2410.04739</a>
                    <span class="tweet-title">TableRAG:  Million-Token Tables? No Problem!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces TableRAG, a framework that uses retrieval-augmented generation (RAG) to efficiently understand large tables. Unlike previous methods that process entire tables, TableRAG focuses on retrieving only the most relevant information, significantly reducing the computational cost and token usage.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">07:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05281" target="_blank">@arXiv 2410.05281</a>
                    <span class="tweet-title">AI-Powered Micrometer:  Predicting Material Behavior with a Transformer Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University, Imperial College London, University of Pennsylvania</span>
                </div>
                <div class="primary-text">
                    This research introduces Micrometer, a transformer-based AI model for predicting the mechanical response of heterogeneous materials. Unlike previous work, Micrometer learns the solution operator of the Lippmann-Schwinger equation, enabling efficient evaluation of mechanical responses across varying microstructures and material properties.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">08:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06238" target="_blank">@arXiv 2410.06238</a>
                    <span class="tweet-title">LLMs Learn to Explore:  Teaching AI to Make Better Decisions</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Google</span>
                </div>
                <div class="primary-text">
                    This research focuses on evaluating and improving the ability of large language models (LLMs) to make optimal decisions in uncertain environments, specifically within the framework of multi-armed bandits and contextual bandits. Unlike previous work that primarily focused on LLMs as predictors, this study explores their decision-making capabilities in scenarios where exploration is crucial for maximizing rewards.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">08:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06317" target="_blank">@arXiv 2410.06317</a>
                    <span class="tweet-title">Q-Learning Goes Deep:  Action-Value Methods Get a Policy Gradient Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Riot Games, Spotify, Google DeepMind</span>
                </div>
                <div class="primary-text">
                    This paper challenges the conventional wisdom that policy gradient methods are better suited for complex action spaces than action-value methods. It identifies three core principles that underpin the success of policy gradients and shows how these principles can be adapted to action-value methods, enabling them to achieve similar performance without relying on policy gradients.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">09:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05292" target="_blank">@arXiv 2410.05292</a>
                    <span class="tweet-title">LLMs Go Flow:  Turning Text into Data with Volterra Magic!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This paper introduces CaLMFlow, a novel framework that uses causal language models (CLMs) to solve Volterra integral equations (VIEs) for flow matching. This approach differs from previous work by leveraging the power of LLMs for continuous data generation, bridging the gap between discrete language modeling and continuous generative modeling.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">09:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06300" target="_blank">@arXiv 2410.06300</a>
                    <span class="tweet-title">Shapley Values Get a Fourier Makeover: Faster Explanations for AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for computing SHAP values, a popular feature attribution method in explainable AI. Unlike previous methods that rely on stochastic sampling or expensive optimization, this approach leverages the sparse Fourier representation of the model to efficiently compute SHAP values.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">09:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05451" target="_blank">@arXiv 2410.05451</a>
                    <span class="tweet-title">LLMs:  No More Prompt Hijacking!  Alignment to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley, Meta</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to defending LLMs against prompt injection attacks by leveraging alignment training. Unlike previous defenses that focus on fine-tuning or prompting, this method constructs a preference dataset containing both desirable and undesirable outputs, effectively teaching the LLM to prioritize the intended instruction even in the presence of malicious prompts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">10:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05026" target="_blank">@arXiv 2410.05026</a>
                    <span class="tweet-title">Robot Learning Gets a Helping Hand: Active Fine-Tuning for Smarter Bots!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research proposes a new algorithm called AMF (Active Multi-task Fine-tuning) for efficiently fine-tuning pre-trained robot policies. Unlike previous work that relies on uniform sampling of tasks for demonstrations, AMF actively selects the most informative tasks to demonstrate, maximizing the information gain about the expert policy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">10:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05553" target="_blank">@arXiv 2410.05553</a>
                    <span class="tweet-title">NMT Models Learn to Follow Instructions:  A Recipe for Customized Translations</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research explores instruction finetuning for Neural Machine Translation (NMT) models, a technique typically used with Large Language Models (LLMs). The paper presents a recipe for adapting NMT models to follow instructions, enabling them to perform various translation-specific tasks, such as formality control or multi-modal translation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">10:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05090" target="_blank">@arXiv 2410.05090</a>
                    <span class="tweet-title">HyperINF:  Schulz's Method Gets a Superpower Boost for Data Influence Estimation!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Ãcole Polytechnique FÃ©dÃ©rale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research proposes HYPERINF, a new method for approximating influence functions. Unlike previous methods like LISSA and DATAINF, HYPERINF leverages the hyperpower method, specifically Schulz's iterative algorithm, to achieve more accurate and stable results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">11:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04996" target="_blank">@arXiv 2410.04996</a>
                    <span class="tweet-title">Unmeasured Confounders? No Problem! New Method Uses Negative Controls for Robust Inference</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University</span>
                </div>
                <div class="primary-text">
                    This research introduces a post-integrated inference method that leverages negative control outcomes to adjust for latent heterogeneity in data integration. Unlike previous methods, this approach is assumption-lean and robust to model misspecification, making it suitable for a wider range of applications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">11:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05078" target="_blank">@arXiv 2410.05078</a>
                    <span class="tweet-title">Tiny Transformers: Compressing Data Like a Boss!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research investigates the compression capabilities of small pre-trained transformers, unlike previous work that focused on large foundation models. It also examines the impact of multimodal training on compression performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">11:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06490" target="_blank">@arXiv 2410.06490</a>
                    <span class="tweet-title">FedL2G:  Learning to Guide Local Training in Federated Learning -  A New Way to Train Models Without Sharing Your Data!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Shanghai Jiao Tong University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method called FedL2G for heterogeneous federated learning (HtFL). Unlike previous methods that rely on sharing prototypes, FedL2G learns to guide local training by updating guiding vectors based on client feedback, ensuring that the guidance benefits the original local task.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">12:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04707" target="_blank">@arXiv 2410.04707</a>
                    <span class="tweet-title">Making LLMs Think Hard: When to Give Your AI More Brainpower</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces a method for adaptively allocating computational resources to language models (LLMs) during inference. Unlike previous work that uses the same decoding procedure for all inputs, this approach predicts the difficulty of each input and allocates more resources to those that are harder to process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">12:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06718" target="_blank">@arXiv 2410.06718</a>
                    <span class="tweet-title">Matryoshka Meets Mamba:  A State Space Model That's Big on Adaptability</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington</span>
                </div>
                <div class="primary-text">
                    This paper introduces MatMamba, a state space model that incorporates a nested Matryoshka structure. Unlike previous work that focused on applying Matryoshka to specific components of models, MatMamba applies it to the entire Mamba2 block, enabling the extraction of numerous submodels without additional training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">12:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06084" target="_blank">@arXiv 2410.06084</a>
                    <span class="tweet-title">Music AI Gets a Diversity Boost: No More Same Old Tunes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel finetuning strategy for generative models that combines distillation and reinforcement learning to improve the quality-diversity trade-off. Unlike previous work that focuses on training a population of models with diverse abilities, this approach optimizes the diversity of generations from a single model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">13:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05954" target="_blank">@arXiv 2410.05954</a>
                    <span class="tweet-title">Video Generation Gets a Pyramid Makeover:  Less Noise, More Efficiency!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel video generative modeling framework called "pyramidal flow matching." Unlike previous cascaded approaches that train separate models for different resolutions, this method uses a unified model to simultaneously generate and decompress visual content across pyramid stages.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">13:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05437" target="_blank">@arXiv 2410.05437</a>
                    <span class="tweet-title">LLMs on a Diet:  Shrinking Language Models Without Losing Their Smarts!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nvidia</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new technique for compressing large language models (LLMs) by focusing on reducing the dimensionality of activations, rather than compressing the weights. This approach allows for retraining without losing expressivity, unlike previous weight-centric methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">14:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06333" target="_blank">@arXiv 2410.06333</a>
                    <span class="tweet-title">Bayesian Optimization Gets a Batch of New Tricks:  Maximizing the Odds of Finding the Best Molecule!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research proposes a new acquisition strategy for batched Bayesian optimization called qPO (multipoint Probability of Optimality). Unlike previous methods that balance exploration and exploitation, qPO focuses solely on exploitation, maximizing the probability that the selected batch contains the true optimum.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">14:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05639" target="_blank">@arXiv 2410.05639</a>
                    <span class="tweet-title">LLMs Need a Makeover:  DecorateLM Gives Training Data a Glamorous Upgrade</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Modelbest Inc, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces DecorateLM, a data engineering method that refines the pretraining corpus for LLMs through a three-phase process: rating, tagging, and editing. Unlike previous work that focuses on data annotation or selection, DecorateLM aims to enhance the quality of existing data by adding metadata, improving structure, and ensuring relevance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">15:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05295" target="_blank">@arXiv 2410.05295</a>
                    <span class="tweet-title">AI Jailbreakers Go Rogue:  LLMs Learn to Break Themselves</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Wisconsin-Madison</span>
                </div>
                <div class="primary-text">
                    This research introduces AutoDAN-Turbo, a method that uses lifelong learning agents to automatically discover and combine jailbreak strategies for LLMs. Unlike previous work that relies on human-designed strategies or optimization algorithms, AutoDAN-Turbo autonomously explores and evolves its attack techniques.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">15:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06698" target="_blank">@arXiv 2410.06698</a>
                    <span class="tweet-title">Penguins, Fourier, and Event Cameras: A New Way to See Wildlife Behavior</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Technische UniversitÃ¤t Berlin, Robotics Institute Germany, Science of Intelligence Excellence Cluster...</span>
                </div>
                <div class="primary-text">
                    This research uses the Fourier Transform to analyze data from event cameras, which are novel sensors that record changes in light intensity instead of images. This approach is different from previous work that relies on deep neural networks and image-like representations of event data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">15:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06209" target="_blank">@arXiv 2410.06209</a>
                    <span class="tweet-title">Theorem Proving Goes Lifelong: AI Learns Math Like a Pro</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Caltech, Stanford University, University of Wisconsin-Madison</span>
                </div>
                <div class="primary-text">
                    This research introduces LeanAgent, a lifelong learning framework for theorem proving. Unlike previous approaches that train on static datasets, LeanAgent continuously learns from new mathematical repositories without forgetting previously learned knowledge.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">16:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04618" target="_blank">@arXiv 2410.04618</a>
                    <span class="tweet-title">Blind Face Restoration:  No More Pixelated Past!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Samsung AI Center Toronto, University of Toronto, Vector Institute for AI</span>
                </div>
                <div class="primary-text">
                    This paper proposes an unsupervised approach to fine-tune pre-trained face restoration models on unseen degradations, using a diffusion model to generate pseudo targets without relying on paired ground-truth images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">16:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04870" target="_blank">@arXiv 2410.04870</a>
                    <span class="tweet-title">SignGD:  The Transformer's  Fast  Learner,  Slow  Thinker</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research analyzes the training dynamics of Sign Gradient Descent (SignGD) when optimizing a two-layer transformer, a task that has been challenging to study theoretically. It differs from previous work by focusing on SignGD, a simpler surrogate for Adam, and by analyzing a transformer with trainable query-key parameterization, a more realistic setting than previous studies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">17:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06205" target="_blank">@arXiv 2410.06205</a>
                    <span class="tweet-title">RoPE's Secret: LLMs Are Spinning a Yarn, Not Just Decaying!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford, Google DeepMind</span>
                </div>
                <div class="primary-text">
                    This research challenges the common belief that Rotary Positional Encodings (RoPE) are primarily useful for decaying attention weights with distance. It instead proposes that RoPE's different frequencies are used for distinct purposes, with high frequencies constructing positional attention patterns and low frequencies carrying semantic information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">17:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05563" target="_blank">@arXiv 2410.05563</a>
                    <span class="tweet-title">LLMs Learn to Think Before They Answer: A Metareasoning Revolution!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Princeton University, EPFL, Anthropic</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to training LLMs that incorporates the concept of "rational metareasoning," enabling them to adaptively adjust their reasoning process based on task complexity. Unlike previous methods that either reduce cost at the expense of performance or vice versa, this approach aims to optimize the trade-off between these two factors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">17:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05146" target="_blank">@arXiv 2410.05146</a>
                    <span class="tweet-title">Speech Translation Gets a Speed Boost:  CTC-GMM Makes MT Data Work Harder!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces CTC-GMM, a method that uses Connectionist Temporal Classification (CTC) to compress speech sequences, allowing the model to leverage machine translation (MT) data for improved streaming speech translation (ST) performance. This approach differs from previous work by directly incorporating MT data into the ST model, rather than relying on pseudo labels or separate text-to-speech systems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">18:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.07064" target="_blank">@arXiv 2410.07064</a>
                    <span class="tweet-title">Data Selection for Language Models:  A Control Freak's Guide to Better AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research formulates data selection for language models as an Optimal Control problem, a mathematical framework that allows for a more precise analysis of how data influences model training. This approach differs from previous methods that rely on heuristics or single-checkpoint performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">18:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05188" target="_blank">@arXiv 2410.05188</a>
                    <span class="tweet-title">Networks Get a Matrix Makeover:  Multidimensional Dynamics Take Center Stage!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nordita, Stockholm University, KTH Royal Institute of Technology...</span>
                </div>
                <div class="primary-text">
                    This research introduces matrix-weighted networks (MWNs), a new framework for modeling multidimensional interactions in complex systems. Unlike traditional networks that use scalar edge weights, MWNs use matrices to represent the complex relationships between nodes, allowing for a more nuanced understanding of how signals propagate through the network.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">19:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05589" target="_blank">@arXiv 2410.05589</a>
                    <span class="tweet-title">Parallel Decoding:  LLMs Get a Speed Boost with a Multi-Token Drafting Trick</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Rice University, Tencent AI Lab, University of Illinois Urbana-Champaign</span>
                </div>
                <div class="primary-text">
                    This research introduces PARALLELSPEC, a new method for speculative decoding that uses a single model to predict multiple future tokens in parallel during the drafting stage. This differs from previous approaches that draft tokens auto-regressively, which can be computationally expensive.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">19:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.07069" target="_blank">@arXiv 2410.07069</a>
                    <span class="tweet-title">LLMs as Graders:  A Meta-Evaluation of Instruction-Following Tests</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This research goes beyond evaluating individual LLMs' ability to follow instructions. It focuses on the evaluation methods themselves, testing how well different protocols work across a wide range of LLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">20:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05248" target="_blank">@arXiv 2410.05248</a>
                    <span class="tweet-title">LLMs:  From  Clueless  to  Confident  with  a  Mixup  Recipe!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research proposes SFTMix, a novel instruction-tuning recipe for LLMs that leverages training dynamics to identify data subsets with varying confidence levels and incorporates a Mixup-based regularization to improve performance. Unlike previous work that focuses on improving the quality of instruction-tuning datasets, SFTMix aims to optimize data utilization through insightful data interpretation and enhance the effectiveness of instruction tuning beyond the conventional NTP paradigm.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">20:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06560" target="_blank">@arXiv 2410.06560</a>
                    <span class="tweet-title">Weather Forecasting Gets a Physics-Fueled Makeover:  A Sandwich of Neural Networks for More Accurate Predictions!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Alibaba</span>
                </div>
                <div class="primary-text">
                    This research introduces WeatherODE, a novel one-stage, physics-driven ordinary differential equation (ODE) model for weather forecasting. Unlike previous methods that rely on fixed time intervals, WeatherODE leverages wave equation theory and a time-dependent source model to address the challenges associated with time-discretization error and dynamic atmospheric processes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">20:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05514" target="_blank">@arXiv 2410.05514</a>
                    <span class="tweet-title">Mapping the World, One Fuzzy Object at a Time: Diffusion Models Make 3D Reconstruction a Breeze</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for object-level mapping that leverages a pre-trained diffusion model as a shape prior. Unlike previous methods that rely on dense observations or single-category models, this approach can reconstruct 3D objects from sparse views and handle multiple object categories.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">21:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06949" target="_blank">@arXiv 2410.06949</a>
                    <span class="tweet-title">LLMs Get a Helping Hand: Multi-Agent Framework Makes Code More Robust</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ByteDance, Tsinghua University, Beihang University</span>
                </div>
                <div class="primary-text">
                    This research proposes a multi-agent framework called Seeker to enhance exception handling in code generated by LLMs. Unlike previous work that focuses on general code generation, Seeker specifically addresses the challenges of detecting, capturing, and handling exceptions in code.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">21:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04619" target="_blank">@arXiv 2410.04619</a>
                    <span class="tweet-title">Social Media's Secret Currency: Likes, Shares, and the Invisible Hand of Influence</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Hong Kong, University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research distinguishes itself by formally modeling social support as a currency within a content market, exploring its impact on market equilibrium and social welfare. It also examines the dual role of influencers as content aggregators and information proxies for social support, a facet not previously explored in depth.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">22:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06231" target="_blank">@arXiv 2410.06231</a>
                    <span class="tweet-title">RelitLRM:  Relighting 3D Objects with a Dash of Diffusion Magic!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Massachusetts Institute of Technology, Stanford University, Cornell University...</span>
                </div>
                <div class="primary-text">
                    This research introduces RelitLRM, a generative model that reconstructs relightable 3D objects from sparse images. Unlike previous methods that rely on dense captures and slow optimization, RelitLRM uses a transformer-based architecture with a diffusion-based appearance generator, enabling efficient and high-quality relighting.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">22:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05352" target="_blank">@arXiv 2410.05352</a>
                    <span class="tweet-title">Multimodal Continual Learning:  When AI Gets a Memory Upgrade!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Chinese University of Hong Kong, Tsinghua University, University of Illinois at Chicago</span>
                </div>
                <div class="primary-text">
                    This research paper presents the first comprehensive survey on multimodal continual learning (MMCL), a field that focuses on training AI models to learn from new data without forgetting previous knowledge, especially when dealing with multiple data types like images, text, and audio.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">22:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06024" target="_blank">@arXiv 2410.06024</a>
                    <span class="tweet-title">Unleashing the Hidden Language of LLMs: Jet Expansion Reveals the Secrets of Deep Learning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London, MIT, Amazon</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel framework called "jet expansion" to analyze the computational graphs of neural networks, particularly deep residual networks. Unlike previous methods that rely on datasets or specific assumptions about the model, jet expansion operates directly on the model's functions, enabling data-free analysis.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">23:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05690" target="_blank">@arXiv 2410.05690</a>
                    <span class="tweet-title">Long-Context Learning:  No Mixing, Just Magic!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">EPFL, Inria, ENS</span>
                </div>
                <div class="primary-text">
                    This paper tackles the problem of identifying linear systems with long-term dependencies, extending previous work that focused solely on first-order dependencies. It establishes a sample complexity bound that matches the i.i.d. parametric rate, revealing a "learning-without-mixing" phenomenon.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">23:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04541" target="_blank">@arXiv 2410.04541</a>
                    <span class="tweet-title">LLMs: Function Modeling Wizards or Data Dummies? A Bayesian Perspective</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge, Microsoft, The Alan Turing Institute</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel evaluation framework for assessing LLMs' function modeling capabilities. It separates the model's ability to understand raw data patterns from its ability to incorporate domain knowledge, providing a more nuanced understanding of LLM performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">24:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05255" target="_blank">@arXiv 2410.05255</a>
                    <span class="tweet-title">Diffusion Models Get a Preference Makeover: No Reward Models Needed!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Rochester, Purdue University, Yonsei University...</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method called Semi-Policy Preference Optimization (SePPO) for aligning diffusion models with human preferences. Unlike previous methods that rely on reward models or paired human-annotated data, SePPO leverages previous checkpoints as reference models to generate on-policy reference samples, effectively replacing "losing images" in preference pairs. This allows the model to learn from only off-policy "winning images."
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">24:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05481" target="_blank">@arXiv 2410.05481</a>
                    <span class="tweet-title">LLMs Learn to Tag Text Like a Boss:  fPLSA Uncovers Hidden Semantic Structures</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces fPLSA, a method that uses foundation models to learn latent semantic structures in document collections by iteratively clustering and tagging document segments based on document-level contexts. This differs from previous work by incorporating document-level context into the tagging process, which allows for more nuanced and informative tags.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">25:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05613" target="_blank">@arXiv 2410.05613</a>
                    <span class="tweet-title">Chatbots Can't Hide Their Racial Biases: A Study Reveals the Shocking Truth</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research goes beyond previous work by examining how large language models (LLMs) generate recommendations based on both explicit and implicit signals of a user's race, even when the user doesn't intend to reveal their identity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">25:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06731" target="_blank">@arXiv 2410.06731</a>
                    <span class="tweet-title">Weather Forecasting Gets a Grid-Based Makeover: Transformers Tackle Unstructured Data</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to handling unstructured data in spatio-temporal modeling, specifically for weather forecasting. Unlike previous methods that rely on structured grids, this paper proposes a "pseudo-token grid encoder" that efficiently encodes unstructured data onto a grid, enabling the use of efficient attention mechanisms in transformer-based architectures.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">25:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05975" target="_blank">@arXiv 2410.05975</a>
                    <span class="tweet-title">Meta-Learning Gets a Makeover:  ConML Makes Models Think Like Humans!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Hong Kong University of Science and Technology</span>
                </div>
                <div class="primary-text">
                    This research introduces ConML, a meta-learning framework that uses task-level contrastive learning to improve model alignment and discrimination. Unlike previous work, ConML doesn't rely on specific model architectures or target models, making it universally applicable.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">26:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04810" target="_blank">@arXiv 2410.04810</a>
                    <span class="tweet-title">One-Shot Federated Learning Gets a Diffusion Makeover:  Personalized Data Synthesis for Privacy-Preserving AI!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Ludwig Maximilian University of Munich, Siemens Technology, University of Oxford...</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method called FedBiP, which personalizes a pretrained Latent Diffusion Model (LDM) to generate synthetic data that aligns with the specific data distributions of individual clients in a One-Shot Federated Learning (OSFL) setting. This approach addresses the challenges of feature space heterogeneity and limited data quantity in OSFL, which are often overlooked by existing methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">26:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04775" target="_blank">@arXiv 2410.04775</a>
                    <span class="tweet-title">Tiny Tech, Big Brains: Earbuds Get Smart with On-Device AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nokia Bell Labs</span>
                </div>
                <div class="primary-text">
                    This research introduces OmniBuds, an earable platform that integrates multiple biosensors and onboard computation powered by a machine learning accelerator, all within a real-time operating system (RTOS). Unlike conventional earables that rely on external data processing, OmniBuds leverage real-time onboard computation to significantly enhance system efficiency, reduce latency, and safeguard privacy by processing data locally.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon" title="Play from here">26:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06735" target="_blank">@arXiv 2410.06735</a>
                    <span class="tweet-title">Code-Trained LLMs:  Smarter Than They Look?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research directly compares the performance of LLMs trained solely on programming languages to those trained on natural language datasets, focusing on logical inference tasks. Previous work often used mixed datasets or didn't conduct fair comparisons.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon" title="Play from here">27:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05102" target="_blank">@arXiv 2410.05102</a>
                    <span class="tweet-title">LLMs Get a Makeover:  Sparse Token Masks for Smarter AI!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Huawei, University College London</span>
                </div>
                <div class="primary-text">
                    This research introduces SparsePO, a method that learns to selectively weight the importance of individual tokens during preference optimization, allowing for more flexible and diverse responses. Unlike previous methods that focus on the entire sequence, SparsePO focuses on specific words or phrases that are crucial for conveying the desired preference.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon" title="Play from here">27:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05534" target="_blank">@arXiv 2410.05534</a>
                    <span class="tweet-title">Tensor Optimization:  MCTS to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to optimizing tensor computation graphs using Monte Carlo Tree Search (MCTS) during the construction phase of equality saturation. This differs from previous work that relied on sequential application of rewrite rules, which could lead to suboptimal results due to the phase-ordering problem.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet67">
            <div class="start-time-icon" title="Play from here">28:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06112" target="_blank">@arXiv 2410.06112</a>
                    <span class="tweet-title">Predicting Packet Latency: Transformers to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The University of Chicago</span>
                </div>
                <div class="primary-text">
                    This research introduces SwiftQueue, a novel L4S queue selection system that uses a custom Transformer to predict packet-level latency. Unlike previous work that focuses on flow-level latency prediction, SwiftQueue aims to predict latency variations at the individual packet level.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet68">
            <div class="start-time-icon" title="Play from here">28:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06875" target="_blank">@arXiv 2410.06875</a>
                    <span class="tweet-title">Shapley Values:  The Fair Way to Explain Counterfactual Simulations</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Columbia University, University of Chicago</span>
                </div>
                <div class="primary-text">
                    This paper introduces the "group Shapley value" to interpret counterfactual simulations in structural economic models. Unlike previous work that focuses on individual parameters, this method quantifies the importance of groups of parameters, providing a more comprehensive understanding of model outcomes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet69">
            <div class="start-time-icon" title="Play from here">28:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06415" target="_blank">@arXiv 2410.06415</a>
                    <span class="tweet-title">AI's Got Opinions: How Biased Chatbots Sway Your Political Views</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington</span>
                </div>
                <div class="primary-text">
                    This study investigates the impact of partisan bias in AI language models on human political decision-making, going beyond previous research that focused on static AI-generated content or inconsequential decisions. It examines how dynamic interactions with biased AI models influence opinions and choices in a more realistic setting.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet70">
            <div class="start-time-icon" title="Play from here">29:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05260" target="_blank">@arXiv 2410.05260</a>
                    <span class="tweet-title">Text-to-Motion:  Turning Words into Walking, Talking Avatars!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH ZÃ¼rich</span>
                </div>
                <div class="primary-text">
                    This research introduces DART, a diffusion-based model that generates continuous, long-term human motion sequences in real-time, conditioned on text prompts. Unlike previous methods that focus on short, isolated motions, DART leverages motion primitives to enable online generation and control.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet71">
            <div class="start-time-icon" title="Play from here">29:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06511" target="_blank">@arXiv 2410.06511</a>
                    <span class="tweet-title">TorchTitan:  LLM Training Gets a  PyTorch-Powered  Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta</span>
                </div>
                <div class="primary-text">
                    This paper introduces TorchTitan, a PyTorch-native distributed training system that unifies and advances state-of-the-art techniques for large language model (LLM) pre-training. Unlike existing solutions, TorchTitan offers a modular and composable approach, enabling seamless integration of 3D parallelism and hardware-software co-designed solutions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet72">
            <div class="start-time-icon" title="Play from here">30:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05462" target="_blank">@arXiv 2410.05462</a>
                    <span class="tweet-title">Attention, Please! New Algorithm Makes Transformers Super-Efficient</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Indian Institute of Science, Google, CMU</span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel approach to attention mechanisms in transformers, focusing on identifying and utilizing a "universal set" of keys. This differs from previous work by not relying on sparsity constraints, low-rank assumptions, or bounded entries.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet73">
            <div class="start-time-icon" title="Play from here">30:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05143" target="_blank">@arXiv 2410.05143</a>
                    <span class="tweet-title">Sidekick for Science: Diffusion Models Get a Multimodal Makeover</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, Air Force Research Laboratory</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to accelerate imaging by leveraging side information from an auxiliary modality. Unlike previous methods that rely on differentiable forward models, this study trains a multimodal diffusion model to capture the joint distribution of different modalities, effectively turning the inverse problem into a linear inpainting problem.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet74">
            <div class="start-time-icon" title="Play from here">30:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05650" target="_blank">@arXiv 2410.05650</a>
                    <span class="tweet-title">Shape-Shifting AI:  How a New Adapter Makes Object Detection See the World Differently</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, University of Science and Technology Beijing</span>
                </div>
                <div class="primary-text">
                    This research focuses on the "image-region gap" in open-vocabulary detection (OVD), a problem where models struggle to accurately classify objects within images because of how regions are extracted. The paper proposes a Shape-Invariant Adapter (SIA) that learns to adjust region features based on their shapes, bridging this gap and improving classification accuracy. This approach differs from previous work by directly addressing the shape deformation issue rather than relying on knowledge distillation or fine-tuning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet75">
            <div class="start-time-icon" title="Play from here">31:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06494" target="_blank">@arXiv 2410.06494</a>
                    <span class="tweet-title">Conformal Prediction:  Data's New Best Friend?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Los Angeles, University of Chicago, University of Illinois at Chicago</span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive overview of conformal prediction (CP) methods from a data-centric perspective, categorizing CP approaches based on different data types, including structured, unstructured, and dynamic data. This approach differs from previous surveys that focused on theoretical foundations or specific data types.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet76">
            <div class="start-time-icon" title="Play from here">31:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04523" target="_blank">@arXiv 2410.04523</a>
                    <span class="tweet-title">Sea-Sick Helicopters?  New Plan Uses Boats to Speed Up Patient Evac!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, University of Massachusetts</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to medical evacuation in maritime environments by utilizing underway watercraft as intermediary exchange points between aircraft. This differs from previous work that focused solely on fixed land-based exchange points or direct transfers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet77">
            <div class="start-time-icon" title="Play from here">32:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05334" target="_blank">@arXiv 2410.05334</a>
                    <span class="tweet-title">Machine Learning's New Bug Zapper:  Human-in-the-Loop Testing for Adversarial Attacks</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research introduces an interactive system called TA3 for testing machine learning models against adversarial attacks. Unlike previous work that primarily relies on automated testing and statistical measures, TA3 incorporates human-in-the-loop (HITL) techniques to enable human experts to steer attack simulations and visualize the impact of attacks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet78">
            <div class="start-time-icon" title="Play from here">32:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06001" target="_blank">@arXiv 2410.06001</a>
                    <span class="tweet-title">Typing Without a Keyboard? This Wristband Makes It Possible!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH ZÃ¼rich</span>
                </div>
                <div class="primary-text">
                    This research introduces TapType, a text entry system that uses wristband sensors to detect finger taps on any surface, unlike previous methods that rely on touchscreens or cameras. TapType combines a Bayesian neural network classifier with an n-gram language model to predict the most likely character sequence.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet79">
            <div class="start-time-icon" title="Play from here">32:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05940" target="_blank">@arXiv 2410.05940</a>
                    <span class="tweet-title">Typing on Air? Nah, TouchInsight Makes Surfaces Your Keyboard!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta Reality Labs, ETH ZÃ¼rich</span>
                </div>
                <div class="primary-text">
                    This research introduces TouchInsight, a method for detecting touch input on surfaces using only egocentric hand tracking. Unlike previous work that relied on external cameras or additional sensors, TouchInsight explicitly models uncertainties in touch location due to both user behavior and sensing inaccuracies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet80">
            <div class="start-time-icon" title="Play from here">33:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05760" target="_blank">@arXiv 2410.05760</a>
                    <span class="tweet-title">Diffusion Models Get a Demon-Powered Upgrade: No Training, Just Better Images!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Academia Sinica, Google</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel inference-time method called Demon for aligning diffusion models with user preferences without requiring model retraining or backpropagation through reward functions. Unlike previous methods that rely on either retraining or differentiable reward functions, Demon guides the denoising process by controlling the noise distribution during inference.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet81">
            <div class="start-time-icon" title="Play from here">33:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05076" target="_blank">@arXiv 2410.05076</a>
                    <span class="tweet-title">LLMs Get a Speed Boost:  TidalDecode Makes Decoding a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces TidalDecode, a new approach to LLM decoding that leverages the spatial coherence of tokens selected for sparse attention. Unlike previous methods that select tokens independently at each layer, TidalDecode reuses the same set of tokens for multiple layers, reducing the overhead of token selection.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet82">
            <div class="start-time-icon" title="Play from here">33:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05356" target="_blank">@arXiv 2410.05356</a>
                    <span class="tweet-title">Bots Behaving Badly? New Research Uses Biased Subgraphs to Catch 'Em!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method called BSG4Bot that uses biased subgraphs to improve the efficiency and accuracy of social bot detection. Unlike previous methods that train on the entire graph, BSG4Bot focuses on smaller, carefully constructed subgraphs that are more likely to contain similar nodes, making it easier for the model to learn patterns.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet83">
            <div class="start-time-icon" title="Play from here">34:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05224" target="_blank">@arXiv 2410.05224</a>
                    <span class="tweet-title">Forget Humans, Teach AI with Random Words!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new framework called COOKBOOK that programmatically generates training data for LLMs using simple patterns over random tokens. This approach differs from previous work that relies on human-generated or LLM-generated data, which can be expensive and time-consuming to curate.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet84">
            <div class="start-time-icon" title="Play from here">34:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05192" target="_blank">@arXiv 2410.05192</a>
                    <span class="tweet-title">Training Language Models Like a River: Why Constant Learning Rates Can Be a Good Thing</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Toyota Technological Institute at Chicago</span>
                </div>
                <div class="primary-text">
                    This research introduces a new learning rate schedule called Warmup-Stable-Decay (WSD) that doesn't require a pre-determined compute budget. Unlike traditional cosine schedules, WSD maintains a constant learning rate for a significant portion of training before decaying, leading to a unique loss curve.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet85">
            <div class="start-time-icon" title="Play from here">35:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06128" target="_blank">@arXiv 2410.06128</a>
                    <span class="tweet-title">Zero-Shot Learning:  Causal Models Get a Mind-Reading Upgrade!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to inferring causal models in a zero-shot manner, meaning it can predict the causal relationships in new datasets without needing to be retrained specifically for them. This differs from previous work that required training a separate model for each dataset.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet86">
            <div class="start-time-icon" title="Play from here">35:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05804" target="_blank">@arXiv 2410.05804</a>
                    <span class="tweet-title">Object Detection's New Trick: Sharing Secrets with Language Models!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Peng Cheng Laboratory</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for incremental object detection that leverages shared attributes from vision-language foundation models. Unlike previous methods, it focuses on capturing common semantic information across categories, mitigating the issue of background drift.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet87">
            <div class="start-time-icon" title="Play from here">36:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05577" target="_blank">@arXiv 2410.05577</a>
                    <span class="tweet-title">Underwater Object Detection:  Deep Dive into the Murky World of AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London, Dalian University of Technology, Ocean University of China...</span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive review of AI-based underwater object detection (UOD) methods, categorizing them based on learning strategies, datasets, features, and learning stages. It also delves into the unique challenges of UOD, such as image quality degradation, small object detection, noisy labels, and class imbalance, and suggests potential solutions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet88">
            <div class="start-time-icon" title="Play from here">36:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04612" target="_blank">@arXiv 2410.04612</a>
                    <span class="tweet-title">Chatbots That Learn to Talk Like Humans: A New Approach to Multi-Turn Conversations</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Cornell University, Princeton University, CMU...</span>
                </div>
                <div class="primary-text">
                    This research introduces REFUEL, a new approach to multi-turn RLHF that addresses the covariate shift problem by iteratively generating on-policy datasets. Unlike previous methods that treat multi-turn tasks as single-turn problems, REFUEL frames the problem as a sequence of regression tasks, enabling efficient policy optimization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet89">
            <div class="start-time-icon" title="Play from here">36:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04734" target="_blank">@arXiv 2410.04734</a>
                    <span class="tweet-title">Token-Level Detective:  Unmasking Hallucinations in Vision Language Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Southern California, Meta</span>
                </div>
                <div class="primary-text">
                    This research introduces a Token-Level Detective Reward (TLDR) model, which provides fine-grained annotations for each token in a vision language model's output, unlike traditional reward models that only provide a single score for the entire response.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet90">
            <div class="start-time-icon" title="Play from here">37:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06781" target="_blank">@arXiv 2410.06781</a>
                    <span class="tweet-title">Fake It Till You Make It: Generating Realistic TEE Images with Anatomical Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research extends a previous pipeline for generating synthetic transthoracic echocardiography (TTE) images to create synthetic transesophageal echocardiography (TEE) images. The new pipeline generates 19 standard TEE views, which is a significant improvement over the previous work that only generated two views.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet91">
            <div class="start-time-icon" title="Play from here">37:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06997" target="_blank">@arXiv 2410.06997</a>
                    <span class="tweet-title">X-ray to MRI:  Turning Bones into Brains with AI!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of OrlÃ©ans, Harvard University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel diffusion-based model, Xray2MRI, that generates pseudo-MRI volumes from a single X-ray image. Unlike previous work, this model integrates target depth, KOA probability distribution, and image intensity distribution modules to guide the synthesis process, ensuring that the generated corresponding slices accurately correspond to the anatomical structures.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet92">
            <div class="start-time-icon" title="Play from here">38:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04960" target="_blank">@arXiv 2410.04960</a>
                    <span class="tweet-title">Segment Anything, Faster: A Survey of SAM's Speed Demons</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UESTC, Lancaster University, Tongji University</span>
                </div>
                <div class="primary-text">
                    This survey focuses specifically on the development of efficient Segment Anything Model (SAM) variants, a topic not addressed in previous surveys. It categorizes these variants based on their acceleration strategies and provides a comprehensive evaluation of their efficiency and accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet93">
            <div class="start-time-icon" title="Play from here">38:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05497" target="_blank">@arXiv 2410.05497</a>
                    <span class="tweet-title">QR Codes Go Hands-Free:  New Tech Makes Scanning a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta</span>
                </div>
                <div class="primary-text">
                    This research focuses on adapting QR code reading to egocentric settings, specifically for wearable devices. Unlike phone-based QR code readers, this system accounts for the unique challenges of egocentric images, such as wider field of view, code distortion, and motion blur.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet94">
            <div class="start-time-icon" title="Play from here">38:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04803" target="_blank">@arXiv 2410.04803</a>
                    <span class="tweet-title">Time Series Forecasting Gets a Long-Term Memory Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to time series forecasting called "multivariate next token prediction." Unlike previous methods that focus on individual time series, this approach considers multiple time series simultaneously, capturing complex dependencies between them.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet95">
            <div class="start-time-icon" title="Play from here">39:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05044" target="_blank">@arXiv 2410.05044</a>
                    <span class="tweet-title">Fusing 3D Models: When Robots Need a Group Hug!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces PhotoReg, a framework that uses 3D foundation models to align multiple 3D Gaussian Splatting (3DGS) models, addressing the challenge of merging these models into a single coherent representation. This approach differs from previous work by leveraging the duality between photorealistic reconstructions and 3D foundation models, enabling robust alignment even with minimal overlap between the models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet96">
            <div class="start-time-icon" title="Play from here">39:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05273" target="_blank">@arXiv 2410.05273</a>
                    <span class="tweet-title">Robot Brains Get a Speed Boost:  Hierarchical Transformers for Faster, Smarter Robots</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research proposes HiRT, a hierarchical framework for robotic control that uses a large vision-language model (VLM) to extract high-level information and a lightweight action policy to react quickly to changes in the environment. This approach differs from previous VLA models that rely solely on large VLMs for both understanding and action generation, leading to slower inference speeds.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet97">
            <div class="start-time-icon" title="Play from here">40:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06615" target="_blank">@arXiv 2410.06615</a>
                    <span class="tweet-title">Stop Saying "I'm 90% Sure!"  New Research Makes AI Confidence Scores More Reliable</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Amsterdam, Amazon, Carnegie Mellon University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new concept called "Î²-calibration" to improve the reliability of confidence scores generated by language models in question-answering systems. Unlike traditional calibration methods that focus on average accuracy, Î²-calibration ensures that confidence scores are accurate across different groups of questions and answers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet98">
            <div class="start-time-icon" title="Play from here">40:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04715" target="_blank">@arXiv 2410.04715</a>
                    <span class="tweet-title">LLMs Get a Data Diet:  Orthogonal Rules for Better Language Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel rule-based framework for selecting high-quality data for training LLMs. Unlike previous methods that rely heavily on human heuristics, this approach leverages LLMs to automatically generate and evaluate rules, using a metric based on the orthogonality of score vectors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet99">
            <div class="start-time-icon" title="Play from here">40:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06703" target="_blank">@arXiv 2410.06703</a>
                    <span class="tweet-title">Web Agents on Trial: New Benchmark Tests Their Trustworthiness</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research introduces ST-WebAgentBench, a new benchmark specifically designed to evaluate the safety and trustworthiness of web agents in enterprise contexts. Unlike previous benchmarks, ST-WebAgentBench not only focuses on task completion but also evaluates adherence to organizational policies, avoidance of unsafe actions, and the agentâs ability to maintain user trust.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet100">
            <div class="start-time-icon" title="Play from here">41:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05533" target="_blank">@arXiv 2410.05533</a>
                    <span class="tweet-title">Persuasion Without a Clue: How to Convince When You Don't Know the Odds</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, Boston University</span>
                </div>
                <div class="primary-text">
                    This paper explores repeated persuasion problems where the information designer doesn't know the prior distribution of the state of the world. Unlike previous work that considers a static setting with a worst-case prior, this paper takes an online learning approach, allowing the information designer to learn and adapt over time.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet101">
            <div class="start-time-icon" title="Play from here">41:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06572" target="_blank">@arXiv 2410.06572</a>
                    <span class="tweet-title">Deepfake Voices: Can We Tell Real From Fake?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Duke University, Google</span>
                </div>
                <div class="primary-text">
                    This research focuses on active malicious attacks against synthetic speech detectors (SSDs), unlike previous work that focused on natural perturbations. It systematically studies the effectiveness and stealthiness of these attacks under different levels of access to the SSD model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet102">
            <div class="start-time-icon" title="Play from here">41:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04738" target="_blank">@arXiv 2410.04738</a>
                    <span class="tweet-title">Diffusion Models in 3D Vision: A Wild Ride Through the Latest Research</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tokyo Institute of Technology, University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive survey of diffusion models applied to 3D vision tasks, highlighting their advantages and limitations, and discussing key techniques, frameworks, and methodologies used to adapt diffusion models for 3D data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet103">
            <div class="start-time-icon" title="Play from here">42:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06541" target="_blank">@arXiv 2410.06541</a>
                    <span class="tweet-title">"Chip-Tuning:  Pruning LLMs Before They Get Too Big for Their Britches"</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Tencent</span>
                </div>
                <div class="primary-text">
                    This research proposes a new structured pruning method called "chip-tuning" that focuses on training probing classifiers on specific layers of LLMs for classification tasks. Unlike previous methods that aim for general pruning, chip-tuning targets task-specific pruning, allowing for more aggressive pruning while maintaining performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet104">
            <div class="start-time-icon" title="Play from here">42:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06795" target="_blank">@arXiv 2410.06795</a>
                    <span class="tweet-title">Vision Models Hallucinating?  Virtual Tokens to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Chinese Academy of Sciences, Renmin University of China, Tsinghua University...</span>
                </div>
                <div class="primary-text">
                    This research focuses on the root cause of object hallucinations in vision-language models (VLMs), proposing that the issue stems from inadequate decoupling of visual features during multi-modal integration. The paper introduces a novel tuning strategy called PATCH, which utilizes trainable virtual tokens to bridge the semantic gap between visual features and text, effectively mitigating hallucinations. This approach differs from previous work by directly addressing the feature decoupling problem, rather than solely focusing on data distribution, training schemes, or decoding strategies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet105">
            <div class="start-time-icon" title="Play from here">43:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04520" target="_blank">@arXiv 2410.04520</a>
                    <span class="tweet-title">Neural Ensemblers:  Dropping Models for Better Predictions</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Freiburg</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel post-hoc ensembling method using neural networks that dynamically assigns weights to base models on a per-instance basis. Unlike traditional approaches, it introduces a regularization technique that randomly drops base model predictions during training, which theoretically enhances ensemble diversity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet106">
            <div class="start-time-icon" title="Play from here">43:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05159" target="_blank">@arXiv 2410.05159</a>
                    <span class="tweet-title">Privacy Panic! New Tool Tests How Easily Hackers Can Steal Your Data</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harbin Institute of Technology, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces MIBench, the first comprehensive benchmark for model inversion attacks and defenses. Unlike previous work, MIBench provides a unified framework for comparing different attack methods and defense strategies under standardized experimental setups.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet107">
            <div class="start-time-icon" title="Play from here">43:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05814" target="_blank">@arXiv 2410.05814</a>
                    <span class="tweet-title">Model Inversion Attacks:  They're Trying to Steal Your Face!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harbin Institute of Technology, Tsinghua University, South China University of Technology</span>
                </div>
                <div class="primary-text">
                    This research delves into the inherent weaknesses of Model Inversion Attacks (MIAs) and proposes a new defense framework called CALoR. Unlike previous defenses, CALoR integrates Confidence Adaptation and Low-Rank compression strategies to comprehensively address the vulnerabilities of MIAs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet108">
            <div class="start-time-icon" title="Play from here">44:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06307" target="_blank">@arXiv 2410.06307</a>
                    <span class="tweet-title">Restless Bandits:  MPC is Almost Optimal,  But Don't Get Too Excited!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">French Institute for Research in Computer Science and Automation</span>
                </div>
                <div class="primary-text">
                    This paper explores the use of Model Predictive Control (MPC) for restless bandits, a type of problem where multiple tasks compete for limited resources. Unlike previous work that focused on steering the system towards an optimal fixed point, this research leverages the concept of dissipativity to show that a simple MPC algorithm with a finite planning horizon can achieve near-optimal performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet109">
            <div class="start-time-icon" title="Play from here">44:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04772" target="_blank">@arXiv 2410.04772</a>
                    <span class="tweet-title">AI Audits:  Peek Inside the Black Box, But Don't Break It!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, MIT</span>
                </div>
                <div class="primary-text">
                    This research focuses on the type and amount of access auditors need to effectively evaluate AI systems. It proposes that black-box access, where auditors can query the model without seeing its internal workings, is minimally necessary for meaningful audits.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet110">
            <div class="start-time-icon" title="Play from here">45:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06213" target="_blank">@arXiv 2410.06213</a>
                    <span class="tweet-title">Don't Do Anything I Mightn't Do: Why KL Regularization Fails in RL</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley, Montreal Institute for Learning Algorithms</span>
                </div>
                <div class="primary-text">
                    This paper explores the limitations of KL regularization when used to control the behavior of reinforcement learning (RL) agents trained on a Bayesian imitative base policy. It demonstrates that even with a tight KL constraint, the agent can still exploit the base policy's uncertainty to achieve near-maximal reward, potentially leading to undesirable outcomes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet111">
            <div class="start-time-icon" title="Play from here">45:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06809" target="_blank">@arXiv 2410.06809</a>
                    <span class="tweet-title">LLMs Get a Safety Upgrade: Decoding-Level Defense to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Chinese Academy of Sciences, Renmin University of China, Tsinghua University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel defense mechanism called RDS (Root Defense Strategy) that operates at the decoding level of LLMs, focusing on the model's ability to recognize harmful outputs during the generation process. Unlike previous methods that rely on prefill-level analysis or single-point evaluation, RDS assesses the safety of each token as it's generated, allowing for more accurate and proactive security measures.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet112">
            <div class="start-time-icon" title="Play from here">46:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05425" target="_blank">@arXiv 2410.05425</a>
                    <span class="tweet-title">AI Designs Tiny Fire Detectors for Space!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Antwerp, imec</span>
                </div>
                <div class="primary-text">
                    This research uses a reinforcement learning-based Neural Architecture Search (NAS) agent to design a small neural network for active fire detection from multispectral satellite imagery. This approach differs from previous work by considering both the network's performance and its computational resource requirements.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet113">
            <div class="start-time-icon" title="Play from here">46:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05499" target="_blank">@arXiv 2410.05499</a>
                    <span class="tweet-title">Unitary Graph Convolutions:  Wave Goodbye to Over-Smoothing!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This paper introduces unitary graph convolutions, a new type of operation that uses unitary matrices to enhance the stability of graph neural networks. Unlike previous methods, this approach avoids over-smoothing, a common problem in deep graph networks where node representations converge too quickly, hindering performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet114">
            <div class="start-time-icon" title="Play from here">46:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05665" target="_blank">@arXiv 2410.05665</a>
                    <span class="tweet-title">Satellite Images:  From Space to Speed,  Edge Computing to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a hybrid edge-cloud collaborative architecture for satellite image analysis, where lightweight models on the satellite initially identify potential man-made structures, reducing the amount of data transmitted to the cloud for further processing. This approach differs from traditional "bent-pipe" methods by leveraging edge computing to optimize data transmission and processing time.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet115">
            <div class="start-time-icon" title="Play from here">47:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04972" target="_blank">@arXiv 2410.04972</a>
                    <span class="tweet-title">Color Me Creative: New AI Lets You Paint Videos With Words!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Beijing University of Posts and Telecommunications, Peking University, Beijing Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This research introduces a language-based video colorization framework that uses user-provided text descriptions to guide the colorization process, unlike previous methods that rely on exemplars or post-processing.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet116">
            <div class="start-time-icon" title="Play from here">47:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04634" target="_blank">@arXiv 2410.04634</a>
                    <span class="tweet-title">AI Art's Dirty Little Secret: Unmasking the Hidden Biases in Text-to-Image Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This research proposes a framework called Concept2Concept to audit text-to-image models by analyzing the distribution of concepts in generated images. Unlike previous work that focuses on specific biases, this framework allows for a more nuanced and comprehensive analysis of concept associations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet117">
            <div class="start-time-icon" title="Play from here">47:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05564" target="_blank">@arXiv 2410.05564</a>
                    <span class="tweet-title">Unsupervised Learning:  How to Teach AI to See the World Like We Do</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Caltech, Harvard University, University of Amsterdam</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for unsupervised representation learning from sequence data by factorizing transformations of latent variables into sparse components. Unlike previous work, this approach does not require supervision of input sequences and leverages the Helmholtz decomposition for more expressive latent flows.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet118">
            <div class="start-time-icon" title="Play from here">48:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06203" target="_blank">@arXiv 2410.06203</a>
                    <span class="tweet-title">LLMs Learn to Plan Before They Write: A Single-Turn Approach to Long-Form Text Generation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google, Emory University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to long-form text generation by directly fine-tuning LLMs to generate the entire document in a single pass. Unlike previous methods that rely on sequential prompting, this approach leverages the token-level attention mechanism in the decoding process, ensuring coherence and consistency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet119">
            <div class="start-time-icon" title="Play from here">48:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06118" target="_blank">@arXiv 2410.06118</a>
                    <span class="tweet-title">Multilingual Machine Translation:  Learning to Learn the Right Way!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">HEIG-VD, HES-SO, Pi School...</span>
                </div>
                <div class="primary-text">
                    This research explores using reinforcement learning to optimize the training schedule of multilingual neural machine translation (NMT) systems. Unlike previous work that focuses on static sampling strategies, this study proposes two algorithms, Teacher-Student Curriculum Learning (TSCL) and Deep Q Network (DQN), that dynamically adjust the order of language presentation during training based on the model's performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet120">
            <div class="start-time-icon" title="Play from here">49:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05522" target="_blank">@arXiv 2410.05522</a>
                    <span class="tweet-title">Mesh-ing with AI: Predicting Stress Fields on Any Shape!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a method to predict scalar fields on arbitrary meshes using a convolutional neural network (CNN) with multi-resolution interpolation. Unlike previous work that relies on fixed grid domains, this approach allows for predictions on complex shapes with varying topology.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet121">
            <div class="start-time-icon" title="Play from here">49:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06845" target="_blank">@arXiv 2410.06845</a>
                    <span class="tweet-title">AI Therapist in Training:  Language Model Learns to Diagnose and Treat Mental Health</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Illinois Urbana-Champaign, Microsoft Research Asia, Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces MentalArena, a self-play framework for training language models to diagnose and treat mental health disorders. Unlike previous methods that rely on existing datasets or social media data, MentalArena generates its own personalized data through simulated patient-therapist interactions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet122">
            <div class="start-time-icon" title="Play from here">49:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05298" target="_blank">@arXiv 2410.05298</a>
                    <span class="tweet-title">LLMs:  Graph Pattern Detectives,  But Can They Crack the Code?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Michigan State University, Hong Kong Polytechnic University, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces a comprehensive benchmark to evaluate the capabilities of large language models (LLMs) in understanding and discovering graph patterns, a crucial aspect of graph mining that has been largely unexplored.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet123">
            <div class="start-time-icon" title="Play from here">50:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04659" target="_blank">@arXiv 2410.04659</a>
                    <span class="tweet-title">AI's Got Zoomies: New Benchmark Tests If LLMs Can See the Big Picture</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Fudan University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark called ActiView, which specifically evaluates the active perception abilities of Multimodal Large Language Models (MLLMs). Unlike previous benchmarks that focus on static image comprehension, ActiView assesses how well LLMs can actively shift their "view" or zoom in on specific details within an image to answer questions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet124">
            <div class="start-time-icon" title="Play from here">50:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06153" target="_blank">@arXiv 2410.06153</a>
                    <span class="tweet-title">LLM Agents:  Lego for Brains, Assemble!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This paper proposes a modular design space for LLM agents, allowing researchers to easily combine and reuse existing modules, rather than starting from scratch for each new task.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet125">
            <div class="start-time-icon" title="Play from here">50:58</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06366" target="_blank">@arXiv 2410.06366</a>
                    <span class="tweet-title">Time-Reversal Symmetry: The Secret Sauce for Accurate Dynamical System Modeling</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Los Angeles, Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel regularization term that enforces Time-Reversal Symmetry (TRS) within a GraphODE model. Unlike previous work that focuses on specific physical priors like energy conservation, this approach leverages TRS as a more general numerical benefit, improving accuracy across a wider range of dynamical systems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet126">
            <div class="start-time-icon" title="Play from here">51:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06405" target="_blank">@arXiv 2410.06405</a>
                    <span class="tweet-title">Vision Transformers Get a Spatial Makeover:  Solving Abstract Reasoning with 2D Encodings</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research explores the use of Vision Transformers (ViT) for solving abstract visual reasoning tasks in the Abstraction and Reasoning Corpus (ARC). Unlike previous work that focused on program synthesis or language-based approaches, this paper investigates the potential of ViT for directly generating output images from input images. The authors propose a novel architecture called ViTARC, which incorporates 2D positional encodings and object-based positional encodings to enhance the spatial reasoning capabilities of ViT.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet127">
            <div class="start-time-icon" title="Play from here">51:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05357" target="_blank">@arXiv 2410.05357</a>
                    <span class="tweet-title">LLM Scaling:  From One to Many,  A Recipe for a Model Zoo!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UNC CH, UMD, UT Austin...</span>
                </div>
                <div class="primary-text">
                    This research introduces Model-GLUE, a framework for scaling LLMs by combining pre-trained models. Unlike previous work that focuses on merging models with similar architectures, Model-GLUE explores both merging and mixture techniques to handle diverse model collections.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet128">
            <div class="start-time-icon" title="Play from here">52:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06483" target="_blank">@arXiv 2410.06483</a>
                    <span class="tweet-title">Predicting Diabetic Eye Trouble: A Deep Learning Ensemble Takes on the Challenge!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Birmingham, University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research uses an ensemble of deep learning models to predict the onset of diabetic macular edema (DME) within a year using ultra-wide-field color fundus photography (UWF-CFP) images. This approach differs from previous studies that focused on diagnosing existing DME.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet129">
            <div class="start-time-icon" title="Play from here">52:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04988" target="_blank">@arXiv 2410.04988</a>
                    <span class="tweet-title">Reinforcement Learning Gets a Boost:  Optimistic Thompson Sampling for Smarter Robots!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to optimistic exploration in model-based reinforcement learning. Unlike previous methods, it leverages a joint model of the reward and state distributions, allowing for more informed and efficient exploration.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet130">
            <div class="start-time-icon" title="Play from here">52:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06407" target="_blank">@arXiv 2410.06407</a>
                    <span class="tweet-title">Skewness-Based Causal Discovery: A New Trick for Noisy Data</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC San Diego, New York University, The University of Melbourne...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel criterion for causal discovery in the presence of heteroscedastic noise, which is a common problem in real-world data. Unlike previous methods that rely on noise extraction or specific noise distribution assumptions, this approach leverages the skewness of the score function to identify causal directions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet131">
            <div class="start-time-icon" title="Play from here">53:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04642" target="_blank">@arXiv 2410.04642</a>
                    <span class="tweet-title">Tuning Up Deep Learning: How Much Feature Learning is Too Much?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This research investigates the effect of a hyperparameter called "gamma" on the performance of neural networks trained online. Unlike previous work that focused on offline training, this study explores the online setting, where data is not repeated.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet132">
            <div class="start-time-icon" title="Play from here">53:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06331" target="_blank">@arXiv 2410.06331</a>
                    <span class="tweet-title">Deep Dive: Editing LLMs for Multi-Hop Reasoning, It's Not Just About the Surface!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">King Abdullah University of Science and Technology, Peking University, South China University of Technology</span>
                </div>
                <div class="primary-text">
                    This research delves into the limitations of existing knowledge editing methods for multi-hop factual recall tasks. It identifies that these methods primarily focus on editing shallow layers of the model, neglecting deeper layers that are crucial for multi-hop reasoning. The paper proposes a novel approach, IFMET, which addresses this limitation by editing both shallow and deep layers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet133">
            <div class="start-time-icon" title="Play from here">54:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06488" target="_blank">@arXiv 2410.06488</a>
                    <span class="tweet-title">Font-tastic! One-Step Chinese Font Synthesis with Super-Resolution</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for few-shot Chinese font synthesis that uses a diffusion model with component-aware conditioning. Unlike previous methods that rely on GANs or pixel-space diffusion models, this approach operates in the latent space, leading to faster training and inference times.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet134">
            <div class="start-time-icon" title="Play from here">54:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06473" target="_blank">@arXiv 2410.06473</a>
                    <span class="tweet-title">Robot Brains Get a Language Boost: Talking to Machines Makes Them Smarter!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, Bosch</span>
                </div>
                <div class="primary-text">
                    This research proposes a multi-agent framework that uses language models to guide robot policies, improving their performance without requiring additional human demonstrations or extensive training. This differs from previous work that often relies on fine-tuning models or handcrafted code interfaces.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet135">
            <div class="start-time-icon" title="Play from here">54:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06497" target="_blank">@arXiv 2410.06497</a>
                    <span class="tweet-title">ERCache:  Don't  Infer,  Just  Cache  It!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta Platforms  Inc.</span>
                </div>
                <div class="primary-text">
                    This research investigates the necessity of performing model inference for every ad request in large-scale social networks, a question overlooked in previous work. It proposes ERCache, a caching framework that leverages user access patterns to reduce the number of inference requests.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet136">
            <div class="start-time-icon" title="Play from here">55:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04534" target="_blank">@arXiv 2410.04534</a>
                    <span class="tweet-title">Music, Motion, and Text: A Unified Model for Multimodal Generation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The Chinese University of Hong Kong, University of Washington, The University of British Columbia...</span>
                </div>
                <div class="primary-text">
                    This research introduces UniMuMo, a unified multimodal model that can generate music, motion, and text in any combination. Unlike previous work that focused on unidirectional generation tasks, UniMuMo leverages a novel music-motion parallel generation scheme to enable simultaneous generation of music and motion.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet137">
            <div class="start-time-icon" title="Play from here">55:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05063" target="_blank">@arXiv 2410.05063</a>
                    <span class="tweet-title">AI Learns to Push Like a Pro: Neural Collapse in Robot Control</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, ETH ZÃ¼rich</span>
                </div>
                <div class="primary-text">
                    This research investigates the phenomenon of neural collapse in the context of image-based robot control, a task that involves continuous control outputs rather than discrete classifications. It proposes a novel "control-oriented" classification strategy to study the clustering of visual features in the representation space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet138">
            <div class="start-time-icon" title="Play from here">55:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06667" target="_blank">@arXiv 2410.06667</a>
                    <span class="tweet-title">LLMs: From Code Writers to Code Runners, Can They Handle the Pressure?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Mohamed bin Zayed University of Artificial Intelligence, Xinjiang University, University of Melbourne...</span>
                </div>
                <div class="primary-text">
                    This research explores the use of LLMs as code executors, going beyond code generation to directly execute code snippets and return outputs. This differs from previous work that primarily focused on code generation or understanding.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet139">
            <div class="start-time-icon" title="Play from here">56:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05773" target="_blank">@arXiv 2410.05773</a>
                    <span class="tweet-title">Remote Sensing Retrieval:  When Simple Samples Just Don't Cut It!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new metric learning approach called GLRTML that incorporates global statistical information during both training and testing phases. Unlike previous methods that rely on local sample relationships, GLRTML leverages the entire dataset to estimate the difficulty of sample pairs, guiding the network to focus on more challenging examples.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet140">
            <div class="start-time-icon" title="Play from here">56:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05935" target="_blank">@arXiv 2410.05935</a>
                    <span class="tweet-title">Manga Faces: One-Shot Detection with a Dash of Gaussian Noise!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research focuses on one-shot object detection in manga, a task not previously explored. It proposes a novel data augmentation method in feature space using Gaussian noise with learnable variances to address the challenges of large variations and long-tailed distributions of manga characters.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet141">
            <div class="start-time-icon" title="Play from here">56:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06191" target="_blank">@arXiv 2410.06191</a>
                    <span class="tweet-title">Neural Networks: Overfitting Without the Guilt Trip!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research analyzes the generalization of two-layer ReLU networks trained by gradient flow in the NTK regime. Unlike previous work, it does not require the regression function to belong to the RKHS of the NTK.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet142">
            <div class="start-time-icon" title="Play from here">57:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.07072" target="_blank">@arXiv 2410.07072</a>
                    <span class="tweet-title">AI for 5G:  Teaching Neural Networks Wireless Etiquette</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Virginia Tech, Massachusetts Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for configuring the weights of recurrent neural networks (RNNs) used in wireless communication systems. Unlike previous approaches that rely solely on training data, this method incorporates domain knowledge, such as channel statistics, to directly set the weights.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet143">
            <div class="start-time-icon" title="Play from here">57:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06340" target="_blank">@arXiv 2410.06340</a>
                    <span class="tweet-title">FedGraph:  Graph Learning's New Playground for Privacy-Preserving Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, University of Illinois at Chicago, University of Southern California...</span>
                </div>
                <div class="primary-text">
                    This research introduces FedGraph, a library specifically designed for federated graph learning, addressing the unique challenges of training graph neural networks on distributed data while preserving user privacy. Unlike existing federated learning libraries, FedGraph focuses on GNN training and supports various state-of-the-art methods, including communication between clients and a central server for model updates and information aggregation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet144">
            <div class="start-time-icon" title="Play from here">57:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05448" target="_blank">@arXiv 2410.05448</a>
                    <span class="tweet-title">Multitasking Makes Language Models Learn Faster:  A Surprising Twist on In-Context Learning!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, Seoul National University, KRAFTON...</span>
                </div>
                <div class="primary-text">
                    This research investigates the impact of task diversity on the training process of language models performing in-context learning. Unlike previous studies that focused on single-task learning, this paper demonstrates that training on multiple diverse tasks simultaneously shortens the learning plateaus, making each task easier to learn.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet145">
            <div class="start-time-icon" title="Play from here">58:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06682" target="_blank">@arXiv 2410.06682</a>
                    <span class="tweet-title">Video Captions Get a Makeover:  LLM Learns to See and Hear!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel multi-round directed preference optimization (mrDPO) approach for training multimodal LLMs for video captioning. Unlike previous work that uses a single round of DPO, mrDPO periodically updates the reference model and incorporates guidance from ground-truth captions to stabilize the training process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet146">
            <div class="start-time-icon" title="Play from here">58:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05626" target="_blank">@arXiv 2410.05626</a>
                    <span class="tweet-title">Neural Networks:  Random Initialization's Hidden Curse?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research investigates the impact of standard random initialization on the generalization ability of neural networks within the Neural Tangent Kernel (NTK) theory. Unlike previous work that focused on mirrored initialization, this study explores the implications of non-zero initial output functions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet147">
            <div class="start-time-icon" title="Play from here">59:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04818" target="_blank">@arXiv 2410.04818</a>
                    <span class="tweet-title">Power Grids Get Smart: AI Solves the AC-OPF Problem in a Flash!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, International School for Advanced Studies</span>
                </div>
                <div class="primary-text">
                    This research introduces PINCO, a physics-informed graph neural network that solves the AC-OPF problem without violating constraints. Unlike previous work, PINCO is unsupervised and can handle power systems with multiple generators per bus.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet148">
            <div class="start-time-icon" title="Play from here">59:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.06886" target="_blank">@arXiv 2410.06886</a>
                    <span class="tweet-title">LLMs Get a Filter:  Say Goodbye to Distracting Documents!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel approach called FltLM, which integrates a context filter with a Long-Context LLM. Unlike previous methods that rely on retrieval-based techniques, FltLM identifies and dynamically excludes irrelevant content within the input context, allowing the model to focus on pertinent information for better comprehension and reasoning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet149">
            <div class="start-time-icon" title="Play from here">59:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.05116" target="_blank">@arXiv 2410.05116</a>
                    <span class="tweet-title">Stable Diffusion Gets a Human Touch:  New AI Learns From Your Feedback, Not Just Data!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sony AI, National Taiwan University, University of Southern California...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework called HERO that fine-tunes Stable Diffusion models using online human feedback. Unlike previous methods that rely on predefined reward functions or pretrained reward models, HERO learns from human feedback collected on the fly during model training.
                </div>
            </div>
        </div>

        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202410101956_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>    
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>
</html>