<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                    Fresh Picks:
                    <span class="highlightNumber">43</span> out of <span class="highlightNumber">192</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-07-29"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>

        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">
                00:55
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18637" target="_blank">
                        @arXiv 2407.18637
                    </a>
                    <span class="tweet-title">
                        Head-Body Tracking: The Secret to Gigapixel Crowds?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces DynamicTrack, a framework that uses both head and body features for
                    tracking in gigapixel images, unlike previous methods that rely solely on body features.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">
                01:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18682" target="_blank">
                        @arXiv 2407.18682
                    </a>
                    <span class="tweet-title">
                        Faster Than a Speeding Bullet: A New Tool for Super-Speedy Object Annotation
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new annotation tool that leverages a pre-trained object detection
                    model
                    (CenterNet) to speed up the process of labeling objects in videos. Unlike previous methods that
                    rely
                    on manual annotation of every frame, this tool uses the model's predictions to automatically
                    label
                    frames, reducing the time required for annotation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">
                01:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18370" target="_blank">
                        @arXiv 2407.18370
                    </a>
                    <span class="tweet-title">
                        LLMs as Judges: Trust the Model, But Only When It's Confident!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington, Allen Institute for Artificial Intelligence
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to LLM-based evaluation by incorporating a confidence
                    measure to determine when to trust the model's judgment. This differs from previous work that
                    relies
                    solely on the model's preference without considering its reliability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">
                02:07
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18695" target="_blank">
                        @arXiv 2407.18695
                    </a>
                    <span class="tweet-title">
                        New Dataset for Computer Vision: PIV3CAMS is the Real Deal!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich, EPFL
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset called PIV3CAMS, which is unique because it includes
                    paired
                    images and videos captured from three different cameras, including a smartphone, a DSLR, and a
                    3D
                    camera. This is different from previous datasets that typically focus on single-camera data or
                    specific applications like autonomous driving.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">
                02:33
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18471" target="_blank">
                        @arXiv 2407.18471
                    </a>
                    <span class="tweet-title">
                        CORD-19 Gets a Vaccine Boost: A New Dataset for COVID-19 Research
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset, CORD-19-Vaccination, specifically tailored for COVID-19
                    vaccine research. It's extracted from the existing CORD-19 dataset but includes additional
                    metadata
                    like language details, author demographics, keywords, and topic information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">
                02:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18322" target="_blank">
                        @arXiv 2407.18322
                    </a>
                    <span class="tweet-title">
                        AI in Pharmacovigilance: Guardrails for LLMs to Stop Hallucinating!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University, MIT, GlaxoSmithKline
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on developing "guardrails" for LLMs specifically designed to mitigate the
                    risk
                    of hallucinations in the pharmacovigilance domain. This is different from previous work that has
                    primarily focused on general LLM safety or on guardrails for other applications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">
                03:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18899" target="_blank">
                        @arXiv 2407.18899
                    </a>
                    <span class="tweet-title">
                        Learning from the Learnt: A Source-Free Active Domain Adaptation Trick
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This paper explores a new paradigm for Source-Free Active Domain Adaptation (SFADA), where the
                    source data is unavailable during adaptation, and a limited annotation budget is available for
                    the
                    target domain. The key innovation lies in the "learn from the learnt" (LFTL) approach, which
                    leverages knowledge from the source-pretrained model and actively iterated models without extra
                    overhead.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">
                03:45
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18290" target="_blank">
                        @arXiv 2407.18290
                    </a>
                    <span class="tweet-title">
                        Visual Signal Decomposition: The Secret Sauce for Better Image Generation?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft Research Asia
                    </span>
                </div>
                <div class="primary-text">
                    This paper doesn't propose a new algorithm but instead focuses on the importance of visual
                    signal
                    decomposition, arguing that it's the key to unlocking better image generation. It highlights the
                    challenges of applying existing decomposition techniques to visual data, particularly the lack
                    of
                    "equivariance" compared to language models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">
                04:07
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18418" target="_blank">
                        @arXiv 2407.18418
                    </a>
                    <span class="tweet-title">
                        LLMs Say "No" to BS: A Framework for Refusal in AI Assistants
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research goes beyond the typical focus on question-answering and examines abstention in a
                    broader chatbot system framework. It proposes a new framework for analyzing abstention behavior
                    from
                    three perspectives: the query, the model, and human values.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">
                04:30
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18908" target="_blank">
                        @arXiv 2407.18908
                    </a>
                    <span class="tweet-title">
                        Wolf: The Video Captioning Superpower That's Smarter Than GPT-4V!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        NVIDIA, UC Berkeley, MIT...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Wolf, a novel video captioning framework that utilizes a
                    mixture-of-experts
                    approach, leveraging complementary strengths of Vision Language Models (VLMs). Unlike previous
                    works
                    that rely on a single model, Wolf combines multiple models to generate more accurate and
                    detailed
                    captions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">
                05:01
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18564" target="_blank">
                        @arXiv 2407.18564
                    </a>
                    <span class="tweet-title">
                        Graph Privacy: It's Not Just About Your Friends, It's About Your Friends' Friends!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Fudan University, Peking University, Zhejiang University
                    </span>
                </div>
                <div class="primary-text">
                    This research goes beyond the usual focus on direct connections in networks and explores how the
                    broader structure of relationships can reveal private information. It introduces a new measure
                    called the Generalized Homophily Ratio to quantify these privacy risks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">
                05:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18624" target="_blank">
                        @arXiv 2407.18624
                    </a>
                    <span class="tweet-title">
                        Labeling Labels: A Dual-Decoupling Approach to Semi-Supervised Multi-Label Learning
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nanjing University of Aeronautics and Astronautics, University of Tokyo
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a dual-decoupling learning framework for semi-supervised multi-label
                    learning. Unlike previous methods that focus on capturing class proportions, this approach aims
                    to
                    improve the quality of model predictions by decoupling correlative and discriminative features,
                    as
                    well as the generation and utilization of pseudo-labels.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">
                06:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18392" target="_blank">
                        @arXiv 2407.18392
                    </a>
                    <span class="tweet-title">
                        Facial Editing Gets a 3D Makeover: Say Goodbye to Flat Faces!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework for local facial attribute editing that leverages a 3D
                    GAN
                    inversion technique to embed attributes from a reference image into a tri-plane space, ensuring
                    3D
                    consistency and realistic viewing from multiple perspectives. This approach differs from
                    previous
                    methods by incorporating semantic masks for precise manipulation and blending techniques to
                    align
                    semantic regions from the reference image with the identity image in a 3D-aware manner.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">
                06:31
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18902" target="_blank">
                        @arXiv 2407.18902
                    </a>
                    <span class="tweet-title">
                        Robots Learn to Spin Pens: From Simulation to Real-World Mastery!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC San Diego, Carnegie Mellon University, UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on teaching robots to spin pen-like objects, a task that has been
                    challenging
                    for learning-based methods due to the lack of high-quality demonstrations and the difficulty of
                    bridging the gap between simulation and the real world. The authors address this by first
                    training
                    an oracle policy in simulation to generate realistic trajectories, then using these trajectories
                    as
                    an open-loop controller in the real world to collect successful demonstrations, and finally
                    fine-tuning the sensorimotor policy with these real-world trajectories.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">
                06:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18568" target="_blank">
                        @arXiv 2407.18568
                    </a>
                    <span class="tweet-title">
                        Vision Models Get a Style Makeover: New Technique Makes Semantic Segmentation Smarter
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Wuhan University, Tencent, Monash University...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel Spectral-Decomposed Token (SET) learning scheme for domain
                    generalized semantic segmentation (DGSS). Unlike previous methods that focus on style decoupling
                    or
                    augmentation, SET leverages the style-invariant properties of Vision Foundation Models (VFMs) by
                    decomposing features into amplitude and phase components in the frequency space. This allows for
                    more effective learning of task-specific information while mitigating the impact of style
                    variations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">
                07:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18676" target="_blank">
                        @arXiv 2407.18676
                    </a>
                    <span class="tweet-title">
                        LLMs Gone Rogue? New Research Tackles Drifting Preferences!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University College London
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Non-Stationary Direct Preference Optimization (NS-DPO), a new approach
                    to
                    fine-tuning LLMs that accounts for changes in human preferences over time. Unlike previous
                    methods,
                    NS-DPO uses a Dynamic Bradley-Terry model to re-weight training data points, giving more
                    importance
                    to recent data and discounting older, potentially outdated preferences.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">
                08:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18519" target="_blank">
                        @arXiv 2407.18519
                    </a>
                    <span class="tweet-title">
                        Stock Market Prediction: A Pre-trained Network That Knows When to Hold 'Em (and When to Fold
                        'Em)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel Temporal-Correlation Graph Pre-trained Network (TCGPN) that
                    combines temporal features and correlation information for stock forecasting. Unlike previous
                    STGNNs, TCGPN utilizes a pre-training method with carefully designed temporal and correlation
                    tasks,
                    making it more robust and effective for stock data prediction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">
                08:52
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18365" target="_blank">
                        @arXiv 2407.18365
                    </a>
                    <span class="tweet-title">
                        Fed Up with Slow Learners? FADAS Makes Federated Learning Asynchronous!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Pennsylvania State University, IBM
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces FADAS, a new method for federated learning that incorporates asynchronous
                    updates into adaptive optimization. Unlike previous work, FADAS allows clients to train the
                    model at
                    their own pace, making it more efficient and resilient to stragglers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">
                09:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18889" target="_blank">
                        @arXiv 2407.18889
                    </a>
                    <span class="tweet-title">
                        Active Learning for Morality: It's Not Always a Good Idea!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Duke University, Carnegie Mellon University, University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This paper investigates the effectiveness of active learning for eliciting moral preferences,
                    highlighting potential issues like preference instability, response variability, and model
                    misspecification. It contrasts active learning with a random query baseline, demonstrating that
                    active learning can be less effective in certain scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">
                09:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18416" target="_blank">
                        @arXiv 2407.18416
                    </a>
                    <span class="tweet-title">
                        PersonaGym: Testing LLMs' Role-Playing Skills with a Dynamic Workout!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces PersonaGym, a dynamic evaluation framework for assessing persona agents
                    in
                    LLMs. Unlike previous work that uses static personas and environments, PersonaGym dynamically
                    selects environments and generates questions based on the persona, allowing for a more
                    comprehensive
                    evaluation of an LLM's ability to act as a persona agent.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">
                10:04
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18414" target="_blank">
                        @arXiv 2407.18414
                    </a>
                    <span class="tweet-title">
                        Decision Transformers Get a Dose of Adversarial Reality: Learning to Play It Safe
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University College London
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a new algorithm called Adversarial Robust Decision Transformer (ARDT) that
                    improves the robustness of Decision Transformers (DT) in adversarial environments. Unlike
                    previous
                    DT methods that rely on observed returns, ARDT learns to condition its policy on the worst-case
                    returns-to-go, making it more resilient to powerful adversaries.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">
                10:33
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18667" target="_blank">
                        @arXiv 2407.18667
                    </a>
                    <span class="tweet-title">
                        Eye Spy: A New Dataset for Ultrasound Report Generation!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        North China University of Technology, Shenyang Ligong University, China Medical University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new ophthalmic ultrasound dataset containing images, reports, and
                    blood
                    flow information, unlike previous datasets that primarily focused on chest X-rays or fundus
                    images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">
                10:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18496" target="_blank">
                        @arXiv 2407.18496
                    </a>
                    <span class="tweet-title">
                        Empathy in Text: Can AI Really Feel Your Feels?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research explores predicting empathy and emotion in text and conversations by combining
                    advanced NLP techniques, transformer-based networks, and linguistic methodologies. The study
                    builds
                    upon previous work by incorporating lexical resources and addressing class imbalance through
                    stratified data sampling.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">
                11:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18488" target="_blank">
                        @arXiv 2407.18488
                    </a>
                    <span class="tweet-title">
                        Dueling Bandits: When AI Wants to Know "Which One Do You Like Better?"
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Princeton University, ByteDance, Oregon State University...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new conversational recommendation system that uses relative feedback,
                    allowing users to express preferences by comparing items or categories. This differs from
                    previous
                    systems that relied solely on explicit binary feedback, which can be ambiguous.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">
                12:11
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18875" target="_blank">
                        @arXiv 2407.18875
                    </a>
                    <span class="tweet-title">
                        AI Tutor's Missing Homework? GANs to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Memphis, CMU, Hong Kong Polytechnic University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a customized Generative Adversarial Imputation Network (GAIN) framework
                    to
                    impute sparse learning performance data in Intelligent Tutoring Systems (ITSs). The key
                    difference
                    from previous work is the use of convolutional neural networks (CNNs) for the input and output
                    layers of the GAIN model, along with a least squares loss function for optimization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">
                12:37
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18907" target="_blank">
                        @arXiv 2407.18907
                    </a>
                    <span class="tweet-title">
                        No More Annotating! AI Learns Dense Keypoints Without Human Help
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces SHIC, a method for learning dense keypoints without manual supervision.
                    Unlike previous methods that rely on extensive human annotations, SHIC leverages pre-trained
                    foundation models like DINO and Stable Diffusion to automatically establish correspondences
                    between
                    images and 3D templates.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">
                13:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18381" target="_blank">
                        @arXiv 2407.18381
                    </a>
                    <span class="tweet-title">
                        Turning Unsigned Distance Fields into Meshes: A Deep Learning Trick
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        École Polytechnique Fédérale de Lausanne
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a deep learning approach to convert unsigned distance fields (UDFs) into
                    signed distance fields (SDFs) locally, enabling the use of traditional meshing algorithms like
                    Marching Cubes. This differs from previous methods that rely on hand-crafted rules or ignore
                    gradient information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">
                13:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18269" target="_blank">
                        @arXiv 2407.18269
                    </a>
                    <span class="tweet-title">
                        Circuit Design Goes AI: Language Models Build Power Converters in One Go!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Duke University, IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces LaMAGIC, a language model-based approach for generating analog circuit
                    topologies. Unlike previous search-based methods that require numerous simulation iterations,
                    LaMAGIC can generate an optimized circuit design in a single pass.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">
                13:48
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18690" target="_blank">
                        @arXiv 2407.18690
                    </a>
                    <span class="tweet-title">
                        AI's New Best Friend: A Data-Centric Development Agent That Learns From Its Mistakes!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to automatic data-centric development (AD2) by
                    proposing
                    an LLM-based agent called Co-STEER. Unlike previous work that focuses on implementation,
                    Co-STEER
                    incorporates a scheduling agent that prioritizes tasks based on their dependencies and learns
                    from
                    implementation feedback. This collaborative evolution strategy allows the agent to continuously
                    improve its scheduling and implementation abilities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">
                14:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18443" target="_blank">
                        @arXiv 2407.18443
                    </a>
                    <span class="tweet-title">
                        Depth from Focus: A Mobile AR Trick to See the World in 3D!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Worcester Polytechnic Institute, Nvidia
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel depth estimation pipeline called HYBRIDDEPTH that combines the
                    strengths of single-image depth estimation and depth from focus (DFF) methods. Unlike previous
                    work
                    that relies solely on one approach, HYBRIDDEPTH leverages both to achieve robust metric depth
                    estimation for mobile AR applications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">
                14:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18525" target="_blank">
                        @arXiv 2407.18525
                    </a>
                    <span class="tweet-title">
                        Big Models, Small Tasks: Do LLMs Really Need to Be Huge for Medical Work?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Beihang University, University of Edinburgh, Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research directly compares the performance of large language models (LLMs) with
                    conventional
                    machine learning and deep learning models on non-generative medical tasks, such as predicting
                    patient mortality and readmission. It also explores the effectiveness of prompting strategies
                    for
                    LLMs in handling structured electronic health record (EHR) data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">
                15:25
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18772" target="_blank">
                        @arXiv 2407.18772
                    </a>
                    <span class="tweet-title">
                        Supply Chains: Unveiling the Hidden Recipes with AI!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, Hitachi America Ltd., Tulane University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new class of graph neural networks (GNNs) specifically designed for
                    temporal production graphs (TPGs). Unlike previous GNNs, these models incorporate an inventory
                    module that learns production functions by explicitly representing each firm's time-varying
                    inventory and updating it based on attention weights.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">
                15:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18802" target="_blank">
                        @arXiv 2407.18802
                    </a>
                    <span class="tweet-title">
                        Neural Network Posteriors: Log-Concave Coupling Makes Sampling a Breeze!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Yale University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel sampling algorithm for single hidden layer neural networks by
                    coupling the posterior density with an auxiliary random variable. This coupling results in a
                    log-concave reverse conditional density, which allows for efficient sampling using existing MCMC
                    methods. This approach differs from previous work by addressing the multimodality and
                    non-concavity
                    challenges often encountered in Bayesian models for complex machine learning algorithms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">
                16:23
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18759" target="_blank">
                        @arXiv 2407.18759
                    </a>
                    <span class="tweet-title">
                        Noise-Busting AI: How to Clean Up Messy Data with a Little Help from Echo State Networks
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Korea Institute for Advanced Study, Ulsan National Institute of Science and Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research extends a previous method for denoising single-variable signals to handle
                    multivariate
                    signals. The key difference is incorporating the interdependencies of the noise into the signal
                    reconstruction process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">
                16:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18498" target="_blank">
                        @arXiv 2407.18498
                    </a>
                    <span class="tweet-title">
                        Socialbot with a Brain: LLMs Meet Commonsense Reasoning for Chatty Companions
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Texas at Dallas, IBM Research, CETINIA Universidad Ray Juan Carlos
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces AutoCompanion, a socialbot that combines large language models (LLMs)
                    with
                    a goal-directed Answer Set Programming (ASP) system for commonsense reasoning. This approach
                    aims to
                    improve the reliability, controllability, and coherence of socialbot conversations compared to
                    LLM-only systems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">
                17:20
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18372" target="_blank">
                        @arXiv 2407.18372
                    </a>
                    <span class="tweet-title">
                        Graph Neural Networks Get a Thermodynamics Makeover!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new type of graph neural network (GE-GNN) that directly predicts the
                    excess
                    Gibbs free energy of binary mixtures, then uses thermodynamic relationships to calculate
                    activity
                    coefficients. This differs from previous work that either directly predicted activity
                    coefficients
                    or used thermodynamic models as constraints during training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">
                17:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18913" target="_blank">
                        @arXiv 2407.18913
                    </a>
                    <span class="tweet-title">
                        Reinforcement Learning Gets a Memory Boost: SOAP-RL Makes Options Stick!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new algorithm, SOAP-RL, for learning option assignments in partially
                    observable environments. Unlike previous methods that rely on complete trajectories, SOAP-RL
                    optimizes the option policy based on the agent's past history, making it more robust for online
                    decision-making.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">
                18:21
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18422" target="_blank">
                        @arXiv 2407.18422
                    </a>
                    <span class="tweet-title">
                        Black Swans: Not Just Bad Luck, It's Our Brains!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley, DeepMind, Virginia Tech
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a new definition of "spatial black swan events" that occur due to human
                    misperception of the world, even in unchanging environments. This differs from previous work
                    that
                    primarily focused on black swans arising from unpredictable environments.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">
                18:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18614" target="_blank">
                        @arXiv 2407.18614
                    </a>
                    <span class="tweet-title">
                        Fake News? Not So Fast! New Dataset Helps AI Unmask Forged Images and Find the Real Deal
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Tokyo, National Institute of Informatics, Academia Sinica
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new task called "image-based automated fact verification," which goes
                    beyond simply detecting forged images and aims to retrieve the original, authentic image. This
                    is
                    achieved through a two-phase framework that combines forgery identification and fact retrieval.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">
                19:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18808" target="_blank">
                        @arXiv 2407.18808
                    </a>
                    <span class="tweet-title">
                        Predicting Chaos: How a Neural Network Learns to See the Future
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new training strategy for the Path-dependent Neural Jump ODE
                    (PD-NJ-ODE)
                    model, enabling it to make long-term predictions for both deterministic and stochastic systems.
                    This
                    differs from previous work, which focused on short-term predictions based on the most recent
                    observations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">
                19:44
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18397" target="_blank">
                        @arXiv 2407.18397
                    </a>
                    <span class="tweet-title">
                        GPs Go Deep: A New Kind of Neural Network with a Touch of Probability
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a probabilistic extension to Kolmogorov-Arnold Networks (KANs) by using
                    Gaussian Processes (GPs) as non-linear neurons. Unlike previous Deep GP models, this approach
                    achieves fully analytical uncertainty propagation, avoiding the need for variational lower
                    bounds or
                    approximations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">
                20:11
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18277" target="_blank">
                        @arXiv 2407.18277
                    </a>
                    <span class="tweet-title">
                        TikTok Addiction? AI Can Spot It Before It's Too Late!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        National Tsing Hua University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on early detection of short-form video addiction (SFVA) using social
                    network
                    data, a novel approach compared to previous mental health detection studies that primarily
                    focused
                    on depression.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">
                20:33
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.18798" target="_blank">
                        @arXiv 2407.18798
                    </a>
                    <span class="tweet-title">
                        Deep Learning Predicts 3D Physics: A Residual Network's Dance with Rigid Bodies
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research extends previous work on 2D object dynamics to the more complex realm of 3D
                    interactions, using a deep residual network to predict the final configuration of a system of 3D
                    rigid bodies.
                </div>
            </div>
        </div>


        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202407290659_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>

</html>