
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY</div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">Fresh Picks: 
                    <span class="highlightNumber" style="font-size: 28px;">152</span> out of <span
                    class="highlightNumber">632</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-10-03"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>


        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">01:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00425" target="_blank">@arXiv 2410.00425</a>
                    <span class="tweet-title">ManiSkill3:  Simulating Robots So Fast, They're Practically Real!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of California San Diego, Carnegie Mellon University, Hillbot...</span>
                </div>
                <div class="primary-text">
                    This research introduces ManiSkill3, a GPU-parallelized robotics simulator that supports heterogeneous simulation and fast parallel rendering, enabling more efficient training of robot learning algorithms. Unlike previous simulators, ManiSkill3 allows for different objects, articulations, and scenes to be simulated in parallel, making it more scalable and generalizable.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">01:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01680" target="_blank">@arXiv 2410.01680</a>
                    <span class="tweet-title">Teacher's Pet: How to Train a Super-Student Model with a Bunch of Teachers</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nvidia</span>
                </div>
                <div class="primary-text">
                    This research focuses on improving the training of "agglomerative models" by studying the impact of different teacher activation statistics on the student model's performance. It introduces a novel normalization technique called "PHI Standardization" (PHI-S) that uses Hadamard matrices to standardize the teacher distributions, leading to better student model performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">01:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01104" target="_blank">@arXiv 2410.01104</a>
                    <span class="tweet-title">Softmax: The Attention-Dispersing Villain of AI Reasoning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google DeepMind, University of Oxford</span>
                </div>
                <div class="primary-text">
                    This paper demonstrates that the softmax function, a cornerstone of attention mechanisms in deep learning, inherently limits the ability of models to maintain sharp attention as the input size increases. This limitation is proven theoretically and validated empirically, setting it apart from previous work that focused on analyzing attention patterns without explicitly attributing them to the softmax function.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">02:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00441" target="_blank">@arXiv 2410.00441</a>
                    <span class="tweet-title">Radiology Reports Get a Makeover: AI Turns Jargon into Engaging Videos!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, Stanford University, Saint Louis University</span>
                </div>
                <div class="primary-text">
                    This research differs from previous work by integrating multiple AI models to create a comprehensive system that translates complex radiology reports into patient-friendly video explanations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">02:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00258" target="_blank">@arXiv 2410.00258</a>
                    <span class="tweet-title">AI's New Best Friend: Learning the World's Secrets, One Cause at a Time</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">VERSES AI Research Lab, Charles University, University of Oxford...</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new approach to AI alignment by focusing on enabling agents to learn the causal structure of the world, including models of other agents' preferences. This differs from previous work that often focuses on explicitly programming in safety constraints or reward functions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">02:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01483" target="_blank">@arXiv 2410.01483</a>
                    <span class="tweet-title">Folding Transformers: A SuperNet Solution for Merging Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Technion â€“ Israel Institute of Technology, Habana Labs, NVIDIA Research</span>
                </div>
                <div class="primary-text">
                    This research focuses on merging large transformer models trained on different tasks from distinct initializations, a more challenging scenario than merging models initialized from a common pre-trained network. It proposes Foldable SuperNet Merge (FS-Merge), a method that optimizes a SuperNet to fuse the original models using a feature reconstruction loss.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">03:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01606" target="_blank">@arXiv 2410.01606</a>
                    <span class="tweet-title">AI Jailbreakers:  The Chatbots That Can't Be Tamed (But Now We Can Tame Them Back)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta</span>
                </div>
                <div class="primary-text">
                    This research introduces GOAT, an automated system that simulates how humans try to "jailbreak" large language models (LLMs) by engaging in multi-turn conversations. Unlike previous work that focused on finding single adversarial prompts, GOAT leverages a dynamic approach, allowing the attacker model to reason and adapt its strategies throughout the conversation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">03:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01560" target="_blank">@arXiv 2410.01560</a>
                    <span class="tweet-title">Open-Source Math AI:  A 14 Million Question Challenge!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">NVIDIA</span>
                </div>
                <div class="primary-text">
                    This research focuses on creating a massive open-source dataset for fine-tuning large language models (LLMs) for mathematical reasoning. Unlike previous work, which often relied on closed-source data, this study uses open-weight models to generate synthetic data, making it accessible for broader research and development.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">04:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01261" target="_blank">@arXiv 2410.01261</a>
                    <span class="tweet-title">Seeing Through the Fog: A New Model for Understanding Occluded Objects</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel multimodal model, OCC-MLLM, specifically designed to understand occluded objects in images. It differs from previous work by incorporating a dual visual encoder module, one for general visual features and another for reconstructing occluded objects using 3D models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">04:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00988" target="_blank">@arXiv 2410.00988</a>
                    <span class="tweet-title">GPT-4: The Idiom Whisperer -  Unlocking Creative Translations with a Playful Prompt</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Santa Barbara, Caltech</span>
                </div>
                <div class="primary-text">
                    This research explores the use of GPT-4 to generate context-aware translations of East Asian idioms, going beyond traditional dictionary-based approaches that often lack nuance and context.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">04:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01469" target="_blank">@arXiv 2410.01469</a>
                    <span class="tweet-title">Speech Separation on a Budget: TIGER Makes It Tiny!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces TIGER, a speech separation model that uses a band-split strategy to reduce computational costs. Unlike previous models that focus on improving performance, TIGER prioritizes efficiency while maintaining high accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">05:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00709" target="_blank">@arXiv 2410.00709</a>
                    <span class="tweet-title">Binding Affinity Prediction: From Old-School Rules to AI's Wild Ride!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Chicago, Argonne National Laboratory, Data Science Institute...</span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive review of binding affinity prediction methods, focusing on the shift from conventional approaches to machine learning and deep learning techniques. It highlights the increasing use of these methods due to the growing availability of protein and small molecule data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">05:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01209" target="_blank">@arXiv 2410.01209</a>
                    <span class="tweet-title">Fed Up with Bias?  A New Algorithm for Fairer Federated Learning!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Northwestern University, Google Research, Carnegie Mellon University</span>
                </div>
                <div class="primary-text">
                    This paper introduces a theoretical framework that models client participation in federated learning (FL) as a Markov chain, allowing for the analysis of optimization convergence when clients have non-uniform and correlated participation across rounds. This is different from previous work that typically assumes independent and uniform client participation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">05:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01111" target="_blank">@arXiv 2410.01111</a>
                    <span class="tweet-title">LEGO-Learning: Building Your Own Instruction Manual</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington</span>
                </div>
                <div class="primary-text">
                    This research introduces InstructioNet, a model that learns to build LEGO structures by creating its own visual instruction book. Unlike previous models that rely on long-term memory, InstructioNet uses an explicit memory stack of images to guide its assembly process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">06:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00859" target="_blank">@arXiv 2410.00859</a>
                    <span class="tweet-title">MPC Gets a Smooth Makeover: Barrier Functions to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Microsoft, University of Oxford</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new method for smoothing Model Predictive Control (MPC) policies using log-barrier functions. Unlike previous work that focused on approximating non-smooth MPC controllers with neural networks, this research directly constructs a smooth expert controller using barrier MPC.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">06:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00435" target="_blank">@arXiv 2410.00435</a>
                    <span class="tweet-title">Spline-tastic! New Networks Make Deep Learning More Symmetrical</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Pazhou Laboratory</span>
                </div>
                <div class="primary-text">
                    This paper introduces Equivariant Kolmogorov-Arnold Networks (EKAN), which combines the benefits of Kolmogorov-Arnold Networks (KANs) with the ability to respect symmetries in data. Unlike previous work that focused on embedding equivariance into Multi-Layer Perceptrons (MLPs), EKAN leverages the learnable activation functions of KANs to achieve equivariance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">07:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01639" target="_blank">@arXiv 2410.01639</a>
                    <span class="tweet-title">Teaching AI Ethics:  How to Make LLMs Play Nice</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for aligning LLMs to human values by directly encoding moral principles into the reward function used during reinforcement learning. This differs from existing approaches that rely on inferring values from human preference data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">07:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01131" target="_blank">@arXiv 2410.01131</a>
                    <span class="tweet-title">Transformers on a Diet:  Hypersphere Training Makes Language Models Slim and Speedy</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nvidia</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new architecture called the Normalized Transformer (nGPT) that normalizes all vectors within the network to lie on a unit norm hypersphere. This approach differs from previous work by focusing on controlling the norms of vectors rather than just adding normalization layers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">07:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00231" target="_blank">@arXiv 2410.00231</a>
                    <span class="tweet-title">DoggyBot: Fetching Your Stuff, One Climb at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, UC San Diego</span>
                </div>
                <div class="primary-text">
                    This research introduces a quadrupedal robot system that can perform open-vocabulary object fetching tasks in unseen indoor environments without requiring any real-world data collection or training. This is achieved by integrating a simple gripper, a learned whole-body controller, and pre-trained vision-language models (VLMs). Unlike previous work, this system focuses on enabling helpful indoor tasks that require both agile locomotion and object manipulation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">07:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01440" target="_blank">@arXiv 2410.01440</a>
                    <span class="tweet-title">Robots Get a Brain Upgrade: Equilibrium Planning Makes Them Smarter!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new approach to robotic planning called "equilibrium sequence modeling." Unlike previous methods that rely on prompting or reinforcement learning, this approach uses a supervised learning framework to train LLMs to self-refine their plans.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">08:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01699" target="_blank">@arXiv 2410.01699</a>
                    <span class="tweet-title">Text-to-Image Generation Gets a Speed Boost:  Jacobi Decoding Goes Probabilistic!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The University of Hong Kong, Huawei Noahâ€™s Ark Lab, CUHK...</span>
                </div>
                <div class="primary-text">
                    This research introduces a probabilistic approach to Jacobi decoding, a technique used to accelerate auto-regressive text-to-image generation. Unlike previous deterministic methods, this new approach allows for sampling-based decoding, which is crucial for generating diverse and high-quality images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">08:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01088" target="_blank">@arXiv 2410.01088</a>
                    <span class="tweet-title">Data Augmentation:  Filling the Gaps with Human-in-the-Loop Help!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, Apple Inc.</span>
                </div>
                <div class="primary-text">
                    This research introduces Amplio, an interactive tool that uses human-in-the-loop techniques to improve data diversity for unstructured text datasets. Unlike previous work that focuses on freeform or structured augmentation, Amplio provides a middle ground by offering three methods: Augment with Concepts, Augment by Interpolation, and Augment with Large Language Model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">09:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01748" target="_blank">@arXiv 2410.01748</a>
                    <span class="tweet-title">LLMs:  Math Whizzes or Just Good at Guessing?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark called Compositional GSM, which tests LLMs' ability to solve chained math problems where the answer to the first problem is used as input for the second. This differs from previous work that primarily focused on evaluating LLMs on single, isolated math problems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">09:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00257" target="_blank">@arXiv 2410.00257</a>
                    <span class="tweet-title">AI Goes Zen: Can Language Loss Unlock Artificial Consciousness?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research explores the role of language in altered states of consciousness by manipulating attention weights in multimodal AI models. Unlike previous work focusing on psychedelic effects on brain regions, this study directly investigates the impact of language breakdown on simulated altered states.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">09:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01731" target="_blank">@arXiv 2410.01731</a>
                    <span class="tweet-title">AI Makes Text-to-Image Art More "Comfy" with Prompt-Adaptive Workflows</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">NVIDIA, Tel Aviv University</span>
                </div>
                <div class="primary-text">
                    This research introduces the novel task of prompt-adaptive workflow generation for text-to-image models. Instead of relying on a single model, the authors propose using an LLM to automatically tailor a workflow to each user prompt, improving image quality. This approach differs from previous work that focuses on improving the diffusion model itself or exploring the input-noise space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">10:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00215" target="_blank">@arXiv 2410.00215</a>
                    <span class="tweet-title">AI's Speed Demon:  How Meta's Multimodal Models Got a 3.88x Speed Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta</span>
                </div>
                <div class="primary-text">
                    This research focuses on characterizing and optimizing the inference performance of emerging multimodal generation models, going beyond traditional LLMs. It pinpoints key system design and optimization opportunities by analyzing the performance of four different generative AI models on real systems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">10:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01708" target="_blank">@arXiv 2410.01708</a>
                    <span class="tweet-title">Can AI Be Your BFF? New Study Explores Social Relationship Alignment in LLMs</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta, Carnegie Mellon University, Stanford University</span>
                </div>
                <div class="primary-text">
                    This research explores the impact of social relationships on LLM-generated content, a gap in existing research that focuses on persona and audience alignment. It examines how LLMs respond to prompts that include information about the commenter's and poster's gender, age, and friendship closeness.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">10:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00350" target="_blank">@arXiv 2410.00350</a>
                    <span class="tweet-title">Training Big Vision Models:  A Speedy, Sustainable, and (Almost) Effortless Approach</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Technology Sydney, Harvard University</span>
                </div>
                <div class="primary-text">
                    This research introduces an automated progressive learning framework called AutoProg, which dynamically adjusts the training workload as the model grows. Unlike previous methods that rely on manual design or fixed schedules, AutoProg automatically determines when, where, and how much to grow the model, leading to significant training acceleration without sacrificing performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">11:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00382" target="_blank">@arXiv 2410.00382</a>
                    <span class="tweet-title">LLMs Pretend to Forget: A New Trick for Selective Unlearning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research introduces "in-context knowledge unlearning," a method that allows LLMs to selectively forget information during inference based on the context of the query. Unlike previous methods that focus on global knowledge unlearning or text classification tasks, this approach specifically targets knowledge unlearning within an in-context learning framework.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">11:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01064" target="_blank">@arXiv 2410.01064</a>
                    <span class="tweet-title">LLMs Play Truth or Dare: A Game-Theoretic Approach to Decoding Reliability</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Imperial College London, University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research introduces a Bayesian Decoding Game (BDG) to enhance the consistency and reliability of LLMs. Unlike previous methods that rely on human feedback or additional training, BDG models the decoding process as a multistage game between a generator and a verifier, allowing for dynamic convergence to a consensus on the most reliable outputs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">12:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00047" target="_blank">@arXiv 2410.00047</a>
                    <span class="tweet-title">Mind's Eye Decoded: AI Sees What You See, Even When You're Just Thinking!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This research goes beyond reconstructing images from direct brain recordings triggered by visual stimuli. It introduces a novel method that captures and visually represents individuals' thoughts stimulated textually.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">12:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00083" target="_blank">@arXiv 2410.00083</a>
                    <span class="tweet-title">Diffusion Models:  Solving Inverse Problems Without Breaking a Sweat!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UT Austin, KAIST, SonyAI...</span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive overview of methods that utilize pre-trained diffusion models to solve inverse problems without requiring further training. It categorizes these methods based on the problems they address and the techniques they employ, highlighting the connections between different approaches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">12:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00713" target="_blank">@arXiv 2410.00713</a>
                    <span class="tweet-title">Robots Go Rogue: New Dataset Tests Anomaly Detection in the Real World</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Tsinghua University, Nanjing University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset called RAD, which is specifically designed for real-life anomaly detection using robotic observations. Unlike previous datasets, RAD captures data from multiple viewpoints and includes various defects, simulating the noisy environments experienced by real robots.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">13:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00168" target="_blank">@arXiv 2410.00168</a>
                    <span class="tweet-title">Speech Models Get a Semantic Makeover: Aligning Speech and Text for Better Understanding</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Johns Hopkins University, Meta AI Research</span>
                </div>
                <div class="primary-text">
                    This research proposes a new approach for fusing speech into pre-trained language models (LLMs) by leveraging speech-text alignments. Unlike previous methods that concatenate speech and text or directly incorporate speech units into the LLM's vocabulary, this approach segments and compresses speech features to match the granularity of text embeddings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">13:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00004" target="_blank">@arXiv 2410.00004</a>
                    <span class="tweet-title">Tiny Memories, Big Language: How a Little Retrieval Goes a Long Way</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research explores the effectiveness of retrieval augmented generation (RAG) with a smaller-scale database, introducing a novel embedding model and regularization technique to improve neighbor search accuracy and generalization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">14:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01481" target="_blank">@arXiv 2410.01481</a>
                    <span class="tweet-title">SonicSim:  Simulating Speech in Motion, One Room at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces SonicSim, a toolkit that generates highly customizable synthetic data for moving sound sources, addressing the limitations of existing real-world and synthetic datasets. Unlike previous work that primarily focused on static sound sources, SonicSim simulates the acoustic characteristics of moving sound sources in various environments.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">14:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00863" target="_blank">@arXiv 2410.00863</a>
                    <span class="tweet-title">LLMs Gone Wild: When Translations Get Chatty</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research focuses on the impact of verbose LLM outputs on translation evaluation, specifically examining how LLMs' tendency to provide additional commentary or refuse to translate affects their performance in existing evaluation frameworks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">14:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01404" target="_blank">@arXiv 2410.01404</a>
                    <span class="tweet-title">3D Object Detection Gets a Skin Makeover: Gaussian-Det Learns to See Surfaces</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This paper proposes Gaussian-Det, a novel approach for 3D object detection that leverages Gaussian Splatting to represent objects as continuous surfaces. Unlike previous methods that rely on discrete point clouds or volumetric representations, Gaussian-Det utilizes surface information to improve objectness deduction and refine detection proposals.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">15:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01529" target="_blank">@arXiv 2410.01529</a>
                    <span class="tweet-title">Robo-MUTUAL: Teaching Robots Multimodal Tasks with Unimodal Data!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel framework called Robo-MUTUAL that enables robots to understand multimodal task instructions by training them solely on unimodal data. This approach differs from previous methods that require meticulously annotated multimodal data, which is often expensive and time-consuming to collect.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">15:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01249" target="_blank">@arXiv 2410.01249</a>
                    <span class="tweet-title">Policy Optimization Gets a Dual Makeover:  Mirror Descent Meets Function Approximation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, Meta</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new framework called Dual Approximation Policy Optimization (DAPO) that incorporates general function approximation into policy mirror descent methods. Unlike previous approaches that use the L2-norm to measure function approximation errors, DAPO utilizes the dual Bregman divergence induced by the mirror map for policy projection.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">15:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00796" target="_blank">@arXiv 2410.00796</a>
                    <span class="tweet-title">AI Power Grid:  Zero False Negatives, No Blackouts!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Caltech, Microsoft, University of Washington</span>
                </div>
                <div class="primary-text">
                    This research proposes using input-convex neural networks (ICNNs) for contingency screening in power systems. Unlike previous methods, this approach guarantees zero false negatives, ensuring that no potentially dangerous scenarios are misclassified as safe.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">16:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00993" target="_blank">@arXiv 2410.00993</a>
                    <span class="tweet-title">Bandit Control:  Beyond Quadratics,  We're  Going  Optimal!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Princeton University</span>
                </div>
                <div class="primary-text">
                    This paper presents an algorithm that achieves an optimal regret bound for bandit non-stochastic control with strongly-convex and smooth cost functions, improving upon the previously known ËœO(T2/3) regret bound.  The key difference is the use of a Newton-based update to exploit the Îº0-convexity of the loss function, which allows for handling general strongly-convex smooth costs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">16:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00184" target="_blank">@arXiv 2410.00184</a>
                    <span class="tweet-title">PET Denoising Gets a 3D Makeover: Diffusion Model Goes Volumetric!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Massachusetts General Hospital, Harvard Medical School, Zhejiang University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a conditional score-based residual diffusion model (CSRD) for PET denoising. Unlike previous diffusion models, CSRD is specifically designed for volumetric data, addressing the limitations of existing models when applied to 3D PET images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">17:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00746" target="_blank">@arXiv 2410.00746</a>
                    <span class="tweet-title">Brain Scans Get a Deep Learning Makeover: AI Zaps Out Noise, Reveals Hidden Metabolites!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Massachusetts General Hospital, Harvard Medical School, Medical University of Vienna...</span>
                </div>
                <div class="primary-text">
                    This research introduces a deep learning method called WALINET for removing water and lipid signals from brain MR spectroscopic imaging (MRSI) data. Unlike conventional methods that rely on linear operations and require tedious parameter tuning, WALINET uses a convolutional neural network to identify and remove these nuisance signals, leading to faster and more accurate metabolite quantification.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">17:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00201" target="_blank">@arXiv 2410.00201</a>
                    <span class="tweet-title">Slides and UIs:  Code-Generated Data Makes Learning a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, Aalto University, University of Texas at Austin</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for generating synthetic, structured visuals with embedded target labels using code generation. Unlike previous work that relies on manual data collection and annotation, this approach allows for on-demand creation of training data, reducing the need for human effort.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">17:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01463" target="_blank">@arXiv 2410.01463</a>
                    <span class="tweet-title">LoRA in FL: Sharing the Love, Not the Matrices!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Hong Kong, Stanford University, Shanghai Institute of Applied Physics</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new method for integrating LoRA with federated learning (FL) by selectively aggregating only the "A" matrices, which learn general knowledge, while keeping the "B" matrices, which capture client-specific knowledge, local. This differs from previous work that either freezes the "A" matrices or aggregates both "A" and "B" matrices, potentially leading to suboptimal performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">18:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01444" target="_blank">@arXiv 2410.01444</a>
                    <span class="tweet-title">Language Models:  They're Not Just Good at Words, They're Good at Geometry!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London, Montreal Institute for Learning Algorithms, University of Toronto...</span>
                </div>
                <div class="primary-text">
                    This research explores the relationship between the compositionality of language and the geometric complexity of representations learned by language models. Unlike previous work, it investigates how this relationship evolves over training and distinguishes between formal and semantic compositionality.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">18:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01153" target="_blank">@arXiv 2410.01153</a>
                    <span class="tweet-title">Text to Physics:  Turning Words into Simulations!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for using latent diffusion models to generate physics simulations. Unlike previous work that focuses on autoregressive methods, this approach generates an entire solution trajectory at once, potentially improving accuracy and efficiency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">19:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01137" target="_blank">@arXiv 2410.01137</a>
                    <span class="tweet-title">LLMs: Not Just for Chatbots, They're Solving PDEs Now!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research integrates text descriptions of physical systems into a neural operator model, FactFormer, to improve its performance in solving partial differential equations (PDEs). Unlike previous work that primarily relies on data-driven approaches, this method leverages the understanding of LLMs to incorporate system information like boundary conditions and governing equations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">19:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00079" target="_blank">@arXiv 2410.00079</a>
                    <span class="tweet-title">AI Planning:  Let Humans Help!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Rutgers University, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces Interactive Speculative Planning, a method that combines an efficient but less accurate agent with a more powerful but slower agent. The key difference is that it allows human users to intervene and provide input during the planning process, potentially speeding up the overall task completion.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">19:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00296" target="_blank">@arXiv 2410.00296</a>
                    <span class="tweet-title">Unlabeled Data: The Secret Weapon for Defending Vision-Language Models from Malicious Prompts</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft, University of Wisconsin-Madison</span>
                </div>
                <div class="primary-text">
                    This research introduces VLMGUARD, a framework that leverages unlabeled user prompts to detect malicious prompts in vision-language models (VLMs). Unlike previous methods that rely on labeled data, VLMGUARD utilizes a novel maliciousness estimation score derived from VLM representations to distinguish between benign and malicious samples within unlabeled data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">20:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01623" target="_blank">@arXiv 2410.01623</a>
                    <span class="tweet-title">LLMs on a Diet: Full-Rank Training with Low-Rank Constraints</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Beijing Institute of Technology, Peking University, Chinese University of Hong Kong</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new training framework called Fira that allows for full-rank training of LLMs while maintaining low-rank constraints, unlike previous methods that either constrain weights or gradients to a low-rank subspace.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">20:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00772" target="_blank">@arXiv 2410.00772</a>
                    <span class="tweet-title">Self-Supervised Learning:  Overfitting?  Not If You Undo Memorization!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Hong Kong University of Science and Technology</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method called Undoing Memorization Mechanism (UMM) to address the overfitting problem in self-supervised learning (SSL) models. UMM works by aligning the feature distributions of early and late layers in the model, maximizing the coding rate reduction of the last layer output. This approach differs from previous work by focusing on the overfitting issue in SSL and using coding rate reduction as a metric to quantify it.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">21:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01101" target="_blank">@arXiv 2410.01101</a>
                    <span class="tweet-title">Offline MARL:  Low Interaction Rank Makes Agents Play Nice!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Princeton University, Meta</span>
                </div>
                <div class="primary-text">
                    This paper introduces the concept of interaction rank (IR) in offline multi-agent reinforcement learning (MARL). Unlike previous work that focused on general function classes, this research explores the benefits of using function classes with low IR, demonstrating that they are more robust to distribution shift.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">22:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00630" target="_blank">@arXiv 2410.00630</a>
                    <span class="tweet-title">Three Photos, One Amazing Face: New AI Makes 3D Portraits From Sparse Snapshots</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, Google</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel volumetric prior for human faces that enables high-fidelity expressive face modeling from as few as three input views. Unlike previous methods that rely on dense multi-view captures or require domain adaptation, this approach leverages a synthetic dataset to train a prior model that can generalize to real-world identities and expressions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">22:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01350" target="_blank">@arXiv 2410.01350</a>
                    <span class="tweet-title">Takin-VC: Voice Conversion Gets a Memory Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Ximalaya Inc., Kyushu University, Northwestern Polytechnical University...</span>
                </div>
                <div class="primary-text">
                    This research introduces Takin-VC, a zero-shot voice conversion framework that uses a hybrid content encoder and a memory-augmented timbre modeling approach to improve speaker similarity and speech naturalness. Unlike previous methods, Takin-VC leverages both phonetic posterior-grams (PPGs) and quantized self-supervised learning (SSL) features to capture linguistic content more effectively. Additionally, it incorporates a memory module to generate high-quality conditional target inputs for the flow matching model, further enhancing speaker similarity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">22:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00354" target="_blank">@arXiv 2410.00354</a>
                    <span class="tweet-title">AI Traders: Can Bots Mimic Wall Street's Best?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National Institute of Advanced Industrial Science and Technology, Ochanomizu University, University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research explores the use of hierarchical multi-agent simulations, where different AI agents with varying levels of expertise interact to make investment decisions, mimicking the decision-making processes found in real-world investment firms. This approach differs from previous studies that primarily focused on single-agent models or simpler communication strategies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">23:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01169" target="_blank">@arXiv 2410.01169</a>
                    <span class="tweet-title">AI Analyst's Dilemma: When to Spill the Tea?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National Institute of Advanced Industrial Science and Technology, Ochanomizu University, University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel task: identifying the optimal timing for expressing opinions triggered by news. Unlike previous work focusing on sentiment analysis or opinion generation, this study explores the decision-making process of professionals, specifically when they choose to release reports based on new information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">23:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00847" target="_blank">@arXiv 2410.00847</a>
                    <span class="tweet-title">Reward Models That Know When They Don't Know: A New Approach to LLM Alignment</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Chinese Academy of Sciences, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces Uncertainty-aware Reward Models (URM) and Uncertainty-aware Reward Model Ensembles (URME) to address the stochasticity of human preferences and the lack of knowledge in reward models. Unlike previous work, which relies on deterministic reward modeling, URM and URME model the distribution of rewards and quantify uncertainty through discrepancies in the ensemble.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">24:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01109" target="_blank">@arXiv 2410.01109</a>
                    <span class="tweet-title">LLMs:  Mixing It Up for Financial Finesse!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research explores the impact of multi-task fine-tuning on LLMs for financial tasks, demonstrating that training on a combination of related tasks can significantly improve performance compared to single-task fine-tuning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">24:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00490" target="_blank">@arXiv 2410.00490</a>
                    <span class="tweet-title">Swimming Robots Learn to Flow: Neural ODEs Take the Plunge!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">TU Delft, Westlake University, University College London</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel data-driven hydrodynamic model for amphibious quadruped robots using Neural Ordinary Differential Equations (ODEs) combined with attention mechanisms. This approach differs from previous work by leveraging real-time sensor data to learn and adapt to varying underwater conditions, enabling more accurate force prediction and robust decision-making.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">24:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00059" target="_blank">@arXiv 2410.00059</a>
                    <span class="tweet-title">DNNs on Lockdown: New Method Keeps Models Safe from Piracy</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of California  San Diego, University of California  Irvine, University of California  Los Angeles</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new method for protecting DNN models from unauthorized access by treating active authorization as an inverse problem of domain adaptation. It uses a mixture-of-experts model to minimize the mutual information between authorized and unauthorized domains, ensuring that the model only performs well on authorized inputs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">25:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00046" target="_blank">@arXiv 2410.00046</a>
                    <span class="tweet-title">AI for Radiotherapy:  One Size Doesn't Fit All, So Let's Mix It Up!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Mayo Clinic, Harvard University</span>
                </div>
                <div class="primary-text">
                    This research introduces the Mixture of Multicenter Experts (MoME) approach, which aims to improve the generalizability of AI models in radiotherapy by integrating specialized expertise from diverse clinical strategies. This differs from previous work that often relies on single-center training, which can lead to biased models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">25:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01620" target="_blank">@arXiv 2410.01620</a>
                    <span class="tweet-title">Ophthalmology's AI Vision Test: Can LLMs See What Doctors Do?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This research introduces LMOD, a new dataset and benchmark specifically designed for evaluating large vision-language models (LVLMs) on ophthalmology images. Unlike previous benchmarks, LMOD covers multiple imaging modalities and tasks, including anatomical understanding, disease diagnosis, and demographic extraction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon" title="Play from here">26:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01405" target="_blank">@arXiv 2410.01405</a>
                    <span class="tweet-title">Looped Transformers:  They're Not Just Efficient, They're Turing Complete!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This paper establishes approximation rates for Looped Transformers by defining the modulus of continuity for sequence-to-sequence functions. It also introduces a time-dependent scaling parameter to improve the model's expressive power.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon" title="Play from here">26:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01644" target="_blank">@arXiv 2410.01644</a>
                    <span class="tweet-title">Federated Learning Goes Hybrid:  When Horizontal Meets Vertical!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge, Commonwealth Scientific and Industrial Research Organisation, Nanyang Technological University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel hybrid federated learning framework called HoVeFL, which combines horizontal and vertical federated learning strategies. Unlike previous work that focuses solely on either horizontal or vertical learning, HoVeFL allows devices to train on both shared features across different datasets and different features from the same dataset.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon" title="Play from here">26:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00835" target="_blank">@arXiv 2410.00835</a>
                    <span class="tweet-title">Solving High-Dimensional Equations:  A Finite Expression Method Takes the Stage!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Purdue University, University of Maryland</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new method, FEX-PG, for solving high-dimensional partial integro-differential equations (PIDEs). It builds upon the original FEX method by incorporating a novel parameter grouping technique and a Taylor series approximation for the integral terms. This approach aims to improve computational efficiency and accuracy while maintaining interpretability of the solutions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet67">
            <div class="start-time-icon" title="Play from here">27:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00454" target="_blank">@arXiv 2410.00454</a>
                    <span class="tweet-title">UniAdapt:  LLMs Get a Knowledge Makeover, No Retraining Required!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google Research, University of California  Berkeley, University of Illinois at Urbana-Champaign...</span>
                </div>
                <div class="primary-text">
                    UniAdapt introduces a novel approach to lifelong model editing by utilizing a vector-assisted router and multiple parallel experts. Unlike previous methods that rely on memory-based routing, UniAdapt leverages semantic similarity to route queries to the most relevant experts, enhancing both generalization and locality.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet68">
            <div class="start-time-icon" title="Play from here">27:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00193" target="_blank">@arXiv 2410.00193</a>
                    <span class="tweet-title">Vision-Language Models:  Diagram Savants or Knowledge Cheaters?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research evaluates the ability of Large Vision-Language Models (LVLMs) to understand diagrams by focusing on their ability to recognize and reason about entities and relations. The study uses a comprehensive test suite with both synthetic and real diagrams, and it finds that while LVLMs can identify entities, their understanding of relations is limited.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet69">
            <div class="start-time-icon" title="Play from here">28:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01093" target="_blank">@arXiv 2410.01093</a>
                    <span class="tweet-title">Missing Data? No Problem! Logistic Regression Gets a High-Dimensional Makeover</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge, Georgia Institute of Technology, Stanford University</span>
                </div>
                <div class="primary-text">
                    This research extends the study of high-dimensional logistic regression to scenarios where data is missing completely at random (MCAR). It provides exact characterizations of both prediction and estimation errors, even when the data matrix used for inference differs from the one used to generate the labels.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet70">
            <div class="start-time-icon" title="Play from here">28:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01366" target="_blank">@arXiv 2410.01366</a>
                    <span class="tweet-title">Style Transfer Without Training:  A Diffusion Model's Magic Trick</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CyberAgent, University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research proposes a new style transfer algorithm called STRDP that doesn't require any additional training. It leverages the latent space of a pre-trained Latent Diffusion Model (LDM) and applies Adaptive Instance Normalization (AdaIN) repeatedly during the reverse diffusion process. This approach differs from previous methods that either require training or operate directly on the image space, leading to higher computational costs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet71">
            <div class="start-time-icon" title="Play from here">28:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00876" target="_blank">@arXiv 2410.00876</a>
                    <span class="tweet-title">Knowledge Graph Completion: Ditch the Paths, Embrace the Bias!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Rensselaer, IBM</span>
                </div>
                <div class="primary-text">
                    This research proposes a new approach to knowledge graph completion that eliminates the need for path encoding modules, which are often time-consuming and require extensive hyperparameter tuning. Instead, the authors introduce connection-biased attention and entity role embeddings into a Transformer-based subgraph encoding module.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet72">
            <div class="start-time-icon" title="Play from here">29:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00363" target="_blank">@arXiv 2410.00363</a>
                    <span class="tweet-title">Fusing Models Like a Pro: Likelihood Composition for Multi-Modal Language Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Shanghai AI Laboratory, CUHK, East China Normal University...</span>
                </div>
                <div class="primary-text">
                    This research proposes a post-hoc framework called "likelihood composition" for fusing heterogeneous models. Unlike previous methods that require training or similar architectures, this approach manipulates the likelihood distributions of candidate answers, making it training-free and adaptable to diverse models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet73">
            <div class="start-time-icon" title="Play from here">29:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01086" target="_blank">@arXiv 2410.01086</a>
                    <span class="tweet-title">Deep Dive into Deep Survival:  Neural Networks Predict When You'll Kick the Bucket</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University</span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive introduction to deep survival analysis models, focusing on how these models are related and the overarching principles behind their development. It distinguishes between innovations specific to time-to-event prediction and those that are not, emphasizing the former.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet74">
            <div class="start-time-icon" title="Play from here">29:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01727" target="_blank">@arXiv 2410.01727</a>
                    <span class="tweet-title">AI Tutor Gets Smarter:  LLMs Help Students Learn Faster!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, Ludwig Maximilian University of Munich</span>
                </div>
                <div class="primary-text">
                    This research proposes a new framework called KCQRL that automates the process of identifying knowledge concepts (KCs) in questions and then uses contrastive learning to generate semantically rich embeddings for questions and solution steps. This differs from previous work by explicitly incorporating the semantics of questions and KCs, rather than treating them as mere identifiers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet75">
            <div class="start-time-icon" title="Play from here">30:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01395" target="_blank">@arXiv 2410.01395</a>
                    <span class="tweet-title">Blurry Robot Vision? This New Method Clears Things Up!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Shanghai University, Peking University, Nanjing University</span>
                </div>
                <div class="primary-text">
                    This research proposes an unsupervised zero-shot dehazing method (RSF-Dehaze) for robotic vision in urological surgery. Unlike previous methods that rely on paired clean and blurry data for training, RSF-Dehaze learns to dehaze from a single input image.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet76">
            <div class="start-time-icon" title="Play from here">30:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01262" target="_blank">@arXiv 2410.01262</a>
                    <span class="tweet-title">Diffusion Models:  Team Up for Fine-Grained Control!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sun Yat-Sen University, University of Toronto</span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel algorithm called AMDM (Aggregation of Multi Diffusion Models) that aggregates intermediate variables from different conditional diffusion models, enhancing their learned representations for fine-grained control. This approach differs from previous work by leveraging the shared theoretical foundation of diffusion models to combine their strengths without requiring additional training or complex datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet77">
            <div class="start-time-icon" title="Play from here">31:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00151" target="_blank">@arXiv 2410.00151</a>
                    <span class="tweet-title">Scheherazade:  LLMs Get Lost in a Math Maze of Chained Problems!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This research introduces Scheherazade, a method for automatically generating challenging math reasoning benchmarks by logically chaining existing problems together. This differs from previous work that focused on manual or template-based approaches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet78">
            <div class="start-time-icon" title="Play from here">31:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00890" target="_blank">@arXiv 2410.00890</a>
                    <span class="tweet-title">Flex3D:  Generating 3D Objects From Scratch, One View at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta, University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research introduces Flex3D, a novel framework that generates high-quality 3D objects from text or images by leveraging an arbitrary number of input views. Unlike previous methods that rely on a fixed number of views, Flex3D generates a pool of candidate views and then selects the best ones for reconstruction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet79">
            <div class="start-time-icon" title="Play from here">31:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00880" target="_blank">@arXiv 2410.00880</a>
                    <span class="tweet-title">AI Matchmaker: Finding the Perfect Team for Your Software Woes</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Illinois, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel system called GEMS, which leverages Large Foundation Models (LFMs) to generate metrics for identifying teams with specific expertise within software corporations. Unlike previous work that relies on pre-defined metrics, GEMS uses prompt engineering to extract and summarize theories from expert literature, transforming them into context-aware metrics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet80">
            <div class="start-time-icon" title="Play from here">32:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01174" target="_blank">@arXiv 2410.01174</a>
                    <span class="tweet-title">Steering LLMs to Safety: A Category-Specific Approach</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nvidia, Arizona State University</span>
                </div>
                <div class="primary-text">
                    This research explores category-specific safety steering of LLMs at inference time, using steering vectors computed from model activations. Unlike previous work, it focuses on fine-grained control over steering and investigates different methods for extracting informative steering vectors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet81">
            <div class="start-time-icon" title="Play from here">32:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01360" target="_blank">@arXiv 2410.01360</a>
                    <span class="tweet-title">Eye-Spy: New Tech Makes Realistic Eyelids From Just Your Phone!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for reconstructing and animating eyelids using only an RGB video captured by a mobile phone. Unlike previous methods that require expensive multi-camera setups or rely on pre-designed blendshapes, this approach leverages eyeball information to achieve detailed results with lightweight captures.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet82">
            <div class="start-time-icon" title="Play from here">33:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01656" target="_blank">@arXiv 2410.01656</a>
                    <span class="tweet-title">Unlocking the Secrets of Truncated Data:  Beyond Gaussians, Faster Than Ever!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This research extends previous work on estimating parameters from truncated data by providing algorithms that work for a wider range of distributions, including general Gaussian distributions and certain exponential families.  It also introduces a new algorithm that achieves polynomial time complexity for simple survival sets like halfspaces and axis-aligned rectangles, unlike previous work which required exponential time.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet83">
            <div class="start-time-icon" title="Play from here">33:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01537" target="_blank">@arXiv 2410.01537</a>
                    <span class="tweet-title">Attention Layers: The Secret Sauce for Sparse Token Regression?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new task called "single-location regression" where the output depends on a single token in a sequence, and the location of that token is a random variable. The authors propose a simplified attention layer that provably solves this task, demonstrating the ability of attention mechanisms to handle sparse token information and internal linear structures.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet84">
            <div class="start-time-icon" title="Play from here">33:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01160" target="_blank">@arXiv 2410.01160</a>
                    <span class="tweet-title">GraphRevisedIE:  Document Extraction Gets a Graph Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Michigan, University of Science and Technology of China</span>
                </div>
                <div class="primary-text">
                    This research introduces GraphRevisedIE, a model that uses graph revision and convolution to enrich multimodal feature embedding for document information extraction. Unlike previous graph-based methods, GraphRevisedIE does not enforce a fully connected graph and supports adding new edges, making it more adaptable to sparse document graphs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet85">
            <div class="start-time-icon" title="Play from here">34:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00373" target="_blank">@arXiv 2410.00373</a>
                    <span class="tweet-title">Traffic Forecasting:  When the Roads Get a Makeover, Can Your Model Keep Up?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Southern University of Science and Technology, Didi Global, Jilin University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel Mixture of Experts (MoE) framework for traffic forecasting that specifically addresses the challenge of spatial shifts in traffic patterns over time. Unlike previous methods that focus on temporal shifts or require manual dataset modifications, this approach learns a set of graph generators (graphons) during training and adaptively combines them to handle spatial distribution shifts during testing.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet86">
            <div class="start-time-icon" title="Play from here">34:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00385" target="_blank">@arXiv 2410.00385</a>
                    <span class="tweet-title">Traffic Forecasting Gets a Speed Boost: STGformer Outperforms the Competition!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Southern University of Science and Technology, Didi Global, Jilin University...</span>
                </div>
                <div class="primary-text">
                    This research introduces STGformer, a novel spatiotemporal graph transformer architecture that efficiently captures both global and local traffic patterns. Unlike previous methods that rely on multiple attention layers, STGformer achieves this in a single layer, significantly reducing computational cost.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet87">
            <div class="start-time-icon" title="Play from here">35:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01686" target="_blank">@arXiv 2410.01686</a>
                    <span class="tweet-title">Transformers Get Positional:  Outsmarting OOD with Fixed Encodings!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This paper introduces positional attention, where attention weights are determined solely by fixed positional encodings, unlike standard self-attention which uses both input values and positional encodings. This approach is shown to improve out-of-distribution (OOD) generalization performance in algorithmic reasoning tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet88">
            <div class="start-time-icon" title="Play from here">35:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00871" target="_blank">@arXiv 2410.00871</a>
                    <span class="tweet-title">Mamba & Transformer: A Visionary Duo Gets a Pretraining Power-Up!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Shanghai AI Laboratory, Shanghai Qi Zhi Institute</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel pretraining strategy called Masked Autoregressive Pretraining (MAP) specifically designed for hybrid Mamba-Transformer vision backbones. Unlike previous methods, MAP leverages both local Masked Autoencoder (MAE) and global autoregressive pretraining to optimize the learning process for both Mamba and Transformer modules within a unified framework.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet89">
            <div class="start-time-icon" title="Play from here">36:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00318" target="_blank">@arXiv 2410.00318</a>
                    <span class="tweet-title">Can AI Build a Bridge? New Study Tests Mechanical Reasoning in Vision Language Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Michigan, CMU</span>
                </div>
                <div class="primary-text">
                    This research investigates the mechanical reasoning abilities of Vision Language Models (VLMs) using a new benchmark called MechBench, which contains 141 cognitive experiments designed to assess understanding of various mechanical systems. This approach differs from previous work by focusing specifically on mechanical reasoning within a comprehensive and diverse set of cognitive tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet90">
            <div class="start-time-icon" title="Play from here">36:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00629" target="_blank">@arXiv 2410.00629</a>
                    <span class="tweet-title">Illuminating the Future: 3D Reconstruction Makes Feature Extraction Shine!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Hong Kong University of Science and Technology, University of Health and Rehabilitation Sciences</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel illumination-robust feature extractor that utilizes relightable 3D reconstruction for data augmentation. Unlike previous methods that rely on large datasets or image-illumination transformations, this approach generates images with varying illumination conditions directly from 3D models, enabling precise control and reducing the need for extensive data collection.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet91">
            <div class="start-time-icon" title="Play from here">36:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00229" target="_blank">@arXiv 2410.00229</a>
                    <span class="tweet-title">Probability Distributions:  The New Frontier for Inverse Problems!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Wisconsin-Madison, Cornell University, University of Minnesota</span>
                </div>
                <div class="primary-text">
                    This research extends the traditional inverse problem framework, which typically operates in Euclidean spaces, to the space of probability distributions. This shift allows for the analysis of problems where the unknown parameter is a probability measure, rather than a single value.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet92">
            <div class="start-time-icon" title="Play from here">37:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01591" target="_blank">@arXiv 2410.01591</a>
                    <span class="tweet-title">CT Scans Get a Makeover: AI Foundation Model Makes Non-Ideal Images Ideal!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Southeast University, Yale University, Case Western Reserve University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel imaging foundation model, TAMP, for universal enhancement of non-ideal measurement computed tomography (NICT) images. Unlike previous specialized models that focus on specific NICT settings and body regions, TAMP is trained on a large-scale simulated dataset and can directly generalize to diverse NICT scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet93">
            <div class="start-time-icon" title="Play from here">37:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01212" target="_blank">@arXiv 2410.01212</a>
                    <span class="tweet-title">Safe RL Gets a High-Probability Guarantee: No More Safety Violations!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a new algorithm, ASCPO, that guarantees high-probability state-wise constraint satisfaction in reinforcement learning. Unlike previous methods that only consider expected constraint violations, ASCPO also accounts for the variance of violations, ensuring a much lower probability of safety breaches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet94">
            <div class="start-time-icon" title="Play from here">38:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00292" target="_blank">@arXiv 2410.00292</a>
                    <span class="tweet-title">AI Eye Doctor: LLMs Diagnose Ocular Surface Diseases with a Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research introduces a multi-modal diagnostic pipeline (MDPipe) that uses large language models (LLMs) to diagnose ocular surface diseases. Unlike previous methods that rely on closed-set predictions, MDPipe leverages LLMs' ability to generate free-form answers with clinical reasoning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet95">
            <div class="start-time-icon" title="Play from here">38:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00844" target="_blank">@arXiv 2410.00844</a>
                    <span class="tweet-title">Unbalanced Dynamics:  Learning How Things Grow and Die with DeepRUOT</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new deep learning method called DeepRUOT for learning regularized unbalanced optimal transport (RUOT). Unlike previous methods, DeepRUOT can infer continuous unbalanced stochastic dynamics from observed snapshots without requiring prior knowledge of growth and death processes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet96">
            <div class="start-time-icon" title="Play from here">39:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01162" target="_blank">@arXiv 2410.01162</a>
                    <span class="tweet-title">Frozen AI Learns to Feel: LLMs Now Understand Your Tone!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Massachusetts Institute of Technology, Meta</span>
                </div>
                <div class="primary-text">
                    This research explores training a speech encoder to capture paralinguistic cues in speech, allowing a frozen LLM to understand emotions and speaking styles without fine-tuning. This differs from previous work that either focused on semantic information or required fine-tuning the LLM.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet97">
            <div class="start-time-icon" title="Play from here">39:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00946" target="_blank">@arXiv 2410.00946</a>
                    <span class="tweet-title">Brain Graph Weights: Unmasking Hidden Sub-Cohorts in Neuroimaging!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel sample weighting scheme that leverages spectral graph theory to capture the interplay between predictive power and cohort-specific factors in neuroimaging-based predictive models. Unlike previous approaches that focus on reweighing challenging samples, this method aims to gain population-level insights into the intrinsic relationship between predictive power and auxiliary factors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet98">
            <div class="start-time-icon" title="Play from here">39:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01658" target="_blank">@arXiv 2410.01658</a>
                    <span class="tweet-title">Outlier-Proofing Causal Inference: When Propensity Scores Go Wild!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new family of estimators called Coarse IPW (CIPW) estimators that are robust to inaccuracies in propensity scores and outliers. Unlike existing IPW estimators, CIPW estimators achieve a smaller RMSE even when propensity scores are slightly inaccurate.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet99">
            <div class="start-time-icon" title="Play from here">40:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01736" target="_blank">@arXiv 2410.01736</a>
                    <span class="tweet-title">Dynamic Data, No Problem:  A Recursive Approach to Retrieval</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research introduces two new algorithms, adRAP and postQFRAP, to improve retrieval performance in dynamic datasets. adRAP efficiently updates the hierarchical structure of retrieved text chunks as new documents are added or removed, while postQFRAP applies query-focused recursive abstractive processing as a post-retrieval layer, enhancing context quality.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet100">
            <div class="start-time-icon" title="Play from here">40:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00903" target="_blank">@arXiv 2410.00903</a>
                    <span class="tweet-title">AI-Powered Text Therapy:  Curing Confounding Bias in Causal Inference</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This research proposes using generative AI models to create treatment texts and then leverage their internal representations for causal inference. This differs from previous work that relies on learning causal representations directly from the data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet101">
            <div class="start-time-icon" title="Play from here">41:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01690" target="_blank">@arXiv 2410.01690</a>
                    <span class="tweet-title">VLMs:  Seeing is Believing, But Context is King!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, German Cancer Research Center</span>
                </div>
                <div class="primary-text">
                    This research focuses on the interplay between image and text modalities in Visual Language Models (VLMs) for visual question answering (VQA) tasks. Unlike previous work that often treats modalities as independent inputs, this study investigates how semantic interventions in both image and text affect VLM performance, uncertainty, and attention attribution.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet102">
            <div class="start-time-icon" title="Play from here">41:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00897" target="_blank">@arXiv 2410.00897</a>
                    <span class="tweet-title">Health Data Privacy: Not Just Black and White, But a Colorful Gradient!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This paper introduces a "privacy gradient" approach to health data management, moving beyond traditional binary models of complete privacy or full accessibility. It proposes a multidimensional concept that considers factors like data sensitivity, stakeholder relationships, purpose of use, and temporal aspects to determine appropriate privacy levels.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet103">
            <div class="start-time-icon" title="Play from here">41:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00812" target="_blank">@arXiv 2410.00812</a>
                    <span class="tweet-title">Brain-Decoding LLMs:  Turning AI into a Language Whisperer</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Texas at Austin, Microsoft Research, University of California  San Francisco...</span>
                </div>
                <div class="primary-text">
                    This research introduces a generative framework called GEM-V, which translates the predictions of large language models (LLMs) into concise, human-understandable explanations of brain activity. This framework then uses these explanations to design new experiments that test the causal relationship between the explanations and brain responses. This approach differs from previous work by directly testing the causality of LLM-based encoding models, rather than simply relying on their predictive power.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet104">
            <div class="start-time-icon" title="Play from here">42:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01671" target="_blank">@arXiv 2410.01671</a>
                    <span class="tweet-title">LLMs Get a Grammar Lesson: Coreference Resolution Makes Long Texts Easier to Understand</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Zhejiang University, MIT, Southeast University...</span>
                </div>
                <div class="primary-text">
                    This research introduces the Long Question Coreference Adaptation (LQCA) method, which focuses on resolving coreference issues in long texts. Unlike previous work that primarily relies on prompt engineering or smaller models, LQCA utilizes a four-step process to systematically identify and manage references within long documents, ultimately improving the performance of LLMs in question answering tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet105">
            <div class="start-time-icon" title="Play from here">42:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01739" target="_blank">@arXiv 2410.01739</a>
                    <span class="tweet-title">Q-Learning Gets a Brain: New Algorithm Mimics Human Intuition</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces Cognitive Belief-Driven Q-Learning (CBDQ), which integrates subjective belief modeling into the Q-learning framework. Unlike previous work that focuses on mathematical optimization, CBDQ draws inspiration from cognitive science to enhance decision-making accuracy by endowing agents with human-like learning and reasoning capabilities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet106">
            <div class="start-time-icon" title="Play from here">43:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01257" target="_blank">@arXiv 2410.01257</a>
                    <span class="tweet-title">Reward Models:  A Head-to-Head Showdown with a Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nvidia</span>
                </div>
                <div class="primary-text">
                    This research directly compares two popular reward model training paradigms, Bradley-Terry and Regression, using a newly released dataset with matched data. It also introduces a novel approach to combine these paradigms, achieving top performance on RewardBench.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet107">
            <div class="start-time-icon" title="Play from here">43:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01698" target="_blank">@arXiv 2410.01698</a>
                    <span class="tweet-title">Satellite Images:  Compressed with a Diffusion of Magic!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Harbin Institute of Technology, Nanyang Technological University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel image compression method for satellites that uses a lightweight encoder on the satellite and a diffusion-based model on the ground to compensate for the loss of information during compression. This approach differs from previous work by leveraging the multi-modal nature of satellite images, using sensor data as a condition for diffusion generation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet108">
            <div class="start-time-icon" title="Play from here">44:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00182" target="_blank">@arXiv 2410.00182</a>
                    <span class="tweet-title">AI Detectives: Can LLMs Crack the Code of Crisis Tweets?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Georgia State University, MIT</span>
                </div>
                <div class="primary-text">
                    This research explores the zero-shot classification capabilities of large language models (LLMs) for analyzing crisis-related tweets, a task typically tackled by specialized machine learning models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet109">
            <div class="start-time-icon" title="Play from here">44:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00345" target="_blank">@arXiv 2410.00345</a>
                    <span class="tweet-title">Lost in the Sauce: A Taxonomy of Loss Functions for Stochastic Optimal Control</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft, Meta</span>
                </div>
                <div class="primary-text">
                    This research introduces a taxonomy of loss functions for stochastic optimal control (SOC) problems, grouping them into classes based on their gradient expectation. This allows for a more systematic understanding of the strengths and weaknesses of different loss functions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet110">
            <div class="start-time-icon" title="Play from here">44:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01450" target="_blank">@arXiv 2410.01450</a>
                    <span class="tweet-title">Mandarin Lyrics:  AI Gets a Tune-Up with Multi-Agent Lyric Generation!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National Tsing Hua University</span>
                </div>
                <div class="primary-text">
                    This research extends previous work on English lyric generation to Mandarin, considering the unique challenges of tonal languages. It introduces a multi-agent system that decomposes the melody-to-lyric task into sub-tasks, each handled by a specialized agent.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet111">
            <div class="start-time-icon" title="Play from here">45:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01707" target="_blank">@arXiv 2410.01707</a>
                    <span class="tweet-title">MCTS Gets a Speed Boost: LLMs Reason Faster with Contrastive Decoding</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Sydney, Peking University, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research focuses on improving the efficiency and interpretability of Monte Carlo Tree Search (MCTS) for Large Language Models (LLMs) by introducing a novel contrastive decoding-based reward model and refining the backpropagation process. Unlike previous work that primarily treated MCTS as a tool, this paper delves into analyzing and improving its components.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet112">
            <div class="start-time-icon" title="Play from here">45:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01259" target="_blank">@arXiv 2410.01259</a>
                    <span class="tweet-title">Overparameterized Models: Degrees of Freedom Get a Random Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley, CMU</span>
                </div>
                <div class="primary-text">
                    This research extends the classical concept of degrees of freedom to the setting of random-X prediction error, which is more relevant to modern machine learning problems. This is achieved by reinterpreting the classical definition and then translating it to the random-X setting.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet113">
            <div class="start-time-icon" title="Play from here">46:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01565" target="_blank">@arXiv 2410.01565</a>
                    <span class="tweet-title">Bayes' Got Your Back: How Neural Networks Learn to Generalize in Context</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Freiburg</span>
                </div>
                <div class="primary-text">
                    This research proposes a new interpretation of neural network training in the context of in-context learning (ICL), viewing it as an approximation of the true posterior distribution rather than the traditional maximum likelihood estimation (MLE). This approach is particularly relevant in the era of large-scale single-epoch training, where datasets are vast and readily available.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet114">
            <div class="start-time-icon" title="Play from here">46:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00944" target="_blank">@arXiv 2410.00944</a>
                    <span class="tweet-title">Hypergraphing Parkinson's: A New Way to Map Motor Impairment</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research uses a heterogeneous hypergraph to analyze multi-modal medical data, which allows for the capture of higher-order relationships between patients beyond simple pairwise connections. This approach is distinct from previous work that often focuses on single modalities or uses homogeneous graphs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet115">
            <div class="start-time-icon" title="Play from here">46:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00132" target="_blank">@arXiv 2410.00132</a>
                    <span class="tweet-title">Traffic Jam? No Problem! New AI Predicts Vehicle Locations and Speeds Using Partial Data.</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Hong Kong, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel deep learning network, CVVLSNet, to estimate vehicle locations and speeds using only partial connected vehicle (CV) trajectory data. Unlike previous methods that rely on full CV data or external sensors, CVVLSNet leverages the limited information available in a mixed traffic environment.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet116">
            <div class="start-time-icon" title="Play from here">47:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01482" target="_blank">@arXiv 2410.01482</a>
                    <span class="tweet-title">Wavelet Magic: Unlocking the Secrets of AI's Black Box</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Paris Sciences et Lettres University, Harvard University, Sorbonne University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for explaining AI models called Wavelet Attribution Method (WAM). Unlike previous methods that focus on pixel-level explanations, WAM leverages the wavelet domain, which captures the structural components of the input data, providing a more comprehensive understanding of the model's decision-making process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet117">
            <div class="start-time-icon" title="Play from here">47:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01208" target="_blank">@arXiv 2410.01208</a>
                    <span class="tweet-title">LLMs:  String Theory for Dummies?  New Research Uncovers Their String Processing Woes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Duke University, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research presents StringLLM, a method for constructing datasets to benchmark the string processing capabilities of LLMs. It differs from previous work by providing a comprehensive evaluation of LLMs across a wide range of string processing tasks and string types.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet118">
            <div class="start-time-icon" title="Play from here">48:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01535" target="_blank">@arXiv 2410.01535</a>
                    <span class="tweet-title">Building Blocks for 3D Scenes:  GaussianBlock Makes Editing a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft, Lancaster University</span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel method called GaussianBlock for 3D scene reconstruction that combines the advantages of both primitives and 3D Gaussians. Unlike previous methods that rely solely on one or the other, GaussianBlock utilizes a hybrid representation, enabling semantically coherent and disentangled representations for precise and physical editing.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet119">
            <div class="start-time-icon" title="Play from here">48:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00117" target="_blank">@arXiv 2410.00117</a>
                    <span class="tweet-title">Robot Perception Gets a Certifiable Makeover: Burer-Monteiro Method to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Massachusetts Institute of Technology, Northeastern University</span>
                </div>
                <div class="primary-text">
                    This paper provides a consolidated overview of the Burer-Monteiro (BM) method, highlighting its application in certifiable robot perception. It emphasizes the importance of the linear independence constraint qualification (LICQ) for successful implementation, a concept not extensively covered in previous research.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet120">
            <div class="start-time-icon" title="Play from here">49:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00857" target="_blank">@arXiv 2410.00857</a>
                    <span class="tweet-title">RAG's Shortcut:  LLMs Take the Easy Way Out!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft, University of Massachusetts  Amherst, University of Maryland  College Park</span>
                </div>
                <div class="primary-text">
                    This research delves into the inner workings of Retrieval Augmented Generation (RAG) systems, specifically examining how language models (LLMs) prioritize information from retrieved context over their own internal knowledge. Unlike previous work that focused on the practical applications of RAG, this study uses mechanistic analysis to understand the model's decision-making process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet121">
            <div class="start-time-icon" title="Play from here">49:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01464" target="_blank">@arXiv 2410.01464</a>
                    <span class="tweet-title">AI Makes Materials Science Move Faster: Flow Matching for Speedy Simulations!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Massachusetts Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This research introduces LIFLOW, a generative framework that accelerates molecular dynamics (MD) simulations for crystalline materials by formulating the task as conditional generation of atomic displacements. Unlike previous methods that focus on biomolecules, LIFLOW accounts for symmetries and handles various atom types under different simulation conditions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet122">
            <div class="start-time-icon" title="Play from here">49:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00288" target="_blank">@arXiv 2410.00288</a>
                    <span class="tweet-title">GARCH-Informed Neural Networks:  Volatility Prediction Gets a Brain Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, Pennsylvania State University, NYU</span>
                </div>
                <div class="primary-text">
                    This research proposes a hybrid deep learning model called GARCH-Informed Neural Network (GINN) that combines the strengths of the GARCH model with the flexibility of an LSTM network. This approach aims to improve the accuracy and generalizability of volatility prediction by incorporating stylized facts and empirical market patterns captured by the GARCH model into the loss function of the LSTM network.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet123">
            <div class="start-time-icon" title="Play from here">50:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00681" target="_blank">@arXiv 2410.00681</a>
                    <span class="tweet-title">Arabic Sign Language Gets a Transformer Makeover: 99.6% Accuracy!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Egypt-Japanese University of Science & Technology, Badya University</span>
                </div>
                <div class="primary-text">
                    This research utilizes transfer learning and transformer-based models for Arabic Alphabet Sign Language recognition, which is a novel approach compared to previous studies that primarily relied on CNN architectures.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet124">
            <div class="start-time-icon" title="Play from here">50:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01316" target="_blank">@arXiv 2410.01316</a>
                    <span class="tweet-title">Kernel Sums: Slicing Through Complexity with QMC!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London, Technical University of Berlin</span>
                </div>
                <div class="primary-text">
                    This paper introduces a quasi-Monte Carlo (QMC) approach to slicing for fast kernel summation, improving upon existing methods like random Fourier features (RFF) and non-QMC slicing. The QMC approach leverages specific sequences of points on the sphere to achieve faster convergence rates.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet125">
            <div class="start-time-icon" title="Play from here">51:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01281" target="_blank">@arXiv 2410.01281</a>
                    <span class="tweet-title">Human Mobility:  When Your GPS Knows You Better Than You Do!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces UIFORMER, a model that incorporates both aleatoric and epistemic uncertainty into human mobility modeling and anomaly detection. Unlike previous work, UIFORMER leverages a dual Transformer architecture to capture complex spatiotemporal dependencies and uses uncertainty estimates to improve prediction accuracy and anomaly scoring.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet126">
            <div class="start-time-icon" title="Play from here">51:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01035" target="_blank">@arXiv 2410.01035</a>
                    <span class="tweet-title">LLMs:  Don't Stop Me Now!  Embedding-Based Scheduling for Faster Responses</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, MIT</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to scheduling LLM requests by using embeddings from the LLM's internal layers to predict output lengths, rather than relying solely on input prompts. This method is more accurate and efficient than previous approaches that used separate models for length prediction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet127">
            <div class="start-time-icon" title="Play from here">52:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00274" target="_blank">@arXiv 2410.00274</a>
                    <span class="tweet-title">AI Conjurers: Building Virtual Worlds with a Magic Wand (and Prompts)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Santa Cruz, Stanford University, University of Texas at Austin...</span>
                </div>
                <div class="primary-text">
                    This research introduces Social Conjurer, a framework for collaborative virtual world creation using AI. Unlike previous work that focused on single-user scene generation, Social Conjurer enables multiple users to build and modify virtual worlds in real-time through language-based prompts and sketches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet128">
            <div class="start-time-icon" title="Play from here">52:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01443" target="_blank">@arXiv 2410.01443</a>
                    <span class="tweet-title">Spine Surgery Gets a Makeover:  No More Radiation, Just a 3D "Mental Map"</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University Hospital Balgrist, University of Zurich</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach for reconstructing 3D spine anatomy using RGB-D data, eliminating the need for radiation-based imaging techniques like CT scans. The method leverages a transformer network, SurgPointTransformer, to learn patterns between visible and hidden anatomical structures, effectively completing the shape of the spine from partial observations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet129">
            <div class="start-time-icon" title="Play from here">52:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00309" target="_blank">@arXiv 2410.00309</a>
                    <span class="tweet-title">AI Makes Friends: New Dataset Helps Robots Understand Human Touch</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Snapchat</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel data generation method for human mesh estimation (HME) that leverages large vision language models (LVLMs) to automatically create paired pseudo-ground truth meshes for scenes with closely interacting individuals. This approach differs from previous work by using LVLMs to annotate contact maps, which guide test-time optimization to produce paired image and pseudo-ground truth meshes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet130">
            <div class="start-time-icon" title="Play from here">53:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01309" target="_blank">@arXiv 2410.01309</a>
                    <span class="tweet-title">LLMs:  Getting Free Bits Back From Rotational Symmetries</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new method for compressing Large Language Models (LLMs) by leveraging rotational symmetries in the weights of Transformer blocks pruned by SliceGPT. This approach differs from previous compression techniques by explicitly accounting for these symmetries, leading to a more efficient encoding of the model weights.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet131">
            <div class="start-time-icon" title="Play from here">53:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01598" target="_blank">@arXiv 2410.01598</a>
                    <span class="tweet-title">Travel Recommender Systems Get a Brain Boost: LLMs Help Find the Perfect Trip!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research introduces a new query reformulation method called Elaborative Subtopic Query Reformulation (EQR) that uses large language models (LLMs) to generate multiple subtopics with detailed elaborations for broad and indirect travel queries. Unlike previous methods that focus on either expanding the breadth or depth of queries, EQR combines both aspects, leading to more effective retrieval of relevant travel destinations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet132">
            <div class="start-time-icon" title="Play from here">54:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01665" target="_blank">@arXiv 2410.01665</a>
                    <span class="tweet-title">Cardiac MRI Gets a Vision Makeover: 36 Million Images, One Super Model!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Siemens Healthineers, Technical University of Munich, Imperial College London</span>
                </div>
                <div class="primary-text">
                    This research introduces a vision foundation model specifically trained on 36 million cardiac MRI images, a significant departure from previous work that often focused on smaller datasets or individual tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet133">
            <div class="start-time-icon" title="Play from here">54:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01340" target="_blank">@arXiv 2410.01340</a>
                    <span class="tweet-title">Neural Networks Learn Physics:  Predicting Vibrations with a Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research uses Physics-Informed Neural Networks (PINNs) to estimate the response and parameters of dynamical systems, incorporating known physical laws directly into the neural network's loss function. This approach differs from previous work by embedding physics knowledge into the learning process, leading to more interpretable models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet134">
            <div class="start-time-icon" title="Play from here">54:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01072" target="_blank">@arXiv 2410.01072</a>
                    <span class="tweet-title">Virtual Staining: From Patchy to Perfect, a Whole Slide Image Revolution!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, UC Los Angeles, University of Pennsylvania</span>
                </div>
                <div class="primary-text">
                    This research introduces CC-WSI-Net, a framework that extends existing GAN models to generate seamless virtual whole slide images (WSIs) by integrating content and color consistency modules. Unlike previous methods that often suffer from inconsistencies at tile boundaries, CC-WSI-Net ensures a smooth and accurate representation of the entire slide.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet135">
            <div class="start-time-icon" title="Play from here">55:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00404" target="_blank">@arXiv 2410.00404</a>
                    <span class="tweet-title">Sparse Views, Big Results: Reconstructing Coronary Arteries with 3D Gaussians</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Science and Technology of China, Suzhou Institute for Advance Research, Key Laboratory of Precision and Intelligent Chemistry...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel 3D Gaussian Representation (3DGR) scheme for coronary artery reconstruction from ultra-sparse 2D X-ray projections. Unlike previous methods that rely on dense view data or struggle with sparse data, 3DGR-CAR leverages the efficiency of 3D Gaussian representation and a Gaussian center predictor to achieve accurate reconstruction with only two views.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet136">
            <div class="start-time-icon" title="Play from here">55:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00036" target="_blank">@arXiv 2410.00036</a>
                    <span class="tweet-title">AI-Powered Interview Assistant:  Say Goodbye to Post-Interview Blues!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington</span>
                </div>
                <div class="primary-text">
                    This research introduces InsightPulse, an IoT-based system that combines hardware and software to enhance the UX interview process. Unlike previous work that focuses solely on software solutions, InsightPulse provides real-time support during interviews through speech analysis and AI-generated summaries and follow-up questions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet137">
            <div class="start-time-icon" title="Play from here">56:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00916" target="_blank">@arXiv 2410.00916</a>
                    <span class="tweet-title">IBM's Quantum Computer: From 5 Qubits to 1,000+ and Counting!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Ain Shams University, Zewail City of Science  Technology and Innovation</span>
                </div>
                <div class="primary-text">
                    This research focuses on the evolution of IBM Quantum's hardware, detailing the progression of their processors from the 5-qubit Canary to the 1,121-qubit Condor. It also highlights the shift in IBM's strategy from prioritizing qubit count to focusing on enhanced error resistance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet138">
            <div class="start-time-icon" title="Play from here">56:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00145" target="_blank">@arXiv 2410.00145</a>
                    <span class="tweet-title">Neural Networks Need a Safety Net: New Algorithm Keeps Robots in Line!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces Constraint-Aware Refinement for Verification (CARV), a new algorithm that uses safety constraints to refine reachable set over-approximations (RSOAs) only when necessary. This approach differs from previous methods by explicitly incorporating safety constraints into the refinement process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet139">
            <div class="start-time-icon" title="Play from here">56:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01669" target="_blank">@arXiv 2410.01669</a>
                    <span class="tweet-title">Covariance Networks Go on a Diet: Sparsity Makes Them Smarter and Faster</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Delft University of Technology, University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research introduces Sparse Covariance Neural Networks (S-VNNs), which apply sparsification techniques to the sample covariance matrix before convolution. This differs from previous work by addressing the limitations of traditional VNNs, which are sensitive to noisy covariance estimates and computationally expensive.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet140">
            <div class="start-time-icon" title="Play from here">57:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01061" target="_blank">@arXiv 2410.01061</a>
                    <span class="tweet-title">Deep-Sea Detectives: AI Uncovers Buried Barrels with 3D Vision!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Scripps Institution of Oceanography, UCSD</span>
                </div>
                <div class="primary-text">
                    This research introduces BarrelNet, a modified PointNet architecture, trained on synthetic data to estimate the pose and burial fraction of barrels from point clouds. This approach differs from previous work by specifically addressing the challenges of underwater imagery and partial visibility of objects.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet141">
            <div class="start-time-icon" title="Play from here">57:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01322" target="_blank">@arXiv 2410.01322</a>
                    <span class="tweet-title">Out-of-Distribution Data?  FORTE Knows How to Spot the Imposters!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Case Western Reserve University, Google</span>
                </div>
                <div class="primary-text">
                    This research proposes a new framework called FORTE that uses multiple representation learning techniques and non-parametric density estimators to detect out-of-distribution (OOD) data and synthetic images. Unlike previous methods that rely solely on likelihood-based generative models, FORTE leverages a combination of diverse feature extraction techniques and anomaly detection algorithms to improve robustness and accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet142">
            <div class="start-time-icon" title="Play from here">58:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00509" target="_blank">@arXiv 2410.00509</a>
                    <span class="tweet-title">Bias Busters:  Unmasking the Hidden Influences in Personalized Medicine</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, University of Zurich</span>
                </div>
                <div class="primary-text">
                    This research delves into the impact of different types of treatment assignment biases on counterfactual prediction and biomarker identification in precision medicine. Unlike previous work that often focuses on fixed treatment policies, this study models various characteristics of the underlying observational treatment policy in distinct clinical settings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet143">
            <div class="start-time-icon" title="Play from here">58:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00948" target="_blank">@arXiv 2410.00948</a>
                    <span class="tweet-title">Shrinking Deep Learning Brains for Real-Time Fluorescence Imaging: A Tiny Model with Big Impact!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Rensselaer, IBM</span>
                </div>
                <div class="primary-text">
                    This research focuses on compressing recurrent neural networks (RNNs) for real-time fluorescence lifetime imaging (FLI) on resource-constrained FPGA boards. The authors explore various compression techniques, including weight reduction, knowledge distillation (KD), post-training quantization (PTQ), and quantization-aware training (QAT), to reduce model size and computational load while preserving inference accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet144">
            <div class="start-time-icon" title="Play from here">58:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00035" target="_blank">@arXiv 2410.00035</a>
                    <span class="tweet-title">Uzbek Speech Gets a Makeover: New Corpus Bridges Cyrillic and Latin Gaps</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Redmond High School, University of Washington</span>
                </div>
                <div class="primary-text">
                    This research introduces FeruzaSpeech, a new speech corpus for the Uzbek language that includes transcripts in both Cyrillic and Latin alphabets. This is unique because previous Uzbek speech corpuses have only used one alphabet, making it difficult to train models that can handle both.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet145">
            <div class="start-time-icon" title="Play from here">59:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.00544" target="_blank">@arXiv 2410.00544</a>
                    <span class="tweet-title">MFBO:  The  Secret  Weapon  for  Speeding  Up  Materials  Discovery</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research provides a systematic evaluation of multi-fidelity Bayesian optimization (MFBO) methods, offering guidelines and recommendations for its application in materials and molecular research. It differs from previous work by focusing on the impact of low-fidelity source cost and informativeness on MFBO performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet146">
            <div class="start-time-icon" title="Play from here">59:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.01628" target="_blank">@arXiv 2410.01628</a>
                    <span class="tweet-title">Predicting the Future of Self-Driving Cars:  Unveiling the Secrets of Uncertainty!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Bosch Center for Artificial Intelligence, University of Freiburg</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method for quantifying and decomposing uncertainty in trajectory prediction models for autonomous driving. Unlike previous work, it utilizes an information-theoretic approach to measure aleatoric and epistemic uncertainty, allowing for a more nuanced understanding of the sources of uncertainty in predictions.
                </div>
            </div>
        </div>

        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202410031450_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>    
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>
</html>