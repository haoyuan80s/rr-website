<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - AI Paper Picks of the Day</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 80px;"></div>
            <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                Freshest
                Top Picks:
                <span class="highlightNumber" style="font-size: 28px;">50</span> out of <span
                    class="highlightNumber">251</span> arXiv AI
                papers
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="line-height: 1.2;">just released on</div>
                <div id="data-default-date" data-date="2024-08-29"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 10px;">
            </div>
        </div>

        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">
                00:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15425" target="_blank">
                        @arXiv 2408.15425
                    </a>
                    <span class="tweet-title">
                        IndyCar AI: From Zero to 150 mph in Three Years!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Carnegie Mellon University, University of Pittsburgh, Massachusetts Institute of Technology...
                    </span>
                </div>
                <div class="primary-text">
                    This research presents a modular and fast autonomy software stack for autonomous racing vehicles,
                    specifically designed for the Indy Autonomous Challenge. The stack is designed to be modular,
                    allowing for easy replacement of modules as requirements change, and fast, ensuring that algorithms
                    finish execution quickly and deterministically.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">
                01:33
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15666" target="_blank">
                        @arXiv 2408.15666
                    </a>
                    <span class="tweet-title">
                        StyleRemix: Obfuscating Authors Like a DJ Remixes Music!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces STYLEREMIX, a method for authorship obfuscation that uses pre-trained LoRA
                    modules to specifically target and modify stylistic elements of a text, making it more interpretable
                    and controllable than previous methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">
                02:07
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15247" target="_blank">
                        @arXiv 2408.15247
                    </a>
                    <span class="tweet-title">
                        Building AI Teams: No-Code Tool Lets You Drag-and-Drop Your Way to Multi-Agent Systems
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces AUTOGEN STUDIO, a no-code tool that allows developers to build and debug
                    multi-agent systems using a drag-and-drop interface. This differs from previous work that primarily
                    focused on code-first approaches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">
                02:27
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15399" target="_blank">
                        @arXiv 2408.15399
                    </a>
                    <span class="tweet-title">
                        RAMs Get a Statistical Makeover: Unlocking the Secrets of Retrieval-Augmented Models
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a statistical framework for analyzing retrieval-augmented models (RAMs),
                    focusing on a unified objective for end-to-end training and deriving excess risk bounds to
                    understand their generalization behavior. This differs from previous work by providing a more
                    comprehensive theoretical understanding of RAMs, including the interplay between retriever and
                    predictor components.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">
                02:58
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15450" target="_blank">
                        @arXiv 2408.15450
                    </a>
                    <span class="tweet-title">
                        Stop AI From Spouting Nonsense: New Trick Makes Image Generators More Polite
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on controlling the output of generative models at inference time, rather than
                    modifying the training dataset or process. It specifically addresses image memorization by nudging
                    the latent representation of the model away from unwanted concepts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">
                03:31
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15484" target="_blank">
                        @arXiv 2408.15484
                    </a>
                    <span class="tweet-title">
                        Binary Brains: NAS-BNN Makes Tiny AI Models Think Big!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, Stony Brook University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel neural architecture search scheme specifically designed for binary
                    neural networks (BNNs). Unlike previous NAS methods for BNNs, which were relatively straightforward,
                    this approach introduces a carefully designed search space and three training techniques tailored
                    for binary supernets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">
                03:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15778" target="_blank">
                        @arXiv 2408.15778
                    </a>
                    <span class="tweet-title">
                        LLMs: Rule-Followers or Rule-Benders? A New Game Tests Their Logic Skills!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Zhipu.AI, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces LOGICGAME, a benchmark specifically designed to evaluate the rule-based
                    reasoning abilities of LLMs. Unlike traditional benchmarks, LOGICGAME focuses on assessing how well
                    LLMs can understand, execute, and plan based on a set of predefined rules, rather than relying on
                    general knowledge or instruction-following alone.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">
                04:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15374" target="_blank">
                        @arXiv 2408.15374
                    </a>
                    <span class="tweet-title">
                        CycleGAN's Got a New Trick: Making Images More Realistic with Less Pixel-Perfect Obsession
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes modifications to the cycle consistency loss used in CycleGAN, a framework for
                    image-to-image translation. Instead of enforcing pixel-level consistency, the authors introduce a
                    new approach that focuses on consistency at the feature level extracted by the discriminator
                    network. This allows for more realistic image translations with fewer artifacts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">
                04:46
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15980" target="_blank">
                        @arXiv 2408.15980
                    </a>
                    <span class="tweet-title">
                        Robots Learn New Tricks Without Training: In-Context Imitation Learning Takes the Stage!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of California Berkeley, Autodesk Research
                    </span>
                </div>
                <div class="primary-text">
                    This research explores in-context imitation learning for robots, where a model learns new tasks by
                    interpreting contextual information provided during the input phase, without updating its underlying
                    policy parameters. This differs from previous work that often requires additional training or
                    fine-tuning for new tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">
                05:11
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15549" target="_blank">
                        @arXiv 2408.15549
                    </a>
                    <span class="tweet-title">
                        Chatbots That Actually Listen: New Research Uses Real-Time Feedback to Make LLMs More
                        Human-Friendly
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Southern California, Texas A&amp;M University, Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces WILDFEEDBACK, a framework that uses real-time user interactions to create
                    preference datasets for training LLMs. Unlike previous methods that rely on human or LLM
                    annotations, WILDFEEDBACK captures authentic user feedback directly from conversations, addressing
                    the limitations of subjectivity and bias in existing approaches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">
                05:40
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15664" target="_blank">
                        @arXiv 2408.15664
                    </a>
                    <span class="tweet-title">
                        MoE Models: Balancing the Load Without the Drama!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new load balancing strategy for Mixture-of-Experts (MoE) models called
                    Loss-Free Balancing. Unlike previous methods that rely on auxiliary losses, this approach directly
                    adjusts the routing scores of each expert based on their load, avoiding the introduction of
                    interference gradients during training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">
                06:10
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15998" target="_blank">
                        @arXiv 2408.15998
                    </a>
                    <span class="tweet-title">
                        Vision Experts Unite! Eagle Soars with Multimodal LLMs
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nvidia
                    </span>
                </div>
                <div class="primary-text">
                    This research systematically explores the design space for multimodal LLMs using a mixture of vision
                    encoders, focusing on the selection and integration of multiple vision experts. Unlike previous
                    work, it emphasizes the importance of unlocking vision encoders during training and finds that
                    simple channel concatenation is a surprisingly effective fusion strategy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">
                06:33
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15495" target="_blank">
                        @arXiv 2408.15495
                    </a>
                    <span class="tweet-title">
                        Stop the Collapse! New Trick Makes Neural Networks More Expressive
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Massachusetts Institute of Technology, NTT Research, École Polytechnique Fédérale de Lausanne
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a simple, theoretically justified method to remove symmetries in neural network
                    loss functions. Unlike previous work that focused on characterizing the effects of symmetries, this
                    research provides a practical solution to prevent models from getting stuck in low-capacity states.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">
                07:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15519" target="_blank">
                        @arXiv 2408.15519
                    </a>
                    <span class="tweet-title">
                        Dementia Detection: Cameras See Deeper Than You Think!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto, Vienna University of Technology, Toronto Metropolitan University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a depth-weighted loss function to train a convolutional autoencoder,
                    addressing the issue of false positives in video anomaly detection systems for dementia care. This
                    approach differs from previous work by incorporating pixel depth as a weight factor, ensuring that
                    events happening both near and far from the camera are analyzed with equal importance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">
                07:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15496" target="_blank">
                        @arXiv 2408.15496
                    </a>
                    <span class="tweet-title">
                        Mamba's Long Memory: How Researchers Gave a Chatbot a Longer Attention Span
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, Meituan
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on improving the ability of Mamba models, a type of language model, to process
                    long sequences of text. Unlike previous approaches that relied on attention mechanisms, ReMamba uses
                    a two-stage process involving selective compression and adaptation to enhance long-context
                    performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">
                07:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15251" target="_blank">
                        @arXiv 2408.15251
                    </a>
                    <span class="tweet-title">
                        TrajFM: The Trajectory Transformer That's Got Your Back (and Your GPS)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Aalborg University, CMU, Beijing Jiaotong University...
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces TrajFM, a vehicle trajectory foundation model that excels in both region and
                    task transferability. Unlike previous methods that rely on embedding vectors, TrajFM utilizes a
                    novel trajectory masking and recovery scheme to unify the generation processes of different tasks,
                    enabling it to transfer between tasks without retraining.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">
                08:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15621" target="_blank">
                        @arXiv 2408.15621
                    </a>
                    <span class="tweet-title">
                        Federated Learning's Privacy Paradox: How More Training Doesn't Mean More Leaks!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Sydney, Sun Yat-sen University, Nanyang Technological University
                    </span>
                </div>
                <div class="primary-text">
                    This research delves into the privacy of federated learning (FL) with differential privacy (DP)
                    using a new framework called f-DP. Unlike previous analyses that relied on composition theorems,
                    this study utilizes shifted interpolation techniques to achieve a tighter and convergent privacy
                    bound, even for non-convex objectives.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">
                08:54
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15710" target="_blank">
                        @arXiv 2408.15710
                    </a>
                    <span class="tweet-title">
                        Conan-Embedding: More Negative Samples, Better Text Embeddings!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a dynamic hard negative mining method that iteratively updates negative
                    examples during training, unlike previous methods that rely on pre-processing.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">
                09:19
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15971" target="_blank">
                        @arXiv 2408.15971
                    </a>
                    <span class="tweet-title">
                        AI Agents: From Solo to Squad, Can They Really Play Nice?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark called BattleAgentBench, which evaluates the collaborative
                    and competitive capabilities of LLMs in multi-agent systems. Unlike previous benchmarks,
                    BattleAgentBench focuses on fine-grained evaluations across different difficulty levels, including
                    single-agent, paired-agent, and multi-agent scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">
                09:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15898" target="_blank">
                        @arXiv 2408.15898
                    </a>
                    <span class="tweet-title">
                        Airfoil Diffusion: A New Way to Design Wings That's Not Just Hot Air!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research uses a diffusion model to generate new airfoil shapes, unlike previous methods that
                    relied on predefined design parameters or template geometries.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">
                09:59
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15313" target="_blank">
                        @arXiv 2408.15313
                    </a>
                    <span class="tweet-title">
                        Stop the Bots from Going Rogue: New AI Training Method Makes LLMs Both Helpful and Harmless
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        King Abdullah University of Science and Technology, University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a supervised learning framework called Bi-Factorial Preference Optimization
                    (BFPO) that directly balances safety and helpfulness in large language models (LLMs). Unlike
                    previous methods that rely on separate reward models for each objective, BFPO re-parameterizes the
                    multi-objective reinforcement learning (RL) objective into a single supervised learning objective.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">
                10:25
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15252" target="_blank">
                        @arXiv 2408.15252
                    </a>
                    <span class="tweet-title">
                        Radio Maps Get a 3D Makeover: SpectrumNet's Got Your Wireless Signals Covered!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Chinese Academy of Sciences, Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces SpectrumNet, a multiband 3D radio map dataset that includes terrain and
                    climate information, unlike previous datasets that focused on 2D urban scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">
                10:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15792" target="_blank">
                        @arXiv 2408.15792
                    </a>
                    <span class="tweet-title">
                        LLMs: Scheduling Like a Boss with Ranking!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UCSD, Tsinghua University, Snowflake...
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel approach to scheduling LLM requests by predicting the relative ranking
                    of their generation lengths, rather than attempting to predict the exact length. This differs from
                    previous work that focused on directly predicting the generation length.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">
                11:20
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15270" target="_blank">
                        @arXiv 2408.15270
                    </a>
                    <span class="tweet-title">
                        Teaching Robots to Ball Like Pros: A Data-Driven Approach to Learning Reusable Basketball Skills
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Hong Kong University of Science and Technology, CMU, Peking University...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a data-driven approach called SkillMimic that learns reusable basketball
                    skills from human-object demonstrations. Unlike previous methods that rely on manually designed
                    rewards, SkillMimic uses a unified configuration to learn diverse skills from human-ball motion
                    datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">
                11:46
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15436" target="_blank">
                        @arXiv 2408.15436
                    </a>
                    <span class="tweet-title">
                        Power Grids: When Inertia Goes on a Rollercoaster Ride!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC San Diego, University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes an online event-triggered switching algorithm for frequency control in power
                    grids with variable inertia. Unlike previous work that focuses on constant inertia, this approach
                    dynamically selects the most suitable controller from a pool of pre-trained controllers, each
                    optimized for a specific inertia level.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">
                12:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15632" target="_blank">
                        @arXiv 2408.15632
                    </a>
                    <span class="tweet-title">
                        Robot Evolution: How AI Designed a Biped That's Lighter Than Air (Almost)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Shanghai University, University of Michigan
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces the SERL algorithm, which combines reinforcement learning for locomotion
                    tasks with evolutionary algorithms to optimize the structural parameters of bipedal robots. Unlike
                    previous work that focuses on either control or structural design, SERL treats these aspects as an
                    integrated whole.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">
                12:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15266" target="_blank">
                        @arXiv 2408.15266
                    </a>
                    <span class="tweet-title">
                        AI Doctors: Can You Tell the Difference?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research goes beyond simply evaluating the accuracy of AI-generated medical responses. It
                    investigates how non-experts perceive and evaluate these responses compared to those provided by
                    doctors, highlighting the potential for over-reliance on AI even when it's inaccurate.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">
                13:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15741" target="_blank">
                        @arXiv 2408.15741
                    </a>
                    <span class="tweet-title">
                        Vectorizing Images with Gradient Fills: A Segmentation-Guided Approach
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a segmentation-guided vectorization framework that extends the capabilities
                    of previous methods to include radial gradients. Unlike prior work that focused on solid colors,
                    this approach leverages gradient-aware segmentation to guide the initialization and optimization of
                    Bézier paths, resulting in more accurate and visually appealing vector graphics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">
                13:27
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15630" target="_blank">
                        @arXiv 2408.15630
                    </a>
                    <span class="tweet-title">
                        Code Validation: No Execution, No Test Cases, Just LLMs!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes CodeSift, a framework that uses LLMs to validate code without requiring
                    execution, reference code, or test cases. This approach differs from previous methods that rely on
                    these elements, making it more scalable and efficient.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">
                13:46
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15281" target="_blank">
                        @arXiv 2408.15281
                    </a>
                    <span class="tweet-title">
                        Encrypting Videos with a Neural Network: It's Like a Secret Code for Your Cat Videos!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, University of California Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new video encryption method that uses implicit neural representation (INR)
                    instead of traditional encryption algorithms. Unlike previous methods that encrypt the entire video
                    stream, this approach represents the video as a neural network, making it more adaptable and
                    efficient.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">
                14:10
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15381" target="_blank">
                        @arXiv 2408.15381
                    </a>
                    <span class="tweet-title">
                        State-ful Factorization: When AI Agents Get a Memory Boost
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT, Northeastern University
                    </span>
                </div>
                <div class="primary-text">
                    This research delves into the use of state information in value factorization for multi-agent
                    reinforcement learning. Unlike previous work that primarily focused on stateless functions, this
                    paper analyzes the impact of state information on existing methods and proposes a new algorithm,
                    DuelMIX, that leverages dueling networks for improved performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">
                14:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15443" target="_blank">
                        @arXiv 2408.15443
                    </a>
                    <span class="tweet-title">
                        Lazy Pathfinding: A Search Algorithm That's Not in a Hurry!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces LaCAS*, a pathfinding algorithm that generates successors gradually as the
                    search progresses, rather than all at once. This approach is different from previous work that
                    either limits the number of successors or generates all successors upfront, potentially sacrificing
                    completeness or efficiency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">
                15:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15272" target="_blank">
                        @arXiv 2408.15272
                    </a>
                    <span class="tweet-title">
                        ECG From One Lead? Deep Learning Makes It Happen!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on estimating ECG intervals using only lead-I ECG, a single lead, unlike
                    previous studies that typically used 12-lead ECGs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">
                15:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15784" target="_blank">
                        @arXiv 2408.15784
                    </a>
                    <span class="tweet-title">
                        Subsampling: The Secret Sauce for Overparameterized Models?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU, UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research extends previous work on the relationship between subsampling and ridge regularization
                    by considering more general feature structures and weight matrices. It also provides a novel
                    interpretation of the equivalence paths in terms of matching effective degrees of freedom.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">
                16:07
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15524" target="_blank">
                        @arXiv 2408.15524
                    </a>
                    <span class="tweet-title">
                        Say Goodbye to Fuzzy 3D Scenes: Ray-Specific Density for Sharper Indoor Reconstructions
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Amsterdam, CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes using the Signed Ray Distance Function (SRDF) instead of the Signed Distance
                    Function (SDF) to model the density function in neural scene reconstruction. Unlike SDF, SRDF
                    considers only the surface along the camera ray, leading to a more accurate representation of the
                    scene's density.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">
                16:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15905" target="_blank">
                        @arXiv 2408.15905
                    </a>
                    <span class="tweet-title">
                        Metadynamics Meets GFlowNets: A Match Made in Sampling Heaven!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        IBM, University of Edinburgh
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Adapted Metadynamics, a variant of metadynamics, as an exploration strategy
                    for continuous GFlowNets. Unlike previous work that focused on discrete GFlowNets, this paper
                    explores the continuous domain, leveraging the local connectedness of continuous spaces for more
                    efficient exploration.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">
                17:04
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15409" target="_blank">
                        @arXiv 2408.15409
                    </a>
                    <span class="tweet-title">
                        LLM Research: A Wild West of Claims and Flaws
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research systematically analyzes over 2,000 LLM papers, evaluating them based on criteria like
                    statistical tests, reproducibility, and claims of emergent behavior. It differs from previous work
                    by quantifying the extent to which these issues occur in the literature.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">
                17:25
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15819" target="_blank">
                        @arXiv 2408.15819
                    </a>
                    <span class="tweet-title">
                        Molecule Matchmaker: AI Helps Unravel Chemical Mixtures
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to analyzing chemical mixtures by combining machine
                    learning molecular embedding methods with a graph-based ranking system. This method leverages the
                    chemical relationships between molecules to improve the accuracy of identifying mixture components,
                    setting it apart from previous methods that primarily rely on spectral matching.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">
                17:46
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15265" target="_blank">
                        @arXiv 2408.15265
                    </a>
                    <span class="tweet-title">
                        BERT Gets a Makeover: Multitasking and GANs for NLP Superpowers
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel BERT architecture for multitask fine-tuning, incorporating layer
                    sharing, a triplet architecture, custom sentence pair tokenization, loss pairing, and gradient
                    surgery. It also applies generative adversarial learning to BERT, constructing a conditional
                    generator model that maps from latent space to create fake embeddings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">
                18:05
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15294" target="_blank">
                        @arXiv 2408.15294
                    </a>
                    <span class="tweet-title">
                        Unmasking the Secrets of Readmission: A Graph-Based Ablation Study
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        KU Leuven, IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research uses a systematic ablation study to identify the most predictive features in
                    person-centric knowledge graphs (PKGs) for readmission prediction. Unlike previous work, it focuses
                    on systematically removing different features from the PKGs to understand their impact on model
                    performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">
                18:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15508" target="_blank">
                        @arXiv 2408.15508
                    </a>
                    <span class="tweet-title">
                        Speech Backdoor Attacks: When Your Alexa Starts Spouting Secrets!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Xiangtan University, Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research explores a new type of speech backdoor attack that leverages emotional voice
                    conversion, a technique that alters the emotional tone of speech while preserving its content. This
                    differs from previous methods that focused on adding noise or modifying specific speech components.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">
                18:48
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15890" target="_blank">
                        @arXiv 2408.15890
                    </a>
                    <span class="tweet-title">
                        Brain Scans Get a Makeover: Diffusion Models Harmonize Multi-Site Neuroimaging Data
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University College London
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new diffusion model called the disentangled diffusion autoencoder (DDAE)
                    for harmonizing multi-site neuroimaging data. Unlike previous methods, the DDAE can control specific
                    aspects of image generation, allowing for more precise site adjustment while preserving biological
                    variability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">
                19:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15901" target="_blank">
                        @arXiv 2408.15901
                    </a>
                    <span class="tweet-title">
                        MoE Models Get a Makeover: Nexus Makes Experts More Adaptable!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Cohere, University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces Nexus, a novel Mixture-of-Experts (MoE) architecture that uses domain-specific
                    data to learn a projection layer for the router. This allows for the efficient addition of new
                    experts trained on unseen data domains without requiring large-scale MoE retraining.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">
                19:49
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15366" target="_blank">
                        @arXiv 2408.15366
                    </a>
                    <span class="tweet-title">
                        COMET's Got Problems: A Deep Dive into the Pitfalls of a Popular Translation Metric
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich, University of Edinburgh
                    </span>
                </div>
                <div class="primary-text">
                    This research delves into the unexpected behaviors of the COMET metric, a popular tool for
                    evaluating machine translation quality. Unlike previous work that focused on COMET's strengths, this
                    paper highlights potential pitfalls arising from technical issues, data biases, and inconsistent
                    usage.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">
                20:10
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15899" target="_blank">
                        @arXiv 2408.15899
                    </a>
                    <span class="tweet-title">
                        Drones Gone Wild: AI Makes Sky Shows a Breeze!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Zaragoza, Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research adapts flow matching algorithms, a technique typically used for 2D image generation,
                    to create 3D point clouds for drone swarms. This approach differs from previous methods that relied
                    on diffusion models, which often resulted in less smooth and collision-prone trajectories.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">
                20:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15261" target="_blank">
                        @arXiv 2408.15261
                    </a>
                    <span class="tweet-title">
                        Civiverse: Unveiling the NSFW Secrets of AI Art Prompts
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This research analyzes the CivitAI platform, a leading open-source text-to-image (TTI) platform, to
                    understand user engagement and the cultural implications of these models. Unlike previous work that
                    focused on training data or visual outputs, this study examines the semantic characteristics of
                    user-generated prompts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">
                21:10
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15533" target="_blank">
                        @arXiv 2408.15533
                    </a>
                    <span class="tweet-title">
                        Hallucinating LLMs? LRP4RAG to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Science and Technology of China
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes LRP4RAG, a method that uses Layer-wise Relevance Propagation (LRP) to detect
                    hallucinations in Retrieval-Augmented Generation (RAG) systems. Unlike previous methods that focus
                    on uncertainty or perturbations, LRP4RAG analyzes the relevance between the input and output of the
                    RAG generator to identify inconsistencies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">
                21:48
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15461" target="_blank">
                        @arXiv 2408.15461
                    </a>
                    <span class="tweet-title">
                        Generating Realistic Hands with Just 1,000 Images: A Handful of Data, a World of Possibilities!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, Singapore Management University, University of Science and Technology of
                        China
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new method called Hand1000 that generates realistic hand images from text
                    descriptions using only 1,000 training images. This is a significant improvement over previous
                    methods that required hundreds of thousands of images. Hand1000 achieves this by incorporating hand
                    gesture information and optimizing text embeddings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">
                22:15
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15297" target="_blank">
                        @arXiv 2408.15297
                    </a>
                    <span class="tweet-title">
                        YOLO-Stutter: Speech Fluency's New BFF, Detecting Dysfluencies with a Region-Wise Twist!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces YOLO-Stutter, an end-to-end method for detecting speech dysfluencies.
                    Unlike previous rule-based systems, YOLO-Stutter directly predicts dysfluencies and their time
                    regions from speech and text input, without relying on handcrafted templates.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">
                22:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.15996" target="_blank">
                        @arXiv 2408.15996
                    </a>
                    <span class="tweet-title">
                        Zero-Shot Action Detection: When CLIP Learns to See the Whole Picture
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        National Tsing Hua University, NVIDIA
                    </span>
                </div>
                <div class="primary-text">
                    This research adapts the CLIP model for zero-shot spatio-temporal action detection, focusing on
                    individual actions within a video. Unlike previous work that primarily focused on fully-supervised
                    learning, this approach leverages the model's visual-language knowledge to recognize unseen actions.
                </div>
            </div>
        </div>


        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202408291104_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>

</html>