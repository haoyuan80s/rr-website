
daily_data = {
    "date": "2024-09-06",
    "tweets": [
            {
                "startTime": "01:01",
                "arxivId": "2409.03753",
                "arxivLink": "https://arxiv.org/abs/2409.03753",
                "title": "Million-Scale Chat Logs: A Visual Feast for Researchers!",
                "institute": "University of Waterloo, Cornell University, University of Southern California...",
                "text": "This research introduces WILDVIS, a tool that allows researchers to analyze large-scale chat logs by combining traditional search functionalities with embedding-based visualization. This approach differs from previous work by offering a more intuitive and interactive way to explore the vast amounts of data generated by user-chatbot interactions.",
                "paper-title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
                "image-path": "flux_paper_image/2409.03753_1725760856.png"
            },

            {
                "startTime": "01:21",
                "arxivId": "2409.03021",
                "arxivLink": "https://arxiv.org/abs/2409.03021",
                "title": "LLMs: Not Just Word Spitters, They Think in Concepts!",
                "institute": "National Taiwan University, CMU, UC Los Angeles",
                "text": "This paper introduces a new framework called CLUE that measures uncertainty in LLMs at the concept level, breaking down generated text into individual concepts and assessing their uncertainty separately. This differs from previous methods that focused on sequence-level uncertainty, treating the entire output as a single unit.",
                "paper-title": "CLUE: Concept-Level Uncertainty Estimation for Large Language Models",
                "image-path": "flux_paper_image/2409.03021_1725760014.png"
            },

            {
                "startTime": "01:49",
                "arxivId": "2409.03682",
                "arxivLink": "https://arxiv.org/abs/2409.03682",
                "title": "MAML's New Trick: First-Order Meta-Learning Gets a Smooth Makeover!",
                "institute": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne",
                "text": "This paper proposes a new first-order variant of MAML, called FO-B-MAML, that avoids the computational and memory burdens of second-order methods. Unlike previous first-order approaches, FO-B-MAML has a bias that can be made arbitrarily small, allowing for convergence to any desired precision. The paper also shows that the MAML objective satisfies a generalized smoothness assumption, suggesting that clipped gradient descent is better suited for this problem.",
                "paper-title": "A New First-Order Meta-Learning Algorithm with Convergence Guarantees",
                "image-path": "flux_paper_image/2409.03682_1725761298.png"
            },

            {
                "startTime": "02:10",
                "arxivId": "2409.03149",
                "arxivLink": "https://arxiv.org/abs/2409.03149",
                "title": "MGP Gets a Time Machine: New Model Tracks Dynamic Correlations!",
                "institute": "Peking University",
                "text": "This research proposes a non-stationary multi-output Gaussian process (MGP) model that can capture both dynamic and sparse correlations among outputs. Unlike previous MGP models, this one uses a spike-and-slab prior to automatically decide which sources are informative to the target output during the training process.",
                "paper-title": "Non-stationary and Sparsely-correlated Multi-output Gaussian Process with Spike-and-Slab Prior",
                "image-path": "flux_paper_image/2409.03149_1725761613.png"
            },

            {
                "startTime": "02:34",
                "arxivId": "2409.03215",
                "arxivLink": "https://arxiv.org/abs/2409.03215",
                "title": "AI Agents Get a New Set of Wheels: xLAM Models Drive Function-Calling Performance",
                "institute": "Salesforce AI Research",
                "text": "This research introduces xLAM, a series of large action models specifically designed for AI agent tasks. Unlike previous work that often relies on proprietary models, xLAM is open-source and trained on a unified, augmented, and synthesized dataset, addressing the scarcity of high-quality agent datasets and the lack of standard protocols in this area.",
                "paper-title": "xLAM: A Family of Large Action Models to Empower AI Agent Systems",
                "image-path": "flux_paper_image/2409.03215_1725760034.png"
            },

            {
                "startTime": "02:58",
                "arxivId": "2409.03685",
                "arxivLink": "https://arxiv.org/abs/2409.03685",
                "title": "Robot Vision: Learning to See Like a Human, One View at a Time!",
                "institute": "Stanford University, Toyota Research Institute",
                "text": "This research explores using single-image novel view synthesis models to augment training data for robotic policies, making them more robust to changes in camera viewpoint. Unlike previous methods that rely on multi-view data or explicit 3D representations, this approach leverages the power of generative models to synthesize novel views from a single image.",
                "paper-title": "View-Invariant Policy Learning via Zero-Shot Novel View Synthesis",
                "image-path": "flux_paper_image/2409.03685_1725760696.png"
            },

            {
                "startTime": "03:19",
                "arxivId": "2409.03512",
                "arxivLink": "https://arxiv.org/abs/2409.03512",
                "title": "MOOCs Got an Upgrade: AI Agents Take Over the Classroom!",
                "institute": "Tsinghua University",
                "text": "This research proposes MAIC (Massive AI-empowered Course), a new online learning model that uses LLM-driven multi-agent systems to create an AI-augmented classroom. Unlike traditional MOOCs, MAIC aims to balance scalability with adaptivity by providing personalized learning experiences for each student.",
                "paper-title": "From MOOC to MAIC: Reshaping Online Teaching and Learning through LLM-driven Agents",
                "image-path": "flux_paper_image/2409.03512_1725760043.png"
            },

            {
                "startTime": "03:38",
                "arxivId": "2409.03302",
                "arxivLink": "https://arxiv.org/abs/2409.03302",
                "title": "Quantum Spin Systems: FNOs Learn the Dance of Tiny Magnets!",
                "institute": "California Institute of Technology, Ahmedabad University, NVIDIA",
                "text": "This research uses Fourier Neural Operators (FNOs) to model the time evolution of quantum spin systems, a task that's usually computationally expensive. Unlike traditional neural networks, FNOs are resolution-invariant, meaning they can be trained at lower resolutions and still accurately predict at higher resolutions.",
                "paper-title": "Fourier Neural Operators for Learning Dynamics in Quantum Spin Systems",
                "image-path": "flux_paper_image/2409.03302_1725762658.png"
            },

            {
                "startTime": "04:08",
                "arxivId": "2409.02977",
                "arxivLink": "https://arxiv.org/abs/2409.02977",
                "title": "LLMs Go Agent: Software Engineering Gets a New AI Buddy!",
                "institute": "Fudan University, Nanyang Technological University, University of Illinois",
                "text": "This research focuses on LLM-based agents, which are AI systems that use LLMs as their core but also have the ability to interact with the environment and use external tools. This sets it apart from previous work that primarily focused on standalone LLMs for software engineering tasks.",
                "paper-title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "image-path": "flux_paper_image/2409.02977_1725760939.png"
            },

            {
                "startTime": "04:32",
                "arxivId": "2409.02965",
                "arxivLink": "https://arxiv.org/abs/2409.02965",
                "title": "Social Media Users: Do We Trust What They Say or What They Do?",
                "institute": "UC Los Angeles, University of Washington",
                "text": "This research proposes a framework called Contribution-Aware Multimodal User Embedding (CAMUE) that can identify and remove misleading information from specific social network users during text-graph fusion. This is different from previous work because it provides personalized explanations for downstream analysis and recommendations.",
                "paper-title": "Do We Trust What They Say or What They Do? A Multimodal User Embedding Provides Personalized Explanations",
                "image-path": "flux_paper_image/2409.02965_1725760487.png"
            },

            {
                "startTime": "05:06",
                "arxivId": "2409.03755",
                "arxivLink": "https://arxiv.org/abs/2409.03755",
                "title": "Diffusion Models: A New Trick to Speed Up Image Generation!",
                "institute": "Tsinghua University",
                "text": "This paper introduces a new technique called \"dynamic compensation\" to improve the accuracy of predictor-corrector diffusion samplers. This method uses a learned compensation ratio to adjust the model's output at each sampling step, mitigating the misalignment issue that arises from the corrector step.",
                "paper-title": "DC-Solver: Improving Predictor-Corrector Diffusion Sampler via Dynamic Compensation",
                "image-path": "flux_paper_image/2409.03755_1725760079.png"
            },

            {
                "startTime": "05:38",
                "arxivId": "2409.03270",
                "arxivLink": "https://arxiv.org/abs/2409.03270",
                "title": "Talking Heads Get a Personality Makeover: New AI Model Injects Style into Videos",
                "institute": "Fudan University, Tencent",
                "text": "This research introduces a novel framework called SVP that incorporates intrinsic style into talking head generation. Unlike previous methods that focus on lip-sync and head movements, SVP learns style-related information from audio and visual cues, enabling the generation of more diverse and expressive videos.",
                "paper-title": "SVP: Style-Enhanced Vivid Portrait Talking Head Diffusion Model",
                "image-path": "flux_paper_image/2409.03270_1725760082.png"
            },

            {
                "startTime": "05:58",
                "arxivId": "2409.03731",
                "arxivLink": "https://arxiv.org/abs/2409.03731",
                "title": "AI-Powered Planning: Deep Learning Makes Robust Optimization Less Conservative",
                "institute": "MIT",
                "text": "This research introduces a new approach for adaptive robust optimization (ARO) that uses a variational autoencoder (VAE) to learn a tighter uncertainty set, improving the accuracy of planning decisions. Unlike previous methods that rely on simpler uncertainty sets or approximate recourse costs, this approach leverages deep generative learning to capture complex, high-dimensional uncertainty more effectively.",
                "paper-title": "A Deep Generative Learning Approach for Two-stage Adaptive Robust Optimization",
                "image-path": "flux_paper_image/2409.03731_1725760945.png"
            },

            {
                "startTime": "06:21",
                "arxivId": "2409.03684",
                "arxivLink": "https://arxiv.org/abs/2409.03684",
                "title": "Predicting Quantum Channels: It's Not All Classical!",
                "institute": "Harvard University, ETH Zurich, CMU...",
                "text": "This research extends previous work on predicting quantum channels by demonstrating that accurate prediction is possible for a broader class of input distributions, specifically those that are not \"classical\" in nature. The key difference lies in the use of a \"biased Pauli analysis\" technique, which overcomes limitations of previous methods that relied on low-degree approximations in the Pauli basis.",
                "paper-title": "Predicting quantum channels over general product distributions",
                "image-path": "flux_paper_image/2409.03684_1725762665.png"
            },

            {
                "startTime": "06:49",
                "arxivId": "2409.03757",
                "arxivLink": "https://arxiv.org/abs/2409.03757",
                "title": "Vision Models for 3D Scenes: Who's Got the Best View?",
                "institute": "University of Illinois Urbana-Champaign, Carnegie Mellon University",
                "text": "This research systematically compares different visual foundation models for 3D scene understanding, evaluating their strengths and weaknesses across various tasks. Unlike previous work that focused on 2D image-based tasks, this study explores the use of video and 3D point cloud encoders for complex 3D scenes.",
                "paper-title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding",
                "image-path": "flux_paper_image/2409.03757_1725760707.png"
            },

            {
                "startTime": "07:16",
                "arxivId": "2409.03632",
                "arxivLink": "https://arxiv.org/abs/2409.03632",
                "title": "Beyond the Model: Unmasking AI Bias with Social Structures",
                "institute": "Google",
                "text": "This research proposes a new type of explanation for machine learning outputs called \"socio-structural explanations.\" Unlike traditional model-centric interpretations, this approach considers the social and structural context in which the model operates, highlighting how societal biases can influence its predictions.",
                "paper-title": "Beyond Model Interpretability: Socio-Structural Explanations in Machine Learning",
                "image-path": "flux_paper_image/2409.03632_1725760114.png"
            },

            {
                "startTime": "07:39",
                "arxivId": "2409.03161",
                "arxivLink": "https://arxiv.org/abs/2409.03161",
                "title": "AI Goes to College: Can ChatGPT Ace Materials Science?",
                "institute": "OMRON SINIC X, Osaka University, University of Tokyo",
                "text": "This research introduces MaterialBENCH, a new benchmark dataset specifically designed to evaluate the problem-solving abilities of large language models (LLMs) in materials science. Unlike previous benchmarks, MaterialBENCH focuses on college-level materials science problems, including those requiring complex calculations and reasoning.",
                "paper-title": "MaterialBENCH: Evaluating College-Level Materials Science Problem-Solving Abilities of Large Language Models",
                "image-path": "flux_paper_image/2409.03161_1725760119.png"
            },

            {
                "startTime": "08:05",
                "arxivId": "2409.02980",
                "arxivLink": "https://arxiv.org/abs/2409.02980",
                "title": "Painting Galaxies with AI: A New Way to Model the Universe",
                "institute": "MIT",
                "text": "This research introduces NeHOD, a generative framework that uses diffusion models and point clouds to model the distribution of galaxies within dark matter halos. Unlike traditional methods like Halo Occupation Distribution (HOD), NeHOD can capture more complex relationships between galaxies and their host halos, achieving accuracy comparable to computationally expensive hydrodynamic simulations.",
                "paper-title": "How DREAMS are made: Emulating Satellite Galaxy and Subhalo Populations with Diffusion Models and Point Clouds",
                "image-path": "flux_paper_image/2409.02980_1725762269.png"
            },

            {
                "startTime": "08:31",
                "arxivId": "2409.03129",
                "arxivLink": "https://arxiv.org/abs/2409.03129",
                "title": "Subsidies: The Secret Weapon to Tame Selfish Agents?",
                "institute": "CMU, Toyota Technological Institute at Chicago",
                "text": "This research explores the use of subsidies to address information avoidance behavior in multi-agent systems, a phenomenon not previously addressed in subsidy design literature. It also establishes formal hardness results for designing optimal subsidy schemes and proposes a data-driven approach to overcome these computational challenges.",
                "paper-title": "Subsidy design for better social outcomes",
                "image-path": "flux_paper_image/2409.03129_1725760505.png"
            },

            {
                "startTime": "08:56",
                "arxivId": "2409.03142",
                "arxivLink": "https://arxiv.org/abs/2409.03142",
                "title": "Unmasking Hidden Actions: New AI Learns to Spot Shifts in Time!",
                "institute": "Carnegie Mellon University, Mohamed bin Zayed University of Artificial Intelligence",
                "text": "This research tackles the challenge of identifying causal relationships in non-stationary time series data without relying on prior knowledge of domain variables. Unlike previous methods that assume a Markov structure or require observing domain indices, this work introduces a novel approach based on sparse transitions and conditional independence constraints.",
                "paper-title": "Causal Temporal Representation Learning with Nonstationary Sparse Transition",
                "image-path": "flux_paper_image/2409.03142_1725761620.png"
            },

            {
                "startTime": "09:23",
                "arxivId": "2409.03060",
                "arxivLink": "https://arxiv.org/abs/2409.03060",
                "title": "Explaining AI: From \"Why?\" to \"How Much?\" with VERIX+",
                "institute": "Stanford University",
                "text": "This research builds upon previous work on verified explainability by introducing new techniques to improve the size and generation time of explanations. It utilizes bound propagation-based sensitivity analysis to obtain more fine-grained feature-level information, leading to smaller explanations. Additionally, it proposes a binary search-inspired traversal approach to process features in batches, significantly reducing the time required to generate explanations.",
                "paper-title": "Better Verified Explanations with Applications to Incorrectness and Out-of-Distribution Detection",
                "image-path": "flux_paper_image/2409.03060_1725760149.png"
            },

            {
                "startTime": "09:48",
                "arxivId": "2409.03733",
                "arxivLink": "https://arxiv.org/abs/2409.03733",
                "title": "LLMs: Thinking Big, Planning Small for Code Generation",
                "institute": "Scale AI, California Institute of Technology, Northeastern University...",
                "text": "This research proposes PLANSEARCH, a novel search algorithm that improves code generation by searching over plans in natural language, rather than directly over code solutions. This approach increases the diversity of generated ideas, leading to more efficient search and better performance.",
                "paper-title": "Planning In Natural Language Improves LLM Search For Code Generation",
                "image-path": "flux_paper_image/2409.03733_1725760169.png"
            },

            {
                "startTime": "10:29",
                "arxivId": "2409.03385",
                "arxivLink": "https://arxiv.org/abs/2409.03385",
                "title": "Graph-Based Referring Expression Comprehension: Making It Great Again!",
                "institute": "Academia Sinica, National Taiwan University",
                "text": "This research introduces a novel \"dynamic gate constraint\" (DGC) module that adaptively disables irrelevant objects during reasoning in graph-based referring expression comprehension (REC) models. This approach differs from previous methods by focusing on sub-expressions of the referring expression, rather than the entire expression, to identify relevant objects.",
                "paper-title": "Make Graph-based Referring Expression Comprehension Great Again through Expression-guided Dynamic Gating and Regression",
                "image-path": "flux_paper_image/2409.03385_1725760880.png"
            },

            {
                "startTime": "10:55",
                "arxivId": "2409.03402",
                "arxivLink": "https://arxiv.org/abs/2409.03402",
                "title": "AI Goes From Player to Coach: Language Models Take the Reins in RL Experiments",
                "institute": "Google",
                "text": "This research proposes a system that uses a large vision language model (VLM) to automate most of the steps in a reinforcement learning (RL) experiment, including task proposition, decomposition, and performance analysis. This differs from previous work that typically focuses on automating individual steps in isolation.",
                "paper-title": "Game On: Towards Language Models as RL Experimenters",
                "image-path": "flux_paper_image/2409.03402_1725760723.png"
            },

            {
                "startTime": "11:16",
                "arxivId": "2409.03034",
                "arxivLink": "https://arxiv.org/abs/2409.03034",
                "title": "Meshing Up Neural Fields: A Multi-Resolution Revolution!",
                "institute": "Technion - Israel Institute of Technology, Broad Institute of MIT and Harvard",
                "text": "This paper proposes a novel framework for representing neural fields on triangle meshes that is multi-resolution across both spatial and frequency domains. Unlike previous work that focuses on Euclidean spaces, this approach leverages the geometry-aware DiffusionNet architecture to decompose the spatial and frequency domains using multiple DiffusionNet components representing different spatial resolutions.",
                "paper-title": "MDNF: Multi-Diffusion-Nets for Neural Fields on Meshes",
                "image-path": "flux_paper_image/2409.03034_1725760189.png"
            },

            {
                "startTime": "11:38",
                "arxivId": "2409.03412",
                "arxivLink": "https://arxiv.org/abs/2409.03412",
                "title": "Doctors' Notes, AI's Sight: Text Boosts Medical Image Segmentation",
                "institute": "Sun Yat-sen University, National Cancer Center, Chinese Academy of Medical Sciences...",
                "text": "This research proposes a novel approach called TG-LMM (Text-Guided Large Multi-Modal Model) that incorporates textual descriptions of organs into the segmentation process. Unlike previous text-visual models that focus on identifying the target, TG-LMM aims to improve segmentation accuracy by leveraging expert descriptions of organ locations.",
                "paper-title": "TG-LMM: Enhancing Medical Image Segmentation Accuracy through Text-Guided Large Multi-Modal Model",
                "image-path": "flux_paper_image/2409.03412_1725762520.png"
            },

            {
                "startTime": "12:05",
                "arxivId": "2409.03274",
                "arxivLink": "https://arxiv.org/abs/2409.03274",
                "title": "LLMs: A Whack-a-Mole Game of Safety and Security",
                "institute": "Chinese Academy of Sciences, Xidian University, Peking University",
                "text": "This research paper provides a comprehensive overview of recent advancements in attack and defense approaches for Large Language Models (LLMs), focusing on studies published in 2023 and beyond. It distinguishes itself from previous surveys by highlighting the unique vulnerabilities of LLMs, analyzing the evolving threat landscape, and examining the effectiveness of contemporary defense mechanisms.",
                "paper-title": "Recent Advances in Attack and Defense Approaches of Large Language Models",
                "image-path": "flux_paper_image/2409.03274_1725760983.png"
            },

            {
                "startTime": "12:32",
                "arxivId": "2409.03444",
                "arxivLink": "https://arxiv.org/abs/2409.03444",
                "title": "LLMs Get a Makeover: Merging Models for Super-Smart AI",
                "institute": "MIT",
                "text": "This research explores the effects of merging multiple fine-tuned LLMs, demonstrating that this process can lead to the emergence of capabilities that surpass the individual contributions of the parent models. This is different from previous work that primarily focused on single model fine-tuning.",
                "paper-title": "Fine-tuning large language models for domain adaptation: Exploration of training strategies, scaling, model merging and synergistic capabilities",
                "image-path": "flux_paper_image/2409.03444_1725761903.png"
            },

            {
                "startTime": "12:48",
                "arxivId": "2409.03277",
                "arxivLink": "https://arxiv.org/abs/2409.03277",
                "title": "ChartMoE: A Multi-Expert Chart Whisperer for Smarter Data Understanding",
                "institute": "Tsinghua University, Peking University, Hong Kong University of Science and Technology Guangzhou...",
                "text": "This research introduces ChartMoE, a new approach to chart understanding that utilizes a Mixture of Experts (MoE) architecture to bridge the gap between visual and language models. Unlike previous work that relies on a single connector, ChartMoE trains multiple connectors with distinct alignment tasks (chart-table, chart-JSON, chart-code), allowing for more specialized and effective chart interpretation.",
                "paper-title": "ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding",
                "image-path": "flux_paper_image/2409.03277_1725760222.png"
            },

            {
                "startTime": "13:17",
                "arxivId": "2409.03012",
                "arxivLink": "https://arxiv.org/abs/2409.03012",
                "title": "More Labels, More Love: Crowdsourcing Apps Get a Boost from User Effort!",
                "institute": "Saint Louis University, IBM, Temple University",
                "text": "This research explores the impact of different levels of user labeling effort on the quantity and quality of data collected in camera-centric mobile crowdsourcing applications. Unlike previous work that focused on minimizing user effort, this study suggests that requesting more effort from users does not necessarily lead to lower engagement or satisfaction.",
                "paper-title": "Design and Evaluation of Camera-Centric Mobile Crowdsourcing Applications",
                "image-path": "flux_paper_image/2409.03012_1725760883.png"
            },

            {
                "startTime": "13:37",
                "arxivId": "2409.03634",
                "arxivLink": "https://arxiv.org/abs/2409.03634",
                "title": "Surface-Centric Modeling: A Neural Network That's Got Your Back (and Your Surface)",
                "institute": "Peking University, Peking University Shenzhen Graduate School",
                "text": "This research introduces a new approach to neural surface reconstruction called \"surface-centric modeling.\" Unlike previous methods that focus on dense volumes, this approach prioritizes regions near the surface, leading to more efficient and accurate reconstructions.",
                "paper-title": "Surface-Centric Modeling for High-Fidelity Generalizable Neural Surface Reconstruction",
                "image-path": "flux_paper_image/2409.03634_1725760244.png"
            },

            {
                "startTime": "14:07",
                "arxivId": "2409.03368",
                "arxivLink": "https://arxiv.org/abs/2409.03368",
                "title": "Training-Free SNNs: No Sweat, Just Spikes!",
                "institute": "Peking University, Hunan University",
                "text": "This research proposes a training-free method for converting pre-trained Artificial Neural Networks (ANNs) into Spiking Neural Networks (SNNs). Unlike previous methods that require retraining or fine-tuning, this approach directly converts the ANN model without any additional training.",
                "paper-title": "Training-free Conversion of Pretrained ANNs to SNNs for Low-Power and High-Performance Applications",
                "image-path": "flux_paper_image/2409.03368_1725760257.png"
            },

            {
                "startTime": "14:33",
                "arxivId": "2409.03583",
                "arxivLink": "https://arxiv.org/abs/2409.03583",
                "title": "Text-Guided Mixup: When Images Learn to Chat with Words!",
                "institute": "University of Washington, Alibaba",
                "text": "This research proposes a novel text-guided mixup technique that leverages the semantic relationships between classes recognized by a pre-trained text encoder to improve the performance of long-tailed image categorization tasks. This approach differs from previous work by incorporating textual information into the mixup process, which helps to alleviate the long-tailed problem by boosting the performance of tail classes.",
                "paper-title": "Text-Guided Mixup Towards Long-Tailed Image Categorization",
                "image-path": "flux_paper_image/2409.03583_1725760262.png"
            },

            {
                "startTime": "15:08",
                "arxivId": "2409.03272",
                "arxivLink": "https://arxiv.org/abs/2409.03272",
                "title": "Self-Driving Cars Get a Brain Upgrade: Occupancy, Language, and Action, Oh My!",
                "institute": "Fudan University, Tsinghua University",
                "text": "This research proposes a new type of world model for autonomous driving that combines visual, language, and action information. Unlike previous models that focus on predicting sensor data, this model aims to understand and reason about the world in a more human-like way.",
                "paper-title": "OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving",
                "image-path": "flux_paper_image/2409.03272_1725760739.png"
            },

            {
                "startTime": "15:33",
                "arxivId": "2409.03365",
                "arxivLink": "https://arxiv.org/abs/2409.03365",
                "title": "Training Giant AI Models: A Juggling Act of Data and Resources!",
                "institute": "Peking University, Purdue University, Alibaba",
                "text": "This research introduces Spindle, a training system for multi-task, multi-modal large models that optimizes resource allocation and scheduling by considering the heterogeneous workloads of different tasks and data modalities. Unlike previous systems designed for single-task models, Spindle decomposes the model into stages and addresses the joint optimization problem sequentially, achieving significant speedups.",
                "paper-title": "Efficient Multi-Task Large Model Training via Data Heterogeneity-aware Model Management",
                "image-path": "flux_paper_image/2409.03365_1725761036.png"
            },

            {
                "startTime": "15:55",
                "arxivId": "2409.03187",
                "arxivLink": "https://arxiv.org/abs/2409.03187",
                "title": "Noise Can Actually Boost Your Brain's Memory! \ud83e\udd2f",
                "institute": "The University of Tokyo",
                "text": "This research delves into the effects of correlated noise on memory in linear recurrent neural networks (RNNs), a departure from previous studies that primarily focused on uncorrelated noise.",
                "paper-title": "How noise affects memory in linear recurrent networks",
                "image-path": "flux_paper_image/2409.03187_1725761936.png"
            },

            {
                "startTime": "16:19",
                "arxivId": "2409.03505",
                "arxivLink": "https://arxiv.org/abs/2409.03505",
                "title": "Newsvendor's Dilemma: From Slow to Fast, a Spectrum of Regret",
                "institute": "Tsinghua University, Columbia University",
                "text": "This research introduces the concept of \"clustered distributions\" to analyze the data-driven Newsvendor problem. This framework unifies previous work by considering a spectrum of regret rates, ranging from 1/\u221an to 1/n, depending on the clustering parameter \u03b2.",
                "paper-title": "Survey of Data-driven Newsvendor: Unified Analysis and Spectrum of Achievable Regrets",
                "image-path": "flux_paper_image/2409.03505_1725761717.png"
            },

            {
                "startTime": "16:45",
                "arxivId": "2409.03282",
                "arxivLink": "https://arxiv.org/abs/2409.03282",
                "title": "Traffic Jams? Not So Fast! New AI Model Predicts Speed with Incident Awareness.",
                "institute": "CMU",
                "text": "This research proposes a Mixture of Experts (MoE) model for traffic speed prediction that explicitly considers incidents, unlike previous models that are incident-agnostic. The MoE model leverages separate recurrent and non-recurrent expert models to capture distinct traffic patterns under different conditions.",
                "paper-title": "Interpretable mixture of experts for time series prediction under recurrent and non-recurrent conditions",
                "image-path": "flux_paper_image/2409.03282_1725760646.png"
            },

            {
                "startTime": "17:09",
                "arxivId": "2409.03103",
                "arxivLink": "https://arxiv.org/abs/2409.03103",
                "title": "Cloud Scaling Gets Smart: AI Predicts Latency, Interprets the Why!",
                "institute": "IBM",
                "text": "This research introduces an interpretable approach to predict end-to-end latency in microservice-based applications, using the Temporal Fusion Transformer (TFT) and Kernel Ridge Regression (KRR). Unlike previous work, this method not only predicts latency but also explains the factors contributing to it, enabling informed, fine-grained autoscaling.",
                "paper-title": "Leveraging Interpretability in the Transformer to Automate the Proactive Scaling of Cloud Resources",
                "image-path": "flux_paper_image/2409.03103_1725760311.png"
            },

            {
                "startTime": "17:26",
                "arxivId": "2409.03614",
                "arxivLink": "https://arxiv.org/abs/2409.03614",
                "title": "Soft Robots Get a Grip on Long-Term Learning!",
                "institute": "CMU",
                "text": "This research proposes a modular parallel robotic manipulation platform specifically designed for long-term data collection in soft robotics. Unlike previous work that often relies on high-fidelity simulations, this platform focuses on collecting real-world data from a robust hardware system.",
                "paper-title": "1 Modular Parallel Manipulator for Long-Term Soft Robotic Data Collection",
                "image-path": "flux_paper_image/2409.03614_1725760745.png"
            },

            {
                "startTime": "17:48",
                "arxivId": "2409.03734",
                "arxivLink": "https://arxiv.org/abs/2409.03734",
                "title": "Safety First, Data Second: How Multi-Objective Learning Levels the AI Playing Field",
                "institute": "University of California Berkeley",
                "text": "This research explores how the need to prioritize safety in large language models can actually create opportunities for new companies to enter the market, even when incumbents have massive datasets. It differs from previous work by focusing on the impact of multi-objective learning on market entry barriers.",
                "paper-title": "Safety vs. Performance: How Multi-Objective Learning Reduces Barriers to Market Entry",
                "image-path": "flux_paper_image/2409.03734_1725762632.png"
            },

            {
                "startTime": "18:09",
                "arxivId": "2409.03563",
                "arxivLink": "https://arxiv.org/abs/2409.03563",
                "title": "100 Instances is All You Need: Predicting LLM Success with a Tiny Test",
                "institute": "University of Cambridge, Polytechnic University of Valencia",
                "text": "This research proposes a novel framework for predicting the performance of a new LLM on unseen data by evaluating it on a small set of reference instances, rather than requiring a full evaluation on a large dataset. This approach leverages the evaluation results of previously tested LLMs to reduce the cost and time required for evaluating new models.",
                "paper-title": "100 instances is all you need: predicting the success of a new LLM on unseen data by testing on a few instances",
                "image-path": "flux_paper_image/2409.03563_1725760379.png"
            },

            {
                "startTime": "18:35",
                "arxivId": "2409.03299",
                "arxivLink": "https://arxiv.org/abs/2409.03299",
                "title": "Can Robots Learn New Tricks? RT-1-X Takes on a 40-Year-Old SCARA Robot!",
                "institute": "Universiteit van Amsterdam",
                "text": "This research investigates the ability of a robotic foundation model, RT-1-X, to generalize to a previously unseen robot type, a SCARA robot. This is different from previous work that focused on generalizing across tasks and environments but not robot embodiments.",
                "paper-title": "Bringing the RT-1-X Foundation Model to a SCARA robot",
                "image-path": "flux_paper_image/2409.03299_1725760760.png"
            },

            {
                "startTime": "18:59",
                "arxivId": "2409.03291",
                "arxivLink": "https://arxiv.org/abs/2409.03291",
                "title": "LLM Detectors: A Case of Fake News, Real Problems",
                "institute": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, University of Applied Sciences and Arts Western Switzerland",
                "text": "This research focuses on the real-world effectiveness of LLM detectors in a specific setting: short news-like posts generated by moderately sophisticated attackers. It goes beyond previous benchmarks by testing detectors against a wider range of attacks, including temperature increase and prompting strategies.",
                "paper-title": "LLM Detectors Still Fall Short of Real World: Case of LLM-Generated Short News-Like Posts",
                "image-path": "flux_paper_image/2409.03291_1725761067.png"
            },

            {
                "startTime": "19:20",
                "arxivId": "2409.03005",
                "arxivLink": "https://arxiv.org/abs/2409.03005",
                "title": "Robots Go Off-Road: Physics-Powered AI Navigates Unseen Terrain",
                "institute": "MIT, Mitsubishi Electric Research Laboratories, George Mason University...",
                "text": "This research introduces PIETRA, a self-supervised learning framework that integrates physics priors into evidential neural networks. Unlike previous methods that avoid out-of-distribution terrain, PIETRA seamlessly transitions between learned and physics-based predictions for unseen terrain.",
                "paper-title": "PIETRA: Physics-Informed Evidential Learning for Traversing Out-of-Distribution Terrain",
                "image-path": "flux_paper_image/2409.03005_1725761086.png"
            },

            {
                "startTime": "19:46",
                "arxivId": "2409.03478",
                "arxivLink": "https://arxiv.org/abs/2409.03478",
                "title": "LLMs: The New Event Planners for Your Smart Home!",
                "institute": "Catholic University of Louvain, Microsoft",
                "text": "This research explores using Large Language Models (LLMs) to automate the process of creating event logs from raw sensor data collected by Internet of Things (IoT) devices. This approach differs from previous work by leveraging the capabilities of LLMs in natural language understanding and generation to abstract sensor readings into meaningful events and integrate logs from multiple sources.",
                "paper-title": "LLM-based event abstraction and integration for IoT-sourced logs",
                "image-path": "flux_paper_image/2409.03478_1725760576.png"
            },

            {
                "startTime": "20:06",
                "arxivId": "2409.03171",
                "arxivLink": "https://arxiv.org/abs/2409.03171",
                "title": "Multi-Task RAG: When One Adapter Isn't Enough!",
                "institute": "Darkhive",
                "text": "This research introduces MARAGS, a multi-adapter system for retrieval augmented generation (RAG) question answering. Unlike previous work that focuses on single-task RAG, MARAGS utilizes multiple adapters to handle diverse tasks within a single LLM, improving efficiency and performance.",
                "paper-title": "MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering",
                "image-path": "flux_paper_image/2409.03171_1725760418.png"
            },

            {
                "startTime": "20:33",
                "arxivId": "2409.03431",
                "arxivLink": "https://arxiv.org/abs/2409.03431",
                "title": "UV-Mamba Strikes: A New Model for Urban Village Boundary Detection!",
                "institute": "Beijing Jiaotong University, Tsinghua University",
                "text": "This research introduces UV-Mamba, a neural network model that uses a state space model (SSM) with deformable convolutions (DCN) to improve urban village boundary identification in high-resolution remote sensing images. This approach addresses the memory loss problem that arises in SSM when dealing with large images.",
                "paper-title": "UV-Mamba: A DCN-Enhanced State Space Model for Urban Village Boundary Identification in High-Resolution Remote Sensing Images",
                "image-path": "flux_paper_image/2409.03431_1725760426.png"
            },

            {
                "startTime": "21:01",
                "arxivId": "2409.03495",
                "arxivLink": "https://arxiv.org/abs/2409.03495",
                "title": "MLE for Multia\ufb03ne Models: A New Algorithm That's Not Afraid of Complexity!",
                "institute": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, ETH Zurich",
                "text": "This paper proposes a novel Alternating and Iteratively-Reweighted Least Squares (AIRLS) algorithm for maximum likelihood estimation (MLE) problems where variables are related by multia\ufb03ne expressions. This approach differs from previous work by considering a wider class of likelihoods and providing a deeper theoretical analysis of the algorithm's convergence.",
                "paper-title": "Maximum likelihood inference for high-dimensional problems with multiaffine variable relations",
                "image-path": "flux_paper_image/2409.03495_1725761754.png"
            },

            {
                "startTime": "21:25",
                "arxivId": "2409.02963",
                "arxivLink": "https://arxiv.org/abs/2409.02963",
                "title": "Fair Clustering: Making Sure Everyone Gets a Seat at the Table",
                "institute": "Stanford University, Georgia Institute of Technology",
                "text": "This paper introduces a new fairness criterion for clustering called \"minimum representation fairness\" (MR-fairness), which requires each group to be represented above a certain threshold in a specified number of clusters. This differs from previous work that focuses on proportional representation across all clusters.",
                "paper-title": "Fair Minimum Representation Clustering via Integer Programming",
                "image-path": "flux_paper_image/2409.02963_1725761369.png"
            },

            {
                "startTime": "21:51",
                "arxivId": "2409.03109",
                "arxivLink": "https://arxiv.org/abs/2409.03109",
                "title": "Fake News, Real Problem: Vision-Language Model Detects and Attributes Deepfakes!",
                "institute": "Univ. Polytechnique Hauts-de-France, Univ. Rennes, CNRS...",
                "text": "This research introduces FIDAVL, a novel approach that combines synthetic image detection and attribution within a single framework. Unlike previous methods that handle these tasks separately, FIDAVL leverages the synergies between vision and language models to achieve more accurate and robust results.",
                "paper-title": "FIDAVL: Fake Image Detection and Attribution using Vision-Language Model",
                "image-path": "flux_paper_image/2409.03109_1725761099.png"
            },

            {
                "startTime": "22:21",
                "arxivId": "2409.03061",
                "arxivLink": "https://arxiv.org/abs/2409.03061",
                "title": "Depth Perception: How 3D Models Learned to See the World Like We Do",
                "institute": "Carnegie Mellon University, Toyota Research Institute",
                "text": "This research introduces a method to incorporate dense metric depth into the training of neural 3D representations. This is different from previous work that relied on sparse depth or monocular depth priors.",
                "paper-title": "Incorporating dense metric depth into neural 3D representations for view synthesis and relighting",
                "image-path": "flux_paper_image/2409.03061_1725760925.png"
            }
    ],
    "stats": {
        "num_pick": 52,
        "num_total": 226,
    },
    "audio": "https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202409062206_audio.mp3"
}