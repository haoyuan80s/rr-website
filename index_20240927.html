
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY</div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">Fresh Picks: 
                    <span class="highlightNumber" style="font-size: 28px;">88</span> out of <span
                    class="highlightNumber">370</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-09-27"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>


        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">01:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17436" target="_blank">@arXiv 2409.17436</a>
                    <span class="tweet-title">Say Goodbye to Live Experiments: YouTube Music's New User Onboarding Simulation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces a simulation methodology for evaluating preference elicitation policies in recommender systems, specifically for onboarding new users. Unlike previous work that primarily focused on algorithm development, this study integrates simulation into production infrastructure, allowing for more realistic evaluation and reducing the need for costly live experiments.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">01:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18121" target="_blank">@arXiv 2409.18121</a>
                    <span class="tweet-title">Robot Learns to Play with Toys by Watching You!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research proposes a method called Robot See Robot Do (RSRD) that enables robots to learn articulated object manipulation from a single monocular human demonstration. Unlike previous work that relies on extensive training data or specific object categories, RSRD leverages large pretrained vision models and a 4D Differentiable Part Model (4D-DPM) to track object motion from a single video.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">01:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17958" target="_blank">@arXiv 2409.17958</a>
                    <span class="tweet-title">Vision Models:  Hard Negatives, Easy Lies?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington</span>
                </div>
                <div class="primary-text">
                    This research introduces the concept of "hard positives" to evaluate vision-language models' understanding of compositionality. Unlike previous work that focused solely on "hard negatives," this study investigates how models handle semantic-preserving changes in captions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">02:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17778" target="_blank">@arXiv 2409.17778</a>
                    <span class="tweet-title">Diffusion Models Get a Speed Boost: Super-Resolution with a Domain Shift</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Advanced Micro Devices, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel diffusion equation that incorporates a "domain shift" strategy. Unlike previous methods that start from random noise, this approach leverages low-resolution images as the starting point for the diffusion process, significantly improving inference efficiency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">02:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17156" target="_blank">@arXiv 2409.17156</a>
                    <span class="tweet-title">AI Censored My Art?  A New Look at Nudity Moderation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ELLIS Alicante, University of Notre Dame, ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research focuses on the specific challenge of AI-based content moderation of artistic nudity, going beyond general NSFW classification. It investigates the performance of existing NSFW classifiers on artistic nudity datasets and proposes a multi-modal approach using CLIP to improve accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">02:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17995" target="_blank">@arXiv 2409.17995</a>
                    <span class="tweet-title">Diffusion Does Navigation: A New Way to Plan Paths with a Little Noise</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This paper explores using diffusion models for joint localization and planning in 2D environments. Unlike previous work that focuses on specific tasks like manipulation or path planning, this research combines perception and planning into a single end-to-end system.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">03:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17691" target="_blank">@arXiv 2409.17691</a>
                    <span class="tweet-title">Training Models Without the Bias Blues: A Hyperparameter-Free Fix for Fairness</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge, Sony</span>
                </div>
                <div class="primary-text">
                    This research proposes a new bias mitigation technique called Targeted Augmentations for Bias Mitigation (TAB). Unlike previous methods, TAB doesn't require group labels during training or validation, making it more practical for real-world scenarios where biases are often unknown.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">03:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17988" target="_blank">@arXiv 2409.17988</a>
                    <span class="tweet-title">Blurry Events? No Problem! Deblur e-NeRF Reconstructs Sharp Scenes from Motion-Blurred Event Cameras</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National University of Singapore</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method, Deblur e-NeRF, that directly reconstructs sharp 3D scenes from motion-blurred events captured by event cameras. Unlike previous work, it incorporates a physically accurate pixel bandwidth model to account for the unique blur characteristics of event cameras.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">04:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18073" target="_blank">@arXiv 2409.18073</a>
                    <span class="tweet-title">Robots Can Now Read Your Mind (Almost) - New Research Decodes Human Intentions!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework called FISER, which explicitly models human intentions as intermediate reasoning steps when following natural language instructions. Unlike previous work that focuses on directly mapping language to actions, FISER separates social and embodied reasoning, allowing AI agents to better understand the context and hidden goals behind human requests.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">04:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17996" target="_blank">@arXiv 2409.17996</a>
                    <span class="tweet-title">Lensless Cameras Get a Photorealistic Makeover: New Algorithm Blurs the Lines Between Reality and Reconstruction!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Chinese University of Hong Kong, Tsinghua University, SenseTime</span>
                </div>
                <div class="primary-text">
                    This research introduces a two-stage approach for lensless image reconstruction that combines a spatially varying deconvolution network with a conditional diffusion model. This approach addresses the limitations of previous methods that either struggled with data consistency or photorealism.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">04:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17376" target="_blank">@arXiv 2409.17376</a>
                    <span class="tweet-title">Lens-ing Trouble: How a Tiny Lens Can Trick Self-Driving Cars</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Michigan State University, Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel physical attack, called LensAttack, that uses optical lenses to manipulate the depth perception of monocular depth estimation (MDE) algorithms used in autonomous driving systems. Unlike previous digital attacks, LensAttack operates in the physical world, making it more realistic and impactful.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">05:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17487" target="_blank">@arXiv 2409.17487</a>
                    <span class="tweet-title">Diffusion Models:  Traffic Lights for Faster Image Generation!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Huawei</span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel approach to accelerate diffusion models by introducing adaptive conditions, which are learned quantized representations of the input image. This method differs from previous work by avoiding trajectory relocation and additional regularization terms, leading to a more efficient and effective training process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">05:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18128" target="_blank">@arXiv 2409.18128</a>
                    <span class="tweet-title">FlowTurbo:  Speeding Up Image Generation with a Velocity Refresher!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes FlowTurbo, a framework that accelerates the sampling process of flow-based generative models by learning a lightweight velocity refiner. Unlike previous one-step distillation methods, FlowTurbo preserves the multi-step sampling paradigm, allowing it to be applied to various tasks such as image editing and inpainting.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">06:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18127" target="_blank">@arXiv 2409.18127</a>
                    <span class="tweet-title">EgoLM:  Giving AI a Body and a Brain, One Motion at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta, Nanyang Technological University, University of Tuebingen</span>
                </div>
                <div class="primary-text">
                    This research proposes EgoLM, a multi-modal language model that unifies egocentric motion tracking and understanding. Unlike previous work, EgoLM incorporates multiple modalities, including sparse motion sensor data and egocentric videos, to provide richer context for motion learning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">06:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17808" target="_blank">@arXiv 2409.17808</a>
                    <span class="tweet-title">Molecular Movies: Deep Learning Makes MD Simulations a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces a new paradigm for learning surrogate models of molecular dynamics (MD) by directly modeling full trajectories as time-series of 3D molecular structures. Unlike previous work that focuses on learning the transition density or equilibrium distribution, this approach leverages the rich dynamical information in MD training data, enabling a wider range of applications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">07:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17490" target="_blank">@arXiv 2409.17490</a>
                    <span class="tweet-title">MathDSL:  Solving Equations with Style (and Less Work!)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces MathDSL, a Domain-Specific Language (DSL) designed for solving linear equations. Unlike previous methods that rely on reinforcement learning, MathDSL uses program synthesis to generate solutions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">07:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17643" target="_blank">@arXiv 2409.17643</a>
                    <span class="tweet-title">Fairness vs. Performance:  A New Way to Find the Sweet Spot</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Technion - Israel Institute of Technology, NVIDIA</span>
                </div>
                <div class="primary-text">
                    This paper proposes a method to compute the optimal Pareto front for fairness and performance in representation learning without training complex models. It leverages structural properties of optimal fair representations to reduce the computation to a compact discrete problem.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">07:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17481" target="_blank">@arXiv 2409.17481</a>
                    <span class="tweet-title">LLMs on a Diet: Learnable Sparsity Makes Big Models Slim and Smart</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National University of Singapore, Nvidia</span>
                </div>
                <div class="primary-text">
                    This research introduces MaskLLM, a method that learns to prune large language models (LLMs) by introducing semi-structured sparsity. Unlike previous methods that rely on handcrafted importance criteria, MaskLLM models the sparsity pattern as a learnable distribution, allowing for end-to-end training on large datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">08:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18114" target="_blank">@arXiv 2409.18114</a>
                    <span class="tweet-title">Meshing with Magic:  A New Auto-Encoder for Artistic 3D Generation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, NVIDIA Research</span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel mesh tokenization algorithm that compresses triangular meshes into 1D token sequences, significantly enhancing training efficiency. It also proposes an Auto-regressive Auto-encoder (ArAE) model that compresses variable-length triangular meshes into fixed-length latent codes, enabling training latent diffusion models for better generalization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">08:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17652" target="_blank">@arXiv 2409.17652</a>
                    <span class="tweet-title">Simulations on Demand:  LLMs Build Games and Robots from Scratch!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces FACTORSIM, a framework that generates full simulations in code from language input. Unlike previous work that focuses on generating specific components of simulations, FACTORSIM generates the entire simulation, including the dynamics, reward functions, and even the user interface.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">09:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17924" target="_blank">@arXiv 2409.17924</a>
                    <span class="tweet-title">Panoramic Photos Get a Neural Makeover:  Stitching and Viewing Made Easy!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Princeton University, Google Inc.</span>
                </div>
                <div class="primary-text">
                    This research introduces a neural light sphere model for panoramic image stitching and view synthesis. Unlike previous methods that rely on volumetric representations, this approach uses a compact spherical representation, decomposing the scene into view-dependent ray offset and color components.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">09:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17285" target="_blank">@arXiv 2409.17285</a>
                    <span class="tweet-title">SpoofCeleb: Deepfakes Go Wild, But Can We Tell?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, Renmin University of China, National Institute of Informatics...</span>
                </div>
                <div class="primary-text">
                    This research introduces SpoofCeleb, a dataset for Speech Deepfake Detection (SDD) and Spoofing-robust Automatic Speaker Verification (SASV) that utilizes real-world, noisy speech data, unlike previous datasets that primarily used clean, studio-quality recordings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">09:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17189" target="_blank">@arXiv 2409.17189</a>
                    <span class="tweet-title">Federated Learning Goes Viral:  New Algorithm Makes Agents Chatty and Efficient!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Arizona State University, Princeton University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new decentralized federated learning algorithm called DSGTm-TV that incorporates gradient tracking and heavy-ball momentum to improve convergence in time-varying directed networks. Unlike previous work, this algorithm allows for uncoordinated stepsizes and momentum parameters, making it more adaptable to heterogeneous network settings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">10:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17659" target="_blank">@arXiv 2409.17659</a>
                    <span class="tweet-title">Autonomous Driving Gets a Bird's-Eye View:  DRL Meets BEV for Safer Roads!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Central South University</span>
                </div>
                <div class="primary-text">
                    This research integrates Bird's-Eye-View (BEV) perception with Deep Reinforcement Learning (DRL) for autonomous driving, bridging the gap between feature extraction and perception. Unlike previous DRL approaches, this method maps the feature extraction network directly to the perception phase, enabling clearer interpretation through semantic segmentation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">10:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18026" target="_blank">@arXiv 2409.18026</a>
                    <span class="tweet-title">Occupancy Networks:  Seeing is Believing, But Can We Trust It?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Zhejiang University, Huawei Noah’s Ark Lab, Udeer.ai</span>
                </div>
                <div class="primary-text">
                    This research focuses on the reliability of semantic occupancy prediction models, particularly those using cameras. It introduces a new method called RELIOCC that enhances reliability by incorporating both individual and relative voxel uncertainties.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">10:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17991" target="_blank">@arXiv 2409.17991</a>
                    <span class="tweet-title">Neural Networks:  Breaking the Curse of Dimensionality, One Smooth Boundary at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford, University of Vienna, Vienna University of Technology</span>
                </div>
                <div class="primary-text">
                    This research focuses on approximating and estimating classification functions with decision boundaries defined by RBV2 functions, a class of functions that can be approximated without the curse of dimensionality. This differs from previous work that focused on Barron functions or imposed different assumptions on the boundaries.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">11:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17505" target="_blank">@arXiv 2409.17505</a>
                    <span class="tweet-title">Unnormalized Densities Get a Sequential Makeover:  Kernel Stein Discrepancy Goes Anytime!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces a sequential version of the kernelized Stein discrepancy (KSD) for goodness-of-fit tests. Unlike previous work, it does not require the Stein kernel to be uniformly bounded, instead exploiting potential boundedness at specific points.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">11:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17917" target="_blank">@arXiv 2409.17917</a>
                    <span class="tweet-title">3D Style Transfer:  It's Not Just About the Colors, It's About the Shapes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta, Texas A&M University, LMU Munich</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method for 3D scene stylization that directly matches the distributions of Gaussians between style and content scenes using the Earth Mover's Distance (EMD). This approach differs from previous work by focusing on geometric stylization rather than just texture transfer.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">11:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18014" target="_blank">@arXiv 2409.18014</a>
                    <span class="tweet-title">LLMs on a Mission:  Role-RL Makes Them Work Together!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">South China Normal University, University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research introduces a new paradigm called Online Long-context Processing (OLP) for handling streaming media transcripts of unlimited length. It also proposes a novel Role Reinforcement Learning (Role-RL) framework to automatically assign different LLMs to their optimal roles within the OLP pipeline based on their performance. This approach differs from previous work by focusing on real-time processing of long contexts and dynamically optimizing LLM deployment for improved efficiency and accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">12:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17448" target="_blank">@arXiv 2409.17448</a>
                    <span class="tweet-title">Financial Sentiment Analysis:  Numbers Talk, LLMs Listen!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National Institute of Advanced Industrial Science and Technology, Ochanomizu University, University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research explores the impact of expert-designed hints on the performance of large language models (LLMs) in financial sentiment analysis. Unlike previous studies that focused on extracting numerical information or analyzing the role of numbers in financial documents, this paper investigates whether LLMs can inherently recognize and utilize these hints without explicit guidance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">12:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17417" target="_blank">@arXiv 2409.17417</a>
                    <span class="tweet-title">Investing Like a Pro:  New AI Uncovers Hidden Gems in Online Opinions</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Academia Sinica, National Taiwan University, National Institute of Advanced Industrial Science and Technology...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to analyzing investment opinions by incorporating argument mining techniques. Unlike previous work that focused on author-centric analysis or simple content analysis, this study delves into the structure of arguments within opinions, examining the relationship between claims and premises to determine their strength and potential profitability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">13:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17622" target="_blank">@arXiv 2409.17622</a>
                    <span class="tweet-title">Neural P3M:  Meshing Up Molecular Geometry for Long-Range Love!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Xi’an Jiaotong University, University of Illinois, Chinese University of Hong Kong...</span>
                </div>
                <div class="primary-text">
                    This paper introduces Neural P3M, a framework that enhances geometric GNNs by incorporating mesh points alongside atoms. This allows for the modeling of long-range interactions, which are often overlooked in traditional GNNs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">13:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17642" target="_blank">@arXiv 2409.17642</a>
                    <span class="tweet-title">AI Delegates:  The Privacy-Conscious Chatbots That Know When to Spill the Beans</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Beijing Institute for General Artificial Intelligence (BIGAI), Microsoft, Beijing Normal University</span>
                </div>
                <div class="primary-text">
                    This research focuses on AI delegates that can strategically self-disclose private information to achieve social goals, unlike previous work that primarily focused on minimizing privacy leaks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">14:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17287" target="_blank">@arXiv 2409.17287</a>
                    <span class="tweet-title">Blockchain-Enabled Data Diet:  How to Slim Down Your Car's Data Without Starving It!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Jiangnan University, Tsinghua University, Shanghai Jiao Tong University...</span>
                </div>
                <div class="primary-text">
                    This research combines blockchain technology with the Variational Information Bottleneck (VIB) method to address the computational burden and security concerns in vehicle-to-vehicle networks. Unlike previous work that focused on either blockchain or VIB separately, this paper proposes a novel approach that integrates both technologies to achieve a more efficient and secure data transmission system.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">14:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17446" target="_blank">@arXiv 2409.17446</a>
                    <span class="tweet-title">FedAWE:  Federated Learning's New Trick to Handle Flaky Clients</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research tackles the problem of clients dropping in and out of federated learning systems, which is a common issue in real-world deployments. Unlike previous work that either assumes perfect knowledge of client availability or requires significant memory overhead, this paper proposes a new algorithm, FedAWE, that handles both heterogeneous and non-stationary client availability with minimal additional memory and computation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">14:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18051" target="_blank">@arXiv 2409.18051</a>
                    <span class="tweet-title">IRL with Multiple Planning Horizons: When Robots Get Short-Term Memory</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC San Francisco, Harvard University, Stanford University</span>
                </div>
                <div class="primary-text">
                    This research focuses on inverse reinforcement learning (IRL) where experts share a common reward function but have different, unknown planning horizons. This is different from previous work that assumes a single planning horizon or that experts optimize for different reward functions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">15:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17216" target="_blank">@arXiv 2409.17216</a>
                    <span class="tweet-title">AI Regulation: It's Not Just About Big Models, It's About Big Data!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of California  Berkeley, OpenAI, Meta...</span>
                </div>
                <div class="primary-text">
                    This research argues that current AI governance efforts are too focused on the size of models and not enough on the data they are trained on. It suggests that smaller models can be just as capable as larger ones if they are trained on the right data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">15:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17486" target="_blank">@arXiv 2409.17486</a>
                    <span class="tweet-title">Global-Local Love:  A New Way to Fine-Tune AI for Medical Images</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Liaoning Technical University, Northeastern University, China Medical University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach called Global-Local Medical SAM Adaptor (GLMed-SA) that combines two techniques: full adaption (GMed-SA) and partial adaption (Med-SA). This allows for a more comprehensive fine-tuning of the Segment Anything Model (SAM) for medical image segmentation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">15:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18119" target="_blank">@arXiv 2409.18119</a>
                    <span class="tweet-title">Mammograms Get a Language Lesson: New AI Learns to "Read" Breast Images</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This research adapts the CLIP model for mammography, a modality with limited labeled data and high-resolution images. It introduces a multi-view contrastive learning framework that leverages the multi-view nature of mammograms and aligns image-image and image-text features.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">16:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17610" target="_blank">@arXiv 2409.17610</a>
                    <span class="tweet-title">Doctor's Orders: AI Learns to See What Matters in Patient Photos</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research focuses on improving vision-language alignment in multi-turn multimodal medical dialogues, a scenario where patients send images and text to doctors in multiple rounds of interaction. Unlike previous work that focuses on single-turn or stitched multi-turn dialogues, this study leverages the preceding text conversations to infer the regions of interest (RoIs) in the image, effectively enhancing the model's understanding of the visual content.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">16:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17677" target="_blank">@arXiv 2409.17677</a>
                    <span class="tweet-title">Transformers: Memory Masters, But Feedforward's the Bottleneck!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This paper analyzes the memorization capacity of Transformers, focusing on the efficiency of parameter usage. Unlike previous work, it establishes both upper and lower bounds on the number of parameters needed for memorization in both next-token prediction and sequence-to-sequence settings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">17:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17527" target="_blank">@arXiv 2409.17527</a>
                    <span class="tweet-title">LLMs:  Unlocking the Secret Recipe of Their Data Diet!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Beijing University of Posts and Telecommunications</span>
                </div>
                <div class="primary-text">
                    This research introduces a new concept called "data proportion detection," which aims to estimate the proportions of different types of data used to train a large language model (LLM) without access to the original training data. This differs from previous work that focuses on analyzing the model's performance or internal representations to understand its training data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">17:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17625" target="_blank">@arXiv 2409.17625</a>
                    <span class="tweet-title">Attention, Please!  Transformers Can Overfit...But It Might Be a Good Thing!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This paper analyzes benign overfitting in the token selection mechanism of attention architectures, a core component of transformer models. Unlike previous work that focused on linear models or two-layer neural networks, this study delves into the unique challenges of understanding overfitting in the context of attention.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">17:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17582" target="_blank">@arXiv 2409.17582</a>
                    <span class="tweet-title">Logit Adjustment Gets a Theoretical Makeover:  Neural Collapse Makes Long-Tailed Recognition Easier!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research provides a theoretical justification for the effectiveness of multiplicative logit adjustment (MLA) in long-tailed recognition (LTR) by connecting it to the concept of neural collapse (NC). Unlike previous work, this paper demonstrates that MLA approximates an optimal decision boundary adjustment method derived from NC.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">18:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17320" target="_blank">@arXiv 2409.17320</a>
                    <span class="tweet-title">Learning to Optimize:  A Multi-Block ADMM That's Not Afraid to Learn!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National University of Singapore, University of Maryland</span>
                </div>
                <div class="primary-text">
                    This research focuses on applying Learning to Optimize (L2O) techniques to a specific type of multi-block ADMM method called MPALM. Unlike previous L2O work that primarily focused on two-block ADMM, this paper explores the application of L2O to a more complex multi-block setting.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">18:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17858" target="_blank">@arXiv 2409.17858</a>
                    <span class="tweet-title">Neural Networks:  Learning to Learn, Faster!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google, University of California  Berkeley, Sloan Foundation...</span>
                </div>
                <div class="primary-text">
                    This paper introduces a solvable model of neural scaling laws that incorporates feature learning, going beyond the kernel limit. It identifies three scaling regimes based on task difficulty and shows that feature learning can improve scaling exponents for hard tasks, but not for easy or super-easy tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">18:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17500" target="_blank">@arXiv 2409.17500</a>
                    <span class="tweet-title">Neural Networks Get a Constraint Makeover:  GLinSAT Makes Them Play by the Rules!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Alibaba</span>
                </div>
                <div class="primary-text">
                    This research introduces GLinSAT, a new neural network layer that can handle general linear constraints on outputs. Unlike previous methods that rely on matrix factorization or black-box solvers, GLinSAT uses an accelerated gradient descent algorithm, making it more efficient and GPU-friendly.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">19:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18057" target="_blank">@arXiv 2409.18057</a>
                    <span class="tweet-title">LightAvatar:  Neural Light Fields Make Head Avatars Super Speedy!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google, Northeastern University, Simon Fraser University</span>
                </div>
                <div class="primary-text">
                    This research introduces LightAvatar, a head avatar model that uses neural light fields (NeLFs) instead of the more common neural radiance fields (NeRFs). This approach allows for faster rendering speeds because it requires only a single network forward pass, unlike NeRFs which require multiple passes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">19:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17567" target="_blank">@arXiv 2409.17567</a>
                    <span class="tweet-title">Derandomizing Multi-Distribution Learning:  Can We Ditch the Dice?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Aarhus University, Yale University, UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This paper explores the computational complexity of derandomizing multi-distribution learning algorithms, which are used to train models that perform well across multiple datasets. Unlike previous work that outputs randomized predictors, this research investigates the possibility of producing deterministic predictors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">20:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17909" target="_blank">@arXiv 2409.17909</a>
                    <span class="tweet-title">Graphing Your Way to Better SME Credit Scores: A Neural Network Revolution</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of California  Irvine, New York University, Trine University...</span>
                </div>
                <div class="primary-text">
                    This research uses a graph neural network (GNN) to model the relationships between financial indicators for SMEs, going beyond traditional methods that rely on isolated feature analysis.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">20:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17655" target="_blank">@arXiv 2409.17655</a>
                    <span class="tweet-title">Office Assistant Bot:  LLM-Powered, Proactive, and (Almost) Human-Like!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces AssistantX, an LLM-powered robot assistant that operates in a physical office environment. Unlike previous service robots, AssistantX utilizes a multi-agent architecture called PPDR4X, which enables it to reason logically and collaborate with humans to complete tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">20:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17502" target="_blank">@arXiv 2409.17502</a>
                    <span class="tweet-title">Tensor Multiplication Gets a Shape-Shifting Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo, Nagoya Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new operator called the "broadcast product" that extends the Hadamard product by aligning the shapes of tensors before performing element-wise multiplication. This addresses the issue of inconsistent mathematical descriptions of broadcast operations in libraries like NumPy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">21:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17954" target="_blank">@arXiv 2409.17954</a>
                    <span class="tweet-title">Big Brains, Tiny Clues: How Attention Differences Help Language Models Learn Faster</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for enhancing knowledge learning in language models by contrasting the attention patterns of large and small models. Unlike previous work that focuses on knowledge distillation from large models to smaller ones, this approach leverages the attention differences to identify elusive clues in text and uses them to guide data augmentation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">21:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18104" target="_blank">@arXiv 2409.18104</a>
                    <span class="tweet-title">Finding Rhinos Without Finding Rhinos:  A New Way to Track Endangered Species</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, MIT, University of Michigan</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to tracking rhinos by mapping their communal defecation sites, called middens, using remotely sensed imagery. This method differs from previous work that focused on directly tracking rhinos, which can be challenging due to their elusive nature.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">21:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17604" target="_blank">@arXiv 2409.17604</a>
                    <span class="tweet-title">RmGPT:  The AI That's Got Your Machinery's Back!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Shanghai Jiao Tong University, Polytechnic University of Milan, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes RmGPT, a unified model for diagnosis and prognosis tasks in rotating machinery. Unlike previous work that relies on task-specific models, RmGPT uses a single set of parameters to handle diverse datasets with varying signal characteristics, fault modes, and operating conditions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">22:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17972" target="_blank">@arXiv 2409.17972</a>
                    <span class="tweet-title">LLMs Get Math-Savvy:  New Technique Makes AI Smarter Than GPT-4!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Chinese Academy of Sciences, Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach called BEATS, which uses a pruning tree search to enhance the mathematical problem-solving abilities of LLMs. Unlike previous methods that rely on supervised fine-tuning or complex search algorithms, BEATS focuses on carefully designed prompts and a back-verification technique to improve accuracy and efficiency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">22:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18061" target="_blank">@arXiv 2409.18061</a>
                    <span class="tweet-title">Forgetful AI Gets a Memory Boost: Optimal Training Protocols for Continual Learning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford, Chalmers University of Technology, University of Gothenburg...</span>
                </div>
                <div class="primary-text">
                    This research differs from previous work by applying optimal control theory to the training dynamics of neural networks in a continual learning setting. It uses statistical physics techniques to reduce the dimensionality of the problem, allowing for the derivation of closed-form formulae for optimal task-selection protocols and learning rate schedules.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">23:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17508" target="_blank">@arXiv 2409.17508</a>
                    <span class="tweet-title">Uni-Med:  A Medical AI That's Got All the Answers (and Images!)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Beijing University of Posts and Telecommunications</span>
                </div>
                <div class="primary-text">
                    This research focuses on improving the "connector" in multi-modal medical AI models, which bridges the gap between visual and textual information. Unlike previous work that primarily focused on the language model component, this paper introduces a novel approach called Connector-MoE (CMoE) to enhance the connector's ability to handle diverse medical tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">23:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17345" target="_blank">@arXiv 2409.17345</a>
                    <span class="tweet-title">SeaSplat:  Underwater Scenes Get a Color Makeover with 3D Gaussians!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Woods Hole Oceanographic Institution</span>
                </div>
                <div class="primary-text">
                    This research introduces SeaSplat, a method that combines 3D Gaussian Splatting with a physically grounded underwater image formation model. Unlike previous methods that rely on dense sampling or per-viewing direction estimation, SeaSplat learns global medium parameters, enabling efficient real-time rendering.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">24:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17264" target="_blank">@arXiv 2409.17264</a>
                    <span class="tweet-title">Serving Up 10 Million Tokens: How Researchers Made LLMs Eat Their Words (and Still Be Fast)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Georgia Institute of Technology, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel 3D parallelism strategy for long-context LLM inference, combining sequence pipeline parallelism (SPP), KV parallelism (KVP), and tensor parallelism (TP). This approach differs from previous work by enabling efficient prefill and decode phases for context lengths up to 10 million tokens, a significant leap compared to existing systems that typically handle up to 1 million tokens.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">24:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17348" target="_blank">@arXiv 2409.17348</a>
                    <span class="tweet-title">AI Teams Learn to Talk Like Humans, But Can They Work Together?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Pittsburgh, Honda Research Institute, CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel computational pipeline that aligns the communication space of multi-agent reinforcement learning (MARL) agents with human natural language. It does this by grounding agent communications on synthetic data generated by embodied large language models (LLMs) in interactive teamwork scenarios. This approach differs from previous work by explicitly incorporating human language into the training process, aiming to create more interpretable and generalizable communication protocols.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">25:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17946" target="_blank">@arXiv 2409.17946</a>
                    <span class="tweet-title">Backdoor Attacks:  LLMs Get Schooled by Tiny Teacher Models!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanyang Technological University</span>
                </div>
                <div class="primary-text">
                    This research explores the effectiveness of backdoor attacks targeting parameter-efficient fine-tuning (PEFT) algorithms for LLMs. It proposes a novel method called W2SAttack, which uses contrastive knowledge distillation to transfer backdoor features from a small-scale poisoned teacher model to a large-scale student model. This approach aims to enhance the effectiveness of backdoor attacks while minimizing computational resource consumption.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">25:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17383" target="_blank">@arXiv 2409.17383</a>
                    <span class="tweet-title">VectorSearch:  Finding Your Needle in the Haystack of Text, Faster!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington</span>
                </div>
                <div class="primary-text">
                    This research proposes VectorSearch, a hybrid system that combines vector embeddings with traditional indexing techniques to improve document retrieval efficiency. Unlike previous work that primarily focuses on index-centric approaches, VectorSearch integrates query engines and CPU optimization for a more comprehensive solution.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">25:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18000" target="_blank">@arXiv 2409.18000</a>
                    <span class="tweet-title">Safeguarding Robots from Time-Traveling Trouble: A New Algorithm for Time-Varying Optimization!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, Norwegian University of Science and Technology, Zurich University of Applied Sciences</span>
                </div>
                <div class="primary-text">
                    This research introduces TVSAFEOPT, an algorithm that tackles time-varying optimization problems with unknown reward and safety functions. Unlike previous methods, TVSAFEOPT uses a spatio-temporal kernel and time Lipschitz constants to safely track changes in the safe region without explicit change detection.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon" title="Play from here">26:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17823" target="_blank">@arXiv 2409.17823</a>
                    <span class="tweet-title">Ranking Loss:  A New Way to Distill Knowledge from Deep Learning Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new ranking loss function based on Kendall's τ coefficient to improve knowledge distillation. Unlike previous methods that rely solely on KL divergence, this approach considers the order of channel values in the logits, providing more inter-class relational information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon" title="Play from here">26:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17565" target="_blank">@arXiv 2409.17565</a>
                    <span class="tweet-title">Pixel-Perfect Post-Training:  Giving Diffusion Models a Vision Checkup!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Princeton University, Meta</span>
                </div>
                <div class="primary-text">
                    This research proposes adding a pixel-space supervision term during the post-training process of latent diffusion models (LDMs). This is different from previous work, which only trained LDMs in the latent space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon" title="Play from here">27:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18084" target="_blank">@arXiv 2409.18084</a>
                    <span class="tweet-title">Robots Learn to Queue Like Humans: New AI Helps Robots Navigate Crowds Without Being a Jerk</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to social navigation for robots by leveraging the visual reasoning capabilities of Large Multimodal Models (LMMs) to understand social structures in crowds. Unlike previous methods that rely on predefined rules or in-domain training, this approach enables zero-shot reasoning of social structure, allowing robots to navigate complex social environments without prior training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet67">
            <div class="start-time-icon" title="Play from here">27:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17166" target="_blank">@arXiv 2409.17166</a>
                    <span class="tweet-title">Bashing the Code: LLMs Automate Script Generation, Assessment, and Refinement</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research proposes a framework called ScriptSmith that automates the generation, assessment, and refinement of Bash scripts without relying on an execution environment. Unlike previous work that focuses on execution-based evaluation, ScriptSmith leverages a text-to-text approach for script assessment, making it suitable for scenarios where execution environments are unavailable or unreliable.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet68">
            <div class="start-time-icon" title="Play from here">27:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17313" target="_blank">@arXiv 2409.17313</a>
                    <span class="tweet-title">VLN Models:  Not as Smart as We Thought?  A Fine-Grained Look at Their Flaws</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">KU Leuven, Peking University, Nanyang Technological University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new evaluation framework for Vision-Language Navigation (VLN) tasks that focuses on assessing models' understanding of individual instructions, rather than just their overall success rate. This framework is based on a context-free grammar (CFG) that systematically breaks down VLN instructions into atomic categories.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet69">
            <div class="start-time-icon" title="Play from here">28:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17431" target="_blank">@arXiv 2409.17431</a>
                    <span class="tweet-title">DPO Gets a Tie-Breaker:  New Research Makes Room for Uncertainty in AI Preference Learning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research introduces two variants of Direct Preference Optimization (DPO) that explicitly model the possibility of ties in pairwise comparisons, unlike the original DPO formulation which only allows for clear preferences.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet70">
            <div class="start-time-icon" title="Play from here">28:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17512" target="_blank">@arXiv 2409.17512</a>
                    <span class="tweet-title">Open-Set Learning: When OODs Get a Label of Their Own!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo, Beijing University of Posts and Telecommunications</span>
                </div>
                <div class="primary-text">
                    This research tackles the "overtrusting" issue in open-set semi-supervised learning (OSSL). Unlike previous methods that rely heavily on limited labeled data, this paper proposes treating out-of-distribution (OOD) samples as a new class, forming a (K+1)-class SSL process. This approach allows the model to learn from both labeled in-distribution (ID) and selected OOD samples, leading to a more robust decision boundary.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet71">
            <div class="start-time-icon" title="Play from here">29:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18044" target="_blank">@arXiv 2409.18044</a>
                    <span class="tweet-title">Speech Translation:  Pretraining?  Nah, We Got This!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta, Polytechnic University of Catalonia</span>
                </div>
                <div class="primary-text">
                    This paper analyzes the training dynamics of direct speech-to-text translation systems, comparing a standard system with a pretrained encoder to one trained from scratch. It proposes a subtle modification in the Transformer architecture to bypass the pretraining stage.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet72">
            <div class="start-time-icon" title="Play from here">29:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17421" target="_blank">@arXiv 2409.17421</a>
                    <span class="tweet-title">Sunspots on the Horizon: AI Predicts Solar Active Regions 12 Hours in Advance!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">NASA Ames Research Center, New Jersey Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This research uses Long Short-Term Memory (LSTM) networks to predict the emergence of solar active regions (ARs) based on acoustic power and magnetic flux data, a novel approach compared to previous studies that focused on detecting ARs using helioseismic methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet73">
            <div class="start-time-icon" title="Play from here">30:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17876" target="_blank">@arXiv 2409.17876</a>
                    <span class="tweet-title">AI Donations:  Not Charity, Just Smart Business</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research goes beyond simply stating that companies donate AI open source software for altruistic reasons. It delves into the specific commercial incentives driving these donations, analyzing 43 cases from various companies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet74">
            <div class="start-time-icon" title="Play from here">30:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17519" target="_blank">@arXiv 2409.17519</a>
                    <span class="tweet-title">Robots Learn to Talk Like Us, See Like Us, and Now, Understand Our World!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research uses pre-trained vision-language models (VLMs) to enable robots to recognize environmental states, like whether a door is open or closed, without requiring specific training for each state. This differs from previous approaches that relied on individual methods for each state.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet75">
            <div class="start-time-icon" title="Play from here">30:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17190" target="_blank">@arXiv 2409.17190</a>
                    <span class="tweet-title">AI in Healthcare:  Guardrails to Stop LLMs from Hallucinating!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Ethriva Inc.</span>
                </div>
                <div class="primary-text">
                    This research proposes a framework that combines existing guardrail systems, Llama Guard and NVIDIA NeMo Guardrails, to enhance the safety and reliability of large language models (LLMs) in healthcare applications. This approach aims to address the unique challenges of hallucinations and misinformation in medical contexts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet76">
            <div class="start-time-icon" title="Play from here">31:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17699" target="_blank">@arXiv 2409.17699</a>
                    <span class="tweet-title">Jailbreak-Proofing LLMs:  A Simple Recipe for Robust Security</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research introduces MoJE, a novel guardrail architecture that utilizes simple linguistic statistical techniques to detect jailbreak attacks on LLMs. Unlike previous guardrails that rely on complex fine-tuning strategies or large language models, MoJE achieves high detection accuracy with minimal computational overhead.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet77">
            <div class="start-time-icon" title="Play from here">31:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18023" target="_blank">@arXiv 2409.18023</a>
                    <span class="tweet-title">Vision Models:  Not So Smart After All?  DARE Tests Their Limits!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge, Google</span>
                </div>
                <div class="primary-text">
                    This research introduces DARE, a new benchmark for evaluating Vision-Language Models (VLMs) that focuses on their robustness to variations in prompts, answer options, output formats, and the number of correct answers. This is different from previous work that primarily focused on evaluating VLM performance on specific tasks and instances.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet78">
            <div class="start-time-icon" title="Play from here">31:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17572" target="_blank">@arXiv 2409.17572</a>
                    <span class="tweet-title">Dr. GPT: Can AI Really Be Our Therapist?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Princeton University, Stanford University</span>
                </div>
                <div class="primary-text">
                    This research explores the potential of LLMs in mental health services by examining student opinions on their use in five specific scenarios, a novel approach compared to previous studies that focused on general applications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet79">
            <div class="start-time-icon" title="Play from here">31:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17704" target="_blank">@arXiv 2409.17704</a>
                    <span class="tweet-title">Transfer Learning:  Support is King, Value is a Jester!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo, Kyoto University</span>
                </div>
                <div class="primary-text">
                    This research focuses on hyperparameter selection for transfer learning in L1 regularized regression, specifically analyzing the impact of transferring support information versus the actual feature vector values.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet80">
            <div class="start-time-icon" title="Play from here">32:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17256" target="_blank">@arXiv 2409.17256</a>
                    <span class="tweet-title">Super-Resolution on a Budget: Making Videos Crystal Clear Without Breaking the Bank!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Würzburg, Sony PlayStation, Meta...</span>
                </div>
                <div class="primary-text">
                    This research focuses on developing efficient video super-resolution methods that are optimized for real-time performance and low computational demands, particularly on mobile devices. Unlike previous work that prioritized high-fidelity results, this study emphasizes the development of solutions with reduced parameters and operations, allowing for high frame rates while maintaining acceptable video quality.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet81">
            <div class="start-time-icon" title="Play from here">33:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17267" target="_blank">@arXiv 2409.17267</a>
                    <span class="tweet-title">Model Aggregation:  Stop Averaging, Start Error-Estimating!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Caltech</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for aggregating predictions from diverse models by minimizing the variance of the aggregate, rather than directly minimizing its prediction error. This approach is distinct from previous work that focuses on training strategies for building and combining multiple models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet82">
            <div class="start-time-icon" title="Play from here">33:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17424" target="_blank">@arXiv 2409.17424</a>
                    <span class="tweet-title">Big ANN Challenge:  Searching for Needles in a Haystack, Faster Than Ever!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft, IT University of Copenhagen, Meta</span>
                </div>
                <div class="primary-text">
                    This research focuses on four challenging variants of Approximate Nearest Neighbor (ANN) search: filtered search, out-of-distribution data, sparse vectors, and streaming scenarios. Unlike previous work that emphasized scaling up classical ANN search, this competition addresses these more complex and realistic scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet83">
            <div class="start-time-icon" title="Play from here">33:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17872" target="_blank">@arXiv 2409.17872</a>
                    <span class="tweet-title">Unmasking the Hidden Cause: A New Method for Peeking Inside Nonlinear Systems</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method for identifying the causal component of input-output data in nonlinear dynamical systems, even when the system is noisy and a perfect model is unavailable. This differs from previous work that relied on complete models or controlled inputs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet84">
            <div class="start-time-icon" title="Play from here">34:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17265" target="_blank">@arXiv 2409.17265</a>
                    <span class="tweet-title">CodonMPNN:  A New Way to Fold Proteins, One Codon at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces CodonMPNN, a model that directly generates codon sequences conditioned on a protein structure and the host organism. This differs from previous inverse folding approaches that generate amino acid sequences and then map them to codons.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet85">
            <div class="start-time-icon" title="Play from here">34:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17332" target="_blank">@arXiv 2409.17332</a>
                    <span class="tweet-title">Retinal AI Gets a Brain Boost: New Model Learns from Natural Images, Avoids Forgetting!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stadtspital Zürich, Spross Research Institute, Gutblick Research...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method called "block expansion" for adapting natural domain vision transformers to retinal imaging tasks. This method aims to mitigate catastrophic forgetting, a common problem in machine learning where models lose performance on previously learned data after being fine-tuned on new data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet86">
            <div class="start-time-icon" title="Play from here">35:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17270" target="_blank">@arXiv 2409.17270</a>
                    <span class="tweet-title">LLMs Get a Logic Lesson:  PROOF OF THOUGHT Makes AI Reasoning Verifiable</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Case Western Reserve University, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces PROOF OF THOUGHT, a framework that combines LLM-generated ideas with formal logic verification. It uses a custom interpreter to convert LLM outputs into First Order Logic constructs for theorem prover scrutiny.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet87">
            <div class="start-time-icon" title="Play from here">35:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17228" target="_blank">@arXiv 2409.17228</a>
                    <span class="tweet-title">Planet Hunters: AI Tool Finds Hidden Worlds in Dusty Disks!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Victoria, Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces Disk2Planet, a machine learning tool that infers planet parameters from protoplanetary disk structures. Unlike previous methods that rely on limited scalar metrics or require extensive simulations, Disk2Planet utilizes a neural network to analyze entire disk images, achieving higher accuracy and robustness.
                </div>
            </div>
        </div>

        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202409271551_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>    
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>
</html>