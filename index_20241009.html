
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY</div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">Fresh Picks: 
                    <span class="highlightNumber" style="font-size: 28px;">98</span> out of <span
                    class="highlightNumber">384</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-10-09"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>


        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">01:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04166" target="_blank">@arXiv 2410.04166</a>
                    <span class="tweet-title">Learning From Mistakes: A New Way to Train AI with Both Good and Bad Feedback</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for preference optimization that can leverage unpaired preferred or dis-preferred examples, unlike existing methods that rely on paired examples. This flexibility allows the method to be applied in scenarios with varying forms of feedback and models, including training generative language models based on human feedback as well as training policies for sequential decision-making problems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">01:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03859" target="_blank">@arXiv 2410.03859</a>
                    <span class="tweet-title">AI Can't See Code: New Benchmark Tests Software Bots on Visual Bugs</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Princeton University</span>
                </div>
                <div class="primary-text">
                    This research introduces SWE-bench Multimodal, a new benchmark for evaluating software engineering systems that focuses on tasks involving visual elements, such as images and videos. Unlike previous benchmarks, which primarily used text-based tasks, SWE-bench Multimodal challenges systems to solve bugs in JavaScript repositories that require understanding visual information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">01:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03806" target="_blank">@arXiv 2410.03806</a>
                    <span class="tweet-title">Time Series Forecasting:  Don't Just Look at the Numbers, Read the Metadata!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to time series forecasting by incorporating metadata, such as dataset descriptions and task details, into the model's training process. This differs from previous work that primarily focused on analyzing the time series data itself.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">02:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04010" target="_blank">@arXiv 2410.04010</a>
                    <span class="tweet-title">LLMs Go Hyperbolic: Fine-Tuning with a Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University, University of Stuttgart, The Chinese University of Hong Kong</span>
                </div>
                <div class="primary-text">
                    This research explores the use of hyperbolic geometry for fine-tuning large language models (LLMs). Unlike previous work that relies on Euclidean space, this study proposes a new method called HypLoRA that directly adapts LLMs in hyperbolic space, preserving the inherent geometric properties.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">02:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04265" target="_blank">@arXiv 2410.04265</a>
                    <span class="tweet-title">AI's Creativity:  A Remix or Original Score?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington</span>
                </div>
                <div class="primary-text">
                    This research introduces CREATIVITY INDEX, a novel metric that quantifies the linguistic creativity of a text by measuring how much of it can be reconstructed from existing web text snippets. This differs from previous work that relied on human evaluators or focused solely on verbatim matches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">02:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03766" target="_blank">@arXiv 2410.03766</a>
                    <span class="tweet-title">FutureFill:  Conquering the Speed Demon of Sequence Generation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This paper introduces FutureFill, a method for speeding up the generation of sequences in convolutional sequence prediction models. Unlike previous methods that require time proportional to the length of the sequence, FutureFill achieves a square root dependence on the sequence length. This is achieved by pre-computing and storing a cache of information that captures the dependence of the generated tokens on the context.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">03:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03868" target="_blank">@arXiv 2410.03868</a>
                    <span class="tweet-title">Can AI Understand Your Inner Values? New Study Tests Language Models on Individualistic Preferences!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington</span>
                </div>
                <div class="primary-text">
                    This research focuses on individualistic value reasoning, a new approach to AI alignment that aims to understand and predict individual preferences without relying on pre-defined categories like demographics or personality traits.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">03:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03825" target="_blank">@arXiv 2410.03825</a>
                    <span class="tweet-title">MonST3R:  A Point Cloud Revolution for Dynamic Scenes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley, Google DeepMind, Stability AI...</span>
                </div>
                <div class="primary-text">
                    This research introduces MonST3R, a novel approach that directly estimates geometry from dynamic scenes by representing them as per-timestep point clouds. Unlike previous methods that rely on multi-stage pipelines or global optimizations, MonST3R takes a geometry-first approach, simplifying the process and improving robustness.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">03:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03747" target="_blank">@arXiv 2410.03747</a>
                    <span class="tweet-title">AI for 6G:  A Distributed Platform to Tame the Data Beast!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research proposes a distributed AI platform architecture specifically tailored for 6G RAN, addressing the challenges of data collection and orchestration of AI models in a distributed network environment. It differs from previous work by focusing on dynamic data collection through programmable probes and a flexible AI processor runtime for seamless deployment across various edge locations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">04:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04368" target="_blank">@arXiv 2410.04368</a>
                    <span class="tweet-title">Random Transformers:  They're Not Just Dumb Luck!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research investigates the capabilities of randomly initialized transformer models by training only their embedding layers, leaving the internal parameters fixed. This approach differs from previous work that focused on training full models or pruning sub-networks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">04:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04207" target="_blank">@arXiv 2410.04207</a>
                    <span class="tweet-title">LoRA-ing Around: Learning on Low-Rank Weights for Fine-Tuned Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley, MIT</span>
                </div>
                <div class="primary-text">
                    This paper introduces "Learning on LoRAs" (LoL), a new paradigm where low-rank weight decompositions (LoRAs) are used as input to machine learning models. Unlike previous work on weight-space learning, which focuses on different symmetry groups, LoL models are specifically designed to handle the unique symmetries of LoRA weights.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">05:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04499" target="_blank">@arXiv 2410.04499</a>
                    <span class="tweet-title">Pre-trained Models Get a Performativity Makeover: Adapting Deep Learning for a Changing World</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Institute of Science and Technology  Austria, Max Planck Institute for Intelligent Systems, ELLIS Institute Tübingen...</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel technique to adjust pretrained backbones for performativity in a modular way, focusing on performative label shift. Unlike previous approaches that require training from scratch with performativity-augmented datasets, this method utilizes existing pretrained models and learns a shallow adapter module to correct for the shift.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">05:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04056" target="_blank">@arXiv 2410.04056</a>
                    <span class="tweet-title">RetNet to the Rescue: Super-Fast Image Completion with a Memory Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This paper introduces RetCompletion, a novel image completion framework that leverages the Retentive Network (RetNet) architecture, originally developed for natural language processing, to achieve faster inference speeds while maintaining high reconstruction quality. Unlike previous Transformer-based methods, RetCompletion utilizes a pixel-wise inference strategy, enabling it to incorporate previously generated pixel information during inference, leading to more accurate and consistent results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">05:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03959" target="_blank">@arXiv 2410.03959</a>
                    <span class="tweet-title">Robots Talking to Each Other:  A New Test for AI Communication</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research introduces a new task and dataset for evaluating how well language models can understand and generate referring expressions in a multi-agent setting. Unlike previous work, this study focuses on embodied agents with different perspectives of the same scene, making it more relevant to real-world applications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">06:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03794" target="_blank">@arXiv 2410.03794</a>
                    <span class="tweet-title">Foundation Models Get a Medical Makeover: Repurposing for Time Series Classification</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of North Carolina at Charlotte, Stanford University, Harvard University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to repurpose pre-trained foundation models for medical time series classification. Unlike previous work that focused on task-specific adaptation, this paper introduces a generalizable adaptation layer that allows the model to handle datasets with varying channel configurations, time series lengths, and diagnostic targets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">06:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04191" target="_blank">@arXiv 2410.04191</a>
                    <span class="tweet-title">Diffusion Models:  One Teacher, Many Students, Faster Images!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Shanghai Jiao Tong University, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new knowledge distillation technique called "one-to-many knowledge distillation" (O2MKD) for accelerating diffusion models. Unlike previous methods that distill a single teacher model into a single student, O2MKD trains multiple student models, each focusing on a specific subset of timesteps in the diffusion process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">07:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03901" target="_blank">@arXiv 2410.03901</a>
                    <span class="tweet-title">Node Embeddings Get a Boost: Target-Aware Contrastive Learning Makes Graph Networks Smarter</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Purdue University, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces Target-Aware Contrastive Learning (Target-aware CL), a self-supervised approach that adapts its contrastive loss function based on the target task. Unlike previous self-supervised methods, Target-aware CL strategically selects positive examples that enhance the mutual information between the target task and node representations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">07:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03972" target="_blank">@arXiv 2410.03972</a>
                    <span class="tweet-title">RNNs:  They're  Not  All  Created  Equal!  Degeneracy  Revealed  and  Controlled.</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This research introduces a unified framework for analyzing degeneracy in task-trained RNNs across three levels: behavior, neural dynamics, and weight space. It goes beyond previous work by quantifying task complexity using information-theoretic measures and demonstrating how this complexity influences degeneracy at each level.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">07:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03720" target="_blank">@arXiv 2410.03720</a>
                    <span class="tweet-title">NeuralQP:  QCQP Solver Gets a Hypergraph Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This paper introduces NeuralQP, a framework that uses hypergraphs to represent and solve large-scale Quadratically Constrained Quadratic Programs (QCQPs). Unlike previous ML-based QCQP optimization frameworks, NeuralQP doesn't rely on strong problem assumptions or large-scale solvers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">08:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04287" target="_blank">@arXiv 2410.04287</a>
                    <span class="tweet-title">GNNs:  Fair When Everyone's in the Same Clique?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Michigan</span>
                </div>
                <div class="primary-text">
                    This research focuses on the impact of local homophily on GNN fairness, going beyond the traditional global homophily analysis. It introduces a new out-of-distribution (OOD) problem for local homophily levels, demonstrating how underrepresented local homophily can lead to unfair predictions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">08:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03804" target="_blank">@arXiv 2410.03804</a>
                    <span class="tweet-title">LLMs on a Diet:  Speculative Decoding Gets a Makeover with Mixture of Attentions</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Huawei, University College London</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new architecture for speculative decoding, a technique that uses smaller models to speed up the generation of text by large language models (LLMs). The key innovation is the introduction of a "Mixture of Attentions" that addresses limitations in previous approaches, such as partial observability and lack of on-policyness during training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">08:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04320" target="_blank">@arXiv 2410.04320</a>
                    <span class="tweet-title">CAVs Get Smart:  Channel-Aware Data Fusion for Autonomous Driving!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">City University of Hong Kong, Tsinghua University, The University of Hong Kong</span>
                </div>
                <div class="primary-text">
                    This research proposes a channel-aware throughput maximization approach for cooperative data fusion in connected and autonomous vehicles (CAVs). Unlike previous work that assumes time-invariant communication channels, this study considers the dynamic nature of wireless channels and leverages a self-supervised autoencoder for adaptive data compression.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">09:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03849" target="_blank">@arXiv 2410.03849</a>
                    <span class="tweet-title">Contextual Shtarkov Sums:  The Secret Sauce for Optimal Online Learning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, Ben-Gurion University of the Negev</span>
                </div>
                <div class="primary-text">
                    This research introduces a new complexity measure called the contextual Shtarkov sum, which characterizes the minimax regret for sequential probability assignment with contexts. This differs from previous work that relied on sequential covering numbers, which were shown to be insufficient for characterizing minimax risk in general.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">09:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03781" target="_blank">@arXiv 2410.03781</a>
                    <span class="tweet-title">AI Tutors: Can We Teach Them to Fail Productively?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, École polytechnique, ETH AI Center...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach called "Pedagogical Steering" to address the lack of pedagogical properties in Large Language Models (LLMs) used for tutoring. Unlike previous work that focuses on fine-tuning LLMs or using extensive prompts, this study proposes an algorithm called StratL that dynamically updates tutoring intents based on student responses, effectively steering the LLM towards a specific pedagogical strategy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">10:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03893" target="_blank">@arXiv 2410.03893</a>
                    <span class="tweet-title">Chess Bot Learns to Think Like a Human, Even Makes Mistakes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, MIT, Visa Research</span>
                </div>
                <div class="primary-text">
                    This research introduces ALLIE, a chess-playing AI trained on human game data. Unlike previous systems that focused on maximizing performance, ALLIE aims to match the skill levels of human players across the spectrum, including non-move behaviors like pondering time and resignation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">10:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04081" target="_blank">@arXiv 2410.04081</a>
                    <span class="tweet-title">Visual Decoding:  Turning Noise into Art with Diffusion!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new approach to visual tokenization by replacing the traditional decoder in an autoencoder with a diffusion process. Instead of reconstructing the image in a single step, the diffusion model iteratively refines noisy data to recover the original image, guided by the latent representation provided by the encoder.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">11:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03983" target="_blank">@arXiv 2410.03983</a>
                    <span class="tweet-title">MetricX-24:  A Machine Translation Metric That's Not Afraid to Say "No" to Bad References!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces a hybrid reference-based/reference-free machine translation evaluation metric, MetricX-24, which can score translations regardless of whether a reference is provided. This differs from previous versions of MetricX, which relied solely on reference-based evaluation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">11:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04224" target="_blank">@arXiv 2410.04224</a>
                    <span class="tweet-title">One-Step Diffusion: Super-Resolution Without the Teacher!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Shanghai Jiao Tong University, ETH Zurich, Westlake University...</span>
                </div>
                <div class="primary-text">
                    This paper proposes a Distillation-Free One-Step Diffusion (DFOSD) model for real-world image super-resolution (Real-ISR). Unlike previous one-step diffusion SR models, DFOSD does not employ knowledge distillation to train its one-step diffusion generator.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">11:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04060" target="_blank">@arXiv 2410.04060</a>
                    <span class="tweet-title">LLMs Get a Tensor Makeover:  Parameter Efficiency Goes 5D!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford, University of Pennsylvania</span>
                </div>
                <div class="primary-text">
                    This paper introduces LoRTA, a novel approach that uses a low-rank tensor model to adapt large language models (LLMs) for downstream tasks. Unlike previous methods that use low-rank matrices, LoRTA leverages the redundancy in weight updates across different layers, heads, and attention matrices by representing them as a unified 5th-order tensor.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">12:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04254" target="_blank">@arXiv 2410.04254</a>
                    <span class="tweet-title">Wikipedia's Missing Links: A New Task for AI to Fill the Gaps</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Aarhus University, Wikimedia Foundation, École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research introduces the task of "entity insertion," which focuses on identifying the most suitable place to add a link to a target entity within a Wikipedia article, even when no mention of that entity exists in the text. This differs from traditional entity linking, which assumes the entity is already mentioned.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">12:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03707" target="_blank">@arXiv 2410.03707</a>
                    <span class="tweet-title">Mamba Strikes Back: A Graph-Powered Stock Prediction Model That's Actually Fast!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of British Columbia, Simon Fraser University, University of Pittsburgh...</span>
                </div>
                <div class="primary-text">
                    This paper proposes SAMBA, a stock prediction model that combines the Mamba architecture with graph neural networks. Unlike previous transformer-based models, SAMBA achieves near-linear computational complexity, making it more practical for real-time trading and long-sequence data processing.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">12:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03812" target="_blank">@arXiv 2410.03812</a>
                    <span class="tweet-title">SLAM-ing the Speed Limit: Event Cameras Boost Neural Mapping</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, INSAIT, Sofia University</span>
                </div>
                <div class="primary-text">
                    This research integrates event cameras into the NICE-SLAM framework, a neural implicit encoding SLAM system. This allows for accurate camera tracking and mapping even when the input RGB-D images are temporally sparse.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">13:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04343" target="_blank">@arXiv 2410.04343</a>
                    <span class="tweet-title">RAG's Big Brain: How More Compute Makes LLMs Smarter</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Illinois, Google</span>
                </div>
                <div class="primary-text">
                    This research explores how increasing the amount of computation used during inference can improve the performance of retrieval-augmented generation (RAG) models, going beyond simply adding more knowledge. It introduces two new strategies, DRAG and IterDRAG, which leverage in-context learning and iterative prompting to effectively scale inference computation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">13:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04489" target="_blank">@arXiv 2410.04489</a>
                    <span class="tweet-title">Grokking: When AI Overthinks and Then Gets It Right</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research explores Grokking in a simplified setting where the optimal solution can always be identified, allowing for a clear definition of "memorization" and "learning." It focuses on a binary classification task with constant labels and analyzes the asymptotic dynamics of the model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">13:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04133" target="_blank">@arXiv 2410.04133</a>
                    <span class="tweet-title">ECG Founder: A Universal Model for Your Heart's Rhythm and Blues</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces ECG Founder, a foundation model trained on over 10 million ECG recordings, which is significantly larger than previous datasets. This allows the model to learn more complex patterns and generalize better to different ECG types and diagnostic tasks. Unlike previous models, ECG Founder can handle both 12-lead and single-lead ECGs, making it suitable for both hospital and portable devices.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">14:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03788" target="_blank">@arXiv 2410.03788</a>
                    <span class="tweet-title">Missing Activity? No Problem! New Model Reconstructs Human Mobility Patterns</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Los Angeles, Texas A&M University, University of Louisville...</span>
                </div>
                <div class="primary-text">
                    This research introduces a semi-supervised iterative transfer learning algorithm to reconstruct human mobility patterns from incomplete datasets. This approach differs from previous work by focusing on semantic activity chains and adapting models to diverse geographical contexts without relying on ground truth data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">14:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03736" target="_blank">@arXiv 2410.03736</a>
                    <span class="tweet-title">AI Chatbot Turns Clinicians into Data Scientists</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research introduces CliMB, an AI-enabled chatbot that guides clinician scientists through the entire medical data science pipeline, enabling them to build predictive models using natural language. This approach differs from previous work by providing a comprehensive, user-friendly solution that integrates data-centric AI, AutoML, and interpretable ML methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">15:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03989" target="_blank">@arXiv 2410.03989</a>
                    <span class="tweet-title">Symmetry Cloning: Teaching AI to See the World Like We Do</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, Vector Institute for Artificial Intelligence</span>
                </div>
                <div class="primary-text">
                    This research introduces "symmetry-cloning," a method for training machine learning models to learn symmetries directly from data, rather than relying on pre-defined architectures. This approach allows for more flexibility and adaptability in handling real-world data that may not perfectly conform to expected symmetries.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">15:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04454" target="_blank">@arXiv 2410.04454</a>
                    <span class="tweet-title">LLMs Got Talent? New Framework Detects Which Datasets Are the Stars!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces CopyLens, a framework that analyzes how copyrighted datasets influence LLM outputs. Unlike previous methods that focus on detecting verbatim copies or individual token contributions, CopyLens examines the impact of entire datasets on LLM responses.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">16:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04039" target="_blank">@arXiv 2410.04039</a>
                    <span class="tweet-title">BlockFound:  A Blockchain Anomaly Detector That's Not Just Rule-Based!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Northwestern University, Meta, NYU...</span>
                </div>
                <div class="primary-text">
                    This research proposes BlockFound, a customized foundation model for detecting anomalous blockchain transactions. Unlike previous methods that rely on rule-based systems or directly apply off-the-shelf large language models, BlockFound introduces a series of customized designs to model the unique data structure of blockchain transactions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">16:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04025" target="_blank">@arXiv 2410.04025</a>
                    <span class="tweet-title">IdeaSynth:  Turning Research Sparks into Full-Blown Projects!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, University of Washington, Allen Institute for AI</span>
                </div>
                <div class="primary-text">
                    This research focuses on helping researchers develop their initial ideas into concrete research briefs, unlike previous work that primarily focused on generating broad ideas. IdeaSynth uses a node-based canvas to represent different facets of a research idea, allowing users to explore variations and connections between them.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">16:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04223" target="_blank">@arXiv 2410.04223</a>
                    <span class="tweet-title">LLMs Go Molecular: Designing Drugs & Materials with a Chatty AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research introduces Llamole, a multimodal large language model (LLM) that integrates text and graph generation for molecular design. Unlike previous LLMs that struggle with molecular data, Llamole can interleave text and graph generation, enabling it to design molecules with specific properties and plan their synthesis.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">17:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03997" target="_blank">@arXiv 2410.03997</a>
                    <span class="tweet-title">LLM-Powered MARL: One-Time Genius for Multi-Agent Teamwork</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Connecticut, University of Pennsylvania, Nvidia</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel framework called YOLO-MARL that leverages the planning capabilities of LLMs to enhance multi-agent reinforcement learning (MARL) policy training. Unlike previous approaches that frequently interact with LLMs during training, YOLO-MARL only requires a one-time interaction with the LLM for each game environment.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">17:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03740" target="_blank">@arXiv 2410.03740</a>
                    <span class="tweet-title">Ophthalmology's New AI Doctor: LEME Sees All, Knows All!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard Medical School, Yale University, Singapore National Eye Centre...</span>
                </div>
                <div class="primary-text">
                    This research introduces LEME, an open-source, ophthalmology-specific large language model (LLM) fine-tuned on a vast dataset of 127,000 training instances. Unlike previous ophthalmology-specific LLMs, LEME was evaluated on clinical scenario-related tasks involving electronic health records (EHRs), demonstrating its potential for real-world applications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">17:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03742" target="_blank">@arXiv 2410.03742</a>
                    <span class="tweet-title">LLMs as Judges:  AI Learns to Judge, Not Just Score!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, University of Copenhagen</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for training LLMs to make preference judgments, called Con-J. Unlike traditional scalar reward models, Con-J leverages the LLM's own judgment abilities by prompting it to generate both positive and negative judgments with rationales. This approach aims to improve interpretability and robustness against biases in datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">18:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03795" target="_blank">@arXiv 2410.03795</a>
                    <span class="tweet-title">Design Patterns:  AI's Secret Weapon for Building Super-Smart Systems</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Kyoto University, Rutgers University, University of Wisconsin-Madison...</span>
                </div>
                <div class="primary-text">
                    This research focuses on adapting classical software design patterns to the unique challenges of building and managing deep learning and machine learning systems. It goes beyond simply applying these patterns to traditional software development and explores their specific use cases in AI systems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">18:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03741" target="_blank">@arXiv 2410.03741</a>
                    <span class="tweet-title">AI Cardiologist: Can a Chatbot Diagnose Your Heart?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Google</span>
                </div>
                <div class="primary-text">
                    This research evaluates a large language model (LLM) specifically designed for medical dialogue, AMIE, on its ability to assess patients with rare, life-threatening inherited cardiomyopathies. Unlike previous studies that used generic LLMs, this study focuses on a specialized LLM and uses real-world clinical data from a subspecialist center.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">19:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04315" target="_blank">@arXiv 2410.04315</a>
                    <span class="tweet-title">Certainty Phrases: Not Just Words, But Distributions!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research models uncertainty as distributions over the probability simplex, rather than assigning a single score to each certainty phrase. This allows for a more nuanced understanding of the semantics of certainty expressions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">19:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04041" target="_blank">@arXiv 2410.04041</a>
                    <span class="tweet-title">NeRF-tastic 3D Reconstruction:  Endoscopy Gets a Depth Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, Chinese University of Hong Kong</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel NeRF-based 3D reconstruction pipeline that utilizes stereo vision derived from monocular endoscopic inputs to achieve high-accuracy depth estimation. This approach differs from previous methods by iteratively refining the NeRF model using depth supervision, leading to improved accuracy and resolution in the final 3D reconstruction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">20:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04419" target="_blank">@arXiv 2410.04419</a>
                    <span class="tweet-title">Visual Localization Goes Lite: Navigating the World with Tiny Maps</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research proposes LiteVLoc, a visual localization framework that uses a lightweight topo-metric map to represent the environment. Unlike traditional methods that rely on detailed 3D maps, LiteVLoc reduces storage overhead by leveraging learning-based feature matching and geometric solvers for metric pose estimation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">20:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04417" target="_blank">@arXiv 2410.04417</a>
                    <span class="tweet-title">Vision Models on a Diet: Sparsifying Visual Tokens for Efficiency</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Fudan University, UC Berkeley...</span>
                </div>
                <div class="primary-text">
                    This research proposes a training-free method for sparsifying visual tokens in vision-language models (VLMs) by leveraging the self-attention matrix to identify and prune irrelevant tokens. Unlike previous methods that rely on additional training data or text-agnostic approaches, this method utilizes text-aware guidance to adaptively determine the sparsification ratio for each layer.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">20:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04324" target="_blank">@arXiv 2410.04324</a>
                    <span class="tweet-title">AI Voices: Can We Tell Real From Fake? New Benchmark Tests the Limits!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Fordham University, IBM</span>
                </div>
                <div class="primary-text">
                    This research introduces SONAR, a new framework and benchmark for evaluating AI-synthesized audio detection models. It differs from previous work by using a larger, more diverse dataset of fake audio generated by cutting-edge TTS models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">21:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03943" target="_blank">@arXiv 2410.03943</a>
                    <span class="tweet-title">Oscillating Your Way to Better Sequence Modeling: MIT Researchers Harness Harmonic Oscillators for AI Breakthrough</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research proposes a new state-space model called LinOSS, which is based on a system of forced harmonic oscillators. Unlike previous state-space models that rely on restrictive parameterizations, LinOSS only requires a nonnegative diagonal state matrix, making it more flexible and potentially more expressive.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">21:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03752" target="_blank">@arXiv 2410.03752</a>
                    <span class="tweet-title">Speech Recognition Gets a Streaming Makeover: LLMs Go Long Form!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta</span>
                </div>
                <div class="primary-text">
                    This research introduces SpeechLLM-XL, a streaming model that processes audio in chunks, enabling efficient speech recognition for long-form utterances. Unlike previous non-streaming models, SpeechLLM-XL scales linearly with audio length, addressing the computational challenges of handling long audio inputs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">21:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04509" target="_blank">@arXiv 2410.04509</a>
                    <span class="tweet-title">AI's Math Homework: Can It Spot Our Mistakes?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">HKUST(GZ), HKUST, Squirrel AI...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark called ERRORRADAR, which focuses on evaluating Multimodal Large Language Models (MLLMs) in the context of error detection in mathematical problem-solving. Unlike previous benchmarks that primarily assess problem-solving accuracy, ERRORRADAR specifically targets the ability of MLLMs to identify incorrect steps and categorize the types of errors made in a student's solution.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">22:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03904" target="_blank">@arXiv 2410.03904</a>
                    <span class="tweet-title">LLMs: Not Just for Chatbots Anymore - They're Making Audio Anomalies a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel framework called AADG, which leverages large language models (LLMs) to generate synthetic audio data containing anomalies. Unlike existing datasets that primarily focus on industrial sounds, AADG aims to create more diverse and realistic audio data, particularly for real-world scenarios where only audio data is available.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">22:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03718" target="_blank">@arXiv 2410.03718</a>
                    <span class="tweet-title">Assamese Language Models: Tokenizing the Tongue Twisters!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The Assam Kaziranga University</span>
                </div>
                <div class="primary-text">
                    This research focuses on evaluating the performance of tokenizers specifically for Assamese language models, a low-resource language, which is a gap in the existing literature.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">22:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03968" target="_blank">@arXiv 2410.03968</a>
                    <span class="tweet-title">Decoding Game:  Why Text Generation Needs a Little Chaos</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Princeton University</span>
                </div>
                <div class="primary-text">
                    This paper introduces Decoding Game, a theoretical framework that reimagines text generation as a game between a Strategist and Nature. Unlike previous work that focuses on maximizing likelihood, Decoding Game considers the adversarial nature of text generation, where Nature can distort the true distribution of language.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">23:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04225" target="_blank">@arXiv 2410.04225</a>
                    <span class="tweet-title">Video Super-Resolution:  A Blind Taste Test for AI!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Moscow State University, University of Würzburg</span>
                </div>
                <div class="primary-text">
                    This research focuses on developing objective quality assessment (QA) methods specifically for video super-resolution (SR) algorithms. Unlike previous work that relied on general image QA metrics, this challenge aims to create metrics that better correlate with human perception of SR quality.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">23:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04346" target="_blank">@arXiv 2410.04346</a>
                    <span class="tweet-title">Ranking LLMs:  Aligning Models with Human Preferences Using NDCG</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, University of Michigan, University of Florida</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel listwise approach called Ordinal Preference Optimization (OPO) for aligning large language models (LLMs) with human preferences. Unlike previous pairwise methods, OPO leverages the relative proximity of responses within a list, utilizing the Normalized Discounted Cumulative Gain (NDCG) metric for training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">24:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03798" target="_blank">@arXiv 2410.03798</a>
                    <span class="tweet-title">LLMs Learn to Speak: Self-Powered Speech Models Ditch the Teacher!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harbin Institute of Technology, Nanyang Technological University</span>
                </div>
                <div class="primary-text">
                    This research addresses the issue of "speech anchor bias" in large speech-text models (LSMs). Unlike previous methods that rely on extensive multi-modal data, this study proposes a self-powered approach where the model generates its own training data, effectively mitigating the bias towards speech inputs and improving instruction-following capabilities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">24:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04234" target="_blank">@arXiv 2410.04234</a>
                    <span class="tweet-title">Jailbreaking LLMs:  A Homotopy-Fueled Attack on Language Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Wisconsin-Madison</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel optimization method called functional homotopy, which leverages the duality between model training and input generation to address the challenges of discrete optimization in language models. Unlike previous methods that rely on token gradients, functional homotopy utilizes gradient descent in the continuous parameter space to create a sequence of easy-to-hard optimization problems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">25:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04279" target="_blank">@arXiv 2410.04279</a>
                    <span class="tweet-title">Deep Learning's Secret Weapon: Reflections and the Lasso</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research extends previous work on shallow networks by showing that deep networks with absolute value activation can be formulated as equivalent convex Lasso problems. This formulation reveals that deep networks favor symmetric structures, with greater depth enabling multilevel symmetries.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">25:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04070" target="_blank">@arXiv 2410.04070</a>
                    <span class="tweet-title">LLMs Get Personal: Decoding-Time Alignment for Tailored Text</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Zhejiang University, National University of Singapore, University of Washington</span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel approach called Personalized Alignment at Decoding-time (PAD) that aligns LLMs with diverse personalized preferences during inference without requiring additional training. Unlike previous methods that rely on training separate policy models for each preference, PAD leverages a single personalized reward model to guide the decoding process, dynamically tailoring the base model's predictions to individual user preferences.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon" title="Play from here">25:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04144" target="_blank">@arXiv 2410.04144</a>
                    <span class="tweet-title">Federated Unlearning Gets a Speed Boost: CONDA Dumps Data, Not Performance!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research introduces CONDA, a federated unlearning method that doesn't require retraining or access to data from remaining clients. It achieves this by selectively dampening parameters influenced by the client whose data is being removed.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon" title="Play from here">26:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03723" target="_blank">@arXiv 2410.03723</a>
                    <span class="tweet-title">AI Wrote It, But You Think It's Human: The Bias We All Have Against AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The Harker School, University of California  Santa Barbara, Carnegie Mellon University</span>
                </div>
                <div class="primary-text">
                    This research focuses on human bias against AI-generated text, a novel perspective compared to previous work that primarily examined AI bias towards different human populations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon" title="Play from here">26:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03818" target="_blank">@arXiv 2410.03818</a>
                    <span class="tweet-title">LLMs: Self-Detoxifying Superheroes of Text Generation!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM, MIT</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel decoding algorithm called Self-disciplined Autoregressive Sampling (SASA) that leverages the internal representations of LLMs to reduce toxicity in generated text without requiring external reward models or retraining. Unlike previous methods that rely on external models or retraining, SASA directly learns a subspace within the LLM's embedding space to identify and steer away from toxic outputs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet67">
            <div class="start-time-icon" title="Play from here">27:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04350" target="_blank">@arXiv 2410.04350</a>
                    <span class="tweet-title">Token-Level Tweaking:  Giving LLMs a Grammar Lesson for Better Behavior</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, University of Illinois at Chicago, Apple Inc.</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method called TIS-DPO, which assigns importance weights to individual tokens in a sequence during direct preference optimization (DPO). This differs from previous DPO methods that treat all tokens equally, potentially leading to less efficient optimization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet68">
            <div class="start-time-icon" title="Play from here">27:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04386" target="_blank">@arXiv 2410.04386</a>
                    <span class="tweet-title">Data Valuation Gets a Distribution Makeover:  How to Tell Which Data is Worth Its Weight in Gold</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National University of Singapore, CMU, A*STAR</span>
                </div>
                <div class="primary-text">
                    This research focuses on valuing data distributions, not just datasets. It proposes a method using Maximum Mean Discrepancy (MMD) to compare the values of distributions based on their samples, addressing the challenge of data heterogeneity across vendors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet69">
            <div class="start-time-icon" title="Play from here">27:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04442" target="_blank">@arXiv 2410.04442</a>
                    <span class="tweet-title">Time Series Forecasting:  Non-Stationarity's Got Talent (and It's Not Just a One-Trick Pony)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes TimeBridge, a framework that tackles the challenge of non-stationarity in time series forecasting by separating its impact on short-term and long-term modeling. Unlike previous methods that either eliminate or retain non-stationarity without considering its distinct effects, TimeBridge utilizes Integrated Attention to mitigate short-term non-stationarity and Cointegrated Attention to preserve it for long-term cointegration.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet70">
            <div class="start-time-icon" title="Play from here">28:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04205" target="_blank">@arXiv 2410.04205</a>
                    <span class="tweet-title">Deepfakes vs. Super-Resolution: A Battle for Authenticity</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ISTI-CNR, CNIT, Mercatorum University</span>
                </div>
                <div class="primary-text">
                    This research delves deeper into the effectiveness of super-resolution (SR) attacks on deepfake detectors. Unlike previous work, it explores the impact of different SR techniques and scaling factors on various deepfake generation methods, including entirely synthetic images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet71">
            <div class="start-time-icon" title="Play from here">28:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04108" target="_blank">@arXiv 2410.04108</a>
                    <span class="tweet-title">Reinforcement Learning's New Trick:  Occupancy Measure Approximation for Scalable Success!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Maryland  College Park, University of Central Florida, ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research proposes a new policy gradient algorithm for reinforcement learning with general utilities (RLGU) that uses maximum likelihood estimation (MLE) to approximate occupancy measures. This approach allows the algorithm to scale to larger state-action spaces, unlike previous methods that relied on tabular estimates.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet72">
            <div class="start-time-icon" title="Play from here">29:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03748" target="_blank">@arXiv 2410.03748</a>
                    <span class="tweet-title">Font-astic Fonts: AI Makes Words Look Like Their Meaning!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Egypt-Japan University of Science and Technology, Mohamed bin Zayed University of Artificial Intelligence, École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research introduces a system that automatically generates semantic typography, where the visual design of a word reflects its meaning. Unlike previous methods that focus on single letters or rely on pre-defined icons, this system morphs multiple letters simultaneously, enhancing readability and handling cursive languages like Arabic.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet73">
            <div class="start-time-icon" title="Play from here">29:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04097" target="_blank">@arXiv 2410.04097</a>
                    <span class="tweet-title">Brain Scans Get a Super-Resolution Makeover: No Training Data Needed!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Basque Center on Cognition  Brain  and Language, Harvard Medical School, Ikerbasque...</span>
                </div>
                <div class="primary-text">
                    This research introduces a self-supervised deep learning method for super-resolution fMRI, eliminating the need for high-resolution ground truth data during training. This differs from previous approaches that relied on paired low-resolution and high-resolution images for training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet74">
            <div class="start-time-icon" title="Play from here">29:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03767" target="_blank">@arXiv 2410.03767</a>
                    <span class="tweet-title">AI's Got a New Reasoning Game: Counterfactual Feedback Makes the Difference!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, Microsoft Research, Cornell Tech</span>
                </div>
                <div class="primary-text">
                    This research focuses on improving the reasoning abilities of language models by introducing a novel fine-tuning approach that leverages counterfactual feedback. Unlike previous work that primarily relies on factual examples, this method incorporates both factual and counterfactual questions to elicit better reasoning mechanisms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet75">
            <div class="start-time-icon" title="Play from here">30:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04112" target="_blank">@arXiv 2410.04112</a>
                    <span class="tweet-title">AI Doctors Get a Flowchart Makeover: New Method Makes Medical Chatbots More Human-Like</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Beijing Jiaotong University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach for annotating preference data in medical dialogue models using a multi-agent system. Unlike previous methods that rely heavily on expert input or general-purpose LLMs, this approach leverages a combination of Constitutional AI and flowchart-based representations of physician preferences to create a more autonomous and adaptable system.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet76">
            <div class="start-time-icon" title="Play from here">30:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03920" target="_blank">@arXiv 2410.03920</a>
                    <span class="tweet-title">Robots Learn Object Properties by Feeling, Not Seeing!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Amazon</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for identifying object properties using only robot proprioception, eliminating the need for external sensors or vision-based tracking systems. Unlike previous work that focuses on identifying robot properties or object properties using object-specific data, this approach calibrates object properties by analyzing the robot's responses to manipulated objects.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet77">
            <div class="start-time-icon" title="Play from here">31:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04347" target="_blank">@arXiv 2410.04347</a>
                    <span class="tweet-title">LLMs: Not Just Chatbots, Now They're Feature Detectives!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of California  Los Angeles, Purdue University, University of Chicago</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel framework, FLAME, that leverages large language models (LLMs) to infer latent features from observed data. Unlike traditional latent feature mining techniques, FLAME incorporates contextual information provided in natural language and produces more interpretable outputs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet78">
            <div class="start-time-icon" title="Play from here">31:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03809" target="_blank">@arXiv 2410.03809</a>
                    <span class="tweet-title">Mammograms:  Not Just for Boobs, They're for... Artefacts?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Imperial College London</span>
                </div>
                <div class="primary-text">
                    This study uniquely investigates the impact of radio-opaque artefacts, like skin markers and implants, on the performance of mammography classification models. Previous research has primarily focused on global image changes, neglecting the localized effects of these artefacts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet79">
            <div class="start-time-icon" title="Play from here">31:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03697" target="_blank">@arXiv 2410.03697</a>
                    <span class="tweet-title">Simulating Success: How Importance Sampling Can Tune Large-Scale Recommenders</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft, University of Texas at Austin</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach called Simulator-Guided Importance Sampling (SGIS) that combines the strengths of open-box simulators and importance sampling for tuning large-scale recommender systems. Unlike previous methods that focus on bias-variance trade-offs, SGIS aims to achieve a computational trade-off by combining two unbiased estimators of key performance indicators (KPIs).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet80">
            <div class="start-time-icon" title="Play from here">32:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03952" target="_blank">@arXiv 2410.03952</a>
                    <span class="tweet-title">Brain-Inspired Hack:  Making AI Less Sensitive to Tiny Tweaks!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new regularizer that mimics the computational function of neural regularizers without requiring neural recordings, expanding the usability and effectiveness of these techniques.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet81">
            <div class="start-time-icon" title="Play from here">32:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04485" target="_blank">@arXiv 2410.04485</a>
                    <span class="tweet-title">AI Chatbots:  Can They Fix Your Code?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Moscow State University</span>
                </div>
                <div class="primary-text">
                    This research explores the potential of conversational program repair using LLMs, specifically focusing on the SWE-Bench dataset. Unlike previous work that primarily focused on repetitive patch generation, this study investigates the effectiveness of a conversational approach where the LLM receives feedback on test failures during the repair process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet82">
            <div class="start-time-icon" title="Play from here">32:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03768" target="_blank">@arXiv 2410.03768</a>
                    <span class="tweet-title">LLMs Hiding Secrets in Plain Sight: Steganographic Collusion Exposed!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">LASR Labs, University College London, University of Amsterdam...</span>
                </div>
                <div class="primary-text">
                    This research demonstrates that robust steganographic collusion in LLMs can arise indirectly from optimization pressure, unlike previous work that focused on predefined steganographic codes or prompting LLMs to devise codes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet83">
            <div class="start-time-icon" title="Play from here">33:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04498" target="_blank">@arXiv 2410.04498</a>
                    <span class="tweet-title">RL's Memory Makeover:  How AI Learns From Both Wins and Fails</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces AdaMemento, a new RL framework that utilizes both successful and failed experiences to refine its policy. Unlike previous methods that solely focus on positive experiences, AdaMemento incorporates a memory-reflection module that learns from both successes and failures, enabling more accurate prediction of trajectories and confidence levels.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet84">
            <div class="start-time-icon" title="Play from here">33:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04135" target="_blank">@arXiv 2410.04135</a>
                    <span class="tweet-title">IceCloudNet:  Turning Satellite Snapshots into 3D Cloud Movies!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research uses machine learning to combine high-resolution satellite images with vertical cloud profiles, creating a 3D dataset of cloud ice properties with unprecedented temporal resolution. This approach differs from previous work by fusing data from different satellite instruments and leveraging the strengths of both.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet85">
            <div class="start-time-icon" title="Play from here">34:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03791" target="_blank">@arXiv 2410.03791</a>
                    <span class="tweet-title">AI Voices: So Convincing, You Can't Tell Fake From Real!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This study expands on previous research by using state-of-the-art voice cloning technology, increasing the number of speakers to over 200, and examining how different tasks (identity and naturalness) impact our ability to distinguish AI-powered voices.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet86">
            <div class="start-time-icon" title="Play from here">34:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04309" target="_blank">@arXiv 2410.04309</a>
                    <span class="tweet-title">Pollution Hotspots:  Hidden in Plain Sight?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">New York University, Google Research, Intel Corporation...</span>
                </div>
                <div class="primary-text">
                    This research augments existing public sensor networks with low-cost sensors to uncover hidden pollution hotspots, demonstrating the limitations of sparse sensor networks and proposing a space-time kriging approach for more accurate hotspot identification.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet87">
            <div class="start-time-icon" title="Play from here">34:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.03756" target="_blank">@arXiv 2410.03756</a>
                    <span class="tweet-title">Building Brains: AI Learns to Save Energy (and the Planet!)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces the SmartBuildingsControlSuite, an open-source dataset and simulator for training AI agents to optimize energy consumption in commercial buildings. Unlike previous work that relies on proprietary data or expensive simulations, this suite provides real-world data and a calibrated simulator, enabling more accessible and scalable research.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet88">
            <div class="start-time-icon" title="Play from here">35:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04383" target="_blank">@arXiv 2410.04383</a>
                    <span class="tweet-title">BrainCodec: Compressing Brain Scans for Smarter AI!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research proposes BrainCodec, a novel fMRI codec inspired by neural audio codecs, which uses a technique called Residual Vector Quantization (RVQ) to compress fMRI data. This differs from previous work that used continuous value-based compression methods like VAEs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet89">
            <div class="start-time-icon" title="Play from here">35:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04253" target="_blank">@arXiv 2410.04253</a>
                    <span class="tweet-title">AI Explanations:  Not Just Why, But Why This, Not That!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, University of Massachusetts Amherst</span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework for generating contrastive explanations that compare an AI's decision to a predicted human response, highlighting the differences between the two. This approach differs from previous work that focuses on explaining the AI's decision in isolation, without considering the user's perspective.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet90">
            <div class="start-time-icon" title="Play from here">36:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04263" target="_blank">@arXiv 2410.04263</a>
                    <span class="tweet-title">DeFoG:  Graph Generation Gets a Flow-Based Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This paper introduces DeFoG, a novel graph generation framework that leverages discrete flow matching. Unlike diffusion models, DeFoG decouples the training and sampling stages, allowing for more efficient optimization of model performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet91">
            <div class="start-time-icon" title="Play from here">36:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04260" target="_blank">@arXiv 2410.04260</a>
                    <span class="tweet-title">Safe Set Expansion:  A Pareto-Perfect Solution for Input-Constrained Systems</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research introduces the Pareto Control Barrier Function (PCBF) algorithm, which maximizes the inner safe set of dynamical systems under input constraints. Unlike previous methods that often fail to account for realistic input limitations, PCBF leverages the Pareto multi-task learning framework to balance safety and safe set volume.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet92">
            <div class="start-time-icon" title="Play from here">36:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04460" target="_blank">@arXiv 2410.04460</a>
                    <span class="tweet-title">AI Predicts Brain Fluid Flow, Fewer Scans Needed!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research uses a U-net deep learning model to predict the distribution of a cerebrospinal fluid (CSF) tracer in the brain, potentially reducing the number of MRI scans needed for clinical analysis. Unlike previous work that relied on complex mathematical models or specific assumptions, this approach leverages data-driven machine learning to predict CSF flow patterns.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet93">
            <div class="start-time-icon" title="Play from here">37:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04422" target="_blank">@arXiv 2410.04422</a>
                    <span class="tweet-title">LLMs:  Long Context, Short Attention Span?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research delves into the specific challenges faced by long-context language models (LCLMs) when tackling complex tasks, identifying two key factors: multi-matching retrieval and logic-based retrieval.  The paper goes beyond simply observing these difficulties and explores the underlying reason for their persistence, proposing the concept of "hyper-multi-step" problems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet94">
            <div class="start-time-icon" title="Play from here">37:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04261" target="_blank">@arXiv 2410.04261</a>
                    <span class="tweet-title">Spacecraft Landing: Diffusion Models Make the Descent a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces TrajDiffuser, a diffusion-based trajectory generator for spacecraft powered descent guidance. Unlike previous AI models that generate trajectories sequentially, TrajDiffuser generates the entire trajectory concurrently, improving stability and allowing for multi-modal distributions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet95">
            <div class="start-time-icon" title="Play from here">38:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04201" target="_blank">@arXiv 2410.04201</a>
                    <span class="tweet-title">Idempotent Test-Time Training:  Making Models Forget What They Don't Know!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne, Bilkent University, Nvidia...</span>
                </div>
                <div class="primary-text">
                    This paper introduces Idempotent Test-Time Training (IT3), a novel approach to addressing distribution shift. Unlike previous Test-Time Training (TTT) methods that rely on domain-specific auxiliary tasks, IT3 leverages the universal property of idempotence, a mathematical concept that ensures repeated application of an operator yields the same result as a single application.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet96">
            <div class="start-time-icon" title="Play from here">38:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04445" target="_blank">@arXiv 2410.04445</a>
                    <span class="tweet-title">X-Ray Vision:  New AI Technique Makes Cephalometric Landmark Detection More Accurate!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research introduces a domain alignment strategy for cephalometric landmark detection, which involves a regional facial extraction module and an X-ray artefact augmentation procedure. This approach aims to improve the model's generalizability to unseen data from different imaging protocols.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet97">
            <div class="start-time-icon" title="Play from here">38:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2410.04264" target="_blank">@arXiv 2410.04264</a>
                    <span class="tweet-title">Deep Learning's Secret Weapon: Unmasking the Minimal Feature Regime</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford, Seoul National University, UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for analyzing feature learning in deep neural networks (DNNs) by decomposing them into a forward feature map and a final linear layer. The authors then diagonalize the forward feature map with respect to the gradient descent operator and track feature learning by measuring how the eigenfunctions and eigenvalues of the feature map change during training.
                </div>
            </div>
        </div>

        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202410091314_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>    
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>
</html>