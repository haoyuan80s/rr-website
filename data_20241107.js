
daily_data = {
    "date": "2024-11-07",
    "tweets": [
        
        {
            "startTime": "01:19",
            "arxivId": "2411.03887",
            "arxivLink": "https://arxiv.org/abs/2411.03887",
            "title": "AI Fingerprinting: A New Way to Track Who's Using Your Brainchild",
            "institute": "Princeton University, Sentient Foundation, University of Washington...",
            "text": "This paper introduces a new field called \"AI-native cryptography\" which uses cryptographic primitives tailored to AI applications. Unlike traditional cryptography, which focuses on discrete data and binary security guarantees, AI-native cryptography exploits the continuous nature of AI data representations and their low-dimensional manifolds, focusing on improving approximate performance.",
            "paper-title": "OML: Open, Monetizable, and Loyal AI",
            "image-path": "flux_paper_image/2411.03887_1731012552.png"
        },

        {
            "startTime": "01:45",
            "arxivId": "2411.03541",
            "arxivLink": "https://arxiv.org/abs/2411.03541",
            "title": "Mice Grok: Hidden Learning in Overtrained Brains",
            "institute": "Harvard University",
            "text": "This research reanalyzes existing neural data from mice trained on an odor discrimination task, finding evidence for continued learning in the piriform cortex even after behavioral performance plateaus. This contrasts with previous work that typically focuses on learning dynamics only up to the point of behavioral mastery.",
            "paper-title": "Do Mice Grok? Glimpses of Hidden Progress During Overtraining in Sensory Cortex",
            "image-path": "flux_paper_image/2411.03541_1731011883.png"
        },

        {
            "startTime": "01:59",
            "arxivId": "2411.03538",
            "arxivLink": "https://arxiv.org/abs/2411.03538",
            "title": "LLMs: Long Context, Big Problems?",
            "institute": "Databricks, Anthropic, OpenAI...",
            "text": "This research investigates the impact of increasing context length on Retrieval Augmented Generation (RAG) performance across 20 popular LLMs. It differs from previous work by evaluating a wider range of models and context lengths, including up to 2 million tokens.",
            "paper-title": "Long Context RAG Performance of Large Language Models",
            "image-path": "flux_paper_image/2411.03538_1731012445.png"
        },

        {
            "startTime": "02:19",
            "arxivId": "2411.03519",
            "arxivLink": "https://arxiv.org/abs/2411.03519",
            "title": "AI Metropolis: Out-of-Order Execution for Faster LLM Agent Simulations",
            "institute": "Stanford University",
            "text": "This research introduces AI Metropolis, a simulation engine that improves the efficiency of LLM agent simulations by incorporating out-of-order execution scheduling. This approach dynamically tracks real dependencies between agents, minimizing false dependencies and enhancing parallelism.",
            "paper-title": "AI Metropolis: Scaling Large Language Model-based Multi-Agent Simulation with Out-of-order Execution",
            "image-path": "flux_paper_image/2411.03519_1731012215.png"
        },

        {
            "startTime": "02:44",
            "arxivId": "2411.03409",
            "arxivLink": "https://arxiv.org/abs/2411.03409",
            "title": "Robots Learn to Grasp Like Humans, No More Clumsy Drops!",
            "institute": "Google DeepMind, UC Berkeley",
            "text": "This research introduces STEER, a framework that relabels existing robot demonstration datasets with detailed language annotations describing specific manipulation skills. This allows for training more flexible, language-conditioned policies that can be steered by a high-level reasoning system, like a human or a Vision-Language Model (VLM). This approach differs from previous work by focusing on making the low-level policies more adaptable to high-level instructions, rather than solely optimizing the high-level reasoning module.",
            "paper-title": "STEER: Flexible Robotic Manipulation via Dense Language Grounding",
            "image-path": "flux_paper_image/2411.03409_1731010875.png"
        },

        {
            "startTime": "03:10",
            "arxivId": "2411.04075",
            "arxivLink": "https://arxiv.org/abs/2411.04075",
            "title": "Sci-Fi QA: Foundation Models Face Multi-Modal Multi-Document Challenge!",
            "institute": "Yale University",
            "text": "This research introduces M3SCIQA, a benchmark for evaluating foundation models' ability to answer questions that require understanding multiple scientific documents and interpreting figures and tables. This differs from previous benchmarks that focus on single-document or text-only tasks.",
            "paper-title": "M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models",
            "image-path": "flux_paper_image/2411.04075_1731009989.png"
        },

        {
            "startTime": "03:38",
            "arxivId": "2411.03768",
            "arxivLink": "https://arxiv.org/abs/2411.03768",
            "title": "Data Point Selection Gets a Bayesian Makeover: It's Like Picking the Best Players for Your Team!",
            "institute": "Microsoft, Samsung, University of Edinburgh",
            "text": "This research proposes a Bayesian approach to data point selection (DPS), a technique used to filter or re-weight training data. Unlike previous methods that rely on bi-level optimization (BLO), this approach treats the problem as a Bayesian inference task, inferring the joint posterior distribution of the main neural network parameters and instance-wise weights.",
            "paper-title": "A Bayesian Approach to Data Point Selection",
            "image-path": "flux_paper_image/2411.03768_1731010207.png"
        },

        {
            "startTime": "03:59",
            "arxivId": "2411.03604",
            "arxivLink": "https://arxiv.org/abs/2411.03604",
            "title": "Dopamine's Distributed Delivery: Can It Teach AI to Play Games?",
            "institute": "University of Toronto, Vector Institute, Schwartz Reisman Institute for Technology and Society",
            "text": "This research proposes a new deep Q-learning algorithm, ARTIFICIAL DOPAMINE (AD), that uses distributed, per-layer temporal-difference (TD) errors to train RL agents. This differs from traditional deep RL algorithms that rely on backpropagation, which propagates errors sequentially through the network.",
            "paper-title": "Temporal-Difference Learning Using Distributed Error Signals",
            "image-path": "flux_paper_image/2411.03604_1731011237.png"
        },

        {
            "startTime": "04:32",
            "arxivId": "2411.03590",
            "arxivLink": "https://arxiv.org/abs/2411.03590",
            "title": "O1-Preview: The Thinking AI That Makes Medprompt Look Like Child's Play!",
            "institute": "Microsoft",
            "text": "This research compares the performance of OpenAI's o1-preview model, which is designed to reason before generating responses, with GPT-4 models enhanced by Medprompt, a set of advanced prompting techniques. The study finds that o1-preview often outperforms GPT-4 with Medprompt, suggesting that the need for elaborate prompting strategies may be diminishing with the advent of reasoning-native models.",
            "paper-title": "From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond",
            "image-path": "flux_paper_image/2411.03590_1731010663.png"
        },

        {
            "startTime": "04:56",
            "arxivId": "2411.03554",
            "arxivLink": "https://arxiv.org/abs/2411.03554",
            "title": "Forget Me Not: A New Benchmark for Unlearning Facial Identities in Vision Language Models",
            "institute": "University of Wisconsin-Madison, USC, University of Michigan-Ann Arbor...",
            "text": "This research introduces a new benchmark called FIUBENCH, specifically designed to evaluate the effectiveness of unlearning algorithms in Vision Language Models (VLMs) under the Right to be Forgotten setting. Unlike previous work that focused on unlearning textual information in LLMs, FIUBENCH addresses the unique challenges of unlearning private information linked to images in VLMs.",
            "paper-title": "Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset",
            "image-path": "flux_paper_image/2411.03554_1731011002.png"
        },

        {
            "startTime": "05:23",
            "arxivId": "2411.03923",
            "arxivLink": "https://arxiv.org/abs/2411.03923",
            "title": "LLMs: Cheating on the Test? A New Method to Catch Them Red-Handed!",
            "institute": "University College London, Boston University, Cohere...",
            "text": "This research proposes a novel method called ConTAM to assess the impact of evaluation data contamination on LLMs. Unlike previous work that focused on identifying contaminated examples, ConTAM directly measures the performance gain models achieve due to contamination.",
            "paper-title": "Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?",
            "image-path": "flux_paper_image/2411.03923_1731012395.png"
        },

        {
            "startTime": "05:51",
            "arxivId": "2411.03628",
            "arxivLink": "https://arxiv.org/abs/2411.03628",
            "title": "Streaming Video Understanding: Can AI Keep Up With Us?",
            "institute": "Tsinghua University, Beijing University of Posts and Telecommunications, OpenAI",
            "text": "This research introduces StreamingBench, a new benchmark specifically designed to evaluate the streaming video understanding capabilities of Multimodal Large Language Models (MLLMs). Unlike previous benchmarks that focus on offline video comprehension, StreamingBench assesses MLLMs' ability to understand video content in real-time, as it unfolds.",
            "paper-title": "StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video Understanding",
            "image-path": "flux_paper_image/2411.03628_1731010265.png"
        },

        {
            "startTime": "06:10",
            "arxivId": "2411.03665",
            "arxivLink": "https://arxiv.org/abs/2411.03665",
            "title": "AI Ethics: Can Robots Pass the Moral Test?",
            "institute": "Peking University, Tsinghua University",
            "text": "This research evaluates the moral beliefs of large language models (LLMs) using a three-module framework that includes moral choice, moral rank, and moral debate. This approach differs from previous work by incorporating a more nuanced and comprehensive assessment of moral beliefs, including the firmness of moral choices and the impact of cultural context.",
            "paper-title": "Evaluating Moral Beliefs across LLMs through a Pluralistic Framework",
            "image-path": "flux_paper_image/2411.03665_1731010334.png"
        },

        {
            "startTime": "06:36",
            "arxivId": "2411.04016",
            "arxivLink": "https://arxiv.org/abs/2411.04016",
            "title": "Zooming In, Zooming Out: How Multi-Scale Modeling Makes Species Maps More Accurate",
            "institute": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, University College London, ETH Z\u00fcrich...",
            "text": "This study explores the impact of different spatial extents (or \"zoom levels\") of environmental data on species distribution models. Unlike previous work that often uses a single scale, this research investigates the benefits of using multiple scales for both single and multiple data types.",
            "paper-title": "Multi-Scale and Multimodal Species Distribution Modeling",
            "image-path": "flux_paper_image/2411.04016_1731011120.png"
        },

        {
            "startTime": "06:58",
            "arxivId": "2411.04034",
            "arxivLink": "https://arxiv.org/abs/2411.04034",
            "title": "Neural Networks Get a Soft Reset: Learning to Adapt to Changing Data",
            "institute": "Google, University College London",
            "text": "This research introduces a novel approach to training neural networks in non-stationary environments by incorporating a learned drift model that implements soft parameter resets. Unlike previous methods that rely on hard resets or fixed heuristics, this approach dynamically adapts to the level of non-stationarity in the data, allowing for more efficient learning.",
            "paper-title": "Non-Stationary Learning of Neural Networks with Automatic Soft Parameter Reset",
            "image-path": "flux_paper_image/2411.04034_1731012282.png"
        },

        {
            "startTime": "07:23",
            "arxivId": "2411.03651",
            "arxivLink": "https://arxiv.org/abs/2411.03651",
            "title": "AI's Got a Vote: How to Make Machines Respect Everyone's Preferences",
            "institute": "University of Toronto, Harvard University",
            "text": "This paper introduces a novel approach to AI value alignment by applying social choice theory to policy aggregation. Unlike previous work that focuses on learning a single \"ground truth\" reward function, this research explores methods that are invariant to affine transformations of rewards, making them more robust to individual biases.",
            "paper-title": "Policy Aggregation",
            "image-path": "flux_paper_image/2411.03651_1731009955.png"
        },

        {
            "startTime": "07:48",
            "arxivId": "2411.03671",
            "arxivLink": "https://arxiv.org/abs/2411.03671",
            "title": "Neural Networks Get a Grip on Contact Problems!",
            "institute": "Queensland University of Technology, Tsinghua University",
            "text": "This research proposes an energy-based physics-informed neural network (PINN) framework for solving frictionless contact problems under large deformation. Unlike previous PINN approaches for contact problems, this framework incorporates a surface contact energy inspired by the Lennard-Jones potential, allowing for more accurate modeling of contact behavior.",
            "paper-title": "Energy-based physics-informed neural network for frictionless contact problems under large deformation",
            "image-path": "flux_paper_image/2411.03671_1731011337.png"
        },

        {
            "startTime": "08:11",
            "arxivId": "2411.03829",
            "arxivLink": "https://arxiv.org/abs/2411.03829",
            "title": "Semantic Segmentation: When Robots Can't Tell a Car From a Cat!",
            "institute": "ShanghaiTech University, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne",
            "text": "This research tackles the challenge of semantic segmentation in open-world scenarios where both novel objects and domain shifts exist. Unlike previous methods that focus on either domain generalization or anomaly detection, this paper proposes a novel approach that jointly handles both types of distribution shifts.",
            "paper-title": "Generalize or Detect? Towards Robust Semantic Segmentation Under Multiple Distribution Shifts",
            "image-path": "flux_paper_image/2411.03829_1731011646.png"
        },

        {
            "startTime": "08:38",
            "arxivId": "2411.03845",
            "arxivLink": "https://arxiv.org/abs/2411.03845",
            "title": "GAE Gets a Makeover: Simple Tricks, Big Performance Boost",
            "institute": "Peking University",
            "text": "This research revisits the Graph Autoencoder (GAE) model for link prediction, demonstrating that with careful hyperparameter tuning and the use of orthogonal embeddings, GAE can achieve performance comparable to more complex models. This approach differs from previous work that focused on enhancing GAE's expressiveness by incorporating pairwise information.",
            "paper-title": "Reconsidering the Performance of GAE in Link Prediction",
            "image-path": "flux_paper_image/2411.03845_1731012026.png"
        },

        {
            "startTime": "09:04",
            "arxivId": "2411.03743",
            "arxivLink": "https://arxiv.org/abs/2411.03743",
            "title": "AI Proteomics Detective: Cracking Biological Mysteries with Language Models",
            "institute": "Tsinghua University, Shanghai Artificial Intelligence Laboratory, National Center for Protein Sciences (Beijing)...",
            "text": "This research introduces PROTEUS, a fully automated system that uses large language models (LLMs) to analyze proteomics data and generate scientific hypotheses. Unlike previous work that focuses on isolated steps within the research process, PROTEUS automates the entire process from raw data to hypothesis generation.",
            "paper-title": "Automating Exploratory Proteomics Research via Language Models",
            "image-path": "flux_paper_image/2411.03743_1731011606.png"
        },

        {
            "startTime": "09:25",
            "arxivId": "2411.03766",
            "arxivLink": "https://arxiv.org/abs/2411.03766",
            "title": "LLMs: Good at Math? Not So Fast! New Benchmark Tests Their Number Sense",
            "institute": "Peking University",
            "text": "This research introduces a new benchmark, the NUPA test, which comprehensively evaluates the numerical understanding and processing abilities (NUPA) of LLMs across four common number representations and 17 distinct tasks. This benchmark goes beyond previous work that primarily focused on integer addition and multiplication.",
            "paper-title": "Number Cookbook: Number Understanding of Language Models and How to Improve It",
            "image-path": "flux_paper_image/2411.03766_1731012159.png"
        },

        {
            "startTime": "09:48",
            "arxivId": "2411.03695",
            "arxivLink": "https://arxiv.org/abs/2411.03695",
            "title": "Surgical Instrument Segmentation: No Labels, No Problem!",
            "institute": "University of Sydney, Harvard University",
            "text": "This research proposes a novel unsupervised surgical instrument segmentation (USIS) method called AMNCutter. Unlike previous USIS methods that rely on pseudo-labels, AMNCutter leverages a graph-cutting loss function and a multi-view normalized cutter module to learn from patch affinities without requiring any manual annotations.",
            "paper-title": "AMNCutter: Affinity-Attention-Guided Multi-View Normalized Cutter for Unsupervised Surgical Instrument Segmentation",
            "image-path": "flux_paper_image/2411.03695_1731010509.png"
        },

        {
            "startTime": "10:13",
            "arxivId": "2411.03562",
            "arxivLink": "https://arxiv.org/abs/2411.03562",
            "title": "Kaggle Grandmaster? Meet Agent K, the Data Science Bot That's Got Skills!",
            "institute": "Huawei, AICentre, UCL...",
            "text": "This research introduces Agent K v1.0, a data science agent that uses a structured reasoning framework to learn from experience and adapt its strategies without traditional fine-tuning or backpropagation. This approach allows the agent to dynamically process memory in a nested structure, effectively learning from accumulated experiences stored to handle complex reasoning tasks.",
            "paper-title": "Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level",
            "image-path": "flux_paper_image/2411.03562_1731010067.png"
        },

        {
            "startTime": "10:41",
            "arxivId": "2411.03524",
            "arxivLink": "https://arxiv.org/abs/2411.03524",
            "title": "Machine Translation's Got a Bias Problem: How to Fix It with Ensembles",
            "institute": "Google",
            "text": "This research investigates the \"metric bias\" problem in Minimum Bayes Risk (MBR) decoding for machine translation. Unlike previous work, which focused on a limited number of metrics and lacked human evaluations, this study analyzes a wide range of metrics across multiple language pairs and includes human evaluations to assess the actual quality of translations.",
            "paper-title": "Mitigating Metric Bias in Minimum Bayes Risk Decoding",
            "image-path": "flux_paper_image/2411.03524_1731010829.png"
        },

        {
            "startTime": "11:07",
            "arxivId": "2411.03456",
            "arxivLink": "https://arxiv.org/abs/2411.03456",
            "title": "Brain Injury Data: A Peek Inside the Tiny Minds of Babies with HIE",
            "institute": "Boston Children\u2019s Hospital, Harvard Medical School, Massachusetts General Hospital",
            "text": "This research expands on a previous dataset by adding 2-year neurocognitive outcomes, providing a more comprehensive picture of HIE's long-term impact.",
            "paper-title": "BOston Neonatal Brain Injury Data for Hypoxic Ischemic Encephalopathy (BONBID-HIE): II. 2-year Neurocognitive Outcome and NICU Outcome",
            "image-path": "flux_paper_image/2411.03456_1731011289.png"
        },

        {
            "startTime": "11:33",
            "arxivId": "2411.03730",
            "arxivLink": "https://arxiv.org/abs/2411.03730",
            "title": "Invoice Processing Gets a Privacy Makeover: Federated Learning Meets Differential Privacy",
            "institute": "University of Helsinki, Computer Vision Center Universitat Aut\u00f2noma de Barcelona, CISPA Helmholtz Center for Information Security...",
            "text": "This research focuses on training a document visual question answering (DocVQA) model in a federated learning setting while ensuring privacy using differential privacy. This is novel because it combines these three areas, which are typically studied separately, to address the challenge of processing sensitive invoice data.",
            "paper-title": "NeurIPS 2023 Competition: Privacy Preserving Federated Learning Document VQA",
            "image-path": "flux_paper_image/2411.03730_1731011012.png"
        },

        {
            "startTime": "11:55",
            "arxivId": "2411.03570",
            "arxivLink": "https://arxiv.org/abs/2411.03570",
            "title": "Learning Circuits in a Nasty Noise World: Outlier Removal to the Rescue!",
            "institute": "University of Texas at Austin",
            "text": "This paper extends the Linial, Mansour, and Nisan algorithm for learning constant-depth circuits to the setting of malicious noise, where both inputs and labels can be corrupted. Previous work only addressed agnostic noise, where only labels were corrupted.",
            "paper-title": "Learning Constant-Depth Circuits in Malicious Noise Models",
            "image-path": "flux_paper_image/2411.03570_1731012012.png"
        },

        {
            "startTime": "12:20",
            "arxivId": "2411.03805",
            "arxivLink": "https://arxiv.org/abs/2411.03805",
            "title": "LLaMA 3: The Lung Cancer Discharge Summary Whisperer",
            "institute": "Yale University",
            "text": "This research compares the performance of several large language models (LLMs) in generating discharge summaries for lung cancer patients, focusing on the LLaMA 3 model's ability to handle varying lengths of clinical notes.",
            "paper-title": "A Comparative Study of Recent Large Language Models on Generating Hospital Discharge Summaries for Lung Cancer Patients",
            "image-path": "flux_paper_image/2411.03805_1731011991.png"
        },

        {
            "startTime": "12:44",
            "arxivId": "2411.04056",
            "arxivLink": "https://arxiv.org/abs/2411.04056",
            "title": "Robot Learning: A New Trick for Out-of-Distribution Tasks",
            "institute": "ETH Zurich",
            "text": "This research introduces problem space transformations to improve the generalization capabilities of behavioral cloning (BC) policies in robotic manipulation. Unlike previous work that focuses on increasing data coverage, this paper leverages practical assumptions about manipulation tasks to enable better performance on unseen scenarios.",
            "paper-title": "Problem Space Transformations for Generalisation in Behavioural Cloning",
            "image-path": "flux_paper_image/2411.04056_1731012103.png"
        },

        {
            "startTime": "13:07",
            "arxivId": "2411.03746",
            "arxivLink": "https://arxiv.org/abs/2411.03746",
            "title": "Stop Those Sneaky Gradients! New Defense Makes Federated Learning Smarter",
            "institute": "Peking University, Columbia University, NYU",
            "text": "This research proposes a new defense mechanism for federated learning that optimizes the trade-off between data leakage and model performance. Unlike previous methods that apply a universal defense strategy to all parameters, this approach customizes defenses for each parameter based on its vulnerability and impact on model utility.",
            "paper-title": "Optimal Defenses Against Gradient Reconstruction Attacks",
            "image-path": "flux_paper_image/2411.03746_1731010653.png"
        },

        {
            "startTime": "13:29",
            "arxivId": "2411.04036",
            "arxivLink": "https://arxiv.org/abs/2411.04036",
            "title": "Training AI on Tiny Brains: Forward Gradients Make It Possible!",
            "institute": "Qualcomm, University of Toronto",
            "text": "This research explores the feasibility of training deep learning models on resource-constrained edge devices using fixed-point forward gradients, a technique that bypasses backpropagation and significantly reduces memory requirements. Unlike previous work that focused on backpropagation-based methods or floating-point forward gradients, this study investigates the effectiveness of quantized forward gradients in the fixed-point space, which is more suitable for low-power hardware.",
            "paper-title": "Stepping Forward on the Last Mile",
            "image-path": "flux_paper_image/2411.04036_1731012558.png"
        },

        {
            "startTime": "13:49",
            "arxivId": "2411.03641",
            "arxivLink": "https://arxiv.org/abs/2411.03641",
            "title": "Constrained Multi-Objective Optimization: A Balancing Act for Finding the Best, Feasible Solutions!",
            "institute": "Chinese University of Hong Kong University of Chicago University at Albany",
            "text": "This research introduces a novel constrained multi-objective Bayesian optimization algorithm (CMOBO) that balances learning the feasible region with optimizing multiple objectives. Unlike previous work that relies on heuristics and approximations, CMOBO provides theoretical guarantees on sample efficiency and the ability to declare infeasibility.",
            "paper-title": "Constrained Multi-objective Bayesian Optimization through Optimistic Constraints Estimation",
            "image-path": "flux_paper_image/2411.03641_1731012221.png"
        },

        {
            "startTime": "14:16",
            "arxivId": "2411.03321",
            "arxivLink": "https://arxiv.org/abs/2411.03321",
            "title": "Can AI Predict the Future? LLMs Take on the 2024 Election!",
            "institute": "University of Southern California, CMU",
            "text": "This research introduces a multi-step reasoning framework for election prediction using LLMs. Unlike previous work that primarily focused on analyzing political text, this study incorporates individual voter behavior modeling, leveraging both real-world and synthetic datasets.",
            "paper-title": "Will Trump Win in 2024? Predicting the US Presidential Election via Multi-step Reasoning with Large Language Models",
            "image-path": "flux_paper_image/2411.03321_1731012515.png"
        },

        {
            "startTime": "14:38",
            "arxivId": "2411.03990",
            "arxivLink": "https://arxiv.org/abs/2411.03990",
            "title": "Robot Choreographers: New AI Makes Robots Learn Tricks with Fewer Demonstrations!",
            "institute": "National University of Singapore, Peking University",
            "text": "This research proposes a new trajectory-level SE(3) equivariant diffusion model for robotic manipulation tasks. Unlike previous work, it only requires one equivariant denoising step during the diffusion process, making it more efficient and less computationally demanding.",
            "paper-title": "ET-SEED: Efficient Trajectory-Level SE(3) Equivariant Diffusion Policy",
            "image-path": "flux_paper_image/2411.03990_1731012234.png"
        },

        {
            "startTime": "15:08",
            "arxivId": "2411.03397",
            "arxivLink": "https://arxiv.org/abs/2411.03397",
            "title": "LLMs Get Chatty: New Platform Lets AI Agents Debate Like Humans (Asynchronously!)",
            "institute": "The Hebrew University of Jerusalem, University of Melbourne, Google Research",
            "text": "This research introduces SAUCE, a platform that allows researchers to simulate multi-agent LLM interactions in both synchronous and asynchronous settings. This differs from previous work that primarily focused on binary interactions between a single human and a single LLM.",
            "paper-title": "SAUCE: Synchronous and Asynchronous User-Customizable Environment for Multi-Agent LLM Interaction",
            "image-path": "flux_paper_image/2411.03397_1731012432.png"
        },

        {
            "startTime": "15:30",
            "arxivId": "2411.03522",
            "arxivLink": "https://arxiv.org/abs/2411.03522",
            "title": "AI's Got Your Back: Decoding LncRNA Regulation with Language Models",
            "institute": "Meta, North Carolina State University",
            "text": "This research explores the use of large language models (LLMs) for analyzing the transcriptional regulation of long non-coding RNAs (lncRNAs). Unlike previous studies that focused on protein-coding genes, this paper investigates the application of LLMs specifically for lncRNA analysis.",
            "paper-title": "Exploring the Potentials and Challenges of Using Large Language Models for the Analysis of Transcriptional Regulation of Long Non-coding RNAs",
            "image-path": "flux_paper_image/2411.03522_1731010646.png"
        },

        {
            "startTime": "15:56",
            "arxivId": "2411.03753",
            "arxivLink": "https://arxiv.org/abs/2411.03753",
            "title": "Symbolic Regression Gets a Makeover: MDLformer to the Rescue!",
            "institute": "Tsinghua University",
            "text": "This research proposes a novel search objective based on the minimum description length (MDL) for symbolic regression, which differs from previous methods that optimize prediction error. The MDL objective reflects the distance from the target formula and decreases monotonically as the search approaches the correct form.",
            "paper-title": "Symbolic regression via MDLformer-guided search: from minimizing prediction error to minimizing description length",
            "image-path": "flux_paper_image/2411.03753_1731010973.png"
        },

        {
            "startTime": "16:19",
            "arxivId": "2411.03497",
            "arxivLink": "https://arxiv.org/abs/2411.03497",
            "title": "AI Doctors Get a Reality Check: New Study Quantifies Uncertainty in Medical Predictions",
            "institute": "Brandeis University, Google",
            "text": "This research focuses on quantifying and reducing uncertainty in clinical predictions made by language models (LMs) using electronic health records (EHRs). Unlike previous work, it explores both white-box (where model parameters are accessible) and black-box (where they are not) settings, adapting methods for both.",
            "paper-title": "Uncertainty Quantification for Clinical Outcome Predictions with (Large) Language Models",
            "image-path": "flux_paper_image/2411.03497_1731010321.png"
        },

        {
            "startTime": "16:43",
            "arxivId": "2411.03637",
            "arxivLink": "https://arxiv.org/abs/2411.03637",
            "title": "Gaussian Splatting Gets a Matching Makeover: Few-Shot View Synthesis with a 3D Structure Boost!",
            "institute": "Peking University, Peking University Shenzhen Graduate School",
            "text": "This paper introduces SCGaussian, a novel approach to few-shot novel view synthesis that leverages matching priors to learn a 3D consistent scene structure. Unlike previous methods that rely on dense inputs or monocular depth priors, SCGaussian utilizes a hybrid Gaussian representation, including ray-based Gaussian primitives, to explicitly optimize the position and shape of Gaussian primitives, leading to more accurate and efficient view synthesis.",
            "paper-title": "Structure Consistent Gaussian Splatting with Matching Prior for Few-shot Novel View Synthesis",
            "image-path": "flux_paper_image/2411.03637_1731012135.png"
        },

        {
            "startTime": "17:08",
            "arxivId": "2411.03551",
            "arxivLink": "https://arxiv.org/abs/2411.03551",
            "title": "Fibrosis Detection Gets a Makeover: AI Learns to \"Inject\" Lung Scars for Better Segmentation!",
            "institute": "Imperial College London, University College London, National Heart and Lung Institute",
            "text": "This research introduces a novel weakly supervised semantic segmentation (WSSS) method called DiffSeg for fibrosis detection in HRCT images. Unlike previous WSSS methods that rely on class activation maps, DiffSeg utilizes a controllable latent space within a generative model to synthesize fibrosis-injected HRCT images from healthy slices, enabling precise localization of fibrosis patterns.",
            "paper-title": "Enhancing Weakly Supervised Semantic Segmentation for Fibrosis via Controllable Image Generation",
            "image-path": "flux_paper_image/2411.03551_1731010715.png"
        },

        {
            "startTime": "17:35",
            "arxivId": "2411.03395",
            "arxivLink": "https://arxiv.org/abs/2411.03395",
            "title": "AI Oncologist: Can a Chatbot Treat Breast Cancer?",
            "institute": "Google",
            "text": "This research explores the performance of a conversational AI system, AMIE, in the subspecialty domain of breast oncology without specific fine-tuning for this domain. The study uses a curated dataset of 50 synthetic breast cancer scenarios and a detailed evaluation rubric to assess the AI's ability to generate management plans.",
            "paper-title": "Exploring Large Language Models for Specialist-level Oncology Care",
            "image-path": "flux_paper_image/2411.03395_1731010407.png"
        },

        {
            "startTime": "17:51",
            "arxivId": "2411.03688",
            "arxivLink": "https://arxiv.org/abs/2411.03688",
            "title": "Implicit Neural Representations: A Deep Dive into the Sine of the Times!",
            "institute": "University of Cambridge, ENS Paris-Saclay, The Hong Kong University of Science and Technology...",
            "text": "This research provides a comprehensive survey of state-of-the-art Implicit Neural Representation (INR) methods, including a taxonomy that categorizes them into four key areas: activation functions, positional encoding, combined strategies, and network structure optimization. It also includes a rigorous analysis of their critical properties and an experimental comparison across various tasks.",
            "paper-title": "Where Do We Stand with Implicit Neural Representations? A Technical and Performance Survey",
            "image-path": "flux_paper_image/2411.03688_1731010025.png"
        },

        {
            "startTime": "18:25",
            "arxivId": "2411.03900",
            "arxivLink": "https://arxiv.org/abs/2411.03900",
            "title": "Retentive Networks: Quantum Chemistry's New Memory Champ!",
            "institute": "University of Michigan",
            "text": "This research introduces a new type of neural network called a Retentive Network (RetNet) as an ansatz for solving electronic ground state problems in quantum chemistry. RetNets are designed to be more efficient than transformers, especially during inference, by processing data in parallel during training and recurrently during inference.",
            "paper-title": "Retentive Neural Quantum States: Efficient Ans\\\"atze for Ab Initio Quantum Chemistry",
            "image-path": "flux_paper_image/2411.03900_1731010601.png"
        },

        {
            "startTime": "18:54",
            "arxivId": "2411.03884",
            "arxivLink": "https://arxiv.org/abs/2411.03884",
            "title": "Transformers Get a Polynomial Makeover: New Activation Function Boosts LLMs!",
            "institute": "Peking University",
            "text": "This research introduces a novel category of activation functions called Polynomial Composition Activations (PolyCom) for transformer models. Unlike traditional activation functions, PolyCom utilizes polynomial compositions to capture higher-order interactions within data, potentially enhancing the expressivity and performance of LLMs.",
            "paper-title": "Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models",
            "image-path": "flux_paper_image/2411.03884_1731011408.png"
        },

        {
            "startTime": "19:14",
            "arxivId": "2411.03349",
            "arxivLink": "https://arxiv.org/abs/2411.03349",
            "title": "LLMs Get Brainy: Logic Rules Boost AI Reasoning Power!",
            "institute": "Eindhoven University of Technology, Peking University, Microsoft...",
            "text": "This research proposes a novel framework called RuAG that automatically distills large volumes of offline data into interpretable first-order logic rules, which are then injected into LLMs to enhance their reasoning capabilities. This approach differs from previous work by using LLMs to define the search process for logic rules and then applying Monte Carlo Tree Search (MCTS) to efficiently discover these rules.",
            "paper-title": "RuAG: Learned-rule-augmented Generation for Large Language Models",
            "image-path": "flux_paper_image/2411.03349_1731011997.png"
        },

        {
            "startTime": "19:45",
            "arxivId": "2411.04109",
            "arxivLink": "https://arxiv.org/abs/2411.04109",
            "title": "LLMs Learn to Think Like Humans: Self-Consistency Makes the Difference!",
            "institute": "Meta, UNC Chapel Hill, New York University",
            "text": "This research introduces Self-Consistency Preference Optimization (SCPO), a novel self-training method for LLMs that leverages the concept of self-consistency during training, unlike previous approaches that apply it only at inference time.",
            "paper-title": "Self-Consistency Preference Optimization",
            "image-path": "flux_paper_image/2411.04109_1731012571.png"
        },

        {
            "startTime": "20:10",
            "arxivId": "2411.03865",
            "arxivLink": "https://arxiv.org/abs/2411.03865",
            "title": "AdaSociety: Where Agents Learn to Play the Social Game!",
            "institute": "Peking University, Beijing Academy of Artificial Intelligence",
            "text": "This research introduces AdaSociety, a multi-agent environment that features both expanding physical surroundings and adaptive social connections. Unlike previous work, AdaSociety allows for the dynamic generation of tasks based on agents' actions, making it a more realistic and challenging environment for studying multi-agent decision-making.",
            "paper-title": "AdaSociety: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making",
            "image-path": "flux_paper_image/2411.03865_1731011873.png"
        },

        {
            "startTime": "20:41",
            "arxivId": "2411.03706",
            "arxivLink": "https://arxiv.org/abs/2411.03706",
            "title": "3D Scene Sleuth: How a New Method Spots Object Changes in a Flash!",
            "institute": "MIT",
            "text": "This research introduces a novel method for detecting 3D object rearrangements in scenes using 3D Gaussian Splatting (3DGS). Unlike previous methods that rely on depth sensors or object models, this approach leverages the real-time rendering capabilities of 3DGS and EfficientSAM's zero-shot segmentation to identify changes from sparse post-change images.",
            "paper-title": "3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical Object Rearrangement",
            "image-path": "flux_paper_image/2411.03706_1731010629.png"
        },

        {
            "startTime": "21:02",
            "arxivId": "2411.03405",
            "arxivLink": "https://arxiv.org/abs/2411.03405",
            "title": "3D Visual Grounding: When Objects Learn to Point at Themselves!",
            "institute": "ETH Zurich",
            "text": "This research introduces two novel losses for 3D visual grounding: an offset loss that helps objects learn their spatial relationships and a span loss that focuses on the specific words in a description that refer to the target object. This is different from previous work that primarily relied on a basic cross-entropy loss, which didn't consider these finer details.",
            "paper-title": "Fine-Grained Spatial and Verbal Losses for 3D Visual Grounding",
            "image-path": "flux_paper_image/2411.03405_1731010515.png"
        },

        {
            "startTime": "21:36",
            "arxivId": "2411.03351",
            "arxivLink": "https://arxiv.org/abs/2411.03351",
            "title": "Tabular Data's Secret Sauce: How to Share Data Without Spilling the Beans!",
            "institute": "Commonwealth Scientific and Industrial Research Organisation, Nanyang Technological University, Xidian University...",
            "text": "This research focuses on tabular data synthesis with differential privacy, a unique aspect compared to previous surveys that primarily focused on pure synthetic data generation methods or only statistical approaches.",
            "paper-title": "Tabular Data Synthesis with Differential Privacy: A Survey",
            "image-path": "flux_paper_image/2411.03351_1731012420.png"
        },

        {
            "startTime": "21:56",
            "arxivId": "2411.03786",
            "arxivLink": "https://arxiv.org/abs/2411.03786",
            "title": "N-Grammys: Speeding Up Language Models with Simple Tricks",
            "institute": "ENS Paris, AWS AI Labs, Meta AI",
            "text": "This paper explores the effectiveness of using simple, learning-free N-gram strategies for speculative decoding in language models. Unlike previous work that relies on training separate draft models, this approach leverages information directly from the base model and the context.",
            "paper-title": "The N-Grammys: Accelerating Autoregressive Inference with Learning-Free Batched Speculation",
            "image-path": "flux_paper_image/2411.03786_1731011861.png"
        },

        {
            "startTime": "22:20",
            "arxivId": "2411.03336",
            "arxivLink": "https://arxiv.org/abs/2411.03336",
            "title": "AI Scheming: How to Build a Safety Case Against Sneaky Bots",
            "institute": "Apollo Research, UK AI Safety Institute, METR...",
            "text": "This paper proposes a framework for constructing safety cases against AI systems that might be scheming, or covertly pursuing goals misaligned with their developers. It focuses on using evaluations to assess the AI's capabilities for scheming and causing harm, and proposes a control evaluation process to ensure that even if the AI system attempts to scheme, it won't be able to cause unacceptable outcomes.",
            "paper-title": "Towards evaluations-based safety cases for AI scheming",
            "image-path": "flux_paper_image/2411.03336_1731012082.png"
        },

        {
            "startTime": "22:52",
            "arxivId": "2411.03978",
            "arxivLink": "https://arxiv.org/abs/2411.03978",
            "title": "Clustering with a Twist: How GPT-4 Makes Your Data Dance to Your Tune!",
            "institute": "University of Washington",
            "text": "This research introduces Multi-Sub, a multiple clustering method that uses a multi-modal subspace proxy learning framework. Unlike previous methods, Multi-Sub leverages CLIP and GPT-4 to align textual prompts expressing user preferences with corresponding visual representations. This allows for customized clustering based on user-specific interests.",
            "paper-title": "Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning",
            "image-path": "flux_paper_image/2411.03978_1731012129.png"
        },

        {
            "startTime": "23:22",
            "arxivId": "2411.03445",
            "arxivLink": "https://arxiv.org/abs/2411.03445",
            "title": "Trojan Horse Hunters: Linear Weights Unmask AI Backdoors!",
            "institute": "Lynx Capital Partners, Peraton Labs, Google",
            "text": "This research proposes a novel Trojan detection method that utilizes linear weight classification. Unlike previous methods that rely on trigger inversion or functional analysis, this approach focuses on identifying patterns in the model's weights themselves.",
            "paper-title": "Solving Trojan Detection Competitions with Linear Weight Classification",
            "image-path": "flux_paper_image/2411.03445_1731011153.png"
        },

        {
            "startTime": "23:46",
            "arxivId": "2411.04118",
            "arxivLink": "https://arxiv.org/abs/2411.04118",
            "title": "Medical AI: Is Pretraining Hype or Just a Big Deal?",
            "institute": "CMU, Johns Hopkins University",
            "text": "This research directly compares the performance of medical LLMs and VLMs to their general-domain counterparts, using a rigorous experimental setup that accounts for model sensitivity to prompting details and statistical uncertainty. This approach differs from previous work, which often relied on less controlled comparisons.",
            "paper-title": "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?",
            "image-path": "flux_paper_image/2411.04118_1731010315.png"
        },

        {
            "startTime": "24:13",
            "arxivId": "2411.03615",
            "arxivLink": "https://arxiv.org/abs/2411.03615",
            "title": "Single Image, No Drama: New Algorithm Fixes Infrared Camera Woes",
            "institute": "\u00c9cole Normale Sup\u00e9rieure, University of California Los Angeles",
            "text": "This research proposes a single-image, locally adaptive algorithm for non-uniformity correction and denoising in uncooled infrared cameras. Unlike previous methods that rely on multiple images or calibration patterns, this approach works on a single image, eliminating the need for motion compensation or temporal accumulation.",
            "paper-title": "ADMIRE: a locally adaptive single-image, non-uniformity correction and denoising algorithm: application to uncooled IR camera",
            "image-path": "flux_paper_image/2411.03615_1731010813.png"
        },

        {
            "startTime": "24:32",
            "arxivId": "2411.03982",
            "arxivLink": "https://arxiv.org/abs/2411.03982",
            "title": "Image Editing Gets a Visual Makeover: Say Goodbye to Text Prompts!",
            "institute": "Indian Institute of Technology Roorkee, Adobe, Microsoft Research...",
            "text": "This research proposes a new method for exemplar-based image editing that doesn't rely on text prompts. Instead, it uses a combination of image and text embeddings to capture the desired edit from an example pair. This approach is faster and more efficient than previous methods that relied on text-based instructions.",
            "paper-title": "ReEdit: Multimodal Exemplar-Based Image Editing with Diffusion Models",
            "image-path": "flux_paper_image/2411.03982_1731010236.png"
        },

        {
            "startTime": "24:56",
            "arxivId": "2411.03814",
            "arxivLink": "https://arxiv.org/abs/2411.03814",
            "title": "AI Jailbreak: How to Trick Chatbots into Saying Bad Things (and Why It Matters)",
            "institute": "Alibaba, Beijing Institute of Technology, Tsinghua University",
            "text": "This research focuses on jailbreaking LLMs in multi-round dialogues, unlike previous work that primarily focused on single-round attacks. The paper proposes a novel multi-round dialogue jailbreaking agent, MRJ-Agent, which utilizes a risk decomposition strategy and psychological tactics to enhance attack effectiveness.",
            "paper-title": "MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue",
            "image-path": "flux_paper_image/2411.03814_1731011925.png"
        },

        {
            "startTime": "25:23",
            "arxivId": "2411.03537",
            "arxivLink": "https://arxiv.org/abs/2411.03537",
            "title": "Molecule Whisperer: New AI Learns to Predict Properties with Less Data",
            "institute": "Max Planck Society, MIT",
            "text": "This research introduces a two-stage pretraining framework for molecular property prediction models. The novelty lies in the use of dynamic denoising with larger noise scales, facilitated by a branching encoder architecture, to improve the model's ability to generalize to diverse downstream datasets. This approach aims to address the challenge of limited labeled data in real-world scenarios.",
            "paper-title": "Two-Stage Pretraining for Molecular Property Prediction in the Wild",
            "image-path": "flux_paper_image/2411.03537_1731010756.png"
        },

        {
            "startTime": "25:49",
            "arxivId": "2411.03885",
            "arxivLink": "https://arxiv.org/abs/2411.03885",
            "title": "AI for Disability Justice: Data's Got a New Look!",
            "institute": "University of Sheffield, Johns Hopkins University, University of Virginia",
            "text": "This research goes beyond simply pointing out the problems with AI and disability data. It proposes a new framework for building more inclusive systems, focusing on disability-led data and AI justice.",
            "paper-title": "Disability data futures: Achievable imaginaries for AI and disability data justice",
            "image-path": "flux_paper_image/2411.03885_1731011600.png"
        },

        {
            "startTime": "26:12",
            "arxivId": "2411.03945",
            "arxivLink": "https://arxiv.org/abs/2411.03945",
            "title": "Can AI Learn Without Updating Its Brain? A Peek Inside the Mind of a Custom Model",
            "institute": "UC Berkeley",
            "text": "This research explores the impact of architectural differences between popular language models like GPT-2 and Llama on their ability to learn in-context, a phenomenon where models learn from examples without updating their parameters. The study goes beyond previous work by examining hybrid architectures, combining components from different models to understand how these changes affect in-context learning performance.",
            "paper-title": "Can Custom Models Learn In-Context? An Exploration of Hybrid Architecture Performance on In-Context Learning Tasks",
            "image-path": "flux_paper_image/2411.03945_1731011092.png"
        },

        {
            "startTime": "26:45",
            "arxivId": "2411.04097",
            "arxivLink": "https://arxiv.org/abs/2411.04097",
            "title": "Vision Models: Seeing the Forest, Missing the Trees? RAVL to the Rescue!",
            "institute": "Stanford University",
            "text": "This research focuses on discovering and mitigating spurious correlations in fine-tuned vision-language models (VLMs) by analyzing local image features rather than relying on global image-level approaches.",
            "paper-title": "RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models",
            "image-path": "flux_paper_image/2411.04097_1731012438.png"
        },

        {
            "startTime": "27:08",
            "arxivId": "2411.03375",
            "arxivLink": "https://arxiv.org/abs/2411.03375",
            "title": "Kernel Computing Goes Analog: A Chip That Thinks Like a Brain",
            "institute": "IBM",
            "text": "This research proposes a new approach to kernel approximation in machine learning algorithms, specifically designed for analog in-memory computing (AIMC) architectures. Unlike previous methods that rely on digital computations, this approach leverages the inherent parallelism of AIMC to perform most operations directly in memory, leading to significant speedups and energy efficiency gains.",
            "paper-title": "Kernel Approximation using Analog In-Memory Computing",
            "image-path": "flux_paper_image/2411.03375_1731012563.png"
        },

        {
            "startTime": "27:29",
            "arxivId": "2411.03759",
            "arxivLink": "https://arxiv.org/abs/2411.03759",
            "title": "Quantum Entanglement: A New Way to Crack the Code of Complex Systems",
            "institute": "Inria, \u00c9cole Normale Sup\u00e9rieure, PSL Research University",
            "text": "This research introduces a novel approach to variational inference, utilizing quantum relaxations of the Kullback-Leibler divergence to derive upper bounds on the log-partition function of pairwise Markov random fields on the Boolean hypercube. This method differs from previous work by employing quantum entropy, a concept rooted in quantum information theory, to approximate the KL divergence.",
            "paper-title": "Variational Inference on the Boolean Hypercube with the Quantum Entropy",
            "image-path": "flux_paper_image/2411.03759_1731010500.png"
        },

        {
            "startTime": "27:46",
            "arxivId": "2411.03840",
            "arxivLink": "https://arxiv.org/abs/2411.03840",
            "title": "Brain's Got Gating: How Fast-Learning Neurons Master Multitasking",
            "institute": "MIT",
            "text": "This research explores a novel linear gated network model where task abstractions emerge dynamically. Unlike previous work that relies on pre-defined task identifiers or struggles with temporally correlated data, this model learns to segment tasks from a stream of data by jointly optimizing weights and gates with neuron-like constraints.",
            "paper-title": "Flexible task abstractions emerge in linear networks with fast and bounded units",
            "image-path": "flux_paper_image/2411.03840_1731012070.png"
        },

        {
            "startTime": "28:09",
            "arxivId": "2411.03596",
            "arxivLink": "https://arxiv.org/abs/2411.03596",
            "title": "TGNs Get a Makeover: Source-Target ID Makes Temporal Graphs More Expressive!",
            "institute": "University of Oxford",
            "text": "This paper identifies a limitation in Temporal Graph Networks (TGNs) for dynamic node affinity prediction. It shows that TGNs cannot represent simple heuristics like moving averages or persistent forecasting. To address this, the authors propose TGNv2, which includes source-target identification in message construction, making it more expressive.",
            "paper-title": "Enhancing the Expressivity of Temporal Graph Networks through Source-Target Identification",
            "image-path": "flux_paper_image/2411.03596_1731010669.png"
        },

        {
            "startTime": "28:32",
            "arxivId": "2411.03494",
            "arxivLink": "https://arxiv.org/abs/2411.03494",
            "title": "Frozen Lake to Real World: Robot Navigation Without Fancy Sensors!",
            "institute": "University of Washington",
            "text": "This research presents a Sim2Real approach for robot navigation that doesn't rely on expensive sensors or complex setups. It uses the simple, accessible Frozen Lake environment to train a robot to navigate a grid-like environment.",
            "paper-title": "An Open-source Sim2Real Approach for Sensor-independent Robot Navigation in a Grid",
            "image-path": "flux_paper_image/2411.03494_1731009998.png"
        },

        {
            "startTime": "28:55",
            "arxivId": "2411.04105",
            "arxivLink": "https://arxiv.org/abs/2411.04105",
            "title": "Transformers: Not Just Chatting, They're Thinking!",
            "institute": "Purdue University, Google",
            "text": "This research goes beyond simply observing that large language models (LLMs) can reason. It delves into the specific mechanisms within the transformer architecture that enable this reasoning ability, focusing on a synthetic propositional logic problem. This approach differs from previous work that primarily focused on evaluating LLMs' reasoning capabilities on complex tasks.",
            "paper-title": "How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis",
            "image-path": "flux_paper_image/2411.04105_1731011652.png"
        },

        {
            "startTime": "29:15",
            "arxivId": "2411.03493",
            "arxivLink": "https://arxiv.org/abs/2411.03493",
            "title": "Attention, Please! New Trick Makes Transformers Learn Faster",
            "institute": "University of Texas at Austin, Google",
            "text": "This paper proposes a new attention mechanism called LASER, which uses an exponential transformation of the input values to address the vanishing gradient problem in standard attention. This approach is different from previous work that focused on reducing the computational complexity of attention.",
            "paper-title": "LASER: Attention with Exponential Transformation",
            "image-path": "flux_paper_image/2411.03493_1731011896.png"
        },

        {
            "startTime": "29:49",
            "arxivId": "2411.03810",
            "arxivLink": "https://arxiv.org/abs/2411.03810",
            "title": "Transfer Learning: Can We Cheat Our Way to Better AI?",
            "institute": "Peking University, Caltech",
            "text": "This paper explores a new setting called Hybrid Transfer RL (HTRL) where an agent learns in a target environment while also having access to data from a source environment with different dynamics. The key difference from previous work is that it focuses on the sample complexity of this transfer setting, proving that while general HTRL doesn't offer sample efficiency gains, a specific type of transfer with \"separable shifts\" can lead to significant improvements.",
            "paper-title": "Hybrid Transfer Reinforcement Learning: Provable Sample Efficiency from Shifted-Dynamics Data",
            "image-path": "flux_paper_image/2411.03810_1731010220.png"
        },

        {
            "startTime": "30:12",
            "arxivId": "2411.03334",
            "arxivLink": "https://arxiv.org/abs/2411.03334",
            "title": "Lens-ing the Universe: AI Gets a Grip on Gravitational Lensing with a Dose of Domain Adaptation",
            "institute": "University of Chicago, Fermilab",
            "text": "This research combines mean-variance estimation (MVE) with unsupervised domain adaptation (UDA) to improve the accuracy of neural network predictions for strong lensing systems. This approach addresses the challenge of training models on simulated data and applying them to real observational data, which often have different noise characteristics.",
            "paper-title": "Neural Network Prediction of Strong Lensing Systems with Domain Adaptation and Uncertainty Quantification",
            "image-path": "flux_paper_image/2411.03334_1731011332.png"
        }
    ],
    "stats": {
        "num_pick": 71,
        "num_total": 235,
    },
    "audio": "https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202411071409_audio.mp3"
}
