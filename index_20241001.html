
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY</div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">Fresh Picks: 
                    <span class="highlightNumber" style="font-size: 28px;">116</span> out of <span
                    class="highlightNumber">542</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-10-01"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>


        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">01:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20537" target="_blank">@arXiv 2409.20537</a>
                    <span class="tweet-title">Robots Learn a New Language:  Pre-trained Transformers Speak Embodiment!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Meta</span>
                </div>
                <div class="primary-text">
                    This research introduces Heterogeneous Pre-trained Transformers (HPT), a novel approach to pre-training robotic policies that leverages data from diverse embodiments, including real robots, simulations, and human videos. Unlike previous methods that often focus on a single embodiment or modality, HPT aligns proprioception and vision inputs from different robots into a shared representation space, enabling generalization across various tasks and environments.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">01:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19817" target="_blank">@arXiv 2409.19817</a>
                    <span class="tweet-title">LLMs Get a Temperature Check: New Method Calibrates Confidence Scores</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts a unique temperature scaling parameter for each token prediction based on the language model's hidden features. This differs from previous methods that apply a single temperature parameter across all outputs, which can lead to miscalibration.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">01:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20324" target="_blank">@arXiv 2409.20324</a>
                    <span class="tweet-title">Blind Navigation Gets a Heads-Up: New Dataset Helps Robots See the World Like We Do</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research introduces HEADS-UP, a new dataset specifically designed for trajectory prediction in blind assistance systems. Unlike existing datasets, HEADS-UP captures data from a head-mounted camera, mimicking the perspective of a blind individual.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">02:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19913" target="_blank">@arXiv 2409.19913</a>
                    <span class="tweet-title">LLMs on a Diet: How to Feed Your Giant Language Model the Right Learning Rate</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research focuses on how the optimal learning rate (LR) for training large language models (LLMs) changes depending on the size of the training dataset, a factor not previously explored in depth.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">02:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19157" target="_blank">@arXiv 2409.19157</a>
                    <span class="tweet-title">Forecasting the Future, Even When It's Trying to Trick You!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Cornell University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework for probabilistic forecasting that guarantees accurate uncertainty estimates, even when the data is changing unpredictably or even being manipulated adversarially. This is different from previous work that often relies on assumptions about the data's stability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">02:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20568" target="_blank">@arXiv 2409.20568</a>
                    <span class="tweet-title">Robots Learn to Do Chores, No Humans Needed!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, AI Institute</span>
                </div>
                <div class="primary-text">
                    This research focuses on enabling robots to learn mobile manipulation skills in real-world environments without extensive human supervision or simulation training. It differs from previous work by incorporating task-relevant autonomy, efficient policy learning using behavior priors, and flexible reward specification.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">03:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19381" target="_blank">@arXiv 2409.19381</a>
                    <span class="tweet-title">LLMs: Code or Words? MetaMath Knows Best!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Shanghai Jiao Tong University, Yale University, Google</span>
                </div>
                <div class="primary-text">
                    This research explores the interplay between natural language and code-based reasoning in LLMs for mathematical problem-solving. It introduces MetaMath, a prompting method that dynamically selects the most appropriate reasoning form based on the problem type.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">03:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19420" target="_blank">@arXiv 2409.19420</a>
                    <span class="tweet-title">MRI Meets CT: A Love Story in the Latent Space</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, University of Hong Kong</span>
                </div>
                <div class="primary-text">
                    This research proposes a data-driven multi-modality imaging (DMI) strategy that utilizes a multi-sensor learning (MSL) framework to synergistically reconstruct images from different modalities. Unlike previous work that focuses on post-hoc fusion of independently reconstructed images, this approach leverages a shared latent space to extract and transfer information across modalities, enabling enhanced image reconstruction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">03:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19808" target="_blank">@arXiv 2409.19808</a>
                    <span class="tweet-title">Can Tiny Language Models Learn to Be Creative?  A New Study Shows They Can!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Princeton University</span>
                </div>
                <div class="primary-text">
                    This research explores whether smaller language models can learn to combine different language skills through fine-tuning, even if they haven't seen those combinations during training. This differs from previous work that focused on the relationship between model size and compositional generalization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">04:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19044" target="_blank">@arXiv 2409.19044</a>
                    <span class="tweet-title">Stacking the Middle: A New Trick for Smarter Language Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research explores the inductive bias of stacking, a training strategy for language models, by proposing a novel variant called MIDAS. Unlike previous stacking methods that duplicate the last layers of a smaller model, MIDAS replicates the middle layers, leading to improved reasoning abilities in downstream tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">04:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20325" target="_blank">@arXiv 2409.20325</a>
                    <span class="tweet-title">Deep Learning Optimizers: It's All About the Norm, Dude!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This paper reinterprets popular deep learning optimizers like Adam, Shampoo, and Prodigy as first-order methods, demonstrating that they can be understood as variations of steepest descent under specific norms. This approach differs from previous work that often framed these optimizers within convex or approximate second-order theory.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">04:58</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19567" target="_blank">@arXiv 2409.19567</a>
                    <span class="tweet-title">Zeroth-Order Optimization:  A Gradient-Free Dance with Variance Reduction!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel variance-reduced gradient estimator for distributed zeroth-order optimization, addressing the trade-off between convergence rate and sampling cost per gradient estimation. Unlike previous work, this estimator leverages historical snapshots for variance correction, achieving a scalable sampling number independent of the problem dimension.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">05:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19345" target="_blank">@arXiv 2409.19345</a>
                    <span class="tweet-title">Transformers Overfitting? No Problem! New Study Reveals Benign Overfitting in Vision</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">RIKEN, Harbin Institute of Technology, University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research delves into the generalization capabilities of Vision Transformers (ViTs) when trained to overfit training data. Unlike previous work that often focused on simplified settings or unrealistic loss functions, this study analyzes a two-layer ViT with softmax attention under gradient descent, providing a more realistic and comprehensive analysis.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">05:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20326" target="_blank">@arXiv 2409.20326</a>
                    <span class="tweet-title">Soccer Bots Learn to Play Like a Team, Not Just Kick the Ball!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research introduces MARLadona, a multi-agent reinforcement learning framework that trains soccer agents to exhibit sophisticated team play. Unlike previous work that often focused on individual skills or simplified 2D environments, MARLadona utilizes a 3D physics-based environment and a modified global entity encoder architecture to achieve effective team play in games with up to 11v11 agents.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">06:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19508" target="_blank">@arXiv 2409.19508</a>
                    <span class="tweet-title">LLMs:  Beyond  Code,  Into  the  Wild!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">TU Darmstadt, IBM Research Europe, National Research Council Canada</span>
                </div>
                <div class="primary-text">
                    This research goes beyond analyzing the impact of LLMs within computer science and explores their influence on 22 diverse fields outside of it. It quantifies the degree to which these fields are citing LLMs and analyzes their usage patterns.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">06:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19505" target="_blank">@arXiv 2409.19505</a>
                    <span class="tweet-title">NLP Papers: A Peek Inside the Mind of a Researcher</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM, National Research Council Canada, TU Darmstadt</span>
                </div>
                <div class="primary-text">
                    This research analyzes the contribution statements in NLP papers to understand the evolution of the field. Unlike previous work that focused on metadata or citation analysis, this study delves into the content of the papers themselves.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">07:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20553" target="_blank">@arXiv 2409.20553</a>
                    <span class="tweet-title">Chess Engine Learns to Think Like a Human, Even When You're Not a Grandmaster!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, Harvard University, Cornell University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a unified model for human-AI alignment in chess, unlike previous work that used separate models for different skill levels. The new model uses a skill-aware attention mechanism to dynamically integrate player strengths with encoded chess positions, enabling it to be sensitive to evolving player skill.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">07:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19134" target="_blank">@arXiv 2409.19134</a>
                    <span class="tweet-title">Secret Prompts: How to Chat with AI Without Spilling Your Secrets</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to confidential prompting for large language models (LLMs) using confidential computing. Unlike previous methods that rely on differential privacy or multi-party computation, this approach leverages confidential virtual machines (CVMs) to isolate user prompts while allowing efficient token generation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">07:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19801" target="_blank">@arXiv 2409.19801</a>
                    <span class="tweet-title">Code Review's New BFF: A Reference-Free Metric for Judging Reviews</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a reference-free metric called CRScore to evaluate code review comments. Unlike previous methods that rely on comparing generated reviews to human-written references, CRScore focuses on assessing the quality of reviews based on their comprehensiveness, conciseness, and relevance to the code changes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">08:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19430" target="_blank">@arXiv 2409.19430</a>
                    <span class="tweet-title">LLMs: Interviewing the Bots, Not the People?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research explores the use of large language models (LLMs) as simulated participants in qualitative research, focusing on semi-structured interviews. It goes beyond previous work by examining the perspectives of qualitative researchers themselves, rather than solely evaluating the technical capabilities of LLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">08:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19613" target="_blank">@arXiv 2409.19613</a>
                    <span class="tweet-title">Mamba's Got a New Trick: Hybrid Mamba for Few-Shot Segmentation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanyang Technological University, Peking University, Singapore University of Technology and Design...</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new approach to few-shot segmentation using a hybrid Mamba network. Unlike previous methods that rely on cross-attention, which has quadratic complexity, this approach utilizes Mamba, a model with linear complexity, to capture inter-sequence dependencies between support and query features.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">08:58</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20534" target="_blank">@arXiv 2409.20534</a>
                    <span class="tweet-title">Conformal Calibration:  Making AI Decisions More Robust, One Uncertainty Set at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Caltech</span>
                </div>
                <div class="primary-text">
                    This research proposes an end-to-end framework for training machine learning models that incorporate uncertainty estimates into decision-making under uncertainty. Unlike previous work, this approach uses conformal prediction to guarantee the quality of uncertainty estimates and trains the model with feedback from the downstream decision-making objective.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">09:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20562" target="_blank">@arXiv 2409.20562</a>
                    <span class="tweet-title">Meshing Madness: A Continuous Representation for Learning Manifold Surface Meshes</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">NVIDIA, University of Toronto, Vector Institute...</span>
                </div>
                <div class="primary-text">
                    This paper introduces SpaceMesh, a continuous representation for meshes that guarantees manifold output and supports complex polygonal connectivity. Unlike previous methods that rely on indirect representations or struggle with complex topologies, SpaceMesh directly generates meshes as the output of a neural network.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">09:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20550" target="_blank">@arXiv 2409.20550</a>
                    <span class="tweet-title">LLMs Gone Wild:  Unmasking the Hallucinations in Code Generation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sun Yat-sen University, Nanyang Technological University</span>
                </div>
                <div class="primary-text">
                    This research focuses on hallucinations in code generation within a repository-level context, unlike previous studies that primarily analyzed standalone function generation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">09:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19150" target="_blank">@arXiv 2409.19150</a>
                    <span class="tweet-title">Decision Trees: Not Just for Tables Anymore!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Texas A&M University</span>
                </div>
                <div class="primary-text">
                    This paper explores the use of Auto-regressive Decision Trees (ARDTs) for language modeling, a novel application of this traditional machine learning technique. Unlike previous work that focused on time-series prediction, this research investigates the theoretical and practical capabilities of ARDTs in generating coherent text and performing reasoning tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">10:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20370" target="_blank">@arXiv 2409.20370</a>
                    <span class="tweet-title">RLHF's New BFF:  Mixture of Judges for Multi-Task LLMs</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel framework called Constrained Generative Policy Optimization (CGPO) for fine-tuning large language models (LLMs) in multi-task learning (MTL) settings. Unlike previous approaches that rely on linear combinations of reward models, CGPO utilizes a mixture of judges (MoJs) to identify and mitigate reward hacking behaviors while optimizing for multiple, potentially conflicting objectives.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">10:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20140" target="_blank">@arXiv 2409.20140</a>
                    <span class="tweet-title">Shiny Objects, Relighted: A Neural Network Makes Inverse Rendering Glow!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, University of Tübingen</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel information-sharing network structure to jointly learn the radiance field and physically based factorization of a scene, enabling high-quality relighting of glossy objects. This approach differs from previous methods that primarily focused on geometry reconstruction and lacked the ability to accurately estimate material properties and relighting.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">11:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19791" target="_blank">@arXiv 2409.19791</a>
                    <span class="tweet-title">Gradient Descent Goes Fourth: Adaptive Stepsize Makes Optimization Linear!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Pennsylvania, University of Washington, Georgia Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This research challenges the common belief that gradient descent requires a function to grow quadratically to achieve linear convergence. It shows that adaptive stepsize gradient descent can achieve nearly linear convergence for functions with fourth-order growth. The key idea is the introduction of a "ravine" manifold, which allows for a decomposition of the function into tangent and normal parts, enabling the algorithm to take advantage of the function's growth properties.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">11:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20557" target="_blank">@arXiv 2409.20557</a>
                    <span class="tweet-title">LLMs: The New Recipe for Video-Based Planning?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">FAIR  Meta, UNCChapelHill</span>
                </div>
                <div class="primary-text">
                    This research proposes VidAssist, a framework that uses large language models (LLMs) for goal-oriented planning in instructional videos. Unlike previous work that relies on extensive training data, VidAssist leverages LLMs' inherent knowledge and reasoning capabilities to achieve zero-shot and few-shot learning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">12:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20043" target="_blank">@arXiv 2409.20043</a>
                    <span class="tweet-title">One-Point-One NeRF:  Giving Neural Rendering a Personal Touch!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This paper introduces OPONeRF, a framework that personalizes neural rendering parameters for each sampled point in a scene. This differs from previous methods that use a single set of parameters for the entire scene, making OPONeRF more adaptable to local variations and test-time scene changes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">12:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19838" target="_blank">@arXiv 2409.19838</a>
                    <span class="tweet-title">Pretrained GNNs:  The New Feature-Finding Superheroes for Molecular Dynamics!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Chicago</span>
                </div>
                <div class="primary-text">
                    This research introduces geom2vec, a method that uses pretrained graph neural networks (GNNs) to learn transferable representations of molecular geometries. This differs from previous work by decoupling GNN training from training for downstream tasks, allowing for analysis of larger molecular graphs with limited computational resources.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">12:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18968" target="_blank">@arXiv 2409.18968</a>
                    <span class="tweet-title">AI in Medicine:  A Safety Checkup for the Future of Healthcare</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge, National University of Singapore</span>
                </div>
                <div class="primary-text">
                    This research goes beyond simply highlighting the potential of AI in medicine. It delves into the specific safety challenges that arise when using AI in real-world healthcare settings, particularly focusing on the reliability and alignment of AI systems with human values.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">13:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19829" target="_blank">@arXiv 2409.19829</a>
                    <span class="tweet-title">Robots on a Mission: Decentralized Learning for Unlabeled Motion Planning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research proposes a decentralized approach to unlabeled motion planning using a Graph Neural Network (GNN) trained with imitation and reinforcement learning. Unlike previous centralized methods, this approach allows robots to coordinate their movements with limited communication and information about the environment.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">13:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19583" target="_blank">@arXiv 2409.19583</a>
                    <span class="tweet-title">Brain Tumor Detection:  A Deep Learning Model That's Not Just a Pretty Face!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, Northeastern University, University of Maryland</span>
                </div>
                <div class="primary-text">
                    This research focuses on developing a convolutional neural network (CNN) specifically designed for brain tumor detection in MRI images, rather than relying on pre-trained models from other domains.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">13:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19835" target="_blank">@arXiv 2409.19835</a>
                    <span class="tweet-title">Land Surface Temperature:  Downscaling Done Right (and Open Source!)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanjing University of Science and Technology, University College London, Nankai University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new deep learning architecture called MoCoLSK-Net for land surface temperature (LST) downscaling. Unlike previous methods, MoCoLSK-Net dynamically adjusts its receptive field based on the input data, allowing it to capture more intricate details and improve accuracy. It also incorporates a modality-conditioned weight generation pathway, enabling more effective multi-modal fusion.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">14:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19435" target="_blank">@arXiv 2409.19435</a>
                    <span class="tweet-title">SBI Just Got Easier: A New Python Package Makes Bayesian Inference a Breeze</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, Swiss Data Science Center, Zurich University of Applied Sciences...</span>
                </div>
                <div class="primary-text">
                    This research introduces sbijax, a Python package that implements a wide variety of state-of-the-art methods in neural simulation-based inference (SBI). Unlike existing packages, sbijax offers functionality for recent developments in the field, such as flow matching posterior estimation (FMPE) and contrastive neural ratio estimation (NRE).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">14:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19359" target="_blank">@arXiv 2409.19359</a>
                    <span class="tweet-title">Quantum Learning Goes Undercover:  Encrypted Data, Secret Computations!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Hefei National Laboratory, Shanghai Qi Zhi Institute</span>
                </div>
                <div class="primary-text">
                    This paper proposes a framework for quantum delegated and federated learning that uses quantum homomorphic encryption to protect data privacy. Unlike previous work that relies on blind quantum computing, this approach allows the server to perform computations on encrypted data without extracting any information about the data itself.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">15:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19702" target="_blank">@arXiv 2409.19702</a>
                    <span class="tweet-title">Relighting Fluffy Friends: New Tech Makes 3D Fur Look Fantastic!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanjing University of Science and Technology, Nanjing University, Adobe</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new way to represent 3D objects, called "Relightable Neural Gaussians" (Rng), which uses Gaussian splatting to represent both hard surfaces and fluffy shapes. Unlike previous methods that rely on surface constraints or analytical shading models, Rng avoids these assumptions, enabling more flexible and efficient relighting.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">15:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19949" target="_blank">@arXiv 2409.19949</a>
                    <span class="tweet-title">Diffusion Models Learn From Mistakes:  A New Way to Train AI Planners</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Northwestern Polytechnical University, China Telecom, Tsinghua University...</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for training diffusion models to act as planners. Unlike previous methods that rely on expert demonstrations or reward labels, this approach leverages large amounts of sub-optimal data, which is more readily available in real-world scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">16:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20247" target="_blank">@arXiv 2409.20247</a>
                    <span class="tweet-title">LLMs on the Go:  Sharing the Training Load for Stable AI on Mobile Devices</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanyang Technological University</span>
                </div>
                <div class="primary-text">
                    This research proposes a collaborative training framework for LLMs that leverages the combined computational power of mobile users and edge servers. This approach differs from previous work by incorporating model stability considerations into the optimization objectives, aiming to reduce performance variance across training instances.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">16:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20098" target="_blank">@arXiv 2409.20098</a>
                    <span class="tweet-title">Facial Expressions: Beyond the Basics, AI Learns to Decode the Nuances!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">China University of Geosciences  University of Cambridge  Baidu Inc  La Trobe University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new task called Facial Expression Category Discovery (FECD) for open-world facial expression recognition (O-FER). Unlike previous O-FER methods that only detect new expressions, FECD aims to simultaneously recognize known expressions and discover new ones. The paper addresses the challenges of theoretical bias (introduced by new categories in unlabeled data) and practical bias (arising from fine-grained and imbalanced facial expression data) by proposing a novel adversarial debiasing framework called FER-GCD.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">17:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19993" target="_blank">@arXiv 2409.19993</a>
                    <span class="tweet-title">LLMs:  Backdoor  Threats  and  How  to  Defend  Them</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Davis, Nvidia, University of Southern California...</span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive survey of backdoor threats to LLMs, covering both training-time and inference-time attacks, as well as defense and detection strategies. It also highlights key challenges in addressing these threats, which is a unique aspect compared to previous work.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">17:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19796" target="_blank">@arXiv 2409.19796</a>
                    <span class="tweet-title">EMR Segmentation:  A Black Box Solution for Healthcare's Textual Chaos!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research focuses on the automatic segmentation of Electronic Medical Records (EMRs) using a novel black-box approach. Unlike previous methods that rely on complex rules or pre-trained language models, this study proposes a simpler sentence embedding technique combined with a neural network for accurate sectioning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">17:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20089" target="_blank">@arXiv 2409.20089</a>
                    <span class="tweet-title">LLMs:  Jailbreaking?  Not So Fast!  New Defense Makes 'em Say "No!"</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, Meta</span>
                </div>
                <div class="primary-text">
                    This research identifies a specific feature in LLMs called the "refusal feature" that plays a key role in how models decide to answer harmful requests. The study shows that adversarial attacks often target this feature, effectively "disabling" the model's safety mechanisms.  The authors propose a new training method called ReFAT that specifically addresses this vulnerability by simulating the effect of these attacks during training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">18:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19824" target="_blank">@arXiv 2409.19824</a>
                    <span class="tweet-title">Ranking Ads:  A Domain-Adapted Reward Model for Offline Evaluation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta, Duke University</span>
                </div>
                <div class="primary-text">
                    This research proposes a domain-adapted reward model for offline evaluation of ranking models in large-scale ad recommendation systems. Unlike traditional methods like Inverse Propensity Scoring (IPS), this approach eliminates the need to account for the complexity of the ad system by leveraging an offline A/B testing simulator.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">18:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19811" target="_blank">@arXiv 2409.19811</a>
                    <span class="tweet-title">SfM Gets a Line:  Hybrid Features Make 3D Reconstruction More Robust</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, Microsoft, Lund University</span>
                </div>
                <div class="primary-text">
                    This research introduces an incremental Structure-from-Motion (SfM) system that incorporates lines and vanishing points alongside traditional point features. This approach improves the robustness and accuracy of 3D reconstruction, especially in challenging scenarios with limited texture.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">18:58</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19690" target="_blank">@arXiv 2409.19690</a>
                    <span class="tweet-title">Neural-Polyptych:  Turning Sketches into Masterpieces, One Patch at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to painting recreation by combining interactive sketches with fragments from original paintings. Unlike previous methods that focus on specific attributes or rely on large datasets, this framework leverages a multi-scale GAN architecture and a Correspondence Attention module to generate high-resolution, content-controllable paintings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">19:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19028" target="_blank">@arXiv 2409.19028</a>
                    <span class="tweet-title">LLMs: Quantum Code Whisperers?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of L’Aquila, King’s College London, University College London...</span>
                </div>
                <div class="primary-text">
                    This research explores the use of large language models (LLMs) to generate explanations for quantum algorithms, a novel application of LLMs in the field of quantum computing.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">19:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19790" target="_blank">@arXiv 2409.19790</a>
                    <span class="tweet-title">Riemann Hypothesis: Can AI Crack the Code of Prime Numbers?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel framework for analyzing the Riemann Hypothesis using probabilistic modeling with cross-entropy optimization and reasoning. This approach differs from previous work by incorporating random sampling of Riemann Zeta function values and updating probability distributions based on performance metrics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">20:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19798" target="_blank">@arXiv 2409.19798</a>
                    <span class="tweet-title">Membership Inference Attacks: Can't Prove You Trained on My Data!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This paper challenges the common assumption that membership inference attacks can reliably prove a model was trained on specific data. It argues that these attacks are fundamentally flawed because it's impossible to accurately estimate their false positive rate.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">20:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20288" target="_blank">@arXiv 2409.20288</a>
                    <span class="tweet-title">LexEval:  Testing AI Lawyers on China's Legal Exams!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces LexEval, a comprehensive benchmark for evaluating large language models (LLMs) in the Chinese legal domain. Unlike previous benchmarks, LexEval focuses on practical legal applications and includes a taxonomy of legal cognitive abilities, encompassing tasks like legal concept memorization, understanding, logic inference, discrimination, generation, and ethics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">20:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19575" target="_blank">@arXiv 2409.19575</a>
                    <span class="tweet-title">Speech vs. Lips:  Who's Got the Most Info?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Beijing University of Posts and Telecommunications</span>
                </div>
                <div class="primary-text">
                    This research uses information theory to quantify the amount of information shared between audio, video, and text in audio-visual speech processing tasks. This approach differs from previous work that primarily focused on qualitative analysis.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">21:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19667" target="_blank">@arXiv 2409.19667</a>
                    <span class="tweet-title">Can LLMs Code Like Pros? New Benchmark Tests Graph Analysis Skills</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Beijing University of Posts and Telecommunications</span>
                </div>
                <div class="primary-text">
                    This research introduces ProGraph, a benchmark that evaluates large language models' (LLMs) ability to analyze graphs by writing code using external APIs, unlike previous benchmarks that focused on direct reasoning over graph descriptions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">21:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20449" target="_blank">@arXiv 2409.20449</a>
                    <span class="tweet-title">Teacher's Pet:  Linear Projections Make Tiny Models Learn Like Big Ones!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google, MIT, University of Michigan</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new method called Learning Embedding Linear Projections (LELP) for knowledge distillation. Unlike previous methods that focus on mimicking the teacher's output probabilities or internal representations, LELP extracts information from the teacher's embedding layer by identifying informative linear subspaces and splitting them into pseudo-subclasses. The student model is then trained on these pseudo-subclasses, improving performance in binary and few-class classification tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">22:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19022" target="_blank">@arXiv 2409.19022</a>
                    <span class="tweet-title">AI vs. Scammers:  A Deep Dive into Online Fraud Detection</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This systematic literature review focuses on AI-based models that analyze textual data to detect online fraud, a unique focus compared to previous reviews that often concentrate on specific scam types.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">22:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19747" target="_blank">@arXiv 2409.19747</a>
                    <span class="tweet-title">Charts Talking:  A New Language for Data Storytelling</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">York University</span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive survey of Natural Language Generation (NLG) for visualizations, focusing on text generation for visualizations and introducing a taxonomy of the problem. It differs from previous work by systematically reviewing the state of the art on NLG for visualizations and introducing a taxonomy of the problem.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">22:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19569" target="_blank">@arXiv 2409.19569</a>
                    <span class="tweet-title">Image Segmentation:  A Love Story Between Words and Pixels</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces four explicit interaction principles for aligning visual and textual information in referring image segmentation. Unlike previous methods that often rely on single-modal mask decoders, this paper proposes a Fully Aligned Network (FAN) that utilizes a multi-modal common space for improved segmentation accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">23:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19024" target="_blank">@arXiv 2409.19024</a>
                    <span class="tweet-title">Elephant in the Room: Reward Models Are the Real MVPs in AI Alignment</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Hong Kong University of Science and Technology, Peking University, Microsoft...</span>
                </div>
                <div class="primary-text">
                    This research focuses on the impact of reward model quality on AI alignment, a crucial aspect often overlooked in previous studies. Unlike prior work that primarily focused on alignment algorithms, this paper systematically investigates the performance of various reward models and their influence on alignment outcomes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">23:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20012" target="_blank">@arXiv 2409.20012</a>
                    <span class="tweet-title">Sentiment Analysis with Missing Data:  A Language-Dominated Approach</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Chinese University of Hong Kong, University College London, Wuhan University</span>
                </div>
                <div class="primary-text">
                    This research focuses on multimodal sentiment analysis with incomplete data, proposing a Language-dominated Noise-resistant Learning Network (LNLN) that prioritizes the language modality for robust sentiment prediction. Unlike previous work, LNLN incorporates a Dominant Modality Correction (DMC) module to enhance the quality of language features, even when data is missing.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">23:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19951" target="_blank">@arXiv 2409.19951</a>
                    <span class="tweet-title">LLMs:  The Weakest Link Holds Back the Chain!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta</span>
                </div>
                <div class="primary-text">
                    This research focuses on evaluating the performance of Large Language Models (LLMs) across multiple capabilities, rather than just individual skills. It introduces a new benchmark, CrossEval, to assess how well LLMs handle tasks that require combining different abilities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">24:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19523" target="_blank">@arXiv 2409.19523</a>
                    <span class="tweet-title">LLMs for Translation:  A Neuron-Specific Approach to Fine-Tuning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tianjin University, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for selectively fine-tuning LLMs for machine translation by identifying and routing language-aware neurons. Unlike previous approaches that focus on full-parameter finetuning, this method aims to mitigate catastrophic forgetting and parameter interference by updating only the parameters of relevant neurons.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">24:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18974" target="_blank">@arXiv 2409.18974</a>
                    <span class="tweet-title">Neural Warp Composition:  A Flow-Based Trick for Faster Rendering</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">McGill University, Adobe Research</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new method for importance sampling in rendering by composing two neural warps: a conditional head warp and an unconditional tail warp. This approach differs from previous work by separating the complexity and conditioning of the target distribution, leading to a more efficient and accurate sampling process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">24:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19884" target="_blank">@arXiv 2409.19884</a>
                    <span class="tweet-title">EEG-Based Attention Decoding:  A Short-Window CNN and Mamba Make a Match!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel model called SWIM, which combines a short-window convolutional neural network (SWCNN) with a sequence model called Mamba. This approach aims to improve the decoding of auditory spatial attention from EEG signals by leveraging both short-term and long-term information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">25:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19149" target="_blank">@arXiv 2409.19149</a>
                    <span class="tweet-title">AI's New Trick:  Text-to-Image Models Can Be Tricked Into Generating Unsafe Content</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research introduces a new type of jailbreak for text-to-image models, called "multimodal pragmatic jailbreak," where the combination of an image and text, each considered safe individually, results in unsafe content. This differs from previous work that focused on jailbreaking language models or image models separately.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon" title="Play from here">25:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20171" target="_blank">@arXiv 2409.20171</a>
                    <span class="tweet-title">Curb Your Enthusiasm: New Method Detects Road Edges Without Any Annotations!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Hong Kong University of Science and Technology, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes an annotation-free curb detection method using Altitude Difference Images (ADIs) derived from LiDAR point clouds. Unlike previous methods that rely on manually annotated data or struggle with lighting variations, this approach leverages the inherent altitude differences in ADIs to identify curbs, making it more robust and efficient.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon" title="Play from here">26:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19846" target="_blank">@arXiv 2409.19846</a>
                    <span class="tweet-title">Semantic Segmentation Without Labels:  PixelCLIP Makes It Happen!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">KAIST, Korea University, Google Research</span>
                </div>
                <div class="primary-text">
                    This research proposes PixelCLIP, a method for open-vocabulary semantic segmentation that doesn't rely on pixel-level semantic labels. Instead, it leverages unlabeled masks generated from vision foundation models like SAM and DINO. This approach differs from previous work that either required pixel-level labels or relied on image-level captions for supervision.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon" title="Play from here">26:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18980" target="_blank">@arXiv 2409.18980</a>
                    <span class="tweet-title">IW-Bench:  Turning Images into Websites, One Element at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Beihang University, Alibaba Group, Tsinghua University...</span>
                </div>
                <div class="primary-text">
                    This research introduces IW-BENCH, a new benchmark specifically designed to evaluate the performance of large multimodal models in converting images to web code. Unlike previous benchmarks that rely on BLEU scores, IW-BENCH focuses on evaluating the completeness of web elements and the accuracy of layout information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet67">
            <div class="start-time-icon" title="Play from here">27:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19242" target="_blank">@arXiv 2409.19242</a>
                    <span class="tweet-title">Scientists Build a Diagram-Making Machine That Actually Listens!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Maryland, IBM, Adobe</span>
                </div>
                <div class="primary-text">
                    This research introduces a new task called "SciDoc2Diagram" which focuses on generating scientific diagrams from academic papers, taking into account user intent. Unlike previous text-to-image models that struggle with long-context inputs, this approach uses a multi-step pipeline to extract relevant information and generate diagrams based on user specifications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet68">
            <div class="start-time-icon" title="Play from here">27:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19005" target="_blank">@arXiv 2409.19005</a>
                    <span class="tweet-title">Digital Twins:  More Than Just a Pretty Face?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National University of Singapore, University of the Philippines, Singapore-ETH Centre...</span>
                </div>
                <div class="primary-text">
                    This research systematically analyzes over 15,000 scientific publications to understand how the definition of a digital twin varies across different domains within the built environment. It contrasts these findings with insights from an expert survey, identifying key components and their significance in each domain.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet69">
            <div class="start-time-icon" title="Play from here">27:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20434" target="_blank">@arXiv 2409.20434</a>
                    <span class="tweet-title">QAEncoder:  Stop  Wasting  Time  Training,  Get  Query-Aligned  Embeddings!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Chinese Academy of Sciences</span>
                </div>
                <div class="primary-text">
                    This research introduces QAEncoder, a training-free approach to bridge the document-query gap in dense retrievers. Unlike previous methods that rely on training or generating pseudo-documents, QAEncoder leverages the conical distribution hypothesis to estimate the expectation of potential queries for each document, effectively aligning representations without additional index size or retrieval latency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet70">
            <div class="start-time-icon" title="Play from here">28:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19625" target="_blank">@arXiv 2409.19625</a>
                    <span class="tweet-title">Argumentation Gets a Time Machine: New Framework Tracks Dialogue Order</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sorbonne University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework for modeling abstract argumentation graphs that incorporates the order of argument enunciation. Unlike previous work, this approach uses an action description language (ADL) to model the dynamic evolution of the argumentation graph, allowing for a more nuanced understanding of the causal relationships between arguments.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet71">
            <div class="start-time-icon" title="Play from here">28:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19691" target="_blank">@arXiv 2409.19691</a>
                    <span class="tweet-title">Chinese Essays Get a Rhetorical Makeover: New Dataset Helps AI Understand and Generate Fancy Writing!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">East China Normal University, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset called CERD, which focuses on understanding and generating rhetorical devices in Chinese essays. Unlike previous datasets that focus on single categories of rhetoric, CERD includes multiple categories and sub-tasks, allowing for a more comprehensive analysis of rhetorical phenomena.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet72">
            <div class="start-time-icon" title="Play from here">28:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20139" target="_blank">@arXiv 2409.20139</a>
                    <span class="tweet-title">Forget Adversarial Training, Just Tame Those Gradients!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research revisits the idea of regularizing the gradient norm of a model with respect to its input, a technique previously deemed less effective than adversarial training. The authors demonstrate that this approach can achieve near state-of-the-art robustness on ImageNet when applied to modern vision transformers, which utilize smooth activation functions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet73">
            <div class="start-time-icon" title="Play from here">29:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19223" target="_blank">@arXiv 2409.19223</a>
                    <span class="tweet-title">Face-Off: Can Cameras Beat Fingertips for High-Altitude Vitals?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Qinghai University, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces the SUMS dataset, a collection of synchronized facial and finger videos recorded at high altitudes during exercise and oxygen recovery scenarios. This dataset is unique because it captures the physiological fluctuations that occur in high-altitude environments, which are not adequately represented in existing datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet74">
            <div class="start-time-icon" title="Play from here">29:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19890" target="_blank">@arXiv 2409.19890</a>
                    <span class="tweet-title">UniMed: One Model to Rule Them All (Medical Images, That Is)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Southeast University, Peking University, Case Western Reserve University</span>
                </div>
                <div class="primary-text">
                    This paper proposes a universal medical image analysis model (UniMed) that can handle various tasks at different levels (image, region, pixel) without requiring task-specific branches or heads. UniMed uses a decomposed-composed decoder architecture to unify the input and output space, enabling flexible model combinations for diverse tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet75">
            <div class="start-time-icon" title="Play from here">30:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19839" target="_blank">@arXiv 2409.19839</a>
                    <span class="tweet-title">AI Forecasters: Can They Predict the Future (Or Just Google It)?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Federal Reserve Bank of Chicago, Forecasting Research Institute, New York University...</span>
                </div>
                <div class="primary-text">
                    This research introduces ForecastBench, a dynamic benchmark for evaluating AI forecasting systems. Unlike previous static benchmarks, ForecastBench continuously updates its questions and resolution values, ensuring it remains relevant and avoids data leakage.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet76">
            <div class="start-time-icon" title="Play from here">30:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19501" target="_blank">@arXiv 2409.19501</a>
                    <span class="tweet-title">Talking Heads with Feelings: New AI Makes Virtual People More Expressive</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stony Brook University, EPFL, Adobe Research...</span>
                </div>
                <div class="primary-text">
                    This research focuses on generating talking heads that express emotions with varying intensity levels, unlike previous methods that often produce static emotional outputs. The authors propose a novel approach to capture and reproduce these subtle shifts in intensity by learning a continuous emotion latent space and using an audio-to-intensity predictor.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet77">
            <div class="start-time-icon" title="Play from here">31:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20329" target="_blank">@arXiv 2409.20329</a>
                    <span class="tweet-title">Federated Learning: When Collaboration Backfires</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research investigates the impact of adversarial clients on personalized federated learning. Unlike previous work, it focuses on fine-tuning the level of collaboration between clients to mitigate the negative effects of adversarial behavior.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet78">
            <div class="start-time-icon" title="Play from here">31:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19764" target="_blank">@arXiv 2409.19764</a>
                    <span class="tweet-title">Spiking Transformers Get a Temporal Tune-Up: New Attention Mechanism Makes Them See the Big Picture</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new spatial-temporal attention mechanism for spiking transformers, which integrates information across both space and time. Previous spiking transformers only considered spatial attention, neglecting the temporal context inherent in the spike-based representation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet79">
            <div class="start-time-icon" title="Play from here">31:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19686" target="_blank">@arXiv 2409.19686</a>
                    <span class="tweet-title">Motion Masking:  Diffusion Models Learn to Dance with a Blindfold!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel masking strategy for diffusion models used in text-driven human motion generation. Unlike previous methods that train motion encoders before inference, this approach directly masks portions of the motion embedding space during the diffusion process, forcing the model to learn contextual relationships between spatial and temporal semantics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet80">
            <div class="start-time-icon" title="Play from here">32:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19278" target="_blank">@arXiv 2409.19278</a>
                    <span class="tweet-title">RNNs:  Building Brains to Mimic the World's Rhythms</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This paper presents an explicit construction of recurrent neural networks (RNNs) that effectively approximate discrete dynamical systems. Unlike previous work, it focuses on minimizing the maximal Lyapunov exponent by using an elementary algebraic approach.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet81">
            <div class="start-time-icon" title="Play from here">32:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19258" target="_blank">@arXiv 2409.19258</a>
                    <span class="tweet-title">VecLSTM:  Turning  Trajectory  Data  Into  a  Grid  for  Activity  Recognition</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington</span>
                </div>
                <div class="primary-text">
                    This research proposes VecLSTM, a framework that uses vectorization to transform raw trajectory data into a structured grid format. This approach differs from previous methods that primarily relied on traditional LSTM architectures.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet82">
            <div class="start-time-icon" title="Play from here">32:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19431" target="_blank">@arXiv 2409.19431</a>
                    <span class="tweet-title">Tilting the Scales: How a New Risk Metric Makes Machine Learning More Robust</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The Alan Turing Institute, University of Cambridge, University of Chicago...</span>
                </div>
                <div class="primary-text">
                    This paper examines the generalization error of the tilted empirical risk (TER), a non-linear risk metric proposed by Li et al. (2021). Unlike previous work focusing on the linear empirical risk, this study provides bounds on the tilted generalization error for both bounded and unbounded loss functions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet83">
            <div class="start-time-icon" title="Play from here">33:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20556" target="_blank">@arXiv 2409.20556</a>
                    <span class="tweet-title">Want to See How a Painting Was Made? AI Can Now Recreate the Process!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for reconstructing the painting process of an image by training a diffusion model on real painting videos. Unlike previous approaches that rely on hand-crafted painting principles, this method learns from real-world data, resulting in more realistic and human-like painting sequences.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet84">
            <div class="start-time-icon" title="Play from here">33:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20251" target="_blank">@arXiv 2409.20251</a>
                    <span class="tweet-title">MRI's New Trick:  Learning to Sharpen Images with a Flip Angle Tweak</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University Hospital Erlangen, Friedrich-Alexander-Universität Erlangen-Nürnberg, University of Würzburg...</span>
                </div>
                <div class="primary-text">
                    This research proposes an end-to-end learning framework to optimize variable flip angle (VFA) schemes for 3D FSE sequences at 7T. Unlike previous heuristic approaches, this method automatically identifies VFAs that minimize image blurring or maximize SNR, considering multiple tissues and their spatial distributions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet85">
            <div class="start-time-icon" title="Play from here">33:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19494" target="_blank">@arXiv 2409.19494</a>
                    <span class="tweet-title">Robots Ditch Depth Sensors, Grasp Objects with Just RGB Images!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, Amazon Robotics</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach for robotic suction grasping that relies solely on RGB images, eliminating the need for depth sensors. Unlike previous methods that often utilize depth information, this approach leverages pre-trained weights from a depth estimation model to predict grasp poses from synthetic training data, achieving generalization to real-world objects without any fine-tuning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet86">
            <div class="start-time-icon" title="Play from here">34:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19750" target="_blank">@arXiv 2409.19750</a>
                    <span class="tweet-title">AstroLLaMA-2-70B:  A 70B-Parameter LLM for Astronomy,  But Can It Handle the Big Questions?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Illinois, University of Pennsylvania, Harvard University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new set of astronomy-focused LLMs, AstroLLaMA-3-8B and AstroLLaMA-2-70B, and benchmarks their performance against existing models. The study also explores the impact of continual pretraining on different model sizes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet87">
            <div class="start-time-icon" title="Play from here">34:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20483" target="_blank">@arXiv 2409.20483</a>
                    <span class="tweet-title">News Recommender Systems: Balancing Accuracy and Editorial Values</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, University of Bari, Polytechnic University of Bari...</span>
                </div>
                <div class="primary-text">
                    This research focuses on the impact of recommender systems on the flow of news, going beyond traditional accuracy metrics to evaluate how these systems align with editorial values and potentially influence democratic functions of news publishers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet88">
            <div class="start-time-icon" title="Play from here">35:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20156" target="_blank">@arXiv 2409.20156</a>
                    <span class="tweet-title">Training Extreme Classifiers:  ANNS to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research proposes a new training algorithm called ASTRA for extreme classifiers, which uses approximate nearest neighbor search (ANNS) on classifier weights to efficiently sample hard negatives. This approach differs from previous methods that either rely on expensive negative sampling strategies or use stale indices, leading to accuracy issues.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet89">
            <div class="start-time-icon" title="Play from here">35:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20067" target="_blank">@arXiv 2409.20067</a>
                    <span class="tweet-title">Breaking the Curse:  Multi-Agent Learning Gets a Robust Makeover</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Caltech, Peking University, Carnegie Mellon University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new class of robust Markov games with "fictitious uncertainty sets" that account for both environmental and agent-specific uncertainties in a correlated manner. This differs from previous work that focused on independent uncertainty sets for each agent.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet90">
            <div class="start-time-icon" title="Play from here">35:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19391" target="_blank">@arXiv 2409.19391</a>
                    <span class="tweet-title">Deep Learning for Multi-Agent Games:  Sparsity is the New Black!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Hong Kong University of Science and Technology</span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel sparse training framework called MAST for value-based deep multi-agent reinforcement learning (MARL). Unlike previous work, MAST addresses the challenges of value estimation errors and policy inconsistency caused by sparsification by introducing a hybrid TD(λ) target mechanism combined with the Soft Mellowmax operator and a dual buffer mechanism.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet91">
            <div class="start-time-icon" title="Play from here">36:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20289" target="_blank">@arXiv 2409.20289</a>
                    <span class="tweet-title">Robots Team Up to See the Whole Picture: NeRFs Go Distributed!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Illinois, Nvidia, UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research proposes a distributed learning approach for multi-agent NeRF reconstruction, where agents share only their learned model weights, reducing communication overhead compared to traditional methods that transfer raw data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet92">
            <div class="start-time-icon" title="Play from here">36:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19926" target="_blank">@arXiv 2409.19926</a>
                    <span class="tweet-title">Risk-Averse Decisions: Debunking the Entropic Risk Underestimation Myth!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Montreal, HEC Montréal, University of Cyprus</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel bootstrapping procedure to debias the empirical entropic risk estimator, addressing the underestimation problem that arises when limited data is available. Unlike previous work, this method utilizes a "bias-aware" distribution fitting technique to better capture tail events, leading to more accurate risk estimates.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet93">
            <div class="start-time-icon" title="Play from here">37:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20440" target="_blank">@arXiv 2409.20440</a>
                    <span class="tweet-title">Optimistic Bandits:  When Noise Makes You Smarter</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne, University of Chicago Booth School of Business</span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to Follow-the-Perturbed-Leader (FTPL) algorithms for multi-armed bandits by allowing for correlated noise across arms. This differs from previous FTPL methods that typically assume independent noise.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet94">
            <div class="start-time-icon" title="Play from here">37:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20385" target="_blank">@arXiv 2409.20385</a>
                    <span class="tweet-title">Tylenol vs. Acetaminophen: Can AI Resist Misinformation Requests?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Mass General Brigham, Brigham and Women’s Hospital, Dana-Farber Cancer Institute...</span>
                </div>
                <div class="primary-text">
                    This research investigates the ability of large language models (LLMs) to resist requests for misinformation, specifically focusing on the generation of misleading medical information. Unlike previous work that primarily explored jailbreaking in the context of extreme content, this study examines LLMs' capacity to recognize and resist illogical or factually flawed requests.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet95">
            <div class="start-time-icon" title="Play from here">37:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19218" target="_blank">@arXiv 2409.19218</a>
                    <span class="tweet-title">List Regression:  It's Not Just for Classifiers Anymore!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This paper extends the concept of list learning, where algorithms output multiple predictions, from classification to regression. It introduces two new combinatorial dimensions, the k-OIG dimension and the k-fat-shattering dimension, to characterize realizable and agnostic list regression, respectively.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet96">
            <div class="start-time-icon" title="Play from here">38:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19978" target="_blank">@arXiv 2409.19978</a>
                    <span class="tweet-title">Violina:  Unveiling the Secrets of Memory in Complex Systems</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research introduces Violina, a new system identification method that can model linear time-invariant non-Markovian dynamics using multiple time-series data. Unlike previous methods, Violina incorporates memory effects and constraints, allowing for more accurate and physically realistic models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet97">
            <div class="start-time-icon" title="Play from here">38:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19001" target="_blank">@arXiv 2409.19001</a>
                    <span class="tweet-title">LLMs Need a Little Help: New Method Makes Them Pay Attention to Instructions</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Huawei Technologies, École Polytechnique, Yale University</span>
                </div>
                <div class="primary-text">
                    This research introduces GUIDE, a method that directly increases attention scores for specific instruction tokens within a prompt. Unlike previous approaches like supervised fine-tuning or reinforcement learning, GUIDE doesn't require additional training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet98">
            <div class="start-time-icon" title="Play from here">39:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19479" target="_blank">@arXiv 2409.19479</a>
                    <span class="tweet-title">Robots Learn to Navigate Like Humans, Avoiding Walls and Dead Ends</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for training deep differentiable planners that can navigate unseen environments. Unlike previous work, this method explicitly models illegal actions and noisy motion, leading to more robust performance in complex environments.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet99">
            <div class="start-time-icon" title="Play from here">39:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19605" target="_blank">@arXiv 2409.19605</a>
                    <span class="tweet-title">DPO's Secret Weapon: Samplers That Make Language Models Learn Faster!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, University of Washington</span>
                </div>
                <div class="primary-text">
                    This research focuses on the optimization properties of Direct Preference Optimization (DPO), specifically analyzing the impact of different sampling strategies on its convergence rates. Unlike previous work that primarily focused on data-driven perspectives, this paper delves into the optimization aspect, revealing a surprising separation in convergence rates between uniform and mixed samplers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet100">
            <div class="start-time-icon" title="Play from here">39:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20167" target="_blank">@arXiv 2409.20167</a>
                    <span class="tweet-title">AI Tutoring Gets a Knowledge Boost:  LLMs Extract Learning Components from Multimedia Content</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for automatically extracting knowledge components (KCs) from educational content using instruction-tuned large multimodal models (LLMs). Unlike previous work that relies on statistical data and human-defined KCs, this approach leverages the capabilities of LLMs to analyze text, images, and even audio to identify and describe KCs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet101">
            <div class="start-time-icon" title="Play from here">40:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19425" target="_blank">@arXiv 2409.19425</a>
                    <span class="tweet-title">Forget Big Models, Tiny Projectors Rule the Multimodal World!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ML Labs, UC Berkeley, TII Abu Dhabi</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to multimodal vision-language alignment by training lightweight projection layers between pretrained, frozen unimodal encoders. This differs from previous work that typically involves training large, joint models from scratch.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet102">
            <div class="start-time-icon" title="Play from here">40:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19592" target="_blank">@arXiv 2409.19592</a>
                    <span class="tweet-title">Whisper It, Not Shout It: New AI Makes Collaborative Perception Ultra-Efficient</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes DiffCP, a collaborative perception (CP) paradigm that uses a diffusion model to compress sensing information from multiple agents. Unlike previous CP methods that rely on transmitting raw data or compressed features, DiffCP leverages the correlations between sensor data to reconstruct the co-agent's perception at the ego-agent's side, significantly reducing communication overhead.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet103">
            <div class="start-time-icon" title="Play from here">40:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19272" target="_blank">@arXiv 2409.19272</a>
                    <span class="tweet-title">LLMs Get a Makeover:  PerceptionCompressor Squeezes Out Redundancy in Long Contexts!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Ant Group, Sun Yat-sen University</span>
                </div>
                <div class="primary-text">
                    This paper introduces PerceptionCompressor, a training-free method for compressing prompts in long context scenarios. Unlike previous methods that focus on removing low-perplexity tokens, PerceptionCompressor leverages contrast perplexity to identify and retain key information tokens while removing noise.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet104">
            <div class="start-time-icon" title="Play from here">41:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19012" target="_blank">@arXiv 2409.19012</a>
                    <span class="tweet-title">LLMs:  Lost in Logic Games, But Can They Reflect Their Way Out?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Princeton University</span>
                </div>
                <div class="primary-text">
                    This research constructs a new dataset of LSAT logic games, including metadata like difficulty and game type, and evaluates the performance of LLMs on this dataset using various prompting strategies. It also explores the effectiveness of a self-reflection framework, where LLMs are given feedback on their incorrect answers and asked to revise their reasoning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet105">
            <div class="start-time-icon" title="Play from here">41:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20409" target="_blank">@arXiv 2409.20409</a>
                    <span class="tweet-title">Brain Tumor Tracking:  A Physics-Fueled Deep Dive into Glioblastoma Growth</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Zurich, Technical University of Munich, Harvard University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method for estimating the full spatial distribution of tumor cells in glioblastoma patients by combining physics-based models with data-driven approaches. Unlike previous methods that rely on hard constraints from physical models, this approach uses a unique discretization scheme to create soft constraints, allowing for greater flexibility in assimilating patient data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet106">
            <div class="start-time-icon" title="Play from here">42:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19439" target="_blank">@arXiv 2409.19439</a>
                    <span class="tweet-title">Double Vision: How Aerial Photos Boost Plant Recognition</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new pre-training method called CRISP, which leverages both ground-level and aerial images to improve representation learning for natural world imagery. Unlike previous work that focuses on single-view pre-training, CRISP utilizes the complementary information from multiple views to enhance downstream tasks like species recognition and distribution mapping.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet107">
            <div class="start-time-icon" title="Play from here">42:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20264" target="_blank">@arXiv 2409.20264</a>
                    <span class="tweet-title">Deep Learning Gets a First-Order Makeover: Solving PDEs with Neural Networks</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, University of Vienna</span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework for solving PDEs using deep neural networks. It leverages a first-order system least squares (FoSLS) formulation, which differs from previous approaches that often rely on variational principles or pointwise residuals.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet108">
            <div class="start-time-icon" title="Play from here">42:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20187" target="_blank">@arXiv 2409.20187</a>
                    <span class="tweet-title">DAG Models:  A New Test to Tell If They're Actually True!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, University of Minnesota</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new statistical test, the Markov Checker, to verify if a directed acyclic graph (DAG) model accurately reflects the relationships in the data. Unlike previous methods that rely on assumptions about the data, the Markov Checker directly assesses the model's ability to predict conditional independence relationships.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet109">
            <div class="start-time-icon" title="Play from here">43:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20342" target="_blank">@arXiv 2409.20342</a>
                    <span class="tweet-title">AI Doctors:  Cancer Imaging Gets a Helping Hand!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">BAMF Health, National Institute of Health, Frederick National Laboratory for Cancer Research...</span>
                </div>
                <div class="primary-text">
                    This research focuses on using AI to generate annotations for cancer images in the National Cancer Institute's Image Data Commons (IDC). Unlike previous work, this study provides both the AI models and the generated annotations, making it easier for researchers to use and build upon.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet110">
            <div class="start-time-icon" title="Play from here">43:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19226" target="_blank">@arXiv 2409.19226</a>
                    <span class="tweet-title">Robots Learn to Open Doors: A Bridge Policy for Planning in Unpredictable Worlds</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to bridge the gap between model-based planning and reinforcement learning (RL) for robots operating in unpredictable environments. The key innovation lies in the use of a "CallPlanner" action within the RL problem, allowing the agent to learn a "bridge policy" that effectively resolves novel situations before handing control back to the planner. This approach differs from previous work by explicitly leveraging the planner's knowledge and avoiding the need to learn the entire task from scratch.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet111">
            <div class="start-time-icon" title="Play from here">43:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19011" target="_blank">@arXiv 2409.19011</a>
                    <span class="tweet-title">Quantum Machine Learning:  Bias is a B*tch, but We've Got the Cure!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of California  San Diego</span>
                </div>
                <div class="primary-text">
                    This research delves into the unique biases that arise in quantum machine learning (QML) systems, going beyond the typical data and algorithmic biases seen in classical machine learning. It identifies five specific biases inherent to QML, including encoding, inductive, realizability, state-dependent, and sampling biases.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet112">
            <div class="start-time-icon" title="Play from here">44:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18995" target="_blank">@arXiv 2409.18995</a>
                    <span class="tweet-title">AI's Got a Triage Problem: Can We Align It With Our Values?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel metric, the Alignment Compliance Index (ACI), to quantify how effectively a large language model (LLM) can be aligned with a given preference function or gold standard. This differs from previous work by focusing on the effect of alignment rather than the process itself.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet113">
            <div class="start-time-icon" title="Play from here">44:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.19308" target="_blank">@arXiv 2409.19308</a>
                    <span class="tweet-title">LLMs Get a Makeover: Fine-Tuning for Opinion-Generating Superpowers!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research focuses on fine-tuning large language models (LLMs) to simulate public opinions on environmental policies, specifically by conditioning them on socio-demographic factors. This approach differs from previous work by emphasizing the importance of tailoring LLMs to specific societal contexts for more accurate and ethical policy simulations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet114">
            <div class="start-time-icon" title="Play from here">45:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20276" target="_blank">@arXiv 2409.20276</a>
                    <span class="tweet-title">NeRF-tastic Mapping:  How a Voronoi Graph Helps Robots Explore Like Pros</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for active neural mapping that uses a generalized Voronoi graph (GVG) to structure information within a neural map. This allows for more efficient and thorough exploration of large-scale indoor environments compared to previous methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet115">
            <div class="start-time-icon" title="Play from here">45:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.20366" target="_blank">@arXiv 2409.20366</a>
                    <span class="tweet-title">Singlish Particles:  Unmasking the Meaning with AI!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Edinburgh, CMU</span>
                </div>
                <div class="primary-text">
                    This research uses task-driven representation learning to disentangle the pragmatic functions of Singlish discourse particles, a novel approach compared to previous linguistic analyses.
                </div>
            </div>
        </div>

        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202410011620_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>    
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>
</html>