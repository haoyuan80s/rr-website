<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                    Fresh Picks:
                    <span class="highlightNumber">74</span> out of <span class="highlightNumber">322</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-08-21"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>

        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">
                00:54
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11039" target="_blank">
                        @arXiv 2408.11039
                    </a>
                    <span class="tweet-title">
                        One Model to Rule Them All: Text and Images, Unified!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Meta, Waymo, University of Southern California
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Transfusion, a method for training a single multi-modal model that can
                    generate both discrete text and continuous images. Unlike previous approaches that either extend
                    language models to use diffusion models as tools or quantize images into discrete tokens,
                    Transfusion integrates both modalities seamlessly by training a single model on both next token
                    prediction and diffusion objectives.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">
                01:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10261" target="_blank">
                        @arXiv 2408.10261
                    </a>
                    <span class="tweet-title">
                        R-GCN: A Knowledge Graph Network That Can't Learn Its Own Rules!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford, Royal Holloway University of London
                    </span>
                </div>
                <div class="primary-text">
                    This paper investigates the explainability of Relational Graph Convolutional Networks (R-GCN), a
                    popular architecture for knowledge graph completion. Unlike previous work that focused on
                    specific
                    subclasses of GNNs, this study analyzes R-GCN and demonstrates that it cannot be faithfully
                    captured
                    by Datalog rules, a logic-based formalism.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">
                01:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10958" target="_blank">
                        @arXiv 2408.10958
                    </a>
                    <span class="tweet-title">
                        AI Predicts Storms: Can a Computer Outsmart Mother Nature?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        NVIDIA Corporation, Lawrence Berkeley National Laboratory, University of Washington...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a generative diffusion model called StormCast that emulates the
                    high-resolution rapid refresh (HRRR) model, a state-of-the-art 3km operational
                    convection-allowing
                    model (CAM). This is different from previous work because it uses a generative model to predict
                    a
                    dense atmospheric state with dozens of vertical levels, which is crucial for capturing the
                    complex
                    dynamics of km-scale convection.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">
                02:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10902" target="_blank">
                        @arXiv 2408.10902
                    </a>
                    <span class="tweet-title">
                        Chatbots Get a Reality Check: New Dataset Evaluates LLMs on Commonsense and Coherence
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        INESC-ID, CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces SODA-EVAL, a new dataset for evaluating open-domain dialogue systems.
                    Unlike previous benchmarks that focused on fluency and relevance, SODA-EVAL targets more complex
                    aspects like coherence and commonsense reasoning, reflecting the challenges of contemporary
                    LLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">
                02:41
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10609" target="_blank">
                        @arXiv 2408.10609
                    </a>
                    <span class="tweet-title">
                        Perturbation Prediction: Simple Models Win the Race!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Altos Labs
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces PerturBench, a comprehensive framework for benchmarking machine
                    learning
                    models that predict the effects of cellular perturbations. It differs from previous work by
                    providing a standardized approach with diverse datasets, metrics for fair comparison, and
                    detailed
                    performance analysis.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">
                03:00
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10490" target="_blank">
                        @arXiv 2408.10490
                    </a>
                    <span class="tweet-title">
                        LLMs Get a Plan: How Planning Can Stop AI from Making Stuff Up!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Southern California, Google
                    </span>
                </div>
                <div class="primary-text">
                    This research explores how planning can be used to guide retrieval in language models, improving
                    the
                    accuracy of generated text by reducing hallucinations. Unlike previous work that focuses on
                    post-hoc
                    verification or heuristic-based retrieval, this study leverages the planning capabilities of
                    instruction-tuned LLMs to proactively gather relevant information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">
                03:30
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10433" target="_blank">
                        @arXiv 2408.10433
                    </a>
                    <span class="tweet-title">
                        CLIP-DPO: Fixing LVLMs' Lies with a Vision-Language Model
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Samsung AICenter Cambridge, Technical University of Iasi, Queen Mary University of London
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes CLIP-DPO, a new method for training Large Vision-Language Models (LVLMs)
                    that
                    uses a pre-trained CLIP model to rank generated captions and identify hallucinated details.
                    Unlike
                    previous approaches, CLIP-DPO doesn't rely on paid APIs, additional data, or external LVLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">
                04:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11053" target="_blank">
                        @arXiv 2408.11053
                    </a>
                    <span class="tweet-title">
                        LLMs for Hardware: VerilogEval Gets a Turbo Boost!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nvidia
                    </span>
                </div>
                <div class="primary-text">
                    This research extends the VerilogEval benchmark to include specification-to-RTL tasks,
                    incorporates
                    in-context learning examples, and introduces a failure classification mechanism. This provides a
                    more comprehensive evaluation framework for Verilog code generation tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">
                04:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10575" target="_blank">
                        @arXiv 2408.10575
                    </a>
                    <span class="tweet-title">
                        MUSE: Seeing the Big Picture (and the Tiny Details) in Text-Video Retrieval
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces MUSE, a multi-scale learning method for text-video retrieval. Unlike
                    previous methods that rely on single-scale representations, MUSE leverages a feature pyramid to
                    extract information from different resolutions, allowing for a more comprehensive understanding
                    of
                    video content.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">
                05:04
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11052" target="_blank">
                        @arXiv 2408.11052
                    </a>
                    <span class="tweet-title">
                        Reinforcement Learning Gets a Speed Boost: JaxGCRL Makes Training a Breeze!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Warsaw University of Technology, University of Warsaw, UC Berkeley...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new codebase and benchmark called JaxGCRL, which uses GPU-accelerated
                    environments and a stable, batched version of the contrastive reinforcement learning algorithm
                    to
                    significantly speed up training times for self-supervised goal-conditioned reinforcement
                    learning
                    (GCRL) agents. This approach allows researchers to collect data from millions of environment
                    steps
                    in minutes on a single GPU, enabling faster experimentation and iteration on new ideas.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">
                05:33
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10365" target="_blank">
                        @arXiv 2408.10365
                    </a>
                    <span class="tweet-title">
                        AI Reviewers: Can Robots Judge Your Research?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Boston University, Tel Aviv University, Columbia University...
                    </span>
                </div>
                <div class="primary-text">
                    This research goes beyond simply using LLMs to generate reviews. It introduces a system for
                    evaluating the quality of these reviews by comparing them to human reviews, both through human
                    and
                    LLM preferences.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">
                06:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10495" target="_blank">
                        @arXiv 2408.10495
                    </a>
                    <span class="tweet-title">
                        AI Code Wizards: Can They Write Secure Code Without Getting Hacked?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Beihang University, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research goes beyond just generating code and examines the ability of large language models
                    (LLMs) to identify and fix security vulnerabilities in the code they produce, a crucial step
                    towards
                    building truly secure software. Unlike previous studies that focused on LLMs' ability to
                    generate
                    code, this paper investigates their end-to-end capability to produce secure code, including
                    self-review and repair.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">
                06:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10255" target="_blank">
                        @arXiv 2408.10255
                    </a>
                    <span class="tweet-title">
                        AI for Finance: Wall Street's New Trading Buddy?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        International Digital Economy Academy, The Hong Kong University of Science and Technology,
                        Shanghai Advanced Institute of Finance...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new framework called the Large Investment Model (LIM) for quantitative
                    investment. Unlike traditional multi-factor models, LIM uses an end-to-end deep learning
                    approach to
                    directly learn trading strategies from diverse financial data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">
                06:49
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10717" target="_blank">
                        @arXiv 2408.10717
                    </a>
                    <span class="tweet-title">
                        Deep Learning Saves the Day: CO2 Storage Gets a Speed Boost!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, TotalEnergies
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new surrogate modeling framework for CO2 storage operations that uses
                    a
                    combination of flow-only and coupled flow-geomechanics simulations for training. This approach
                    significantly reduces the computational cost of training compared to previous methods that
                    relied
                    solely on coupled simulations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">
                07:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10635" target="_blank">
                        @arXiv 2408.10635
                    </a>
                    <span class="tweet-title">
                        LLMs Learn to Play Games: A Self-Improving AI That's Smarter Than the Average Bear
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Rensselaer Polytechnic Institute, Shenzhen University, University of California Los
                        Angeles...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces STRATEGIST, a method that uses LLMs to learn strategic skills in
                    multi-agent games. Unlike previous LLM-based approaches, STRATEGIST incorporates a bi-level tree
                    search, allowing it to learn both high-level strategies and low-level action planning through
                    simulated self-play.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">
                07:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10443" target="_blank">
                        @arXiv 2408.10443
                    </a>
                    <span class="tweet-title">
                        Training Giant Speech Models on Your Phone: Federated Learning Gets Real!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google LLC
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on training large Automatic Speech Recognition (ASR) models using
                    Federated
                    Learning (FL), a technique that allows training on user devices without sharing their data. The
                    paper presents a novel approach to improve the quality of FL-trained ASR models by leveraging
                    user-edited transcripts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">
                07:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10491" target="_blank">
                        @arXiv 2408.10491
                    </a>
                    <span class="tweet-title">
                        Sigmoid Squeeze: New Trick Makes Neural Network Verification Faster!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Vermont, University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a new method for tightening the bounds of sigmoid activation functions in
                    neural networks, which are commonly used in verification tasks. Unlike previous approaches that
                    rely
                    on static linear approximations, this method uses tunable hyperplanes that dynamically adjust to
                    achieve the tightest possible element-wise relaxation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">
                08:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10353" target="_blank">
                        @arXiv 2408.10353
                    </a>
                    <span class="tweet-title">
                        ICA Gets a Second Wind: Sparsity Solves Gaussian Source Separation
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new identifiability theory for Independent Component Analysis (ICA)
                    that
                    relies on second-order statistics, unlike traditional methods that assume non-Gaussianity. It
                    achieves this by focusing on the connective structure between sources and observed variables,
                    introducing a novel assumption called "structural variability."
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">
                08:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10599" target="_blank">
                        @arXiv 2408.10599
                    </a>
                    <span class="tweet-title">
                        Vision Calorimeter: Seeing Anti-neutrons with Deep Learning Eyes
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Chinese Academy of Sciences, Peking University, École Polytechnique Fédérale de Lausanne
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method called Vision Calorimeter (ViC) for reconstructing
                    anti-neutrons using deep learning. Unlike conventional methods that rely on clustering
                    algorithms,
                    ViC leverages the contextual information embedded in the energy distribution of anti-neutrons
                    deposited in the electromagnetic calorimeter (EMC).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">
                09:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10733" target="_blank">
                        @arXiv 2408.10733
                    </a>
                    <span class="tweet-title">
                        CNNs and Transformers Team Up to Spot GI Trouble!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Pashchimanchal Campus, Northwestern University
                    </span>
                </div>
                <div class="primary-text">
                    This research combines Convolutional Neural Networks (CNNs) and Transformers, specifically the
                    Swin
                    Transformer, to classify endoscopic images. This approach differs from previous work that
                    primarily
                    relied on CNNs alone.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">
                09:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10920" target="_blank">
                        @arXiv 2408.10920
                    </a>
                    <span class="tweet-title">
                        RNNs Learn to Store Secrets in Onion Layers: A New Twist on Neural Network Representation
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This paper challenges the Linear Representation Hypothesis (LRH) by demonstrating that recurrent
                    neural networks (RNNs) can learn to represent sequences using a non-linear "onion"
                    representation,
                    where information is stored at different magnitudes rather than in distinct linear subspaces.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">
                10:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10392" target="_blank">
                        @arXiv 2408.10392
                    </a>
                    <span class="tweet-title">
                        Teaching AI Ethics Without the Sermon: New Method Aligns LLMs to Values in Unstructured Text
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for aligning LLMs to values embedded in unstructured text,
                    unlike previous approaches that rely heavily on curated datasets or rule-based systems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">
                10:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10463" target="_blank">
                        @arXiv 2408.10463
                    </a>
                    <span class="tweet-title">
                        TTS Overfitting? Not With This Adversarial Trick!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes using adversarial training to prevent keyword spotting (KWS) models from
                    overfitting to synthetic text-to-speech (TTS) data. This approach differs from previous work by
                    focusing on domain adaptation within the KWS domain, specifically addressing the mismatch
                    between
                    TTS and real speech.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">
                10:48
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10556" target="_blank">
                        @arXiv 2408.10556
                    </a>
                    <span class="tweet-title">
                        Honor of Kings: A MOBA Dataset for Offline RL's Big Game
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Tencent
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Hokoff, a new dataset for offline reinforcement learning (RL) and
                    offline
                    multi-agent RL (MARL) based on the popular MOBA game Honor of Kings. Unlike previous datasets,
                    Hokoff focuses on a complex, real-world game environment, offering a more realistic and
                    practical
                    setting for offline RL research.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">
                11:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10646" target="_blank">
                        @arXiv 2408.10646
                    </a>
                    <span class="tweet-title">
                        LLMs: Speaking the Same Language, But Thinking Different Thoughts?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Hebrew University of Jerusalem, Google
                    </span>
                </div>
                <div class="primary-text">
                    This research goes beyond simply looking at whether LLMs can answer the same question correctly
                    in
                    different languages. It investigates whether the models are actually storing and retrieving that
                    knowledge in a shared, language-independent way. To do this, the researchers use knowledge
                    editing
                    techniques to modify the model's understanding of a fact in one language and then observe how
                    that
                    change affects the model's performance in other languages.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">
                11:44
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10805" target="_blank">
                        @arXiv 2408.10805
                    </a>
                    <span class="tweet-title">
                        Lifting 3D Poses from 2D: A Multi-view Magic Trick!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Université Catholique de Louvain, École Polytechnique Fédérale de Lausanne
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to 3D human pose estimation by decoupling the process
                    into
                    two stages: 2D pose estimation and 3D pose lifting. This allows the model to be trained on
                    synthetic
                    2D-3D pose pairs, eliminating the need for real-world images paired with 3D poses, which are
                    often
                    scarce and limited to controlled environments.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">
                12:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11019" target="_blank">
                        @arXiv 2408.11019
                    </a>
                    <span class="tweet-title">
                        Dendrites: The Brain's Secret Superpower for Faster Learning
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Stirling, University of Wolverhampton, University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the role of context-sensitive dendrites in learning, going beyond the
                    traditional focus on feedback connections. It proposes that dendrites integrate diverse
                    contextual
                    information, amplifying relevant signals and suppressing irrelevant ones, leading to faster and
                    more
                    efficient learning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">
                12:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10488" target="_blank">
                        @arXiv 2408.10488
                    </a>
                    <span class="tweet-title">
                        Sign Language Translation Gets a High-Def Upgrade: Event Streams to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Anhui University, Peking University, Chinese Academy of Sciences
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes using high-definition event streams for sign language translation, a
                    departure from traditional methods relying on visible light videos. Event streams offer
                    advantages
                    like high dynamic range, dense temporal signals, and privacy protection due to their spatial
                    sparsity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">
                13:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10291" target="_blank">
                        @arXiv 2408.10291
                    </a>
                    <span class="tweet-title">
                        Chatbots: More Insight, Less Weights, Please!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Italian Institute of Technology, Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This paper argues that the pursuit of ever-increasing weights in large-scale machine learning
                    applications is unsustainable and potentially manipulative. It proposes a shift from focusing on
                    more weights to prioritizing insight and understanding.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">
                13:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10807" target="_blank">
                        @arXiv 2408.10807
                    </a>
                    <span class="tweet-title">
                        Unmixing Music: Scientists Untangle the Secrets of Pitch and Timbre in Multi-Instrument
                        Tracks
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Queen Mary University of London, Sony
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on disentangling pitch and timbre from mixtures of musical instruments, a
                    task
                    not addressed by previous work which primarily focused on single-instrument audio.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">
                14:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10446" target="_blank">
                        @arXiv 2408.10446
                    </a>
                    <span class="tweet-title">
                        AI Watermarks: Paraphrasing Away the Proof!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        NIT Silchar, IIIT Delhi, BITS Pilani...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces the concept of "visual paraphrasing attacks" as a method to circumvent
                    existing image watermarking techniques. Unlike previous work that focused on traditional image
                    manipulation methods, this paper explores how AI-generated images can be subtly altered to
                    remove
                    watermarks while preserving their semantic content.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">
                14:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10943" target="_blank">
                        @arXiv 2408.10943
                    </a>
                    <span class="tweet-title">
                        LLMs on Trial: Can They Follow Instructions Like a Boss?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces SysBench, a benchmark specifically designed to evaluate how well large
                    language models (LLMs) follow system messages. Unlike previous work that focused on single-turn
                    conversations or simplified instructions, SysBench analyzes LLMs' ability to follow complex
                    instructions across multiple turns of conversation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">
                14:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10284" target="_blank">
                        @arXiv 2408.10284
                    </a>
                    <span class="tweet-title">
                        MoE Models Get a Speed Boost: AdapMoE Makes LLMs Run Faster on Edge Devices
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces AdapMoE, a framework that combines adaptive expert gating, prefetching,
                    and
                    caching to optimize MoE inference on edge devices. Unlike previous work, AdapMoE dynamically
                    adjusts
                    the number of activated experts based on sensitivity analysis, improving efficiency without
                    sacrificing accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">
                15:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10906" target="_blank">
                        @arXiv 2408.10906
                    </a>
                    <span class="tweet-title">
                        ShapeSplat: Giving 3D Objects a Gaussian Makeover!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich, INSAIT, Sofia University...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces ShapeSplat, a large-scale dataset of 3D Gaussian Splats, which are 3D
                    representations of objects using Gaussian primitives. This dataset is unique because it focuses
                    on
                    the trained parameters of Gaussian Splats, unlike previous work that primarily focused on
                    rendering
                    quality or scene compression.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">
                15:48
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10263" target="_blank">
                        @arXiv 2408.10263
                    </a>
                    <span class="tweet-title">
                        Fraud Detection: When Spline Interpolation Meets KANs
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cincinnati, Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the application of Kolmogorov–Arnold Networks (KAN) for fraud detection,
                    identifying limitations and proposing a quick assessment method to determine its suitability for
                    a
                    given problem. It also introduces a heuristic approach for selecting KAN hyperparameters,
                    significantly reducing training time compared to traditional grid search methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">
                16:16
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10504" target="_blank">
                        @arXiv 2408.10504
                    </a>
                    <span class="tweet-title">
                        Prompt Engineering Gets a Query-Specific Makeover: Offline RL to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        SenseTime, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on query-dependent prompt optimization, a concept often overlooked in
                    previous
                    work. It utilizes multi-loop offline reinforcement learning to fine-tune a small language model
                    to
                    generate prompts tailored to specific input queries, reducing the need for frequent interactions
                    with the target LLM.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">
                16:44
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11029" target="_blank">
                        @arXiv 2408.11029
                    </a>
                    <span class="tweet-title">
                        Scaling Laws Just Got a Learning Rate Makeover!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new scaling law formulation that incorporates learning rate
                    annealing,
                    allowing for accurate prediction of loss at any training step across various learning rate
                    schedules. Unlike previous scaling laws that only describe the final loss, this new formulation
                    captures the dynamics of the entire training process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">
                17:10
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10474" target="_blank">
                        @arXiv 2408.10474
                    </a>
                    <span class="tweet-title">
                        LLMs Under the Microscope: New Criteria for Testing Their Trustworthiness
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Alberta, The University of Tokyo, New York University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a set of multi-level testing criteria, LECOV, for LLMs, focusing on three
                    internal components: attention mechanism, feed-forward neurons, and uncertainty. This approach
                    differs from previous work by going beyond input/output analysis and examining the internal
                    workings
                    of LLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">
                17:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10811" target="_blank">
                        @arXiv 2408.10811
                    </a>
                    <span class="tweet-title">
                        LLMs: Do They Think in English, Even When Speaking Japanese?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Kyoto University, National Institute of Informatics, University of Tokyo
                    </span>
                </div>
                <div class="primary-text">
                    This research investigates the internal "latent language" of multilingual language models (LLMs)
                    by
                    analyzing the probability distribution of tokens in their intermediate layers. Unlike previous
                    work
                    that focused on English-centric models, this study examines models trained on Japanese data and
                    explores how their internal language representation influences their performance on tasks
                    involving
                    different languages.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">
                18:10
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11049" target="_blank">
                        @arXiv 2408.11049
                    </a>
                    <span class="tweet-title">
                        MagicDec: Decoding Long Texts Faster, Without Sacrificing Accuracy!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research challenges the conventional wisdom that speculative decoding is less effective for
                    large batch sizes. It shows that for moderate to long sequences, speculative decoding can
                    actually
                    improve throughput and reduce latency without compromising accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">
                18:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10668" target="_blank">
                        @arXiv 2408.10668
                    </a>
                    <span class="tweet-title">
                        LLMs: Safe? Not So Fast! Decoding the Hidden Danger
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Tencent
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the safety boundaries of LLMs by focusing on the decoding process rather
                    than
                    the input or parameter space. It proposes a novel approach using a Cost Value Model (CVM) to
                    identify potential harmful paths within the decoding process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">
                19:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10987" target="_blank">
                        @arXiv 2408.10987
                    </a>
                    <span class="tweet-title">
                        Ultrasound Denoising Gets a Diffusion Makeover: From Noisy to Nifty!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Concordia University, University of Toronto, Institut de Recherche en Informatique de
                        Toulouse
                    </span>
                </div>
                <div class="primary-text">
                    This research adapts Denoising Diffusion Probabilistic Models (DDPMs) to enhance the quality of
                    plane wave ultrasound images. Unlike previous work that focused on denoising B-mode images, this
                    study directly denoises the radiofrequency (RF) data, which is the raw signal captured by the
                    ultrasound transducer.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">
                19:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10683" target="_blank">
                        @arXiv 2408.10683
                    </a>
                    <span class="tweet-title">
                        Rejection in Argumentation: It's Harder to Say No Than Yes!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Linköping University, Massachusetts Institute of Technology, Universität Paderborn...
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces the concept of "rejection conditions" in abstract argumentation
                    frameworks.
                    Unlike previous work that focused on acceptance conditions, this research explores how arguments
                    can
                    be rejected based on specific constraints.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">
                19:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10669" target="_blank">
                        @arXiv 2408.10669
                    </a>
                    <span class="tweet-title">
                        Tensor Trees: Unveiling Hidden Relationships in Data, One Branch at a Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Kyoto University, University of Tokyo
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for constructing generative models using a tensor tree
                    network.
                    The key innovation lies in dynamically optimizing the tree structure by minimizing the bond
                    mutual
                    information, which measures the information flow between different parts of the network. This
                    approach differs from previous work that used fixed tensor network structures, such as tensor
                    trains
                    or balanced trees.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">
                20:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10264" target="_blank">
                        @arXiv 2408.10264
                    </a>
                    <span class="tweet-title">
                        Shrinking Big Data: How to Squeeze Scientific Data Without Losing the Good Stuff
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington, Pacific Northwest National Laboratory
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new method called Order-Preserving Dimension Reduction (OPDR) to reduce
                    the
                    dimensionality of embedding vectors generated by large language models while preserving the
                    relationships between data points. This differs from previous work that focused on preserving
                    pairwise distances or local neighborhoods, but not the specific order of nearest neighbors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">
                20:48
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10436" target="_blank">
                        @arXiv 2408.10436
                    </a>
                    <span class="tweet-title">
                        Graph Inverse Problems: When Data Hides, GNNs Come to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge, University of British Columbia
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a framework for solving Graph Inverse Problems (GRIP) by integrating
                    learned regularization techniques with Graph Neural Networks (GNNs). Unlike previous work, this
                    approach leverages meta-data associated with the graph, which can be used to improve the
                    accuracy of
                    the solution.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">
                21:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11042" target="_blank">
                        @arXiv 2408.11042
                    </a>
                    <span class="tweet-title">
                        Teaching Machines to Think Like Algorithms: GraphFSA Learns the Rules of the Game!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces GraphFSA, a framework for learning finite state automata on graphs. Unlike
                    previous work that focuses on continuous state spaces, GraphFSA uses discrete states, making it
                    easier to interpret and visualize the learned rules.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">
                21:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10946" target="_blank">
                        @arXiv 2408.10946
                    </a>
                    <span class="tweet-title">
                        LLMs: The New Chatty Recommender System?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Meta, Google
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the use of large language models (LLMs) in recommendation systems,
                    focusing
                    on how LLMs can be used to generate personalized recommendations and explanations through
                    natural
                    language interactions. It differs from previous work by emphasizing the use of LLMs for general
                    reasoning and conversational recommendation, rather than solely relying on standardized user
                    feedback like purchases or clicks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">
                22:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10383" target="_blank">
                        @arXiv 2408.10383
                    </a>
                    <span class="tweet-title">
                        BrewCLIP: When Speech Gets a Second Wind, Images Get a Better Match!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a "bifurcated" model, meaning it uses two separate channels to process
                    audio:
                    one for transcription and another for raw audio. This allows the model to capture both textual
                    and
                    non-textual information from speech, which is often overlooked by traditional methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">
                22:34
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10689" target="_blank">
                        @arXiv 2408.10689
                    </a>
                    <span class="tweet-title">
                        Genesis: A Robot Scientist Aims to Crack the Code of Life
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge, University of Manchester
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on developing a robot scientist called Genesis, designed to automate the
                    process of improving complex systems biology models. Unlike previous robot scientists, Genesis
                    aims
                    to handle models with thousands of interacting components, a significant leap in complexity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">
                23:01
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10818" target="_blank">
                        @arXiv 2408.10818
                    </a>
                    <span class="tweet-title">
                        Transformers Go Rogue: Learning Randomized Algorithms with Deep Neural Networks
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google, ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel training objective for transformers that encourages the
                    emergence
                    of randomized algorithms within the model's architecture. Unlike previous work that focuses on
                    deterministic strategies, this approach leverages randomness as a key component of the learning
                    process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">
                23:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10524" target="_blank">
                        @arXiv 2408.10524
                    </a>
                    <span class="tweet-title">
                        Speech Recognition Gets Bilingual: A New Trick for Understanding Code-Switching
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Huawei
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel Cross-lingual Contextual Biasing (XCB) module to improve speech
                    recognition accuracy for phrases in a secondary language within code-switching scenarios. Unlike
                    previous approaches that rely on large-scale training data or separate language-specific
                    representations, XCB leverages a lightweight module and a language-specific loss function to
                    enhance
                    the learning of representations for the secondary language.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">
                23:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10871" target="_blank">
                        @arXiv 2408.10871
                    </a>
                    <span class="tweet-title">
                        Radio U-Net: A Deep Learning Recipe for Finding Cosmic Radio Sources
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        INAF, University of Bologna, Hamburger Sternwarte
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Radio U-Net, a convolutional neural network based on the U-Net
                    architecture, specifically designed to detect faint and extended radio sources in radio surveys.
                    This approach differs from previous work by utilizing synthetic radio observations built upon
                    cosmological simulations for training, allowing for accurate detection even in low-quality
                    images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">
                24:21
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10711" target="_blank">
                        @arXiv 2408.10711
                    </a>
                    <span class="tweet-title">
                        LLMs: Are They as Biased as We Are?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University College London, University of Surrey, University of Southampton...
                    </span>
                </div>
                <div class="primary-text">
                    This research investigates whether large language models (LLMs) exhibit the same order bias as
                    humans when making similarity judgments. Unlike previous work that focused on replicating human
                    biases in LLMs, this study specifically examines the context-sensitive nature of similarity
                    judgments.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">
                24:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10368" target="_blank">
                        @arXiv 2408.10368
                    </a>
                    <span class="tweet-title">
                        Deep Learning: The New Wall Street Wizard?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto, York University
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces Deep-MacroFin, a framework that uses deep learning to solve partial
                    differential equations (PDEs) in continuous-time economic models. Unlike traditional numerical
                    methods, Deep-MacroFin can handle higher-dimensional problems and offers a more user-friendly
                    implementation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">
                25:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10351" target="_blank">
                        @arXiv 2408.10351
                    </a>
                    <span class="tweet-title">
                        Doomscrolling Teens: Algorithms, Anxiety, and a Call to Action
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Dakota State University
                    </span>
                </div>
                <div class="primary-text">
                    This research goes beyond simply highlighting the negative impacts of social media on teenagers.
                    It
                    delves into the specific role of algorithms and AI in shaping these impacts, calling for a
                    multi-pronged approach involving government regulation, industry initiatives, and educational
                    programs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">
                25:40
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10693" target="_blank">
                        @arXiv 2408.10693
                    </a>
                    <span class="tweet-title">
                        Chaos &amp; Quanta: Feature Selection Gets a Quantum Boost!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to feature selection by incorporating chaos-generated
                    variables into quantum-inspired metaheuristics. This differs from previous work by utilizing the
                    Lyapunov exponent to ensure the algorithm operates in a truly chaotic regime.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">
                26:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10557" target="_blank">
                        @arXiv 2408.10557
                    </a>
                    <span class="tweet-title">
                        Speech Models: "What" vs. "How" - A New Way to Learn!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Indraprastha Institute of Information Technology Delhi, Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a modified version of the HuBERT model, called Other HuBERT (O-HuBERT),
                    which
                    uses separate learnable parameters to model "other" information (like speaker characteristics)
                    in
                    speech, distinct from the "content" information (what is being said). This approach differs from
                    previous work that typically uses a single set of parameters to encode both types of
                    information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">
                26:42
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11017" target="_blank">
                        @arXiv 2408.11017
                    </a>
                    <span class="tweet-title">
                        Committee Chaos: When Voters Change Their Minds, Who Gets the Boot?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford, Hasso Plattner Institute, Alan Turing Institute
                    </span>
                </div>
                <div class="primary-text">
                    This research explores how winning committees in multiwinner elections adapt to changes in voter
                    preferences, focusing on a specific class of voting rules called Thiele rules. Unlike previous
                    work,
                    this study considers the scenario where the second-stage preferences are unknown at the time of
                    the
                    first-stage election.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">
                27:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10595" target="_blank">
                        @arXiv 2408.10595
                    </a>
                    <span class="tweet-title">
                        Game On! When Learning Syncs with Change, Chaos Reigns!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CyberAgent, UBI The University of Tokyo, RCIES Soken University...
                    </span>
                </div>
                <div class="primary-text">
                    This research explores learning in games where the rules change periodically, unlike previous
                    studies that focused on games with fixed rules. It discovers that when the speed of learning
                    matches
                    the speed of the game's change, the learning process diverges from the optimal strategy, leading
                    to
                    unpredictable outcomes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">
                27:34
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11032" target="_blank">
                        @arXiv 2408.11032
                    </a>
                    <span class="tweet-title">
                        AI Takes the Wheel: Neural Networks Drive CO2 Transport Modeling
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Max Planck Society
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces CarbonBench, a new dataset specifically designed for training machine
                    learning models to emulate atmospheric CO2 transport. It also explores the use of
                    SwinTransformer, a
                    deep neural network architecture, for this task, and demonstrates its ability to achieve stable
                    and
                    mass-conserving transport for over 6 months.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">
                28:03
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10270" target="_blank">
                        @arXiv 2408.10270
                    </a>
                    <span class="tweet-title">
                        AI Alignment: Is Your Robot Really Listening?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces new metrics to evaluate the effectiveness of aligning language models
                    with
                    human values. It goes beyond simply measuring how well a model performs on a task and delves
                    into
                    the internal mechanisms of how the model learns and represents those values.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">
                28:30
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10935" target="_blank">
                        @arXiv 2408.10935
                    </a>
                    <span class="tweet-title">
                        Point Clouds to 3D: A New Way to Generate Realistic Objects from Images
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach for image-to-3D generation that utilizes point clouds as
                    input to generate 3D Gaussian parameters, instead of directly mapping 2D image features to 3D
                    Gaussian representations. This method leverages the geometric priors provided by point clouds,
                    which
                    are generated from a pre-trained 3D diffusion model, to facilitate the learning of 3D Gaussian
                    representations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">
                28:58
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10500" target="_blank">
                        @arXiv 2408.10500
                    </a>
                    <span class="tweet-title">
                        Emotion-LLaMA Gets a Boost: Conv-Attention Makes Multimodal Emotion Recognition Smarter!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Shenzhen University, Shenzhen Technology University, CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Conv-Attention, a hybrid framework that combines convolutional and
                    attention mechanisms for multimodal feature fusion. This approach differs from previous work by
                    mitigating the limitations of each individual method, leading to improved performance,
                    especially
                    when dealing with limited data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon" title="Play from here">
                29:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10808" target="_blank">
                        @arXiv 2408.10808
                    </a>
                    <span class="tweet-title">
                        Telecom Talk: Tiny AI Gets Big Brains for Tech Questions
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on improving the performance of smaller language models (LLMs) on
                    domain-specific question answering tasks, specifically in the telecommunications field. The
                    authors
                    achieve this by combining a retrieval-augmented generation (RAG) pipeline with a novel ensemble
                    scoring system for responses. This approach differs from previous work by focusing on enhancing
                    the
                    capabilities of smaller LLMs, rather than solely relying on larger, more resource-intensive
                    models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon" title="Play from here">
                29:58
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10258" target="_blank">
                        @arXiv 2408.10258
                    </a>
                    <span class="tweet-title">
                        Ultrasound in the Wild: NeRF-US Removes Artifacts and Makes 3D Imaging a Breeze!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces NeRF-US, a novel approach that incorporates 3D geometric guidance into
                    NeRF
                    training for ultrasound imaging. Unlike previous methods, NeRF-US utilizes a diffusion model to
                    learn priors for border probability and scattering density, resulting in artifact-free
                    reconstructions from casually captured ultrasound data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon" title="Play from here">
                30:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10437" target="_blank">
                        @arXiv 2408.10437
                    </a>
                    <span class="tweet-title">
                        AI's Secret Code: Unmasking the Hidden Language of Generative Models
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT, Pacific Northwest National Laboratory, Rutgers University
                    </span>
                </div>
                <div class="primary-text">
                    This research uses pre-trained deep neural networks (DNNs) as feature extractors to analyze the
                    internal representations of data, called embeddings. This approach allows for the identification
                    of
                    latent biases and high-level information within unstructured data, such as text and images.
                    Unlike
                    previous work that focused on hand-crafted features, this study leverages the automated feature
                    engineering capabilities of DNNs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet67">
            <div class="start-time-icon" title="Play from here">
                30:42
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10872" target="_blank">
                        @arXiv 2408.10872
                    </a>
                    <span class="tweet-title">
                        AI Road Safety Auditors: Can Language Models Replace Human Inspectors?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University College London, Chulalongkorn University
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the use of Vision Language Models (VLMs) for road safety assessment, a
                    departure from traditional Convolutional Neural Networks (CNNs) that require extensive training
                    data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet68">
            <div class="start-time-icon" title="Play from here">
                31:04
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10239" target="_blank">
                        @arXiv 2408.10239
                    </a>
                    <span class="tweet-title">
                        Evaluating AI: It's Not Just About the Model, It's About the Test!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU, Northwestern University, Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the ethical implications of evaluating machine learning systems, a
                    topic
                    often overlooked in Responsible AI discussions. It proposes a utility framework that balances
                    information gain from an evaluation against potential ethical harms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet69">
            <div class="start-time-icon" title="Play from here">
                31:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10240" target="_blank">
                        @arXiv 2408.10240
                    </a>
                    <span class="tweet-title">
                        Blind Artists, Meet Your New Muse: AI-Powered Tile-Based Image Editor
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, University of Michigan
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces AltCanvas, a tile-based image editor that integrates generative AI with
                    a
                    constructive approach, providing users with enhanced control and editing capabilities. Unlike
                    previous text-to-image tools that lack precise control over image composition, AltCanvas allows
                    users to build images incrementally, adding, editing, and arranging objects while receiving
                    speech
                    and audio feedback.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet70">
            <div class="start-time-icon" title="Play from here">
                31:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10619" target="_blank">
                        @arXiv 2408.10619
                    </a>
                    <span class="tweet-title">
                        Change Detection: Stable Diffusion Makes Remote Sensing See the Light!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of California Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel change detection framework that combines Stable Diffusion models
                    with
                    the Structural Similarity Index (SSIM) to improve accuracy and interpretability. This approach
                    differs from previous work by leveraging the generative capabilities of diffusion models to
                    model
                    changes between images, rather than relying solely on pixel-wise differences.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet71">
            <div class="start-time-icon" title="Play from here">
                32:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10720" target="_blank">
                        @arXiv 2408.10720
                    </a>
                    <span class="tweet-title">
                        MLP-Mixer: The New Kid on the Block for Chemical Kinetics Forecasting
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        IBM, University of Surrey
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes using a novel MLP-Mixer architecture, PatchTSMixer, to model the
                    time-series
                    evolution of stiff chemical kinetics. This approach differs from previous work by utilizing a
                    multi-layer perceptron mixer architecture instead of traditional numerical techniques.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet72">
            <div class="start-time-icon" title="Play from here">
                32:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10665" target="_blank">
                        @arXiv 2408.10665
                    </a>
                    <span class="tweet-title">
                        Point Cloud Compression: A Deep Dive into Attribute Compression with a Twist!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Friedrich-Alexander-Universität Erlangen-Nürnberg
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces an end-to-end learned dynamic lossy attribute coding approach for point
                    cloud compression, utilizing a high-dimensional convolution to capture inter-point dependencies.
                    This differs from previous methods that primarily focused on geometry compression or relied on
                    geometry-dependent transformations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet73">
            <div class="start-time-icon" title="Play from here">
                33:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.10845" target="_blank">
                        @arXiv 2408.10845
                    </a>
                    <span class="tweet-title">
                        Self-Driving Cars Get Chatty: New Dataset Teaches AI to Talk and Drive!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Turing Inc., University of Tokyo, University of Tsukuba...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces CoVLA, a dataset for autonomous driving that combines vision, language,
                    and
                    action data. Unlike previous datasets, CoVLA uses automated methods to generate captions and
                    trajectories, making it more scalable and comprehensive.
                </div>
            </div>
        </div>


        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202408211831_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>

</html>