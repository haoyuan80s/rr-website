
daily_data = {
    "date": "2024-10-25",
    "tweets": [
        
        {
            "startTime": "01:03",
            "arxivId": "2410.18923",
            "arxivLink": "https://arxiv.org/abs/2410.18923",
            "title": "SegLLM: Chatting with Images, One Mask at a Time!",
            "institute": "UC Berkeley, UCLA, Panasonic AI Research...",
            "text": "This research introduces SegLLM, a model that remembers past interactions with images, including the masks it generated. This allows it to understand complex queries that refer to previously segmented objects, unlike previous models that only work on single-round tasks.",
            "paper-title": "SegLLM: Multi-round Reasoning Segmentation",
            "image-path": "flux_paper_image/2410.18923_1729884292.png"
        },

        {
            "startTime": "01:33",
            "arxivId": "2410.18404",
            "arxivLink": "https://arxiv.org/abs/2410.18404",
            "title": "Privacy by the Numbers: A Bayesian Twist on Data Protection",
            "institute": "Rice University, University of California Berkeley, INRIA",
            "text": "This paper introduces a Bayesian Coordinate Differential Privacy (BCDP) framework, which allows for feature-specific privacy quantification. Unlike traditional LDP, which applies uniform protection to all data features, BCDP adjusts privacy protection based on the sensitivity of each feature.",
            "paper-title": "Enhancing Feature-Specific Data Protection via Bayesian Coordinate Differential Privacy",
            "image-path": "flux_paper_image/2410.18404_1729885989.png"
        },

        {
            "startTime": "02:02",
            "arxivId": "2410.18861",
            "arxivLink": "https://arxiv.org/abs/2410.18861",
            "title": "Open-Source LLMs Get Watermarked: A New Kind of AI Proof",
            "institute": "Columbia University, UC Berkeley, Google",
            "text": "This paper introduces a watermarking scheme for open-source LLMs that modifies the model's weights, unlike previous approaches that focused on altering the sampling algorithm.",
            "paper-title": "Provably Robust Watermarks for Open-Source Language Models",
            "image-path": "flux_paper_image/2410.18861_1729886116.png"
        },

        {
            "startTime": "02:23",
            "arxivId": "2410.18469",
            "arxivLink": "https://arxiv.org/abs/2410.18469",
            "title": "LLMs Gone Rogue: New AI Tool Makes Jailbreaking Easier Than Ever!",
            "institute": "UC San Diego, University of Virginia, Microsoft",
            "text": "This research introduces ADV-LLM, an iterative self-tuning process that crafts adversarial LLMs with enhanced jailbreak ability. Unlike previous methods that rely on expensive search algorithms, ADV-LLM learns from its own generated data, significantly reducing computational costs and achieving high attack success rates.",
            "paper-title": "Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities",
            "image-path": "flux_paper_image/2410.18469_1729884668.png"
        },

        {
            "startTime": "02:52",
            "arxivId": "2410.18234",
            "arxivLink": "https://arxiv.org/abs/2410.18234",
            "title": "Multi-Draft Speculative Sampling: It's Like Having Multiple Brains for Text Generation!",
            "institute": "University of Toronto, Qualcomm",
            "text": "This research introduces a new approach to multi-draft speculative sampling, where the optimal token selection scheme is decomposed into two steps: importance sampling followed by single-draft speculative sampling. This differs from previous work by providing a more efficient and effective method for maximizing the acceptance probability of draft tokens.",
            "paper-title": "Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits",
            "image-path": "flux_paper_image/2410.18234_1729885832.png"
        },

        {
            "startTime": "03:18",
            "arxivId": "2410.18856",
            "arxivLink": "https://arxiv.org/abs/2410.18856",
            "title": "LLMs for Medicine: A Primer to Tame the AI Beast!",
            "institute": "University of Pennsylvania, University of California San Diego, University of Maryland...",
            "text": "This paper provides a structured framework for using large language models (LLMs) in medicine, addressing the lack of practical guidelines for their application in healthcare. It goes beyond simply describing LLMs and focuses on actionable steps for healthcare professionals to integrate these tools into their workflows.",
            "paper-title": "Demystifying Large Language Models for Medicine: A Primer",
            "image-path": "flux_paper_image/2410.18856_1729884428.png"
        },

        {
            "startTime": "03:37",
            "arxivId": "2410.18932",
            "arxivLink": "https://arxiv.org/abs/2410.18932",
            "title": "Robots Get a New Ear for Quiet Navigation",
            "institute": "Carnegie Mellon University",
            "text": "This research focuses on predicting how loud a robot's actions will be at a listener's location, using visual information about the environment. Unlike previous work that focused on finding the source of a sound, this paper explores how robots can be aware of the noise they generate.",
            "paper-title": "ANAVI: Audio Noise Awareness using Visuals of Indoor environments for NAVIgation",
            "image-path": "flux_paper_image/2410.18932_1729884636.png"
        },

        {
            "startTime": "04:00",
            "arxivId": "2410.18200",
            "arxivLink": "https://arxiv.org/abs/2410.18200",
            "title": "Forget \"Similar\" - This AI Learns From Totally Unrelated Things!",
            "institute": "Surrey Institute for People-Centred AI, Carnegie Mellon University, Jiangnan University",
            "text": "This research challenges the traditional assumption in contrastive learning that positive pairs must be closely related. Instead, it proposes a method to learn from arbitrary pairs, even those that seem completely unrelated, like a snake and a lamp. This is achieved by creating subspaces for each pair, allowing the model to focus on shared features that might not be obvious to humans.",
            "paper-title": "Rethinking Positive Pairs in Contrastive Learning",
            "image-path": "flux_paper_image/2410.18200_1729886295.png"
        },

        {
            "startTime": "04:28",
            "arxivId": "2410.18979",
            "arxivLink": "https://arxiv.org/abs/2410.18979",
            "title": "PixelGaussian: Giving 3D Gaussians a Makeover for Better Views!",
            "institute": "Tsinghua University",
            "text": "This paper introduces PixelGaussian, a method that dynamically adjusts the number and distribution of 3D Gaussians based on the complexity of the scene, unlike previous methods that use a fixed number of Gaussians per pixel.",
            "paper-title": "PixelGaussian: Generalizable 3D Gaussian Reconstruction from Arbitrary Views",
            "image-path": "flux_paper_image/2410.18979_1729884804.png"
        },

        {
            "startTime": "04:49",
            "arxivId": "2410.18117",
            "arxivLink": "https://arxiv.org/abs/2410.18117",
            "title": "FedAda2: Adaptive Learning Without the Communication Hangover!",
            "institute": "University of Chicago, Columbia University, Google",
            "text": "This paper introduces FedAda2, a new class of adaptive algorithms for federated learning that avoids transmitting preconditioners between the server and clients, thereby improving communication efficiency.",
            "paper-title": "Efficient Adaptive Federated Optimization",
            "image-path": "flux_paper_image/2410.18117_1729886190.png"
        },

        {
            "startTime": "05:15",
            "arxivId": "2410.18210",
            "arxivLink": "https://arxiv.org/abs/2410.18210",
            "title": "Multilingual LLMs: One Bad Apple Spoils the Whole Bunch!",
            "institute": "University of Illinois, Meta",
            "text": "This research investigates the cross-lingual vulnerability of fine-tuning attacks against multilingual LLMs. Unlike previous work that focused solely on English, this study demonstrates that fine-tuning attacks in one language can compromise the safety alignment of other languages within the same model.",
            "paper-title": "Towards Understanding the Fragility of Multilingual LLMs against Fine-Tuning Attacks",
            "image-path": "flux_paper_image/2410.18210_1729886554.png"
        },

        {
            "startTime": "05:38",
            "arxivId": "2410.18779",
            "arxivLink": "https://arxiv.org/abs/2410.18779",
            "title": "Tiny Teachers, Big Brains: How Small Language Models Can Train Giant Ones Faster!",
            "institute": "Google, Google DeepMind",
            "text": "This research explores a novel approach to training large language models (LLMs) by leveraging smaller, less powerful language models (SLMs) as teachers. Unlike traditional knowledge distillation where a larger, more capable model acts as the teacher, this paper investigates the potential of using SLMs to guide the training of LLMs, particularly in the early stages.",
            "paper-title": "A Little Help Goes a Long Way: Efficient LLM Training by Leveraging Small LMs",
            "image-path": "flux_paper_image/2410.18779_1729886542.png"
        },

        {
            "startTime": "06:02",
            "arxivId": "2410.18907",
            "arxivLink": "https://arxiv.org/abs/2410.18907",
            "title": "Robot Skills on Autopilot: New System Makes Robot Learning Easier!",
            "institute": "NVIDIA",
            "text": "This research introduces SkillMimicGen (SkillGen), a system that automatically generates demonstration datasets for robot manipulation tasks by decomposing them into motion and skill segments. This differs from previous work by focusing on adapting and replaying only the skill segments, which are contact-rich interactions with objects, while using motion planning for the rest of the task.",
            "paper-title": "SkillMimicGen: Automated Demonstration Generation for Efficient Skill Learning and Deployment",
            "image-path": "flux_paper_image/2410.18907_1729886727.png"
        },

        {
            "startTime": "06:19",
            "arxivId": "2410.18122",
            "arxivLink": "https://arxiv.org/abs/2410.18122",
            "title": "Yesterday's News, Today's Lies: A New Benchmark for Misinformation Detection",
            "institute": "University of Amsterdam, University of Cambridge",
            "text": "This research introduces a new benchmark dataset called \"misinfo-general\" specifically designed to evaluate the ability of misinformation detection models to generalize to unseen data. Unlike previous datasets, this one focuses on six axes of generalisation: time, event, topic, publisher, political bias, and misinformation type.",
            "paper-title": "Yesterday's News: Benchmarking Multi-Dimensional Out-of-Distribution Generalisation of Misinformation Detection Models",
            "image-path": "flux_paper_image/2410.18122_1729886310.png"
        },

        {
            "startTime": "06:40",
            "arxivId": "2410.18209",
            "arxivLink": "https://arxiv.org/abs/2410.18209",
            "title": "SLMs Get Smart: Self-Correcting AI Without Big Brother LLMs",
            "institute": "University of Washington, Microsoft",
            "text": "This research introduces CORRECTIONLM, a framework that enables small language models (SLMs) to self-correct using in-context exemplars without relying on large language models (LLMs). This approach differs from previous work that heavily relied on LLMs for feedback and refinement.",
            "paper-title": "CorrectionLM: Self-Corrections with SLM for Dialogue State Tracking",
            "image-path": "flux_paper_image/2410.18209_1729886401.png"
        },

        {
            "startTime": "07:06",
            "arxivId": "2410.18889",
            "arxivLink": "https://arxiv.org/abs/2410.18889",
            "title": "LLMs: Not Just Smart, They're Also Super Sleuths!",
            "institute": "Technion - Institute of Technology, Google Research",
            "text": "This research explores the use of large language models (LLMs) to detect label errors in existing NLP datasets. Unlike previous work that focused on LLMs as annotators, this study leverages LLMs to identify potential mislabeled examples, which can then be corrected by human experts.",
            "paper-title": "Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance",
            "image-path": "flux_paper_image/2410.18889_1729886211.png"
        },

        {
            "startTime": "07:23",
            "arxivId": "2410.18928",
            "arxivLink": "https://arxiv.org/abs/2410.18928",
            "title": "Learning Hamiltonians: A Sparse, Non-Local, and Speedy Approach",
            "institute": "California Institute of Technology, Virginia Tech, Phasecraft Inc....",
            "text": "This research introduces a new Hamiltonian learning protocol that leverages compressed sensing techniques. Unlike previous methods, it doesn't require geometric locality or relaxed locality conditions, making it applicable to a wider range of Hamiltonians.",
            "paper-title": "Learning $k$-body Hamiltonians via compressed sensing",
            "image-path": "flux_paper_image/2410.18928_1729884899.png"
        },

        {
            "startTime": "07:57",
            "arxivId": "2410.18312",
            "arxivLink": "https://arxiv.org/abs/2410.18312",
            "title": "AI Hackers Beware: Honeypots Now Fight Back with Prompt Injections!",
            "institute": "University of Cambridge",
            "text": "This research differs from previous work by evaluating the cyber capabilities of downloadable foundation models, which are more difficult to govern than proprietary models. It also introduces a novel countermeasure technique called Defensive Prompt Injection (DPI) to combat AI-powered cyberattacks.",
            "paper-title": "Countering Autonomous Cyber Threats",
            "image-path": "flux_paper_image/2410.18312_1729884740.png"
        },

        {
            "startTime": "08:22",
            "arxivId": "2410.18904",
            "arxivLink": "https://arxiv.org/abs/2410.18904",
            "title": "Weather Forecasting Gets a Time Machine: Modulated AFNO Interpolates for Smoother Predictions",
            "institute": "Nvidia",
            "text": "This research introduces a novel \"Modulated Adaptive Fourier Neural Operator\" (ModAFNO) layer that modifies the existing AFNO architecture to enable temporal interpolation of weather data. This approach allows for the reconstruction of weather states at higher temporal resolutions, filling in the gaps between coarser time steps.",
            "paper-title": "Modulated Adaptive Fourier Neural Operators for Temporal Interpolation of Weather Forecasts",
            "image-path": "flux_paper_image/2410.18904_1729885710.png"
        },

        {
            "startTime": "08:46",
            "arxivId": "2410.18975",
            "arxivLink": "https://arxiv.org/abs/2410.18975",
            "title": "Unleashing the Infinite: A Generative Video Game That Never Ends",
            "institute": "Google, The University of North Carolina at Chapel Hill",
            "text": "This research introduces the concept of a \"generative infinite game,\" where game mechanics, narratives, and visuals are generated by AI models in real-time, transcending the limitations of traditional, hard-coded systems. This differs from previous work by leveraging recent advancements in generative AI to create a truly open-ended and dynamic gaming experience.",
            "paper-title": "Unbounded: A Generative Infinite Game of Character Life Simulation",
            "image-path": "flux_paper_image/2410.18975_1729885418.png"
        },

        {
            "startTime": "09:16",
            "arxivId": "2410.18858",
            "arxivLink": "https://arxiv.org/abs/2410.18858",
            "title": "Bilinear Sequence Regression: A Model That's Not Just Linearly Awesome!",
            "institute": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, Universita Bocconi, ETH Zurich",
            "text": "This research introduces a new model called \"Bilinear Sequence Regression\" (BSR) for learning from long sequences of high-dimensional data. Unlike previous work that focused on vectorizing data, BSR explicitly models the sequential nature of the data, allowing for more efficient learning.",
            "paper-title": "Bilinear Sequence Regression: A Model for Learning from Long Sequences of High-dimensional Tokens",
            "image-path": "flux_paper_image/2410.18858_1729885435.png"
        },

        {
            "startTime": "09:40",
            "arxivId": "2410.18956",
            "arxivLink": "https://arxiv.org/abs/2410.18956",
            "title": "Unposed Images to Semantic 3D: A Transformer's Tale of 3D Reconstruction",
            "institute": "UT Austin, NVIDIA Research, XMU...",
            "text": "This research introduces a unified framework called Large Spatial Model (LSM) that directly processes unposed RGB images into semantic radiance fields, eliminating the need for traditional multi-stage approaches like Structure-from-Motion (SfM). LSM simultaneously estimates geometry, appearance, and semantics in a single feed-forward pass, enabling real-time semantic 3D reconstruction.",
            "paper-title": "Large Spatial Model: End-to-end Unposed Images to Semantic 3D",
            "image-path": "flux_paper_image/2410.18956_1729886698.png"
        },

        {
            "startTime": "10:04",
            "arxivId": "2410.18701",
            "arxivLink": "https://arxiv.org/abs/2410.18701",
            "title": "LLM Inference: Baton's Relay Race for Faster Results!",
            "institute": "Peking University",
            "text": "This research proposes Baton, a new method for batch-wise inference of LLMs that dynamically adjusts the processing batch to minimize idle computations. Unlike previous methods that replicate model structures or rely on padding, Baton uses vector shaping and embedding strategies to achieve seamless query insertion and removal, resulting in near-zero idle computations.",
            "paper-title": "BATON: Enhancing Batch-wise Inference Efficiency for Large Language Models via Dynamic Re-batching",
            "image-path": "flux_paper_image/2410.18701_1729886915.png"
        },

        {
            "startTime": "10:30",
            "arxivId": "2410.18881",
            "arxivLink": "https://arxiv.org/abs/2410.18881",
            "title": "One-Step Text-to-Image Models Get a Human Touch: Diff-Instruct++ Makes AI Art More Aligned with Our Tastes!",
            "institute": "Peking University",
            "text": "This research introduces Diff-Instruct++, a method for aligning one-step text-to-image generator models with human preferences. Unlike previous work that focused on aligning diffusion models, Diff-Instruct++ directly targets the one-step generator, offering a more efficient and potentially faster approach.",
            "paper-title": "Diff-Instruct++: Training One-step Text-to-image Generator Model to Align with Human Preferences",
            "image-path": "flux_paper_image/2410.18881_1729885405.png"
        },

        {
            "startTime": "10:55",
            "arxivId": "2410.18111",
            "arxivLink": "https://arxiv.org/abs/2410.18111",
            "title": "Data Diet for Big Recommendation Models: How to Feed Them Less and Get More!",
            "institute": "Google",
            "text": "This research focuses on optimizing the training data requirements for large recommendation models (LRMs). Unlike previous work that primarily focused on scaling up data and model size, this paper explores strategies to reduce data volume while maintaining model performance. It introduces concepts like continuous downsampling and distillation to accelerate model convergence and achieve greater efficiency.",
            "paper-title": "Data Efficiency for Large Recommendation Models",
            "image-path": "flux_paper_image/2410.18111_1729886225.png"
        },

        {
            "startTime": "11:12",
            "arxivId": "2410.18094",
            "arxivLink": "https://arxiv.org/abs/2410.18094",
            "title": "ECG's Got Rhythm: New AI Learns to Spot Atrial Fibrillation Like a Pro!",
            "institute": "Tsinghua University",
            "text": "This research introduces a novel self-supervised learning approach for ECG representation that incorporates medical knowledge about atrial fibrillation. Unlike previous methods, it focuses on both inter-period and intra-period representations, aiming to capture the irregularity in RR intervals and the absence of P-waves.",
            "paper-title": "Self-supervised inter\u2013intra period-aware ECG representation learning for detecting atrial fibrillation",
            "image-path": "flux_paper_image/2410.18094_1729885117.png"
        },

        {
            "startTime": "11:34",
            "arxivId": "2410.18737",
            "arxivLink": "https://arxiv.org/abs/2410.18737",
            "title": "Diffusion Models: Guidance Rectified, Expectations Aligned",
            "institute": "Tsinghua University, Ant Group, Shanghai Jiao Tong University...",
            "text": "This research addresses a theoretical flaw in Classifier-Free Guidance (CFG), a technique used for conditional generation in diffusion models. CFG combines conditional and unconditional score functions with coefficients that sum to one, but this approach can lead to an expectation shift in the generated distribution. The paper proposes ReCFG, which relaxes the constraint on the coefficients, allowing for a more flexible control over the generated distribution and ensuring compatibility with diffusion theory.",
            "paper-title": "Rectified Diffusion Guidance for Conditional Generation",
            "image-path": "flux_paper_image/2410.18737_1729886581.png"
        },

        {
            "startTime": "12:08",
            "arxivId": "2410.18396",
            "arxivLink": "https://arxiv.org/abs/2410.18396",
            "title": "DAGs Gone Wild: \u21131 Penalty's Inconsistency and the Rise of \u21130",
            "institute": "UC San Diego, CMU",
            "text": "This research delves into the limitations of using the \u21131 penalty in differentiable structure learning, particularly in the linear Gaussian case. It demonstrates that even when the global optimum of the optimization problem is found, the \u21131 penalty can lead to inconsistent solutions, resulting in structures that deviate from the true DAG or its Markov equivalence class. To address this, the paper proposes a new method called CALM, which utilizes an \u21130-penalized likelihood with hard acyclicity constraints and incorporates a moral graph estimation procedure.",
            "paper-title": "Revisiting Differentiable Structure Learning: Inconsistency of $\\ell_1$ Penalty and Beyond",
            "image-path": "flux_paper_image/2410.18396_1729885700.png"
        },

        {
            "startTime": "12:37",
            "arxivId": "2410.18194",
            "arxivLink": "https://arxiv.org/abs/2410.18194",
            "title": "Zipping Up Data: How Compression Makes Language Models Learn Faster",
            "institute": "Stanford University",
            "text": "This research introduces ZIP-FIT, a data selection method that uses gzip compression to measure the alignment between potential training data and the target task distribution. Unlike previous methods that rely on embeddings or hashed n-grams, ZIP-FIT directly measures the similarity between data based on how well they compress together.",
            "paper-title": "ZIP-FIT: Embedding-Free Data Selection via Compression-Based Alignment",
            "image-path": "flux_paper_image/2410.18194_1729885764.png"
        },

        {
            "startTime": "12:55",
            "arxivId": "2410.18775",
            "arxivLink": "https://arxiv.org/abs/2410.18775",
            "title": "Watermarks That Can't Be Edited: A New Weapon Against Image Thieves",
            "institute": "Nanyang Technological University",
            "text": "This research introduces W-Bench, a benchmark for evaluating watermarking methods against image editing techniques powered by large-scale generative models. It also proposes VINE, a watermarking method that leverages a pretrained diffusion model to enhance robustness against image editing while maintaining high image quality.",
            "paper-title": "Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances",
            "image-path": "flux_paper_image/2410.18775_1729884735.png"
        },

        {
            "startTime": "13:19",
            "arxivId": "2410.18965",
            "arxivLink": "https://arxiv.org/abs/2410.18965",
            "title": "Matrix Factorization: A Little Push Gets You a Big Boost!",
            "institute": "ETH Zurich, University of Texas at Austin",
            "text": "This research focuses on the impact of initialization on the convergence rate of Scaled Gradient Descent (ScaledGD) for matrix factorization. Unlike previous work that primarily focused on small initializations, this paper introduces Nystr\u00f6m initialization, which significantly improves the convergence rate, achieving quadratic convergence in some cases.",
            "paper-title": "On the Crucial Role of Initialization for Matrix Factorization",
            "image-path": "flux_paper_image/2410.18965_1729886355.png"
        },

        {
            "startTime": "13:40",
            "arxivId": "2410.18822",
            "arxivLink": "https://arxiv.org/abs/2410.18822",
            "title": "Binocular Vision: The Secret Sauce for Super-Realistic 3D Scenes",
            "institute": "Tsinghua University, Wayne State University",
            "text": "This research proposes a novel method for synthesizing novel views from sparse inputs using 3D Gaussian Splatting. Unlike previous methods that rely on external depth priors, this approach leverages the inherent self-supervision present in binocular stereo consistency.",
            "paper-title": "Binocular-Guided 3D Gaussian Splatting with View Consistency for Sparse View Synthesis",
            "image-path": "flux_paper_image/2410.18822_1729884851.png"
        },

        {
            "startTime": "13:59",
            "arxivId": "2410.18262",
            "arxivLink": "https://arxiv.org/abs/2410.18262",
            "title": "Symplectic Neural Networks: Hamiltonian Matching for a Smooth Ride Through Time",
            "institute": "University of Cambridge",
            "text": "This research introduces SympFlow, a novel neural network architecture for integrating Hamiltonian systems. Unlike previous symplectic neural networks, SympFlow incorporates a time dependence that allows for the identification of an underlying Hamiltonian function.",
            "paper-title": "Hamiltonian Matching for Symplectic Neural Integrators",
            "image-path": "flux_paper_image/2410.18262_1729886861.png"
        },

        {
            "startTime": "14:19",
            "arxivId": "2410.18862",
            "arxivLink": "https://arxiv.org/abs/2410.18862",
            "title": "FedSPD: Soft Clustering Makes Decentralized Learning a Breeze!",
            "institute": "Carnegie Mellon University",
            "text": "This paper proposes FedSPD, a decentralized federated learning algorithm that uses soft clustering to personalize models for each client. Unlike previous work, FedSPD avoids requiring clients to train models for all clusters in every round, reducing communication overhead.",
            "paper-title": "FedSPD: A Soft-clustering Approach for Personalized Decentralized Federated Learning",
            "image-path": "flux_paper_image/2410.18862_1729885501.png"
        },

        {
            "startTime": "14:39",
            "arxivId": "2410.18352",
            "arxivLink": "https://arxiv.org/abs/2410.18352",
            "title": "FedBaF: Foundation Models Get a Secret Upgrade for Better FL!",
            "institute": "Carnegie Mellon University",
            "text": "This research introduces FedBaF, a method that integrates pre-trained foundation model weights during the aggregation phase of federated learning (FL) without disclosing the foundation model to clients. This differs from previous work that typically initializes the global FL model with the foundation model's weights, exposing it to security risks.",
            "paper-title": "FedBaF: Federated Learning Aggregation Biased by a Foundation Model",
            "image-path": "flux_paper_image/2410.18352_1729886733.png"
        },

        {
            "startTime": "14:57",
            "arxivId": "2410.18136",
            "arxivLink": "https://arxiv.org/abs/2410.18136",
            "title": "AI Chemists: Large Language Models Design Metal Complexes",
            "institute": "Deep Principle Inc., Cornell University",
            "text": "This research integrates large language models (LLMs) into the evolutionary optimization framework (LLM-EO) for designing transition metal complexes (TMCs). Unlike traditional genetic algorithms (GAs), LLM-EO leverages the chemical knowledge embedded within LLMs, acquired during their extensive pretraining.",
            "paper-title": "Generative Design of Functional Metal Complexes Utilizing the Internal Knowledge of Large Language Models",
            "image-path": "flux_paper_image/2410.18136_1729886708.png"
        },

        {
            "startTime": "15:26",
            "arxivId": "2410.18626",
            "arxivLink": "https://arxiv.org/abs/2410.18626",
            "title": "Offline RL Gets a Boost: New Method Ditches Data, Leverages Model Wisdom!",
            "institute": "Zhejiang University, Peking University",
            "text": "This paper introduces a new offline-to-online (O2O) reinforcement learning paradigm called SAMG. Unlike previous O2O RL algorithms that rely on large offline datasets for fine-tuning, SAMG leverages a pre-trained offline critic model to guide the online learning process, eliminating the need for retraining with offline data.",
            "paper-title": "SAMG: State-Action-Aware Offline-to-Online Reinforcement Learning with Offline Model Guidance",
            "image-path": "flux_paper_image/2410.18626_1729886078.png"
        },

        {
            "startTime": "15:53",
            "arxivId": "2410.18574",
            "arxivLink": "https://arxiv.org/abs/2410.18574",
            "title": "Teaching Tiny Brains Big Reasoning Skills: A New Trick for AI",
            "institute": "University of Zurich, ETH Zurich",
            "text": "This research introduces a new knowledge distillation method called SIKeD, which uses an iterative approach to train smaller models to learn multiple reasoning strategies from a larger model. Unlike traditional methods that often leave smaller models biased towards a single strategy, SIKeD incorporates self-generated data to help the smaller model learn to choose the most suitable strategy for a given task.",
            "paper-title": "SIKeD: Self-guided Iterative Knowledge Distillation for mathematical reasoning",
            "image-path": "flux_paper_image/2410.18574_1729886835.png"
        },

        {
            "startTime": "16:23",
            "arxivId": "2410.18698",
            "arxivLink": "https://arxiv.org/abs/2410.18698",
            "title": "Brain Tumor Detection: When Low-Quality Scans Meet High-Tech Solutions",
            "institute": "Chinese University of Hong Kong, University College London",
            "text": "This research explores the use of transfer learning to improve brain tumor segmentation in low-quality MRI scans, specifically focusing on the challenges faced in Sub-Saharan Africa. The study compares different training strategies, including fine-tuning a model trained on high-quality data with a dataset of low-quality scans.",
            "paper-title": "Transferring Knowledge from High-Quality to Low-Quality MRI for Adult Glioma Diagnosis",
            "image-path": "flux_paper_image/2410.18698_1729886775.png"
        },

        {
            "startTime": "16:48",
            "arxivId": "2410.18634",
            "arxivLink": "https://arxiv.org/abs/2410.18634",
            "title": "Tiny Models, Big Data: How to Train Embedding Models on the Cheap",
            "institute": "Renmin University of China, Microsoft",
            "text": "This research focuses on aligning small, open-source language models (LLMs) for synthesizing large-scale embedding data, unlike previous work that heavily relied on expensive proprietary LLMs like GPT-4.",
            "paper-title": "Little Giants: Synthesizing High-Quality Embedding Data at Scale",
            "image-path": "flux_paper_image/2410.18634_1729886042.png"
        },

        {
            "startTime": "17:14",
            "arxivId": "2410.18712",
            "arxivLink": "https://arxiv.org/abs/2410.18712",
            "title": "Time Series Forecasting Gets a Memory Boost: Retrieval-Augmented Diffusion Models to the Rescue!",
            "institute": "Peking University",
            "text": "This research introduces a Retrieval-Augmented Time series Diffusion Model (RATD) that leverages a database of relevant time series to guide the diffusion process. This differs from previous work by explicitly incorporating retrieval mechanisms to address the limitations of insufficient and imbalanced time series datasets.",
            "paper-title": "Retrieval-Augmented Diffusion Models for Time Series Forecasting",
            "image-path": "flux_paper_image/2410.18712_1729886614.png"
        },

        {
            "startTime": "17:42",
            "arxivId": "2410.18142",
            "arxivLink": "https://arxiv.org/abs/2410.18142",
            "title": "Nobel Prize Literature Gets AI-Reviewed: Can Robots Understand Our Stories?",
            "institute": "Peking University, University of Georgia, The University of Texas at Arlington...",
            "text": "This research compares the literary analysis capabilities of a large language model (LLM) to those of human graduate students, focusing on Nobel Prize-winning short stories. It goes beyond previous work by evaluating the LLM's performance across multiple dimensions of literary analysis, including thematic analysis, intertextuality, and emotional tone.",
            "paper-title": "Analyzing Nobel Prize Literature with Large Language Models",
            "image-path": "flux_paper_image/2410.18142_1729886532.png"
        },

        {
            "startTime": "18:05",
            "arxivId": "2410.18570",
            "arxivLink": "https://arxiv.org/abs/2410.18570",
            "title": "Robots Get Brainy: Navigating with Tree-of-Thought Reasoning!",
            "institute": "New York University Abu Dhabi, New York University, China Academic of Electronics and Information Technology...",
            "text": "This research introduces a new method for zero-shot object navigation (ZSON) that uses a Vision-Language Model with a Tree-of-Thought Network (VLTNet). Unlike previous methods that rely on simple object categories, VLTNet incorporates natural language instructions and uses a tree-based reasoning framework to select the most promising frontier for exploration.",
            "paper-title": "Zero-shot Object Navigation with Vision-Language Models Reasoning",
            "image-path": "flux_paper_image/2410.18570_1729885311.png"
        },

        {
            "startTime": "18:27",
            "arxivId": "2410.18391",
            "arxivLink": "https://arxiv.org/abs/2410.18391",
            "title": "Faster Privacy: New Algorithms Speed Up User-Level Data Protection",
            "institute": "University of Wisconsin-Madison",
            "text": "This research introduces novel algorithms for user-level private stochastic convex optimization (SCO) that achieve state-of-the-art excess risk with significantly improved runtime compared to previous methods. The key innovation lies in the use of outlier removal techniques applied to both SGD iterates and user gradients, enabling faster convergence without sacrificing privacy.",
            "paper-title": "Faster Algorithms for User-Level Private Stochastic Convex Optimization",
            "image-path": "flux_paper_image/2410.18391_1729885547.png"
        },

        {
            "startTime": "18:54",
            "arxivId": "2410.18966",
            "arxivLink": "https://arxiv.org/abs/2410.18966",
            "title": "LLMs: Memorizing Data or Just Being Smart? A Deep Dive into Contamination Detection Assumptions",
            "institute": "University of Washington, George Mason University",
            "text": "This research systematically reviews 47 papers on data contamination detection in LLMs, categorizes the underlying assumptions, and assesses their validity through case studies. Unlike previous surveys, it focuses on evaluating the assumptions themselves, rather than just listing detection techniques.",
            "paper-title": "Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions",
            "image-path": "flux_paper_image/2410.18966_1729886343.png"
        },

        {
            "startTime": "19:16",
            "arxivId": "2410.18355",
            "arxivLink": "https://arxiv.org/abs/2410.18355",
            "title": "Real-Time Face Relighting: Making Your Video Calls Look Like a Hollywood Movie!",
            "institute": "Chinese Academy of Sciences, City University of Hong Kong, Peking University",
            "text": "This research introduces a real-time method for relighting portrait videos using Neural Radiance Fields (NeRF). Unlike previous methods, this approach can adjust both the viewpoint and lighting conditions of the face in real-time, enabling more dynamic and realistic relighting effects.",
            "paper-title": "Real-Time 3D-Aware Portrait Video Relighting",
            "image-path": "flux_paper_image/2410.18355_1729885634.png"
        },

        {
            "startTime": "19:39",
            "arxivId": "2410.18938",
            "arxivLink": "https://arxiv.org/abs/2410.18938",
            "title": "Neural Networks Learn Features: A Random Matrix Theory Perspective",
            "institute": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, Harvard University, \u00c9cole Normale Sup\u00e9rieure...",
            "text": "This research provides a rigorous mathematical analysis of feature learning in two-layer neural networks trained with a single, large gradient step. Unlike previous work, it holds in the challenging maximal learning rate regime and allows for finitely supported second-layer initialization.",
            "paper-title": "A Random Matrix Theory Perspective on the Spectrum of Learned Features and Asymptotic Generalization Capabilities",
            "image-path": "flux_paper_image/2410.18938_1729886492.png"
        },

        {
            "startTime": "20:02",
            "arxivId": "2410.18917",
            "arxivLink": "https://arxiv.org/abs/2410.18917",
            "title": "Turbulent Flows: PINNs Make a Splash with Parametric Predictions!",
            "institute": "Siemens Corporation",
            "text": "This research extends the RANS-PINN framework to predict turbulent flow fields for both internal and external flows, going beyond the previous focus on flow over a cylinder. It also introduces a novel sampling approach to ensure training convergence with a more complex loss function.",
            "paper-title": "Using Parametric PINNs for Predicting Internal and External Turbulent Flows",
            "image-path": "flux_paper_image/2410.18917_1729884905.png"
        },

        {
            "startTime": "20:28",
            "arxivId": "2410.18241",
            "arxivLink": "https://arxiv.org/abs/2410.18241",
            "title": "Open Source Co-opetition: When Rivals Become BFFs (But One's Still the Boss)",
            "institute": "University of Oxford",
            "text": "This research focuses on open source co-opetition in projects hosted and governed by a single company, a scenario not extensively explored in previous studies.",
            "paper-title": "Characterising Open Source Co-opetition in Company-hosted Open Source Software Projects: The Cases of PyTorch, TensorFlow, and Transformers",
            "image-path": "flux_paper_image/2410.18241_1729885207.png"
        },

        {
            "startTime": "20:53",
            "arxivId": "2410.18503",
            "arxivLink": "https://arxiv.org/abs/2410.18503",
            "title": "Cardiac Segmentation: Bridging the Semantic Gap with Swin Filtering Blocks!",
            "institute": "Sorbonne Universit\u00e9, CNRS, INSERM",
            "text": "This research introduces a new Swin Filtering Block (SFB) network that uses a combination of convolutional and transformer layers to bridge the semantic gap in cardiac image segmentation. Unlike previous methods that rely solely on convolutions or self-attention, SFB-net incorporates a filtering mechanism in the skip connection paths between the encoder and decoder, allowing the decoder to focus on semantically rich features while filtering out noise from the encoder.",
            "paper-title": "SFB-net for Cardiac Segmentation: Bridging the Semantic Gap with Attention",
            "image-path": "flux_paper_image/2410.18503_1729884880.png"
        },

        {
            "startTime": "21:19",
            "arxivId": "2410.18875",
            "arxivLink": "https://arxiv.org/abs/2410.18875",
            "title": "Hunting for Cosmic Oddities: AI Helps Astronomers Find Hidden Treasures",
            "institute": "Russian Academy of Sciences, University of Illinois at Urbana-Champaign, National Center for Supercomputing Applications...",
            "text": "This research focuses on using active learning algorithms to identify astronomical anomalies in large datasets, particularly photometric light curves. This approach differs from previous work by incorporating human expertise into the anomaly detection process, allowing for more accurate identification of rare or unusual objects.",
            "paper-title": "Exploring the Universe with SNAD: Anomaly Detection in Astronomy",
            "image-path": "flux_paper_image/2410.18875_1729886499.png"
        },

        {
            "startTime": "21:41",
            "arxivId": "2410.18359",
            "arxivLink": "https://arxiv.org/abs/2410.18359",
            "title": "AI's Got a New Critic: FENCE Helps Language Models Tell the Truth!",
            "institute": "Carnegie Mellon University, Meta",
            "text": "This research introduces FENCE, a fine-grained critique-based evaluator that provides textual critiques for each model-generated claim. Unlike previous work that relies on proprietary models or the generator itself for evaluation, FENCE leverages diverse knowledge sources and public datasets to provide more accurate and informative feedback.",
            "paper-title": "Improving Model Factuality with Fine-grained Critique-based Evaluator",
            "image-path": "flux_paper_image/2410.18359_1729886126.png"
        },

        {
            "startTime": "22:06",
            "arxivId": "2410.18974",
            "arxivLink": "https://arxiv.org/abs/2410.18974",
            "title": "3D-Adapter: Giving Diffusion Models a Geometry Lesson!",
            "institute": "Stanford University, Apparate Labs, UC San Diego...",
            "text": "This paper introduces 3D-Adapter, a plug-in module that enhances the geometric consistency of multi-view diffusion models by incorporating 3D feedback augmentation. Unlike previous methods that rely on input/output synchronization, 3D-Adapter reconstructs a 3D representation mid-way through the denoising process and feeds the rendered views back into the network.",
            "paper-title": "3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D Generation",
            "image-path": "flux_paper_image/2410.18974_1729886182.png"
        },

        {
            "startTime": "22:43",
            "arxivId": "2410.18955",
            "arxivLink": "https://arxiv.org/abs/2410.18955",
            "title": "Med-ChatGPT: A Doctor's Prescription for Better Language Understanding",
            "institute": "University of Washington, George Mason University",
            "text": "This research proposes a unified prompting format for seven medical natural language understanding (NLU) tasks and develops a new instruction-tuning dataset, MNLU-Instruct, to improve the generalizability of large language models (LLMs) in medical domains. This approach differs from previous work by focusing on a wider range of NLU tasks and utilizing a dataset-agnostic prompting strategy.",
            "paper-title": "BioMistral-NLU: Towards More Generalizable Medical Language Understanding through Instruction Tuning",
            "image-path": "flux_paper_image/2410.18955_1729886823.png"
        },

        {
            "startTime": "23:05",
            "arxivId": "2410.18852",
            "arxivLink": "https://arxiv.org/abs/2410.18852",
            "title": "Deep Learning Makes Mesh Generation a Breeze!",
            "institute": "Donghua University, CMU",
            "text": "This research integrates deep learning into the polycube method for hex mesh generation, automating the surface segmentation and polycube construction process. Previous work often relied on manual intervention for these steps.",
            "paper-title": "DL-Polycube: Deep learning enhanced polycube method for high-quality hexahedral mesh generation and volumetric spline construction",
            "image-path": "flux_paper_image/2410.18852_1729886265.png"
        },

        {
            "startTime": "23:28",
            "arxivId": "2410.18096",
            "arxivLink": "https://arxiv.org/abs/2410.18096",
            "title": "Entity Linking Gets a Multi-Modal Makeover: 79K Instances and Counting!",
            "institute": "Peking University, Chinese Academy of Sciences",
            "text": "This research introduces a new dataset, M3EL, for multi-modal entity linking. Unlike previous datasets, M3EL is larger, covers a wider range of tasks, and includes more diverse topics.",
            "paper-title": "$M^3EL$: A Multi-task Multi-topic Dataset for Multi-modal Entity Linking",
            "image-path": "flux_paper_image/2410.18096_1729886506.png"
        },

        {
            "startTime": "23:59",
            "arxivId": "2410.18636",
            "arxivLink": "https://arxiv.org/abs/2410.18636",
            "title": "Learning to Cooperate: When Selfish AI Agents Become Team Players",
            "institute": "Google",
            "text": "This research introduces a new policy gradient algorithm for learning-aware reinforcement learning, which takes into account that other agents are also learning through trial and error. This algorithm is unbiased, higher-derivative-free, and applicable to scalable architectures based on recurrent sequence policy models.",
            "paper-title": "Multi-agent cooperation through learning-aware policy gradients",
            "image-path": "flux_paper_image/2410.18636_1729884300.png"
        },

        {
            "startTime": "24:22",
            "arxivId": "2410.18705",
            "arxivLink": "https://arxiv.org/abs/2410.18705",
            "title": "Concept-Fueled AI: Making Machines See the World Like We Do",
            "institute": "ETH Z\u00fcrich",
            "text": "This research explores how to integrate concept information into existing machine learning models, going beyond simply using concepts for classification. It proposes two new methods: Concept-Guided Conditional Diffusion and Concept-Guided Prototype Networks, which leverage concept knowledge to generate visual representations of concepts and perform interpretable concept prediction.",
            "paper-title": "Exploiting Interpretable Capabilities with Concept-Enhanced Diffusion and Prototype Networks",
            "image-path": "flux_paper_image/2410.18705_1729886271.png"
        },

        {
            "startTime": "24:45",
            "arxivId": "2410.18425",
            "arxivLink": "https://arxiv.org/abs/2410.18425",
            "title": "Beta-licious Matrix Factorization: A Stable Recipe for Bounded Data",
            "institute": "University of Massachusetts, University of Chicago",
            "text": "This research introduces a new method for matrix factorization that uses the doubly non-central beta (DNCB) distribution as a likelihood term. This approach is particularly useful for analyzing data with bounded support, such as DNA methylation data. Unlike previous methods, this approach allows for the number of latent feature factors to be independently determined from the number of sample factors.",
            "paper-title": "Doubly Non-Central Beta Matrix Factorization for Stable Dimensionality Reduction of Bounded Support Matrix Data",
            "image-path": "flux_paper_image/2410.18425_1729885269.png"
        },

        {
            "startTime": "25:07",
            "arxivId": "2410.18834",
            "arxivLink": "https://arxiv.org/abs/2410.18834",
            "title": "K-Space K-razy: Deep Learning Makes MRI Motion Correction Super Speedy!",
            "institute": "University Hospital of Tuebingen, Technical University of Munich, Imperial College London...",
            "text": "This research proposes a novel deep learning-based framework called LAPANet for non-rigid motion estimation directly from k-space, the raw data acquired by MRI scanners. Unlike previous methods that rely on image reconstruction, LAPANet operates directly on the k-space data, enabling faster and more accurate motion correction, especially at high acceleration rates.",
            "paper-title": "Highly efficient non-rigid registration in k-space with application to cardiac Magnetic Resonance Imaging",
            "image-path": "flux_paper_image/2410.18834_1729886196.png"
        },

        {
            "startTime": "25:31",
            "arxivId": "2410.18718",
            "arxivLink": "https://arxiv.org/abs/2410.18718",
            "title": "LLMs: Not Just for Chatbots Anymore! Predicting Wind Speed with Graph Signals",
            "institute": "Tsinghua University",
            "text": "This research proposes a novel framework that leverages LLMs for predicting missing values in time-varying graph signals by exploiting spatial and temporal smoothness. Unlike previous work that focuses on noise reduction, this approach utilizes the LLM's ability to recognize and predict patterns in the data.",
            "paper-title": "LLM-based Online Prediction of Time-varying Graph Signals",
            "image-path": "flux_paper_image/2410.18718_1729884753.png"
        },

        {
            "startTime": "25:54",
            "arxivId": "2410.18112",
            "arxivLink": "https://arxiv.org/abs/2410.18112",
            "title": "Traffic Jams? Not With OPTIMA: AI Makes Cars Cooperate Like a Dream Team!",
            "institute": "CMU",
            "text": "This research introduces OPTIMA, a distributed reinforcement learning framework for cooperative autonomous vehicle tasks. Unlike previous work that often relies on simplified scenarios, OPTIMA tackles complex, real-world traffic situations, including multi-vehicle collisions and intersections without traffic lights.",
            "paper-title": "OPTIMA: Optimized Policy for Intelligent Multi-Agent Systems Enables Coordination-Aware Autonomous Vehicles",
            "image-path": "flux_paper_image/2410.18112_1729884887.png"
        },

        {
            "startTime": "26:15",
            "arxivId": "2410.18792",
            "arxivLink": "https://arxiv.org/abs/2410.18792",
            "title": "LLMs Go Geo: A New Agent Helps AI Code for Earth Science",
            "institute": "Agence Nationale de la Recherche",
            "text": "This research introduces GeoAgent, a framework that combines a code interpreter, static analysis, and Retrieval-Augmented Generation (RAG) within a Monte Carlo Tree Search (MCTS) algorithm to help LLMs handle geospatial data processing tasks. This approach differs from previous work by integrating these techniques into a single framework and focusing on multi-turn tasks, which are more complex and realistic.",
            "paper-title": "An LLM Agent for Automatic Geospatial Data Analysis",
            "image-path": "flux_paper_image/2410.18792_1729886796.png"
        },

        {
            "startTime": "26:44",
            "arxivId": "2410.18294",
            "arxivLink": "https://arxiv.org/abs/2410.18294",
            "title": "Fake News Foiled: A New Index That Sniffs Out Lies!",
            "institute": "University of Washington",
            "text": "This research proposes NexusIndex, a framework that integrates multi-model embeddings with an innovative FAISSNexusIndex layer for efficient similarity searches. This approach differs from previous work by directly incorporating the indexing system into the neural network, enabling rapid and precise similarity searches during both training and inference.",
            "paper-title": "NexusIndex: Integrating Advanced Vector Indexing and Multi-Model Embeddings for Robust Fake News Detection",
            "image-path": "flux_paper_image/2410.18294_1729886301.png"
        },

        {
            "startTime": "27:01",
            "arxivId": "2410.18804",
            "arxivLink": "https://arxiv.org/abs/2410.18804",
            "title": "Diffusion Models Get a Speed Boost: New Trick Makes Image Inpainting a Breeze!",
            "institute": "Stony Brook University, Microsoft",
            "text": "This research proposes a new algorithm for fast constrained sampling in pre-trained diffusion models. Unlike previous methods that rely on expensive backpropagation operations, this approach uses a numerical approximation to the gradients, resulting in significant speed-ups.",
            "paper-title": "Fast constrained sampling in pre-trained diffusion models",
            "image-path": "flux_paper_image/2410.18804_1729885884.png"
        },

        {
            "startTime": "27:19",
            "arxivId": "2410.18580",
            "arxivLink": "https://arxiv.org/abs/2410.18580",
            "title": "Spiking Neural Networks: A Search for the Perfect Brain-Inspired Architecture",
            "institute": "Peking University, Huawei",
            "text": "This research introduces a novel spatial-temporal search method for optimizing spiking neural network (SNN) architectures. Unlike previous NAS methods that primarily focus on the spatial dimension, this work incorporates temporal dynamics, a crucial aspect of SNNs.",
            "paper-title": "Spatial-Temporal Search for Spiking Neural Networks",
            "image-path": "flux_paper_image/2410.18580_1729886143.png"
        },

        {
            "startTime": "27:46",
            "arxivId": "2410.18268",
            "arxivLink": "https://arxiv.org/abs/2410.18268",
            "title": "Model Selection Gets a Stability Boost: Inflated Argmax to the Rescue!",
            "institute": "University of Chicago, NSF-Simons National Institute for Theory and Mathematics in Biology",
            "text": "This paper introduces a new method for stabilizing model selection by combining bagging with an \"inflated argmax\" operation. This approach differs from previous work by returning a set of candidate models instead of a single model, allowing for the expression of uncertainty in the selection process.",
            "paper-title": "Stabilizing black-box model selection with the inflated argmax",
            "image-path": "flux_paper_image/2410.18268_1729886438.png"
        },

        {
            "startTime": "28:08",
            "arxivId": "2410.18895",
            "arxivLink": "https://arxiv.org/abs/2410.18895",
            "title": "ArterialNet: Turning Wearables into Blood Pressure Detectives!",
            "institute": "MIT, Texas A&M University",
            "text": "This research introduces ArterialNet, a two-stage model that leverages a \"pretrain on cohort and finetune on individual\" paradigm to reconstruct arterial blood pressure waveforms from wearable pulsatile signals. Unlike previous work, ArterialNet incorporates a personalized feature extractor and a cohort-aware regularization mechanism to address the challenge of individual variability in physiological data.",
            "paper-title": "ArterialNet: Reconstructing Arterial Blood Pressure Waveform with Wearable Pulsatile Signals, a Cohort-Aware Approach",
            "image-path": "flux_paper_image/2410.18895_1729885922.png"
        },

        {
            "startTime": "28:30",
            "arxivId": "2410.18610",
            "arxivLink": "https://arxiv.org/abs/2410.18610",
            "title": "CT Scans Get Smart: AI Predicts Heart Trouble with a Twist!",
            "institute": "Alibaba Group, Tsinghua University, Hupan Laboratory...",
            "text": "This research proposes a novel joint representation method that integrates both discrete quantitative biomarkers and continuous deep features extracted from chest CT scans for CVD risk prediction. This approach differs from previous work by combining traditional clinical knowledge with deep learning features, allowing for more accurate and interpretable predictions.",
            "paper-title": "A Joint Representation Using Continuous and Discrete Features for Cardiovascular Diseases Risk Prediction on Chest CT Scans",
            "image-path": "flux_paper_image/2410.18610_1729886108.png"
        },

        {
            "startTime": "28:55",
            "arxivId": "2410.18605",
            "arxivLink": "https://arxiv.org/abs/2410.18605",
            "title": "Candy Crush: A Language Model Learns to Play the Game",
            "institute": "Microsoft",
            "text": "This research explores the use of language models (LMs) to model game event sequences, treating them as a customized language. This approach differs from previous work by directly modeling the rich and fine-grained game events in a scalable way without requiring any labels.",
            "paper-title": "Understanding Players as if They Are Talking to the Game in a Customized Language: A Pilot Study",
            "image-path": "flux_paper_image/2410.18605_1729885860.png"
        },

        {
            "startTime": "29:16",
            "arxivId": "2410.18630",
            "arxivLink": "https://arxiv.org/abs/2410.18630",
            "title": "Microscopic Mouse Surgery: A New Way to See and Stitch Tiny Skulls!",
            "institute": "University of Tokyo",
            "text": "This research introduces a new registration scheme for microscopic manipulation, specifically focusing on the partially exposed cranial surface of mice. Unlike previous methods that rely on landmarks or contours, this scheme utilizes a CNN-based approach to segment and colorize features like sutures and surfaces, enabling more accurate and robust registration.",
            "paper-title": "A Cranial-Feature-Based Registration Scheme for Robotic Micromanipulation Using a Microscopic Stereo Camera System",
            "image-path": "flux_paper_image/2410.18630_1729886487.png"
        },

        {
            "startTime": "29:44",
            "arxivId": "2410.18588",
            "arxivLink": "https://arxiv.org/abs/2410.18588",
            "title": "Tiny Brains, Big Ideas: Distilling Knowledge from a Giant LLM",
            "institute": "Microsoft",
            "text": "This research explores the effectiveness of knowledge distillation using a large, capable Llama-3.1-405B-Instruct model as a teacher to train smaller student models. The study focuses on using task-specific synthetic data generated through tailored prompts to enhance the training process, which is a novel approach compared to previous work.",
            "paper-title": "Knowledge Distillation Using Frontier Open-source LLMs: Generalizability and the Role of Synthetic Data",
            "image-path": "flux_paper_image/2410.18588_1729886149.png"
        },

        {
            "startTime": "30:06",
            "arxivId": "2410.18160",
            "arxivLink": "https://arxiv.org/abs/2410.18160",
            "title": "Predicting the Future: A Language Model That Thinks Ahead",
            "institute": "Iprova SA",
            "text": "This research introduces a new pretraining method called Future Token Prediction (FTP). Unlike traditional language models that predict only the next token, FTP models are trained to predict multiple future tokens, capturing a broader semantic context. This is achieved by generating embedding vectors for each token position and using a separate decoder to predict the next N tokens.",
            "paper-title": "Future Token Prediction -- Causal Language Modelling with Per-Token Semantic State Vector for Multi-Token Prediction",
            "image-path": "flux_paper_image/2410.18160_1729884934.png"
        },

        {
            "startTime": "30:28",
            "arxivId": "2410.18248",
            "arxivLink": "https://arxiv.org/abs/2410.18248",
            "title": "LLMs on a Diet: Scheduling Strategies for Memory-Hungry Models",
            "institute": "Harvard University, Tsinghua University",
            "text": "This research introduces a novel scheduling framework called LAMPS, which integrates API handling strategies directly into the scheduling policy for augmented LLMs. Unlike previous work that focuses on managing memory during API calls, LAMPS considers the total length of requests and their handling strategies during API calls to minimize request completion time.",
            "paper-title": "Efficient Inference for Augmented Large Language Models",
            "image-path": "flux_paper_image/2410.18248_1729886790.png"
        },

        {
            "startTime": "30:52",
            "arxivId": "2410.18100",
            "arxivLink": "https://arxiv.org/abs/2410.18100",
            "title": "Typing with Your Ring: A Deep-Learning Word Wizard for AR Glasses",
            "institute": "Meta, University of Bristol",
            "text": "This research proposes a novel ring-based mid-air gesture typing system called RingGesture, which utilizes electrodes to mark the start and end of gesture trajectories and IMU sensors for hand tracking. It also introduces a deep-learning word prediction framework, Score Fusion, that integrates three key components: a word-gesture decoding model, a spatial spelling correction model, and a lightweight contextual language model. This framework fuses the scores from the three models to predict the most likely words with higher precision.",
            "paper-title": "RingGesture: A Ring-Based Mid-Air Gesture Typing System Powered by a Deep-Learning Word Prediction Framework",
            "image-path": "flux_paper_image/2410.18100_1729885483.png"
        },

        {
            "startTime": "31:16",
            "arxivId": "2410.18099",
            "arxivLink": "https://arxiv.org/abs/2410.18099",
            "title": "Pre-trained Word-Gesture Keyboard Decoder: No More Fat Fingers in VR!",
            "institute": "Meta, University of Bristol",
            "text": "This research proposes a novel approach to pre-training a neural decoder for word-gesture keyboards by using a coarse discretization method to encode word-gesture trajectories. This approach differs from previous work by using a more generalizable method that can be applied to different word-gesture keyboard systems without requiring specific fine-tuning.",
            "paper-title": "Gesture2Text: A Generalizable Decoder for Word-Gesture Keyboards in XR Through Trajectory Coarse Discretization and Pre-Training",
            "image-path": "flux_paper_image/2410.18099_1729885647.png"
        },

        {
            "startTime": "31:38",
            "arxivId": "2410.18556",
            "arxivLink": "https://arxiv.org/abs/2410.18556",
            "title": "Smaller Brains, Bigger Defense: How Model Complexity Impacts Adversarial Robustness",
            "institute": "University of Cambridge",
            "text": "This research investigates the relationship between a model's effective dimensionality, a measure of its complexity, and its robustness against adversarial attacks. Unlike previous work that focused on parameter count or other metrics, this study explores how effective dimensionality, which considers both model architecture and training data, influences a model's ability to withstand adversarial examples.",
            "paper-title": "Complexity Matters: Effective Dimensionality as a Measure for Adversarial Robustness",
            "image-path": "flux_paper_image/2410.18556_1729886749.png"
        },

        {
            "startTime": "32:06",
            "arxivId": "2410.18676",
            "arxivLink": "https://arxiv.org/abs/2410.18676",
            "title": "Graph Transformers Get a Motif Makeover: Counting Homomorphisms for Better Structure",
            "institute": "University of Oxford, TU Wien, AITHYRA",
            "text": "This research introduces a new structural encoding method called Motif Structural Encoding (MoSE) for graph Transformers. Unlike previous methods like random-walk structural encoding (RWSE), MoSE leverages homomorphism counts to capture graph structure.",
            "paper-title": "Homomorphism Counts as Structural Encodings for Graph Learning",
            "image-path": "flux_paper_image/2410.18676_1729886721.png"
        },

        {
            "startTime": "32:28",
            "arxivId": "2410.18400",
            "arxivLink": "https://arxiv.org/abs/2410.18400",
            "title": "Video Compression: Now Smarter Than Ever, Thanks to Deep Learning!",
            "institute": "Tsinghua University, Peking University, Peng Cheng Laboratory",
            "text": "This research introduces a novel video compression framework, DMVC, specifically designed for machine learning applications. Unlike traditional methods that prioritize human visual perception, DMVC focuses on preserving semantic information crucial for deep learning accuracy while efficiently reducing data size.",
            "paper-title": "DMVC: Multi-Camera Video Compression Network aimed at Improving Deep Learning Accuracy",
            "image-path": "flux_paper_image/2410.18400_1729886525.png"
        },

        {
            "startTime": "32:56",
            "arxivId": "2410.18399",
            "arxivLink": "https://arxiv.org/abs/2410.18399",
            "title": "CloudEye: Seeing Clearly Through the Fog of Mobile Vision",
            "institute": "Tsinghua University, Peking University, Peng Cheng Laboratory",
            "text": "This research proposes CloudEye, a system that leverages historical inference results from edge servers under cloud server guidance to optimize mobile visual tasks. This approach differs from previous work by fully exploiting the spatio-temporal correlations in video frames, reducing redundant computations and improving efficiency.",
            "paper-title": "CloudEye: A New Paradigm of Video Analysis System for Mobile Visual Scenarios",
            "image-path": "flux_paper_image/2410.18399_1729885733.png"
        },

        {
            "startTime": "33:24",
            "arxivId": "2410.18538",
            "arxivLink": "https://arxiv.org/abs/2410.18538",
            "title": "\"SMITE: One Image, A Thousand Segments - Video Segmentation Gets a Makeover!\"",
            "institute": "Simon Fraser University, Autodesk Research, University of Toronto...",
            "text": "This research introduces a novel video segmentation method called SMITE that leverages a pre-trained text-to-image diffusion model to segment videos based on a few reference images, even if those images are not from the video itself. This differs from previous work by allowing for flexible granularity segmentation, meaning the number of segments can vary arbitrarily, and by using a tracking mechanism to ensure temporal consistency across frames.",
            "paper-title": "SMITE: Segment Me In TimE",
            "image-path": "flux_paper_image/2410.18538_1729885175.png"
        },

        {
            "startTime": "33:54",
            "arxivId": "2410.18836",
            "arxivLink": "https://arxiv.org/abs/2410.18836",
            "title": "Bilingual LLMs: Speaking Your Language, Not Just English!",
            "institute": "OpenBabylon, ARG-Tech, University of Dundee...",
            "text": "This research proposes a model-agnostic approach to developing bilingual LLMs by extending the vocabulary of the target language while preserving the English tokenization. This method aims to improve language performance and reduce computational costs compared to previous approaches that rely on extensive pre-training on massive datasets.",
            "paper-title": "From English-Centric to Effective Bilingual: LLMs with Custom Tokenizers for Underrepresented Languages",
            "image-path": "flux_paper_image/2410.18836_1729885750.png"
        },

        {
            "startTime": "34:20",
            "arxivId": "2410.18539",
            "arxivLink": "https://arxiv.org/abs/2410.18539",
            "title": "Making Videos Understand Physics: A Deep Learning Trick for Imagining the Impossible",
            "institute": "University of Oxford",
            "text": "This research introduces a new method for learning interpretable representations of videos by incorporating nonlinear priors based on physical laws. Unlike previous work that focuses on predicting future behavior, this approach allows for counterfactual reasoning, enabling the generation of videos depicting hypothetical scenarios not observed during training.",
            "paper-title": "Interpretable Representation Learning from Videos using Nonlinear Priors",
            "image-path": "flux_paper_image/2410.18539_1729884764.png"
        },

        {
            "startTime": "34:46",
            "arxivId": "2410.18151",
            "arxivLink": "https://arxiv.org/abs/2410.18151",
            "title": "Music's Secret Code: AI Composer Learns the Math Behind Chords",
            "institute": "MIT",
            "text": "This research introduces Music102, a transformer-based model that leverages the inherent symmetries of music, specifically the transposition and reflection of pitch classes, to improve chord progression prediction. This approach differs from previous work by explicitly incorporating group theory into the model architecture, allowing it to learn more effectively from the underlying mathematical structure of music.",
            "paper-title": "Music102: An $D_{12}$-equivariant transformer for chord progression accompaniment",
            "image-path": "flux_paper_image/2410.18151_1729885983.png"
        },

        {
            "startTime": "35:10",
            "arxivId": "2410.18385",
            "arxivLink": "https://arxiv.org/abs/2410.18385",
            "title": "Linking Documents for Zero-Shot Retrieval: A Universal Solution!",
            "institute": "University of Toronto, Amazon",
            "text": "This research introduces a novel Universal Document Linking (UDL) algorithm for zero-shot information retrieval. Unlike previous methods that focus on single document-query pairs, UDL links similar documents to generate synthetic queries that cover multiple documents, enhancing the model's ability to learn the distribution of unseen domains.",
            "paper-title": "Link, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval",
            "image-path": "flux_paper_image/2410.18385_1729884694.png"
        },

        {
            "startTime": "35:33",
            "arxivId": "2410.18441",
            "arxivLink": "https://arxiv.org/abs/2410.18441",
            "title": "AI's New Math: Optimizing Language Models with Probabilistic Twists",
            "institute": "MIT",
            "text": "This paper delves into the mathematical foundations of generative AI, specifically the Transformer model, and proposes novel probabilistic optimization techniques for key components like sub-word encoding and attention computation. It differs from previous work by introducing probabilistic approaches to address challenges like vocabulary size and computational efficiency.",
            "paper-title": "The Nature of Mathematical Modeling and Probabilistic Optimization Engineering in Generative AI",
            "image-path": "flux_paper_image/2410.18441_1729885441.png"
        }
    ],
    "stats": {
        "num_pick": 86,
        "num_total": 349,
    },
    "audio": "https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202410251327_audio.mp3"
}
