
daily_data = {
    "date": "2024-07-30",
    "tweets": [
            {
                "startTime": "00:48",
                "arxivId": "2407.20080",
                "arxivLink": "https://arxiv.org/abs/2407.20080",
                "title": "TTA's Got Talent: A Unified Benchmark for Test-Time Adaptation",
                "institute": "Tsinghua University",
                "text": "This research introduces a new benchmark, UniTTA, for evaluating Test-Time Adaptation (TTA) methods. UniTTA is unique because it considers both domain and class shifts, creating a more realistic and comprehensive evaluation environment compared to previous benchmarks.",
                "paper-title": "UniTTA: Unified Benchmark and Versatile Framework Towards Realistic Test-Time Adaptation",
                "image-path": ""
            },

            {
                "startTime": "01:10",
                "arxivId": "2407.19564",
                "arxivLink": "https://arxiv.org/abs/2407.19564",
                "title": "Motion Forecasting: A Fine-Tuned Dance of Parameters",
                "institute": "University of Bonn, Stanford University, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne",
                "text": "This research introduces Forecast-PEFT, a parameter-efficient fine-tuning strategy for pre-trained motion forecasting models. Unlike traditional full fine-tuning methods, Forecast-PEFT freezes most of the model's parameters, focusing adjustments on newly introduced prompts and adapters. This approach preserves pre-learned representations and significantly reduces the number of parameters that need retraining, thereby enhancing efficiency.",
                "paper-title": "Forecast-PEFT: Parameter-Efficient Fine-Tuning for Pre-trained Motion Forecasting Models",
                "image-path": ""
            },

            {
                "startTime": "01:39",
                "arxivId": "2407.19409",
                "arxivLink": "https://arxiv.org/abs/2407.19409",
                "title": "Shrinking Super Brains: How to Make Tiny Multimodal AI Models That Still Rock!",
                "institute": "Peking University, Nanyang Technological University, UC Merced",
                "text": "This research focuses on knowledge distillation for multimodal large language models (MLLMs), specifically exploring what factors are most important in training smaller, more efficient MLLMs while retaining their ability to understand both text and images. Unlike previous work that focused on specific tasks or model architectures, this study investigates a broader range of distillation techniques and their impact on MLLM performance.",
                "paper-title": "LLAVADI: What Matters For Multimodal Large Language Models Distillation",
                "image-path": ""
            },

            {
                "startTime": "02:09",
                "arxivId": "2407.19055",
                "arxivLink": "https://arxiv.org/abs/2407.19055",
                "title": "LLMs Learn to Debug: A Tree Search for Code-Writing Perfection",
                "institute": "NVIDIA",
                "text": "This research introduces a novel algorithm called BESTER, which uses a best-first tree search approach to enable LLMs to debug their own code by iteratively generating self-reflections and program repairs. This differs from previous work that primarily focused on single-shot code generation or simpler self-reflection techniques.",
                "paper-title": "Effective Large Language Model Debugging with Best-first Tree Search",
                "image-path": ""
            },

            {
                "startTime": "02:33",
                "arxivId": "2407.20230",
                "arxivLink": "https://arxiv.org/abs/2407.20230",
                "title": "Reinforcement Learning's New Trick: Split, Aggregate, and Conquer!",
                "institute": "Carnegie Mellon University",
                "text": "This paper introduces a new on-policy reinforcement learning algorithm called SAPG that leverages large-scale parallel environments by splitting them into chunks and fusing them back together via importance sampling. This approach differs from previous methods that typically train a single policy across all environments, leading to wasted data.",
                "paper-title": "SAPG: Split and Aggregate Policy Gradients",
                "image-path": ""
            },

            {
                "startTime": "03:03",
                "arxivId": "2407.19359",
                "arxivLink": "https://arxiv.org/abs/2407.19359",
                "title": "Forecasting Your Future: How AI Learns to Pick the Best Medical Data for Predictions",
                "institute": "Google",
                "text": "This research introduces a method for automatically selecting the most relevant auxiliary tasks for pretraining a model for clinical outcome prediction. Unlike previous work that uses all available data or relies on manual selection, this approach uses a nested-loop meta-learning process to learn the optimal task weights.",
                "paper-title": "Learning to Select the Best Forecasting Tasks for Clinical Outcome Prediction",
                "image-path": ""
            },

            {
                "startTime": "03:23",
                "arxivId": "2407.19014",
                "arxivLink": "https://arxiv.org/abs/2407.19014",
                "title": "Semantic Segmentation Gets a Speed Boost with Sparse Refinement!",
                "institute": "MIT, NVIDIA, Tsinghua University...",
                "text": "This paper introduces SparseRefine, a method that enhances low-resolution semantic segmentation predictions with sparse high-resolution refinements. Unlike previous work that focuses on dense refinements, SparseRefine selectively refines only a sparse set of pixels, leading to significant computational savings.",
                "paper-title": "Sparse Refinement for Efficient High-Resolution Semantic Segmentation",
                "image-path": ""
            },

            {
                "startTime": "03:49",
                "arxivId": "2407.19325",
                "arxivLink": "https://arxiv.org/abs/2407.19325",
                "title": "Language Models: No Critical Period for Learning?",
                "institute": "ETH Z\u00fcrich",
                "text": "This study investigates the critical period for language acquisition in language models (LMs) by training them on bilingual data with varying ages of exposure to the second language. Unlike previous work that focused on fine-tuning pre-trained models, this research trains LMs from scratch, providing a more controlled environment for studying the critical period.",
                "paper-title": "Do Language Models Have a Critical Period for Language Acquisition?",
                "image-path": ""
            },

            {
                "startTime": "04:08",
                "arxivId": "2407.20034",
                "arxivLink": "https://arxiv.org/abs/2407.20034",
                "title": "MaskInversion: Giving CLIP a Local Makeover!",
                "institute": "University of Bonn, Goethe University Frankfurt, University of Oxford...",
                "text": "MaskInversion learns a localized embedding for a specific image region without modifying the pre-trained vision-language model. This differs from previous methods that either fine-tune the model or rely on simple cropping or token aggregation.",
                "paper-title": "MaskInversion: Localized Embeddings via Optimization of Explainability Maps",
                "image-path": ""
            },

            {
                "startTime": "04:39",
                "arxivId": "2407.18940",
                "arxivLink": "https://arxiv.org/abs/2407.18940",
                "title": "Scientists Build a Search Engine for Research Papers, and It's Actually Pretty Smart!",
                "institute": "Princeton University",
                "text": "This research introduces LitSearch, a new benchmark for evaluating scientific literature search systems. Unlike previous benchmarks that focus on specific tasks like citation recommendation, LitSearch uses realistic literature search questions that researchers actually ask.",
                "paper-title": "LitSearch: A Retrieval Benchmark for Scientific Literature Search",
                "image-path": ""
            },

            {
                "startTime": "04:57",
                "arxivId": "2407.18982",
                "arxivLink": "https://arxiv.org/abs/2407.18982",
                "title": "Deep Learning's Speed Demon: Secure MPC Gets a Turbo Boost!",
                "institute": "Tsinghua University",
                "text": "This research proposes a new method for reducing communication latency in secure multi-party computation (MPC) protocols used for privacy-preserving deep learning. The key innovation lies in optimizing multivariate multiplication by leveraging pre-shared information and reducing communication rounds. This approach differs from previous work that focused on improving polynomial computations within finite rings.",
                "paper-title": "Low-Latency Privacy-Preserving Deep Learning Design via Secure MPC",
                "image-path": ""
            },

            {
                "startTime": "05:17",
                "arxivId": "2407.20228",
                "arxivLink": "https://arxiv.org/abs/2407.20228",
                "title": "Vision Models Get a Brain Boost: FlexAttention Makes High-Res Images a Breeze!",
                "institute": "University of Massachusetts, Princeton University",
                "text": "This paper introduces FlexAttention, a new attention mechanism that allows vision-language models to efficiently process high-resolution images. Unlike previous methods that process all high-resolution tokens, FlexAttention dynamically selects a small subset of relevant tokens, significantly reducing computational cost.",
                "paper-title": "FlexAttention for Efficient High-Resolution Vision-Language Models",
                "image-path": ""
            },

            {
                "startTime": "05:43",
                "arxivId": "2407.19594",
                "arxivLink": "https://arxiv.org/abs/2407.19594",
                "title": "LLMs Judge Themselves: A Meta-Rewarding Approach to Self-Improvement",
                "institute": "Meta, University of California Berkeley, New York University",
                "text": "This research introduces a novel \"Meta-Rewarding\" approach to self-improvement in LLMs. Unlike previous methods that focus on improving the model's responses, this paper trains the model to judge its own judgments, leading to a more refined understanding of instructions and better responses.",
                "paper-title": "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge",
                "image-path": ""
            },

            {
                "startTime": "06:17",
                "arxivId": "2407.20219",
                "arxivLink": "https://arxiv.org/abs/2407.20219",
                "title": "SfM Gets a Global Makeover: Faster, More Accurate 3D Reconstruction!",
                "institute": "ETH Zurich, Microsoft",
                "text": "This paper proposes a new global SfM system called GLOMAP that directly combines camera and point position estimation in a single step, eliminating the need for separate translation averaging and triangulation. This approach is different from previous global SfM systems, which typically perform these steps individually.",
                "paper-title": "Global Structure-from-Motion Revisited",
                "image-path": ""
            },

            {
                "startTime": "06:36",
                "arxivId": "2407.19371",
                "arxivLink": "https://arxiv.org/abs/2407.19371",
                "title": "Predicting Patient Doom: A Deep Dive into Correlated Time-to-Event Predictions",
                "institute": "Google",
                "text": "This research proposes a deep latent state-space generative model to capture the interactions among multiple types of correlated clinical events. Unlike previous work that focuses on single event prediction, this model simultaneously predicts mortality risk and organ failure risk trajectories by leveraging the temporal correlations between past measurements and clinical interventions.",
                "paper-title": "Deep State-Space Generative Model For Correlated Time-to-Event Predictions",
                "image-path": ""
            },

            {
                "startTime": "07:01",
                "arxivId": "2407.19778",
                "arxivLink": "https://arxiv.org/abs/2407.19778",
                "title": "AI for Bioimages: From Pixels to Insights, One Prompt at a Time!",
                "institute": "Peking University",
                "text": "This research proposes using Multimodal Large Language Models (MLLMs) for bioimage analysis, going beyond traditional image processing methods by integrating diverse information sources like text, metadata, and even experimental protocols.",
                "paper-title": "Multimodal Large Language Models for Bioimage Analysis",
                "image-path": ""
            },

            {
                "startTime": "07:32",
                "arxivId": "2407.19389",
                "arxivLink": "https://arxiv.org/abs/2407.19389",
                "title": "Federated Learning Gets a Makeover: Importance-Aware Submodels to the Rescue!",
                "institute": "Purdue University, Google",
                "text": "This research proposes a new method called FIARSE for federated learning that dynamically adjusts submodels based on the importance of model parameters. Unlike previous static and dynamic submodel extraction methods, FIARSE doesn't require additional information beyond the model parameters themselves to determine parameter importance, reducing overhead on clients.",
                "paper-title": "FIARSE: Model-Heterogeneous Federated Learning via Importance-Aware Submodel Extraction",
                "image-path": ""
            },

            {
                "startTime": "07:57",
                "arxivId": "2407.18957",
                "arxivLink": "https://arxiv.org/abs/2407.18957",
                "title": "AI Stockbrokers: Can LLMs Predict the Market?",
                "institute": "University of Liverpool, Peking University, Rutgers University...",
                "text": "This research introduces StockAgent, a multi-agent AI system that uses large language models (LLMs) to simulate real-world stock trading environments. Unlike previous work, StockAgent focuses on the impact of external factors, such as macroeconomic events and company fundamentals, on trading behavior.",
                "paper-title": "When AI Meets Finance (StockAgent): Large Language Model-based Stock Trading in Simulated Real-world Environments",
                "image-path": ""
            },

            {
                "startTime": "08:25",
                "arxivId": "2407.20060",
                "arxivLink": "https://arxiv.org/abs/2407.20060",
                "title": "Relational Databases Get a Deep Learning Makeover: No More Feature Engineering!",
                "institute": "Stanford University, Kumo.AI, Max Planck Institute for Informatics",
                "text": "This research introduces RELBENCH, a benchmark for relational deep learning (RDL). Unlike previous work that focused on manual feature engineering, RDL uses graph neural networks to learn predictive models directly from relational databases.",
                "paper-title": "RelBench: A Benchmark for Deep Learning on Relational Databases",
                "image-path": ""
            },

            {
                "startTime": "08:53",
                "arxivId": "2407.19115",
                "arxivLink": "https://arxiv.org/abs/2407.19115",
                "title": "RNNs Go Parallel: Newton's Method Gets a Speed Boost!",
                "institute": "Stanford University",
                "text": "This research builds upon DEER, a method for parallelizing nonlinear RNNs, by introducing quasi-Newton approximations and a connection between damped Newton's method and Kalman smoothing to improve scalability and stability.",
                "paper-title": "Towards Scalable and Stable Parallelization of Nonlinear RNNs",
                "image-path": ""
            },

            {
                "startTime": "09:16",
                "arxivId": "2407.19548",
                "arxivLink": "https://arxiv.org/abs/2407.19548",
                "title": "Cycle3D: Image-to-3D Generation Gets a Consistency Makeover!",
                "institute": "Peking University, Pengcheng Laboratory, National University of Singapore",
                "text": "This paper introduces Cycle3D, a framework that combines a pre-trained 2D diffusion model with a 3D reconstruction model during the multi-step diffusion process. This differs from previous methods that rely solely on multi-view diffusion models, which often struggle to generate consistent multi-view images.",
                "paper-title": "Cycle3D: High-quality and Consistent Image-to-3D Generation via Generation-Reconstruction Cycle",
                "image-path": ""
            },

            {
                "startTime": "09:48",
                "arxivId": "2407.18990",
                "arxivLink": "https://arxiv.org/abs/2407.18990",
                "title": "LLM Tuning: Hyperparameter Hunt for the Perfect Fit!",
                "institute": "IBM",
                "text": "This research focuses on providing practical recommendations for hyperparameter configurations when fine-tuning LLMs, using a comprehensive grid search across a wide range of datasets and tasks. Unlike previous studies that often focused on a limited set of datasets or tasks, this paper aims to provide more robust recommendations for real-world applications.",
                "paper-title": "Stay Tuned: An Empirical Study of the Impact of Hyperparameters on LLM Tuning in Real-World Applications",
                "image-path": ""
            },

            {
                "startTime": "10:09",
                "arxivId": "2407.20057",
                "arxivLink": "https://arxiv.org/abs/2407.20057",
                "title": "Daily CO2 Emissions: Hotter Days, Hotter Planet, Hotter Headaches",
                "institute": "Tsinghua University",
                "text": "This research reconstructs global daily CO2 emissions from 1970 to 2022 using a machine learning algorithm, providing a higher temporal resolution than previous datasets.",
                "paper-title": "Reconstructing Global Daily CO2 Emissions via Machine Learning",
                "image-path": ""
            },

            {
                "startTime": "10:46",
                "arxivId": "2407.19633",
                "arxivLink": "https://arxiv.org/abs/2407.19633",
                "title": "AI Solves Optimization Problems: No More Math Headaches!",
                "institute": "Stanford University",
                "text": "This research introduces OptiMUS-0.3, a system that uses a modular approach to break down complex optimization problems into smaller, manageable chunks. This differs from previous work that relied on single prompts for LLMs, which struggled with long and complex problems.",
                "paper-title": "OptiMUS-0.3: Using Large Language Models to Model and Solve Optimization Problems at Scale",
                "image-path": ""
            },

            {
                "startTime": "11:14",
                "arxivId": "2407.19777",
                "arxivLink": "https://arxiv.org/abs/2407.19777",
                "title": "Agnostic Learning: ERM Gets a Reality Check (and a New Algorithm)",
                "institute": "Aarhus University, UC Berkeley",
                "text": "This paper revisits agnostic PAC learning, a model for studying supervised learning. It shows that Empirical Risk Minimization (ERM), a common learning algorithm, is sub-optimal when the error of the best hypothesis is considered as a parameter. The paper then introduces a new, improper learning algorithm that outperforms ERM for almost the full range of error values.",
                "paper-title": "Revisiting Agnostic PAC Learning",
                "image-path": ""
            },

            {
                "startTime": "11:39",
                "arxivId": "2407.19284",
                "arxivLink": "https://arxiv.org/abs/2407.19284",
                "title": "Fake It 'Til You Make It: How Synthetic Tumors Are Helping Doctors See Pancreatic Cancer Better",
                "institute": "Northwestern University, Mayo Clinic",
                "text": "This research focuses on the impact of synthetic tumor size and boundary precision on the performance of pancreatic tumor segmentation models. Unlike previous studies that primarily focused on generating realistic tumor textures, this paper investigates the influence of these specific factors on model accuracy.",
                "paper-title": "Optimizing Synthetic Data for Enhanced Pancreatic Tumor Segmentation",
                "image-path": ""
            },

            {
                "startTime": "12:00",
                "arxivId": "2407.19280",
                "arxivLink": "https://arxiv.org/abs/2407.19280",
                "title": "Driving with Chatbots: LLMs Take the Wheel!",
                "institute": "The University of Tokyo",
                "text": "This research explores the use of Large Language Models (LLMs) in autonomous driving systems, focusing on their application in both modular decision-making pipelines and end-to-end driving systems. It differs from previous work by systematically reviewing the latest progress of LLMs in AD and proposing a taxonomy analysis framework covering perception, decision-making, planning, and control.",
                "paper-title": "Large Language Models for Human-like Autonomous Driving: A Survey",
                "image-path": ""
            },

            {
                "startTime": "12:29",
                "arxivId": "2407.19523",
                "arxivLink": "https://arxiv.org/abs/2407.19523",
                "title": "Meta-Learning Gets a Game-Changing Makeover: Adversarial Task Distribution Generation!",
                "institute": "Tsinghua University",
                "text": "This research introduces a novel approach to meta-learning by explicitly modeling task distribution shifts using normalizing flows. Unlike previous work that relies on hand-crafted or simple prior distributions, this method generates task distributions adversarially, aiming to improve the robustness of fast adaptation.",
                "paper-title": "Robust Fast Adaptation from Adversarially Explicit Task Distribution Generation",
                "image-path": ""
            },

            {
                "startTime": "12:59",
                "arxivId": "2407.20198",
                "arxivLink": "https://arxiv.org/abs/2407.20198",
                "title": "Fetal Brain Motion: A Deep Dive into Equivariant Representations",
                "institute": "Boston Children's Hospital, Harvard Medical School, Northeastern University...",
                "text": "This research introduces SpaER, a method for fetal motion tracking that dynamically learns spatio-temporal representations using equivariant filters and self-attention mechanisms. Unlike previous approaches that statically estimate motion from image pairs, SpaER tracks the rigid movement patterns of the fetal head across time and space.",
                "paper-title": "SpaER: Learning Spatio-temporal Equivariant Representations for Fetal Brain Motion Tracking",
                "image-path": ""
            },

            {
                "startTime": "13:26",
                "arxivId": "2407.19269",
                "arxivLink": "https://arxiv.org/abs/2407.19269",
                "title": "Ellipsoid Fitting: A Bayesian Approach to Outlier-Proofing in High Dimensions",
                "institute": "Chinese Academy of Sciences, Peking University",
                "text": "This research introduces a Bayesian approach to fitting multidimensional ellipsoids, which differs from previous methods by considering each model point as a potential source for generating each measurement sample. This approach enhances robustness against noise and outliers, particularly in higher-dimensional spaces.",
                "paper-title": "A Bayesian Approach Toward Robust Multidimensional Ellipsoid-Specific Fitting.",
                "image-path": ""
            },

            {
                "startTime": "13:51",
                "arxivId": "2407.19605",
                "arxivLink": "https://arxiv.org/abs/2407.19605",
                "title": "Look Hear, See There: AI Predicts Your Gaze as You Listen to Instructions",
                "institute": "Stony Brook University, UC Berkeley, Waymo LLC...",
                "text": "This research focuses on predicting human gaze during an incremental object referral task, where a person is asked to find a specific object in an image while listening to a spoken description. This differs from previous work that either predicted gaze during free viewing or during a standard object referral task where the entire description is provided at once.",
                "paper-title": "Look Hear: Gaze Prediction for Speech-directed Human Attention",
                "image-path": ""
            },

            {
                "startTime": "14:21",
                "arxivId": "2407.19422",
                "arxivLink": "https://arxiv.org/abs/2407.19422",
                "title": "AI Therapists: Can Robots Really Help You Feel Better?",
                "institute": "Beijing University of Technology, Wuhan University, The Hong Kong Polytechnic University...",
                "text": "This research provides a comprehensive review of AI's role in enhancing CBT across various stages, from pre-treatment assessment to post-treatment follow-up, which is a unique contribution compared to previous reviews that focused mainly on specific applications.",
                "paper-title": "A Generic Review of Integrating Artificial Intelligence in Cognitive Behavioral Therapy",
                "image-path": ""
            },

            {
                "startTime": "14:44",
                "arxivId": "2407.20121",
                "arxivLink": "https://arxiv.org/abs/2407.20121",
                "title": "Say Goodbye to Bad Recommendations: A New Framework for Cross-Domain Interest Transfer",
                "institute": "Meituan, Peking University",
                "text": "This research introduces an explicit interest transfer framework called EXIT, which uses supervised learning to model the transfer of user interests across different domains. Unlike previous implicit methods, EXIT directly learns which interests from the source domain are beneficial to the target domain, preventing negative transfer.",
                "paper-title": "EXIT: An EXplicit Interest Transfer Framework for Cross-Domain Recommendation",
                "image-path": ""
            },

            {
                "startTime": "15:05",
                "arxivId": "2407.20105",
                "arxivLink": "https://arxiv.org/abs/2407.20105",
                "title": "Language Models: No More Copycats, Just Creative Cats!",
                "institute": "ETH Zurich",
                "text": "This paper proposes a new algorithm called Copyright-Protecting Fusion (CP-Fuse) that combines multiple language models to minimize the reproduction of copyrighted material. Unlike previous work that focuses on mitigating memorization during training, CP-Fuse intervenes during the inference phase, adaptively adjusting the weights of the models based on the generated sequence history.",
                "paper-title": "Strong Copyright Protection for Language Models via Adaptive Model Fusion",
                "image-path": ""
            },

            {
                "startTime": "15:29",
                "arxivId": "2407.20164",
                "arxivLink": "https://arxiv.org/abs/2407.20164",
                "title": "Robots That Understand You: New AI Makes Multi-Robot Teams Talk and Work Together!",
                "institute": "University of Cambridge",
                "text": "This research presents a method for training multi-robot navigation policies that can interpret and follow natural language instructions. Unlike previous work that relies on LLMs within the control loop, this approach conditions policies on embeddings from pretrained LLMs, allowing for low-latency control.",
                "paper-title": "Language-Conditioned Offline RL for Multi-Robot Navigation",
                "image-path": ""
            },

            {
                "startTime": "15:55",
                "arxivId": "2407.20021",
                "arxivLink": "https://arxiv.org/abs/2407.20021",
                "title": "Attention, Please! New Method Makes Vision Transformers Slimmer Without Data",
                "institute": "Seoul National University, Google, Korea Advanced Institute of Science and Technology",
                "text": "This research focuses on data-free quantization of Vision Transformers (ViTs), a technique that makes these powerful models more efficient for use on devices with limited resources. Unlike previous methods that struggle with low-bit quantization, this paper proposes a novel approach that leverages the similarity between attention maps across different heads within the ViT architecture.",
                "paper-title": "MimiQ: Low-Bit Data-Free Quantization of Vision Transformers",
                "image-path": ""
            },

            {
                "startTime": "16:26",
                "arxivId": "2407.19838",
                "arxivLink": "https://arxiv.org/abs/2407.19838",
                "title": "RNA Design: Flow Matching Makes It Flow!",
                "institute": "Tsinghua University",
                "text": "This research introduces RNACG, a universal RNA sequence generation model based on flow matching. Unlike previous methods that focus on specific tasks, RNACG can handle various conditional inputs, making it more versatile.",
                "paper-title": "RNACG: A Universal RNA Sequence Conditional Generation model based on Flow-Matching",
                "image-path": ""
            },

            {
                "startTime": "16:52",
                "arxivId": "2407.19155",
                "arxivLink": "https://arxiv.org/abs/2407.19155",
                "title": "Graph Attack: When AI Goes Rogue, It's Biased Towards Training Data!",
                "institute": "Korea Advanced Institute of Science and Technology",
                "text": "This research identifies a bias in existing graph poisoning attacks, where they primarily target edges connected to training nodes. The paper proposes a new attack method, Metacon, which uses contrastive surrogate objectives to alleviate this bias and target a wider range of edges.",
                "paper-title": "Debiased Graph Poisoning Attack via Contrastive Surrogate Objective",
                "image-path": ""
            },

            {
                "startTime": "17:19",
                "arxivId": "2407.19370",
                "arxivLink": "https://arxiv.org/abs/2407.19370",
                "title": "Click, Click, Boom! New AI Makes Grasping More Controllable",
                "institute": "Peking University, University of Central Florida",
                "text": "This research introduces a new method called Semantic Contact Map (SCM) for controllable grasp generation. Unlike previous methods that rely on general contact information, SCM provides fine-grained details about finger-object interactions, allowing for more precise control over how a hand grasps an object.",
                "paper-title": "ClickDiff: Click to Induce Semantic Contact Map for Controllable Grasp Generation with Diffusion Models",
                "image-path": ""
            },

            {
                "startTime": "17:55",
                "arxivId": "2407.18992",
                "arxivLink": "https://arxiv.org/abs/2407.18992",
                "title": "AI Recipe Book: LLMs Cook Up Asset Management Solutions",
                "institute": "IBM",
                "text": "This research introduces a novel approach to automated solution recipe generation for industrial asset management by leveraging Large Language Models (LLMs) and a taxonomy-guided prompting process. This differs from previous work by automating the creation of solution recipes, which traditionally required extensive collaboration between data scientists and domain experts.",
                "paper-title": "Towards Automated Solution Recipe Generation for Industrial Asset Management with LLM",
                "image-path": ""
            },

            {
                "startTime": "18:15",
                "arxivId": "2407.20083",
                "arxivLink": "https://arxiv.org/abs/2407.20083",
                "title": "Word Completion Gets a Boost: Energy-Based Model Makes Translation a Breeze!",
                "institute": "Tsinghua University, Tencent",
                "text": "This research proposes an energy-based model for word-level autocompletion in computer-aided translation. Unlike previous classification models, this approach directly defines the energy function on top of both the input context and the candidate target word, allowing it to capture more valuable information from the source sentence.",
                "paper-title": "An Energy-based Model for Word-level AutoCompletion in Computer-aided Translation",
                "image-path": ""
            },

            {
                "startTime": "18:41",
                "arxivId": "2407.19415",
                "arxivLink": "https://arxiv.org/abs/2407.19415",
                "title": "Music for Your Videos: A New Loss Function That's Not All Noise!",
                "institute": "Tsinghua University, University of Science and Technology of China",
                "text": "This research introduces an inter-intra modal loss (II Loss) to address the issue of noisy data in self-supervised cross-modal retrieval. Unlike previous methods that focus solely on minimizing the distance between positive samples, II Loss also minimizes variations in feature distributions within each modality, effectively mitigating overfitting to noisy data.",
                "paper-title": "Start from Video-Music Retrieval: An Inter-Intra Modal Loss for Cross Modal Retrieval",
                "image-path": ""
            },

            {
                "startTime": "19:11",
                "arxivId": "2407.19451",
                "arxivLink": "https://arxiv.org/abs/2407.19451",
                "title": "Hair Today, Gone Tomorrow: New Model Makes 3D Hair Editing a Breeze!",
                "institute": "Yale University, Adobe Research, CAU...",
                "text": "This research introduces Perm, a parametric model for 3D hair that disentangles global hair shape and local strand details using a PCA-based strand representation in the frequency domain. This differs from previous work that jointly models these aspects, limiting editing capabilities.",
                "paper-title": "\textsc{Perm}: A Parametric Representation for Multi-Style 3D Hair Modeling",
                "image-path": ""
            },

            {
                "startTime": "19:37",
                "arxivId": "2407.19984",
                "arxivLink": "https://arxiv.org/abs/2407.19984",
                "title": "AI Diagnoses Depression & Alzheimer's, But How Confident Are We?",
                "institute": "University of Cambridge, Tsinghua University",
                "text": "This research introduces a novel Bayesian approach for confidence estimation in automatic detection of Alzheimer's disease and depression, using a dynamic Dirichlet prior distribution to model the second-order probability of the predictive distribution. This differs from previous work that relied on standard neural network classifiers with softmax activation, which often struggle with reliable uncertainty estimations.",
                "paper-title": "Confidence Estimation for Automatic Detection of Depression and Alzheimer's Disease Based on Clinical Interviews",
                "image-path": ""
            },

            {
                "startTime": "20:04",
                "arxivId": "2407.19474",
                "arxivLink": "https://arxiv.org/abs/2407.19474",
                "title": "Visual Riddles: Can AI Crack These Brain Teasers?",
                "institute": "Ben-Gurion University of the Negev, Google",
                "text": "This research introduces Visual Riddles, a benchmark that uses synthetic images generated specifically for the challenge, rather than relying on pre-existing images. This approach allows for a wider range of scenarios and more creative freedom in designing riddles.",
                "paper-title": "Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision and Language Models",
                "image-path": ""
            },

            {
                "startTime": "20:31",
                "arxivId": "2407.19829",
                "arxivLink": "https://arxiv.org/abs/2407.19829",
                "title": "E-commerce Search Gets a Makeover: Generative Retrieval with a Preference for Clicks!",
                "institute": "JD.com, Tsinghua University",
                "text": "This research proposes a novel framework for e-commerce search called \"generative retrieval with preference optimization\" (GenR-PO). Unlike traditional methods that rely on dense representations of queries and items, GenR-PO directly generates identifiers of target items using an autoregressive language model. This approach simplifies the generation process by transforming the task of generating lengthy item titles from short queries into a task of generating multi-span identifiers.",
                "paper-title": "Generative Retrieval with Preference Optimization for E-commerce Search",
                "image-path": ""
            },

            {
                "startTime": "21:02",
                "arxivId": "2407.18941",
                "arxivLink": "https://arxiv.org/abs/2407.18941",
                "title": "Label Errors? LEMON Squeezes Out the Noise!",
                "institute": "MIT",
                "text": "This research proposes LEMON, a method for identifying label errors in multimodal datasets. Unlike previous work that focuses on unimodal representations, LEMON leverages the neighborhood structure of contrastively pretrained multimodal embeddings.",
                "paper-title": "LEMoN: Label Error Detection using Multimodal Neighbors",
                "image-path": ""
            },

            {
                "startTime": "21:25",
                "arxivId": "2407.19547",
                "arxivLink": "https://arxiv.org/abs/2407.19547",
                "title": "Diffusion Models: Time Travel for Better Images?",
                "institute": "SenseTime Research, Beihang University, Monash University...",
                "text": "This research focuses on the unique temporal features in diffusion models, which are crucial for image generation. Unlike previous work, it proposes a framework that specifically addresses the disturbance caused by quantization on these features.",
                "paper-title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "image-path": ""
            },

            {
                "startTime": "21:58",
                "arxivId": "2407.18962",
                "arxivLink": "https://arxiv.org/abs/2407.18962",
                "title": "Robots on a Roll: Deep Learning Makes Path Planning a Breeze!",
                "institute": "Northeastern University, NYU, Peking University",
                "text": "This research focuses on using the Deep Deterministic Policy Gradient (DDPG) algorithm for autonomous navigation, which is different from previous work that used Deep Q-Network (DQN) and Double Deep Q-Network (DDQN) algorithms. The DDPG algorithm is specifically designed for continuous action spaces, making it more suitable for path planning tasks where the robot needs to make smooth and continuous movements.",
                "paper-title": "Autonomous Navigation of Unmanned Vehicle Through Deep Reinforcement Learning",
                "image-path": ""
            },

            {
                "startTime": "22:25",
                "arxivId": "2407.20157",
                "arxivLink": "https://arxiv.org/abs/2407.20157",
                "title": "LLMs for Tables: A New Framework for Relational Table Learning",
                "institute": "Shanghai Jiao Tong University, Tsinghua University",
                "text": "This research introduces rLLM, a framework for relational table learning (RTL) that combines modules from graph neural networks (GNNs), large language models (LLMs), and table neural networks (TNNs). This approach differs from previous work by providing a standardized way to construct novel RTL models using a \"combine, align, and co-train\" strategy.",
                "paper-title": "rLLM: Relational Table Learning with LLMs",
                "image-path": ""
            },

            {
                "startTime": "22:54",
                "arxivId": "2407.20061",
                "arxivLink": "https://arxiv.org/abs/2407.20061",
                "title": "Quantum Dots Get a Helping Hand: AI-Free Bootstrapping for Faster Qubit Tuning",
                "institute": "National Institute of Standards and Technology",
                "text": "This research focuses on automating the initial setup of quantum dot devices, a crucial step before more complex qubit operations. Unlike previous work that relied on machine learning, this approach uses physics-based heuristics, making it more interpretable and less prone to unexpected failures.",
                "paper-title": "Autonomous Bootstrapping of Quantum Dot Devices",
                "image-path": ""
            },

            {
                "startTime": "23:19",
                "arxivId": "2407.19035",
                "arxivLink": "https://arxiv.org/abs/2407.19035",
                "title": "Gaussian Splatting Gets a Boost: 3D Content Creation Goes Super-Fast!",
                "institute": "East China University of Science and Technology, University of Washington, University of Copenhagen",
                "text": "This research introduces a new 3D content creation framework called ScalingGaussian, which combines 3D and 2D diffusion models to generate detailed textures and geometric consistency in 3D assets. Unlike previous methods that rely solely on 3D diffusion, ScalingGaussian leverages the strengths of both 3D and 2D diffusion models, resulting in a more efficient and effective approach.",
                "paper-title": "ScalingGaussian: Enhancing 3D Content Creation with Generative Gaussian Splatting",
                "image-path": ""
            },

            {
                "startTime": "23:51",
                "arxivId": "2407.20177",
                "arxivLink": "https://arxiv.org/abs/2407.20177",
                "title": "LLMs on a Diet: AutoScale Finds the Perfect Data Recipe for Training",
                "institute": "Virginia Tech, University of Illinois, University of Washington...",
                "text": "This research proposes AutoScale, a tool that automatically determines the optimal data composition for training LLMs at any desired scale. Unlike previous work that optimizes data composition for smaller proxy models, AutoScale leverages scaling laws to predict optimal compositions for larger models, eliminating the need for expensive re-training.",
                "paper-title": "AutoScale: Automatic Prediction of Compute-optimal Data Composition for Training LLMs",
                "image-path": ""
            },

            {
                "startTime": "24:15",
                "arxivId": "2407.19866",
                "arxivLink": "https://arxiv.org/abs/2407.19866",
                "title": "Deep Image Priors: MRF Gets a Speed Boost!",
                "institute": "University of Bristol, INFN Pisa, University of Bath...",
                "text": "This research introduces BARDIP, a new method for MRF reconstruction that uses a pretrained Bloch-consistent denoising autoencoder (BDAE) to improve speed and accuracy compared to previous DIP-MRF approaches.",
                "paper-title": "Deep Image Priors for Magnetic Resonance Fingerprinting with pretrained Bloch-consistent denoising autoencoders",
                "image-path": ""
            },

            {
                "startTime": "24:38",
                "arxivId": "2407.18946",
                "arxivLink": "https://arxiv.org/abs/2407.18946",
                "title": "Doggy Dance Moves: AI Learns to Align Motion Across Species",
                "institute": "Meta, ETH Zurich",
                "text": "This research proposes a new approach for understanding motion datasets that is independent of the character's morphology and skeletal structure. Unlike previous methods that rely on sparse high-dimensional latent spaces, this paper introduces a phase manifold consisting of multiple closed curves, each representing a latent amplitude. This allows for a more compact and structured representation of motion, enabling alignment across different characters.",
                "paper-title": "WalkTheDog: Cross-Morphology Motion Alignment via Phase Manifolds",
                "image-path": ""
            },

            {
                "startTime": "25:10",
                "arxivId": "2407.19039",
                "arxivLink": "https://arxiv.org/abs/2407.19039",
                "title": "Tokenizing Molecules: BPE Goes From Words to Graphs!",
                "institute": "CMU",
                "text": "This research proposes GRAPHBPE, a novel tokenization method for molecular graphs that adapts the Byte-Pair Encoding (BPE) algorithm commonly used in Natural Language Processing (NLP). Unlike previous methods that rely on external knowledge or trained neural networks, GRAPHBPE solely utilizes a given molecular graph corpus and is model-agnostic.",
                "paper-title": "GraphBPE: Molecular Graphs Meet Byte-Pair Encoding",
                "image-path": ""
            },

            {
                "startTime": "25:41",
                "arxivId": "2407.19937",
                "arxivLink": "https://arxiv.org/abs/2407.19937",
                "title": "Recommending Restaurants? It's All About the Order!",
                "institute": "Fudan University, Microsoft",
                "text": "This research introduces the \"Aspect Order Tree\" (AOTree) model, which considers the order in which users evaluate aspects of an item when making a decision. This is different from previous work that primarily focused on the importance of aspects, neglecting their sequential relationship.",
                "paper-title": "AOTree: Aspect Order Tree-based Model for Explainable Recommendation",
                "image-path": ""
            },

            {
                "startTime": "26:05",
                "arxivId": "2407.19410",
                "arxivLink": "https://arxiv.org/abs/2407.19410",
                "title": "Prompt Compression: Making AI Code Like a Pro, But Shorter!",
                "institute": "Tokyo Institute of Technology, National Institute of Informatics, OMRON SINIC X Corporation",
                "text": "This research introduces AdaCoder, a framework that compresses prompts used in visual programmatic models (VPMs). Unlike previous methods that rely on fine-tuning or additional training, AdaCoder uses a single frozen large language model (LLM) to adaptively select compressed prompts based on the question type.",
                "paper-title": "AdaCoder: Adaptive Prompt Compression for Programmatic Visual Question Answering",
                "image-path": ""
            },

            {
                "startTime": "26:40",
                "arxivId": "2407.19772",
                "arxivLink": "https://arxiv.org/abs/2407.19772",
                "title": "Code Tests From Thin Air: LLMs Get a New Benchmarking Challenge",
                "institute": "IBM",
                "text": "This research introduces a method for generating benchmark variations that generalize across coding tasks and programming languages, using Abstract Syntax Trees (ASTs). This approach mitigates the issue of benchmark data leaking into the training data of Large Language Models (LLMs).",
                "paper-title": "Generating Unseen Code Tests In Infinitum",
                "image-path": ""
            },

            {
                "startTime": "27:03",
                "arxivId": "2407.19094",
                "arxivLink": "https://arxiv.org/abs/2407.19094",
                "title": "Robots Get a Brain: Vision-Language Models Solve Tasks Without Training!",
                "institute": "Northwestern University, Yale University",
                "text": "This research introduces Wonderful Team, a multi-agent system that uses a single off-the-shelf vision-language model (VLLM) to solve robotic tasks without any prior training. Unlike previous work that relies on fine-tuning or separate vision encoders, Wonderful Team leverages the advanced capabilities of VLLMs to handle all aspects of a robotic task, from high-level planning to low-level action execution.",
                "paper-title": "Solving Robotics Problems in Zero-Shot with Vision-Language Models",
                "image-path": ""
            },

            {
                "startTime": "27:30",
                "arxivId": "2407.19893",
                "arxivLink": "https://arxiv.org/abs/2407.19893",
                "title": "Zero-Shot IoT Sensing: Teaching AI to Smell the Coffee (and Recognize Unseen Things)",
                "institute": "Delft University of Technology, Google, University of Pittsburgh",
                "text": "This research proposes a novel approach to zero-shot IoT sensing by leveraging foundation models (FMs) and aligning IoT data embeddings with semantic embeddings generated by an FM's text encoder. This differs from previous work by utilizing cross-attention to combine learnable soft prompts and auxiliary hard prompts for effective prompt engineering, and by employing data augmentation to synthesize unseen class IoT data for fine-tuning the IoT feature extractor and embedding projector.",
                "paper-title": "Leveraging Foundation Models for Zero-Shot IoT Sensing",
                "image-path": ""
            },

            {
                "startTime": "27:54",
                "arxivId": "2407.19922",
                "arxivLink": "https://arxiv.org/abs/2407.19922",
                "title": "Sentiment Analysis Gets a Boost: LLMs Explain Currency Pair Predictions",
                "institute": "IBM",
                "text": "This research explores using LLMs to explain sentiment analysis results, specifically in the context of predicting currency pair prices. Unlike previous work that focused on sentiment analysis itself, this paper investigates how LLMs can be used to identify key terms that explain the sentiment, enriching the input for more accurate predictions.",
                "paper-title": "Monetizing Currency Pair Sentiments through LLM Explainability",
                "image-path": ""
            },

            {
                "startTime": "28:13",
                "arxivId": "2407.20229",
                "arxivLink": "https://arxiv.org/abs/2407.20229",
                "title": "2D Vision Models Get a 3D Makeover: Fine-Tuning with Gaussian Splatting",
                "institute": "ETH Zurich, Max Planck Institute for Informatics, Google",
                "text": "This research proposes a novel two-stage approach to improve the 3D understanding of 2D vision models. Unlike previous work that focuses on fusing multi-view 2D features into 3D representations, this paper explores the reverse direction, incorporating 3D awareness into 2D representation learning through fine-tuning.",
                "paper-title": "Improving 2D Feature Representations by 3D-Aware Fine-Tuning",
                "image-path": ""
            },

            {
                "startTime": "28:39",
                "arxivId": "2407.19853",
                "arxivLink": "https://arxiv.org/abs/2407.19853",
                "title": "Learning to Adapt on the Fly: A New Trick for Multi-Source Domain Adaptation",
                "institute": "Universit\u00e9 Paris-Saclay, CEA, \u00c9cole Centrale de Lyon",
                "text": "This paper introduces a novel approach for online multi-source domain adaptation (MSDA) by leveraging Gaussian Mixture Models (GMMs) and the Wasserstein geometry of Gaussian measures. Unlike previous methods that assume all data is available during training, this work tackles the challenge of adapting to a target domain in a streaming fashion.",
                "paper-title": "Online Multi-Source Domain Adaptation through Gaussian Mixtures and Dataset Dictionary Learning",
                "image-path": ""
            },

            {
                "startTime": "29:10",
                "arxivId": "2407.20020",
                "arxivLink": "https://arxiv.org/abs/2407.20020",
                "title": "ImagiNet: Spotting Fake Photos with a Multi-Content Dataset",
                "institute": "Stanford University",
                "text": "This research introduces ImagiNet, a new dataset for synthetic image detection that includes a wider range of content types and generators compared to previous datasets. This approach aims to address the limitations of existing datasets, which often lack diversity and can lead to biased detectors.",
                "paper-title": "ImagiNet: A Multi-Content Dataset for Generalizable Synthetic Image Detection via Contrastive Learning",
                "image-path": ""
            },

            {
                "startTime": "29:42",
                "arxivId": "2407.19965",
                "arxivLink": "https://arxiv.org/abs/2407.19965",
                "title": "KNN-MT Gets a Speed Boost: A Single-Layer Network Makes Translation Faster and More Accurate!",
                "institute": "Microsoft",
                "text": "This research proposes a single-layer neural network for kNN-MT interpolation, which is simpler and faster to train than previous methods. It also integrates kNN-MT into GPU inference, demonstrating its feasibility for large-scale models.",
                "paper-title": "Simply Trainable Nearest Neighbour Machine Translation with GPU Inference",
                "image-path": ""
            },

            {
                "startTime": "30:03",
                "arxivId": "2407.19089",
                "arxivLink": "https://arxiv.org/abs/2407.19089",
                "title": "AI Chemists: Building Better Drugs with a Million-Word Memory!",
                "institute": "Sanofi, Digital Data, Integrated Drug Discovery...",
                "text": "This research introduces a semi-supervised learning method for many-shot in-context learning (ICL) in molecular inverse design. Unlike previous work, this approach iteratively includes LLM-generated molecules with high predicted performance, along with experimental data, to overcome the lack of available experimental data.",
                "paper-title": "Many-Shot In-Context Learning for Molecular Inverse Design",
                "image-path": ""
            },

            {
                "startTime": "30:33",
                "arxivId": "2407.19110",
                "arxivLink": "https://arxiv.org/abs/2407.19110",
                "title": "Fedspeak Decoded: GPT-4 Uncovers Hidden Dissent in FOMC Meetings",
                "institute": "Princeton University, University of Washington, University of Maryland",
                "text": "This research uses GPT-4 to analyze FOMC transcripts and statements, going beyond previous work that relied on a single document type. It quantifies dissent among committee members, revealing that transcripts contain more disagreement than public statements.",
                "paper-title": "GPT Deciphering Fedspeak: Quantifying Dissent Among Hawks and Doves",
                "image-path": ""
            },

            {
                "startTime": "30:59",
                "arxivId": "2407.19034",
                "arxivLink": "https://arxiv.org/abs/2407.19034",
                "title": "MangaUB: A Benchmark for Large Multimodal Models to Understand the Art of Comics",
                "institute": "The University of Tokyo",
                "text": "This research introduces MangaUB, a new benchmark specifically designed to evaluate the manga understanding capabilities of large multimodal models (LLMs). Unlike previous benchmarks that focus on individual elements like text or characters, MangaUB assesses the LLM's ability to comprehend the narrative conveyed across multiple panels.",
                "paper-title": "MangaUB: A Manga Understanding Benchmark for Large Multimodal Models",
                "image-path": ""
            },

            {
                "startTime": "31:27",
                "arxivId": "2407.19396",
                "arxivLink": "https://arxiv.org/abs/2407.19396",
                "title": "MiniGrid Gets a JAX Makeover: 200,000x Speed Boost for Deep RL!",
                "institute": "University College London, Technical University of Berlin, University of Oxford...",
                "text": "This research introduces NAVIX, a JAX-based reimplementation of the MiniGrid environment suite. Unlike previous work, NAVIX focuses on providing a fully batched and differentiable implementation of the entire MiniGrid suite, enabling significant speedups and scalability for Deep RL training.",
                "paper-title": "NAVIX: Scaling MiniGrid Environments with JAX",
                "image-path": ""
            },

            {
                "startTime": "31:55",
                "arxivId": "2407.20199",
                "arxivLink": "https://arxiv.org/abs/2407.20199",
                "title": "Grokking Modular Arithmetic: It's Not Just for Neural Networks Anymore!",
                "institute": "MIT, Indian Institute of Technology Bombay",
                "text": "This paper demonstrates that the phenomenon of \"grokking\" modular arithmetic, previously observed in neural networks, also occurs in Recursive Feature Machines (RFM), an algorithm that uses the Average Gradient Outer Product (AGOP) for feature learning. This finding suggests that grokking is not specific to neural networks or gradient-based optimization methods.",
                "paper-title": "Emergence in non-neural models: grokking modular arithmetic via average gradient outer product",
                "image-path": ""
            },

            {
                "startTime": "32:27",
                "arxivId": "2407.19540",
                "arxivLink": "https://arxiv.org/abs/2407.19540",
                "title": "Missing Data? No Problem! NECHO v2 Predicts Diagnoses with a Knowledge Distillation Twist",
                "institute": "University College London",
                "text": "This research introduces NECHO v2, a framework that tackles the challenge of predicting patient diagnoses when some medical records are missing. Unlike previous methods, NECHO v2 uses a systematic knowledge distillation approach to transfer knowledge from a teacher model trained on complete data to a student model learning with incomplete data.",
                "paper-title": "Overcoming Uncertain Incompleteness for Robust Multimodal Sequential Diagnosis Prediction via Knowledge Distillation and Random Data Erasing",
                "image-path": ""
            },

            {
                "startTime": "32:59",
                "arxivId": "2407.19072",
                "arxivLink": "https://arxiv.org/abs/2407.19072",
                "title": "Deep Learning's Got a New Trick: Recognizing Objects by Their Shape, Not Just Their Parts!",
                "institute": "Korea University, Fujitsu, MIT",
                "text": "This research explores the role of \"configural processing\" in object recognition, where neural networks learn to identify objects based on the spatial relationships between their components, rather than just individual features. This approach is distinct from previous work that primarily focused on local feature-based recognition.",
                "paper-title": "Configural processing as an optimized strategy for robust object recognition in neural networks",
                "image-path": ""
            },

            {
                "startTime": "33:23",
                "arxivId": "2407.19118",
                "arxivLink": "https://arxiv.org/abs/2407.19118",
                "title": "AI Co-Pilots: Saving Medical Studies from Bias-Induced Meltdowns!",
                "institute": "UC Berkeley, UCSF, Microsoft Research",
                "text": "This research proposes using large language models (LLMs) as \"causal co-pilots\" to assist researchers in identifying and addressing potential biases in medical studies based on real-world data (RWD). This approach differs from previous work by focusing on the collaborative role of LLMs in study design and analysis, rather than solely relying on them for data analysis or interpretation.",
                "paper-title": "Large Language Models as Co-Pilots for Causal Inference in Medical Studies",
                "image-path": ""
            },

            {
                "startTime": "33:49",
                "arxivId": "2407.19256",
                "arxivLink": "https://arxiv.org/abs/2407.19256",
                "title": "ChatGPT in the ICU: Can AI Really Be a Doctor?",
                "institute": "Peking University",
                "text": "This research differs from previous work by focusing specifically on the application of large language models (LLMs) in critical care medicine (CCM). It provides a comprehensive overview of current applications, challenges, and future directions for LLMs in this field.",
                "paper-title": "Stochastic Parrots or ICU Experts? Large Language Models in Critical Care Medicine: A Scoping Review",
                "image-path": ""
            },

            {
                "startTime": "34:12",
                "arxivId": "2407.20067",
                "arxivLink": "https://arxiv.org/abs/2407.20067",
                "title": "Explaining AI: When It Can't Explain, It's Time to Drop It!",
                "institute": "University of Cambridge",
                "text": "This research introduces a novel approach to GNN training called xAI-Drop, which leverages explainability to identify and exclude noisy network elements during training. Unlike previous dropping methods that rely on random or heuristic-based selection criteria, xAI-Drop uses explainability scores to pinpoint potentially harmful components.",
                "paper-title": "xAI-Drop: Don't Use What You Cannot Explain",
                "image-path": ""
            },

            {
                "startTime": "34:33",
                "arxivId": "2407.19985",
                "arxivLink": "https://arxiv.org/abs/2407.19985",
                "title": "Vision Transformers on a Diet: MoNE Makes Models Slimmer Without Losing Sight!",
                "institute": "Google",
                "text": "This research introduces Mixture of Nested Experts (MoNE), a framework that dynamically allocates computational resources to different visual tokens during inference. Unlike previous approaches that process all tokens with the same compute, MoNE uses nested models with varying computational capacities, allowing for more efficient processing.",
                "paper-title": "Mixture of Nested Experts: Adaptive Processing of Visual Tokens",
                "image-path": ""
            },

            {
                "startTime": "34:57",
                "arxivId": "2407.19074",
                "arxivLink": "https://arxiv.org/abs/2407.19074",
                "title": "Neural Networks: Solving Geotechnical Problems with a Parsimonious Twist!",
                "institute": "The Hong Kong Polytechnic University, University of Cambridge, University of Leeds",
                "text": "This research introduces a novel parsimonious loss function for physics-informed neural networks (PINNs) to solve complex geotechnical problems. Unlike previous work, this approach simplifies the governing equations and reduces the output dimensions, leading to more accurate and efficient solutions.",
                "paper-title": "Parsimonious Universal Function Approximator for Elastic and Elasto-Plastic Cavity Expansion Problems",
                "image-path": ""
            },

            {
                "startTime": "35:22",
                "arxivId": "2407.19048",
                "arxivLink": "https://arxiv.org/abs/2407.19048",
                "title": "Gravitational Wave Inference Gets a Speed Boost with AI!",
                "institute": "MIT",
                "text": "This research introduces AMPLFI, a parameter estimation algorithm for gravitational waves that uses likelihood-free inference with normalizing flows. Unlike previous methods, AMPLFI is designed for real-time inference, running alongside a neural-network-based search pipeline.",
                "paper-title": "Rapid Likelihood Free Inference of Compact Binary Coalescences using Accelerated Hardware",
                "image-path": ""
            },

            {
                "startTime": "35:50",
                "arxivId": "2407.19447",
                "arxivLink": "https://arxiv.org/abs/2407.19447",
                "title": "Nudging Consent: How England's New Opt-Out System Is Tricking Us All!",
                "institute": "University College London, University of Oxford",
                "text": "This research examines the revised opt-out system for secondary use of health data in England, focusing on how the system's design and communication influence patient choices. It contrasts this approach with previous opt-out systems and explores the potential for \"hard paternalism\" in this context.",
                "paper-title": "Nudging Consent and the New Opt Out System to the Processing of Health Data in England",
                "image-path": ""
            },

            {
                "startTime": "36:14",
                "arxivId": "2407.20207",
                "arxivLink": "https://arxiv.org/abs/2407.20207",
                "title": "Stop the Information Loss! New Text Augmentation Framework Makes Dense Retrieval Smarter",
                "institute": "Tsinghua University",
                "text": "This research introduces a novel text augmentation framework called QAEA-DR, which focuses on enhancing the original text itself rather than just improving the embedding model or retrieval process. It utilizes large language models (LLMs) to generate question-answer pairs and element-driven events from the original text, effectively concentrating key information and removing noise.",
                "paper-title": "QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval",
                "image-path": ""
            },

            {
                "startTime": "36:41",
                "arxivId": "2407.18949",
                "arxivLink": "https://arxiv.org/abs/2407.18949",
                "title": "AI Tries to Crack the New Yorker's Caption Contest: Can Machines Be Funny?",
                "institute": "Stanford University",
                "text": "This research focuses on generating humorous captions for New Yorker cartoons, a task that requires understanding not just the visual elements but also cultural nuances and humor. It explores the use of vision transformer encoder-decoder models and compares their performance to GPT-4V, a large language model with multi-modal capabilities.",
                "paper-title": "Predicting Winning Captions for Weekly New Yorker Comics",
                "image-path": ""
            },

            {
                "startTime": "37:01",
                "arxivId": "2407.19460",
                "arxivLink": "https://arxiv.org/abs/2407.19460",
                "title": "Missing Brain Data? WMG-Diff to the Rescue!",
                "institute": "Harvard University",
                "text": "This research proposes a novel deep learning model called WMG-Diff to impute missing tissue microstructure data in diffusion MRI tractography. Unlike previous methods that rely on random data selection, WMG-Diff leverages the geometric relationships between white matter fiber clusters to guide the imputation process.",
                "paper-title": "White Matter Geometry-Guided Score-Based Diffusion Model for Tissue Microstructure Imputation in Tractography Imaging",
                "image-path": ""
            },

            {
                "startTime": "37:25",
                "arxivId": "2407.19618",
                "arxivLink": "https://arxiv.org/abs/2407.19618",
                "title": "A/B Testing Got a Makeover: Sharing Data Makes Experiments Smarter!",
                "institute": "Shanghai Jiao Tong University, MIT",
                "text": "This research focuses on experiments with local treatments in Markov Decision Processes (MDPs), where the treatment only affects specific states. It introduces a variance reduction technique that leverages this local structure by sharing information across states unaffected by the treatment. This approach contrasts with previous work that typically considers global treatments applied uniformly across all states.",
                "paper-title": "Experimenting on Markov Decision Processes with Local Treatments",
                "image-path": ""
            },

            {
                "startTime": "37:49",
                "arxivId": "2407.20232",
                "arxivLink": "https://arxiv.org/abs/2407.20232",
                "title": "Ambiguous Edits? SANE Makes 'Em Specific!",
                "institute": "T\u00e9l\u00e9com Paris Institut Polytechnique de Paris, University of Oxford",
                "text": "This paper introduces SANE, a method that uses a large language model (LLM) to break down ambiguous user instructions into specific editing tasks for text-based image editing diffusion models. This differs from previous work by explicitly decomposing instructions rather than relying on implicit concept interpretations learned by visual-language models.",
                "paper-title": "Specify and Edit: Overcoming Ambiguity in Text-Based Image Editing",
                "image-path": ""
            },

            {
                "startTime": "38:21",
                "arxivId": "2407.19346",
                "arxivLink": "https://arxiv.org/abs/2407.19346",
                "title": "Polynomial Playground: Unlocking the Secrets of In-Context Learning with a Simple Function",
                "institute": "UC Berkeley",
                "text": "This research uses univariate polynomial regression as a toy problem to study in-context learning in transformer-based architectures. Unlike previous work that used linear regression or multi-layer perceptrons, this approach allows for the exploration of prompting and alignment within a more structured function class.",
                "paper-title": "Polynomial Regression as a Task for Understanding In-context Learning Through Finetuning and Alignment",
                "image-path": ""
            }
    ],
    "stats": {
        "num_pick": 86,
        "num_total": 454,
    },
    "audio": "https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202407301805_audio.mp3"
}