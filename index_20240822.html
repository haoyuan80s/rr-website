<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                    Fresh Picks:
                    <span class="highlightNumber">61</span> out of <span class="highlightNumber">276</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-08-22"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>

        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">
                00:52
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11817" target="_blank">
                        @arXiv 2408.11817
                    </a>
                    <span class="tweet-title">
                        New Benchmark Makes Even the Smartest AI Models Look Dumb!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge, University of Hong Kong
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark called GRAB, specifically designed to evaluate the
                    graph
                    analysis capabilities of large multimodal models (LMMs). Unlike previous benchmarks, GRAB
                    focuses on
                    tasks that require visual analytical reasoning, such as estimating means, intercepts, and
                    correlations from graphs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">
                01:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11796" target="_blank">
                        @arXiv 2408.11796
                    </a>
                    <span class="tweet-title">
                        Shrinking LLMs: How to Make Big Models Tiny Without Losing Their Smarts
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        NVIDIA
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on compressing large language models (LLMs) using pruning and
                    distillation,
                    but it introduces a new step called "teacher correction" where the teacher model is fine-tuned
                    on
                    the distillation dataset before the compression process. This addresses the issue of data
                    distribution mismatch that can occur when the original training data is unavailable.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">
                01:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11441" target="_blank">
                        @arXiv 2408.11441
                    </a>
                    <span class="tweet-title">
                        AI's Got a Bias Problem: How Generative Models Are Perpetuating Epistemic Injustice
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This paper expands the concept of epistemic injustice to generative AI, identifying four
                    distinct
                    configurations of harm: amplified testimonial injustice, manipulative testimonial injustice,
                    hermeneutical ignorance, and access injustice.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">
                02:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11283" target="_blank">
                        @arXiv 2408.11283
                    </a>
                    <span class="tweet-title">
                        Inference Plans: Giving Probabilistic Programs a Little Control
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT, Binghamton University, IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces inference plans, a programming interface that allows developers to
                    control
                    how random variables are partitioned during hybrid particle filtering. This is different from
                    previous work, which relied on heuristics that were not necessarily aligned with the developer's
                    performance goals.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">
                02:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11150" target="_blank">
                        @arXiv 2408.11150
                    </a>
                    <span class="tweet-title">
                        Deep Learning Deciphers Medieval Scribes' Secrets
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        École des Ponts ParisTech
                    </span>
                </div>
                <div class="primary-text">
                    This research adapts a deep learning method called the Learnable Typewriter to analyze the
                    morphology of medieval handwriting, focusing on character prototypes rather than classification.
                    This approach allows for a more nuanced and interpretable analysis of script variations compared
                    to
                    previous methods that relied on global impressions or simple classification tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">
                02:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11527" target="_blank">
                        @arXiv 2408.11527
                    </a>
                    <span class="tweet-title">
                        Vizier's Secret Sauce: How Google's Black Box Optimization Algorithm Got a Python Makeover
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google DeepMind, Google Research
                    </span>
                </div>
                <div class="primary-text">
                    This research paper formalizes and open-sources the Google Vizier algorithm, which is a Bayesian
                    optimization algorithm used for tuning millions of objectives across Google. The paper also
                    includes
                    a Python implementation of the algorithm, powered by TFProbability on JAX, and benchmarks it
                    against
                    industry-wide baselines.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">
                03:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11564" target="_blank">
                        @arXiv 2408.11564
                    </a>
                    <span class="tweet-title">
                        AI Movie Director: From Script to Screen, AutoDirector Makes Filmmaking a Breeze!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces AutoDirector, an interactive agent system that automates the scheduling
                    of
                    tasks in multi-sensory film production. Unlike previous systems, AutoDirector dynamically plans
                    tasks based on user feedback, allowing for real-time adjustments and optimization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">
                03:41
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11811" target="_blank">
                        @arXiv 2408.11811
                    </a>
                    <span class="tweet-title">
                        Segment Anything in 3D, Online and in Real Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Nanyang Technological University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach for online 3D instance segmentation by lifting 2D masks
                    generated by a vision foundation model (VFM) to 3D queries. This differs from previous methods
                    that
                    directly project 2D masks to 3D, which can lead to inaccurate and inconsistent results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">
                04:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11062" target="_blank">
                        @arXiv 2408.11062
                    </a>
                    <span class="tweet-title">
                        Text-to-SQL: Chatting with Databases, One Query at a Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, Zuoyebang Education Technology Co. Ltd.
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Interactive-T2S, a framework that uses large language models (LLMs) to
                    generate SQL queries through multi-turn interactions with databases. Unlike previous methods,
                    Interactive-T2S focuses on a step-by-step, interpretable SQL generation process, using a set of
                    tools to guide the LLM in identifying relevant information and relationships within the
                    database.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">
                04:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11210" target="_blank">
                        @arXiv 2408.11210
                    </a>
                    <span class="tweet-title">
                        SAM2 in 3D: A Click-by-Click Adventure in Medical Image Segmentation
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        NVIDIA
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on evaluating the performance of Segment Anything Model 2 (SAM2) for 3D
                    medical image segmentation, specifically using a standardized evaluation protocol based on the
                    original SAM2 paper. It differs from previous studies by employing an iterative annotation
                    simulation that mimics user clicks, allowing for a more realistic assessment of SAM2's
                    capabilities
                    in interactive segmentation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">
                05:00
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11745" target="_blank">
                        @arXiv 2408.11745
                    </a>
                    <span class="tweet-title">
                        LLMs Get a Memory Boost: Parallel Decoding Makes Long Texts a Breeze!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Xiamen University
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces FocusLLM, a framework that uses parallel decoding to process long texts.
                    Unlike previous methods that compress or modify attention mechanisms, FocusLLM divides the text
                    into
                    chunks and processes them concurrently, allowing it to handle much longer sequences without
                    sacrificing accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">
                05:23
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11791" target="_blank">
                        @arXiv 2408.11791
                    </a>
                    <span class="tweet-title">
                        Reward Models That Talk Back: Critiques Boost AI Performance
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Critique-out-Loud (CLoud) reward models, which differ from traditional
                    reward models by generating natural language critiques of responses before predicting a reward
                    score. This allows the model to explicitly reason about the quality of a response, rather than
                    relying on implicit reasoning within a single forward pass.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">
                05:47
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11200" target="_blank">
                        @arXiv 2408.11200
                    </a>
                    <span class="tweet-title">
                        Unbound KANs: Breaking Free from Grids with a GPU Boost!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nvidia
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Unbounded Kolmogorov-Arnold Networks (UKANs), which eliminate the need
                    for
                    a bounded grid in traditional KANs. This is achieved by replacing the fixed B-spline
                    coefficients
                    with a coefficient generator (CG) model, allowing for an unbounded domain.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">
                06:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11084" target="_blank">
                        @arXiv 2408.11084
                    </a>
                    <span class="tweet-title">
                        Stop Wasting Samples! New Method Makes Biased Oracles Less Painful
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        École Polytechnique Fédérale de Lausanne, Georgia Institute of Technology, ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a family of multi-level Monte Carlo (MLMC) gradient methods for
                    stochastic
                    optimization problems where only biased gradient oracles are available. Unlike previous work
                    that
                    focuses on iteration complexity, this paper considers the cost of obtaining low-bias gradients,
                    leading to a more comprehensive analysis of total computational complexity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">
                06:44
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11065" target="_blank">
                        @arXiv 2408.11065
                    </a>
                    <span class="tweet-title">
                        Physics Equations: A Hidden Language with a Secret Code!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford, Institut d’Astrophysique de Paris, University of Portsmouth
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the statistical patterns of operators in physics equations, going beyond
                    previous work that focused on the distribution of words in natural languages.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">
                07:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11481" target="_blank">
                        @arXiv 2408.11481
                    </a>
                    <span class="tweet-title">
                        E-Bench: Giving AI Video Editors a Reality Check (and a Score!)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, Peng Cheng Laboratory
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces E-Bench, a benchmark suite specifically designed for evaluating
                    text-driven
                    video editing. Unlike previous metrics that focus on visual quality, E-Bench QA prioritizes
                    text-video alignment and the relationship between the original and edited videos.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">
                07:40
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11469" target="_blank">
                        @arXiv 2408.11469
                    </a>
                    <span class="tweet-title">
                        Negation Nation: Can AI Tell the Difference Between "Likes" and "Doesn't Like"?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Paris, École Normale Supérieure
                    </span>
                </div>
                <div class="primary-text">
                    This research builds upon previous work by Gubelmann and Handschuh (2022) but refines the
                    methodology to create a more controlled test for evaluating how well language models understand
                    negation. The authors propose a new test, the Self-Contained Neg Test, which uses minimal pairs
                    varying only in the presence or absence of negation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">
                08:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11382" target="_blank">
                        @arXiv 2408.11382
                    </a>
                    <span class="tweet-title">
                        Positional Embeddings: Swap 'em Out, Don't Throw 'em Out!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft Corporation
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the feasibility of replacing positional embeddings in pre-trained neural
                    machine translation models with alternative approaches like RoPE and ALiBi, without significant
                    performance loss. This differs from previous work that focused on training models from scratch
                    with
                    these new embeddings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">
                08:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11334" target="_blank">
                        @arXiv 2408.11334
                    </a>
                    <span class="tweet-title">
                        LLM for Breast Ultrasound Reports: GPT-4's Little Helper
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        NYU, CMU, NYU Langone Health
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the development of an in-house LLM for extracting clinical information
                    from
                    breast ultrasound reports. Unlike previous work that relies on costly proprietary LLMs or
                    struggles
                    with large datasets, this study proposes a pipeline using GPT-4 to generate a small labeled
                    dataset
                    for fine-tuning an open-source LLM, Llama3-8B.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">
                08:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11812" target="_blank">
                        @arXiv 2408.11812
                    </a>
                    <span class="tweet-title">
                        One Policy to Rule Them All: A Transformer for Cross-Embodied Robot Learning
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces CrossFormer, a transformer-based policy that can control diverse robots
                    without requiring manual alignment of observation or action spaces. Unlike prior work,
                    CrossFormer
                    is trained on a much larger and more diverse dataset, encompassing 900K trajectories across 20
                    different robot embodiments.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">
                09:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11186" target="_blank">
                        @arXiv 2408.11186
                    </a>
                    <span class="tweet-title">
                        Negotiating Without Spilling the Beans: A Cone-Shaped Approach to Trading
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        The University of Texas at Austin
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new algorithm, ST-CR, for autonomous negotiation that relies on
                    comparison-based gradient estimation. Unlike previous methods that require agents to share
                    extensive
                    information about their preferences, ST-CR only needs acceptance or rejection responses.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">
                09:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11751" target="_blank">
                        @arXiv 2408.11751
                    </a>
                    <span class="tweet-title">
                        Robot Fleet Design: A Bayesian Optimization Recipe for Exploration Success!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto, Korea Advanced Institute of Science and Technology, Inha University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel framework called Bayesian Optimization for Fleet Design (BOFD)
                    that
                    uses multi-objective Bayesian optimization to efficiently design heterogeneous robot fleets for
                    exploration tasks. Unlike previous work that focused on single-objective optimization or
                    homogeneous
                    fleets, BOFD considers the trade-off between performance and cost, making it more suitable for
                    real-world applications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">
                10:07
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11237" target="_blank">
                        @arXiv 2408.11237
                    </a>
                    <span class="tweet-title">
                        Attention, Please! New Trick Makes Document Classifiers Smarter (and Less Overconfident)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Bristol, CMU, San Diego State University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on improving out-of-distribution (OOD) detection in document
                    classification
                    systems. Unlike previous methods that modify the training process, this paper proposes a novel
                    technique called Attention Head Masking (AHM) that operates during inference. AHM selectively
                    masks
                    attention heads in transformer-based models to enhance the separation between in-distribution
                    (ID)
                    and OOD data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">
                10:46
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11381" target="_blank">
                        @arXiv 2408.11381
                    </a>
                    <span class="tweet-title">
                        RAGLAB: A Unified Framework for Retrieval-Augmented Generation, Because LLMs Need a Little
                        Help!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nanjing University, Squirrel AI, Peking University...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces RAGLAB, a modular and research-oriented open-source library that
                    provides a
                    unified framework for comparing and developing Retrieval-Augmented Generation (RAG) algorithms.
                    Unlike previous work, RAGLAB standardizes key experimental variables, enabling fair comparisons
                    between different RAG algorithms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">
                11:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11357" target="_blank">
                        @arXiv 2408.11357
                    </a>
                    <span class="tweet-title">
                        Dress Up Your Digital Self: New AI Makes Layered 3D Humans with Swappable Clothes!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tianjin University, Changzhou Institute of Technology, Tsinghua University...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method for generating layered 3D humans with physically
                    decoupled
                    clothing, allowing for separate editing and reuse of clothing items. Unlike previous methods
                    that
                    generate 3D humans as a whole or only support simple clothing, this approach enables the
                    creation of
                    complex, multi-layered outfits that can be transferred between different avatars.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">
                11:48
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11363" target="_blank">
                        @arXiv 2408.11363
                    </a>
                    <span class="tweet-title">
                        ProteinGPT: Chatting with Proteins, One Amino Acid at a Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Los Angeles, Georgia Institute of Technology, Meta
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces ProteinGPT, a multimodal LLM that integrates protein sequence and
                    structure
                    encoders with a large language model. Unlike previous work that relies on a single modality,
                    ProteinGPT leverages both sequence and structure information to provide more comprehensive
                    protein
                    analysis.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">
                12:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11727" target="_blank">
                        @arXiv 2408.11727
                    </a>
                    <span class="tweet-title">
                        Toxic Prompts? No Problem! New AI Tool Sniffs Out Bad Behavior
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nanyang Technological University, ShanghaiTech University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes ToxicDetector, a greybox method that uses embeddings from the last token
                    of
                    each layer in a large language model (LLM) to identify toxic prompts. This approach differs from
                    previous blackbox and whitebox methods by leveraging internal model states for efficient and
                    scalable detection.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">
                12:41
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11104" target="_blank">
                        @arXiv 2408.11104
                    </a>
                    <span class="tweet-title">
                        PINNs Get a Gradient Makeover: ConFIG Fights Loss Term Conflicts!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Technical University of Munich, Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces the ConFIG method, which addresses the issue of conflicting gradients
                    during
                    the training of Physics-Informed Neural Networks (PINNs). Unlike previous methods that focus on
                    weighting strategies, ConFIG directly manipulates the gradient direction to ensure a
                    conflict-free
                    update.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">
                13:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11316" target="_blank">
                        @arXiv 2408.11316
                    </a>
                    <span class="tweet-title">
                        LLMs: Predicting the Future of Medicine, One Probability at a Time
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This research delves into the reliability of probability outputs generated by Large Language
                    Models
                    (LLMs) in medical predictions. Unlike previous work that focused on explicit probabilities
                    derived
                    from text generation, this study compares those to implicit probabilities calculated based on
                    the
                    likelihood of predicting the correct label token.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">
                13:37
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11326" target="_blank">
                        @arXiv 2408.11326
                    </a>
                    <span class="tweet-title">
                        AI Planning Gets a Brain: Automating Thought of Search for 100% Accuracy
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Cornell University, IBM Research
                    </span>
                </div>
                <div class="primary-text">
                    This research automates the Thought of Search (ToS) method, which previously required human
                    feedback. AutoToS uses unit tests and feedback loops to guide the language model towards
                    generating
                    sound and complete search components.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">
                14:01
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11537" target="_blank">
                        @arXiv 2408.11537
                    </a>
                    <span class="tweet-title">
                        Robots Learn to Grasp Like Humans: A New Embodied Learning Survey
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Hong Kong Polytechnic University, Tsinghua University, Chinese University of Hong Kong
                    </span>
                </div>
                <div class="primary-text">
                    This survey focuses on embodied learning for object-centric robotic manipulation, categorizing
                    existing work into three main branches: embodied perceptual learning, embodied policy learning,
                    and
                    embodied task-oriented learning. It also provides a comprehensive overview of public datasets,
                    evaluation metrics, representative applications, current challenges, and potential future
                    research
                    directions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">
                14:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11302" target="_blank">
                        @arXiv 2408.11302
                    </a>
                    <span class="tweet-title">
                        Shopping Cart Savvy: How AI Learns Your Hidden Product Preferences
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nanyang Technological University, Beihang University, University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel deep learning framework called ArcRec that models
                    reference-dependent
                    preferences in recommender systems. Unlike previous work, ArcRec incorporates both personalized
                    experiences and crowd influences to instantiate reference points at the attribute level,
                    capturing
                    consumers' heterogeneous salience of product attributes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">
                14:58
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11119" target="_blank">
                        @arXiv 2408.11119
                    </a>
                    <span class="tweet-title">
                        LLMs Go Sparse: A Decoder-Only Revolution in Retrieval
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Indian Institute of Technology Bombay, IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the use of decoder-only large language models (LLMs) for learned sparse
                    retrieval (LSR), a technique that combines the strengths of traditional keyword-based and
                    embedding-based retrieval methods. Unlike previous LSR systems that primarily used encoder-only
                    models, this study proposes a novel approach leveraging the power of decoder-only LLMs for
                    semantic
                    keyword expansion.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">
                15:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11227" target="_blank">
                        @arXiv 2408.11227
                    </a>
                    <span class="tweet-title">
                        OCTCube: Seeing the Retina in 3D, One Cube at a Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces OCTCube, a 3D foundation model for optical coherence tomography (OCT)
                    that
                    leverages the 3D structure of OCT volumes, unlike previous models that focused on 2D image
                    slices.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">
                16:01
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11490" target="_blank">
                        @arXiv 2408.11490
                    </a>
                    <span class="tweet-title">
                        Tired of Text Answers? Get Your Info in Tables!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, University of Science and Technology of China, Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces DocTabQA, a new approach to question answering that uses structured
                    tables
                    instead of plain text to present answers. Unlike previous text-to-table methods, DocTabQA
                    focuses on
                    generating hierarchical tables from long documents, making it more suitable for complex
                    information
                    retrieval tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">
                16:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11085" target="_blank">
                        @arXiv 2408.11085
                    </a>
                    <span class="tweet-title">
                        Camera Pose Refinement: 3D Gaussian Splatting Makes Localization a Breeze!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        HKUST, University of Oxford, Dartmouth College
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes GSLoc, a test-time camera pose refinement framework that utilizes 3D
                    Gaussian
                    Splatting (3DGS) for scene representation. Unlike previous NeRF-based methods, GSLoc avoids
                    training
                    specialized feature extractors and relies on direct RGB matching, making it faster and easier to
                    deploy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">
                16:49
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11219" target="_blank">
                        @arXiv 2408.11219
                    </a>
                    <span class="tweet-title">
                        Small Language Models Learn to Chat Like Humans with "CoDi"
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Meta
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel data distillation framework called "CoDi" to synthesize
                    large-scale, diverse, and steerable multi-turn conversations from black-box LLMs. This approach
                    aims
                    to improve the conversational abilities of smaller language models (SLMs) by leveraging the
                    knowledge of larger models. Unlike previous work that focuses on instruction-tuning, CoDi
                    targets
                    the shortcomings of LLMs in generating natural multi-turn conversations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">
                17:10
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11187" target="_blank">
                        @arXiv 2408.11187
                    </a>
                    <span class="tweet-title">
                        Drones and Trucks: A Delivery Duo That's Got the Route Down!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research extends the Flying Sidekick Traveling Salesman Problem (FSTSP) to a multi-agent
                    setting, considering multiple trucks and drones operating on real-world road networks. It
                    differs
                    from previous work by incorporating the complexities of road networks and proposing a
                    three-phase
                    algorithm for efficient route optimization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">
                17:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11138" target="_blank">
                        @arXiv 2408.11138
                    </a>
                    <span class="tweet-title">
                        Robot Grasping Gets a Local Makeover: New AI Focuses on Target, Not the Whole Scene!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new approach to robotic grasping that focuses on specific target
                    regions
                    rather than analyzing the entire scene. This "region-focal" method aims to improve efficiency
                    and
                    accuracy by concentrating on the area of interest, reducing unnecessary computations and
                    potential
                    interference from irrelevant objects.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">
                17:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11813" target="_blank">
                        @arXiv 2408.11813
                    </a>
                    <span class="tweet-title">
                        Token-Level Alignment: Giving LLMs a Visual Vocabulary Boost!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Science and Technology of China, Peking University, Kuaishou Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel token-level supervised alignment paradigm called Supervised
                    Embedding Alignment (SEA) to address the misalignment issue in Multimodal Large Language Models
                    (MLLMs). Unlike previous image-level methods, SEA aligns visual tokens with the LLM's embedding
                    space through contrastive learning, ensuring a more coherent integration of visual and language
                    representations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">
                18:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11443" target="_blank">
                        @arXiv 2408.11443
                    </a>
                    <span class="tweet-title">
                        Subword Regularization: Is Randomness Really Random?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich, Tokyo Institute of Technology
                    </span>
                </div>
                <div class="primary-text">
                    This paper investigates the distributions formed by stochastic subword tokenization schemes like
                    BPE-Dropout and MaxMatch-Dropout. It finds that these schemes are heavily biased towards a small
                    set
                    of tokenizations per word, which may limit their effectiveness. The paper proposes a new
                    algorithm
                    for uniformly sampling tokenizations, which improves machine translation quality.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">
                18:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11540" target="_blank">
                        @arXiv 2408.11540
                    </a>
                    <span class="tweet-title">
                        Rainy Day Reconstruction: How to Make 3D Scenes Shine Even When It's Pouring!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        The University of Tokyo, Nanjing University of Science and Technology, Dalian Maritime
                        University...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new task called 3D Reconstruction in Rainy Environments (3DRRE),
                    focusing
                    on reconstructing 3D scenes from images captured in rainy conditions. Unlike previous work that
                    primarily addressed transient occlusions, this paper tackles the unique challenges posed by
                    rain,
                    which continuously affects visibility and scene integrity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">
                19:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11182" target="_blank">
                        @arXiv 2408.11182
                    </a>
                    <span class="tweet-title">
                        Jailbreaking LLMs: Hiding Malicious Queries in Benign Narratives
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        The Pennsylvania State University, Northern Arizona University, Worcester Polytechnic
                        Institute...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel jailbreak attack method that hides prohibited queries within
                    carrier
                    articles, leveraging a knowledge graph and a composer LLM to generate these articles. Unlike
                    previous methods that rely on logic traps or prompt mutations, this approach focuses on shifting
                    the
                    attention of the LLM by inserting the malicious query into a text that is similar in topic but
                    does
                    not violate the model's safeguards.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">
                19:59
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11450" target="_blank">
                        @arXiv 2408.11450
                    </a>
                    <span class="tweet-title">
                        Tired of Balls? Ellipsoids Are the New Shape of Topological Data Analysis!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich, University of Fribourg
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new type of simplicial complex called an ellipsoid complex, which
                    uses
                    ellipsoids instead of balls to approximate the shape of data. This approach is particularly
                    effective for data that lies on manifolds or has bottlenecks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">
                20:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11530" target="_blank">
                        @arXiv 2408.11530
                    </a>
                    <span class="tweet-title">
                        Logic Program Refactoring: A New Way to Shrink Knowledge Bases!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford, Czech Academy of Sciences
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to knowledge refactoring that uses constrained
                    optimization
                    (COP) to compress logic programs. Unlike previous methods that enumerate all possible subsets of
                    rules, this approach focuses on literals rather than rules, significantly reducing the number of
                    decision variables required.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">
                20:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11433" target="_blank">
                        @arXiv 2408.11433
                    </a>
                    <span class="tweet-title">
                        Forget Me Not: A New Way to Remove Data From AI Models Without Breaking Them
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Xidian University, Wormpex AI Research, Meta
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a "Twin Machine Unlearning" approach, which uses a separate, related
                    unlearning problem to predict which data points are harder to forget. This helps to improve the
                    alignment between the unlearned model and the original model, ensuring that the model doesn't
                    lose
                    its ability to perform well on the remaining data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">
                21:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11293" target="_blank">
                        @arXiv 2408.11293
                    </a>
                    <span class="tweet-title">
                        Robot Vision: Seeing is Believing, and Now, Moving!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Sun Yat-sen University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a flow-based vision method called ViIK that directly outputs diverse
                    collision-free robot configurations, fusing inverse kinematics and collision checking. Unlike
                    previous methods that require separate execution of IK and collision checking, ViIK integrates
                    these
                    steps into a single process, significantly reducing the preparation time for motion planning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">
                21:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11799" target="_blank">
                        @arXiv 2408.11799
                    </a>
                    <span class="tweet-title">
                        Token Pruning: Making AI Assistants Think Faster, Not Harder!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a practical multi-task token pruning procedure for transformer
                    architectures used in conversational virtual assistant systems. Unlike previous work that
                    requires
                    task-specific training, this approach can be applied directly to pre-trained models without
                    affecting accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">
                22:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11746" target="_blank">
                        @arXiv 2408.11746
                    </a>
                    <span class="tweet-title">
                        Transformers on a Diet: New Training Method Cuts FLOPs by 4x!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Mixed Sparsity Training (MST), a novel method that combines dynamic
                    sparse
                    training with sparsity variation and hybrid sparse attention to reduce FLOPs during transformer
                    pretraining. Unlike previous work, MST dynamically adjusts the sparsity of the model throughout
                    training, allowing for greater flexibility and efficiency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">
                22:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11164" target="_blank">
                        @arXiv 2408.11164
                    </a>
                    <span class="tweet-title">
                        Gaussian Kernel? Nah, Epanechnikov is the New Black!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Hawaii, University of Texas at Austin
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes the Ensemble Epanechnikov Mixture Filter (EnEMF), which uses the
                    Epanechnikov
                    kernel instead of the commonly used Gaussian kernel for kernel density estimation in particle
                    filtering. This approach is shown to be more efficient, especially in high-dimensional settings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">
                23:04
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11447" target="_blank">
                        @arXiv 2408.11447
                    </a>
                    <span class="tweet-title">
                        Occupancy Estimation Gets a Gaussian Makeover: Self-Supervised and Speedy!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Tokyo
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Gaussian Splatting for 3D occupancy estimation, a technique that
                    differs
                    from previous volume rendering methods by offering a fully self-supervised approach and
                    achieving
                    faster rendering speeds.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">
                23:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11336" target="_blank">
                        @arXiv 2408.11336
                    </a>
                    <span class="tweet-title">
                        Climate Change Forecasting Gets a 3D Makeover: New Model Predicts Temperature with Tensorial
                        Flair!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel temperature prediction model called FATE, which utilizes a
                    tensor-based FocalNet Transformer architecture. Unlike previous models that flatten input data,
                    FATE
                    preserves the spatial-temporal structure of climate data in a 3D tensor format, allowing for
                    more
                    nuanced pattern recognition.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">
                23:54
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11413" target="_blank">
                        @arXiv 2408.11413
                    </a>
                    <span class="tweet-title">
                        Panorama to Room: Turning a Single Snap into a 3D World!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes Pano2Room, a novel view synthesis method that reconstructs a 3D scene
                    from a
                    single panoramic image. Unlike previous methods that rely on multiple images or text prompts,
                    Pano2Room leverages a panoramic RGBD inpainter to iteratively refine a mesh generated from the
                    panorama, ultimately creating a 3D Gaussian Splatting field (3DGS) capable of synthesizing
                    photo-realistic novel views.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">
                24:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11380" target="_blank">
                        @arXiv 2408.11380
                    </a>
                    <span class="tweet-title">
                        Robot Navigation: No Map? No Problem! Vision-Language Models to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Tokyo
                    </span>
                </div>
                <div class="primary-text">
                    This research explores open-vocabulary navigation without relying on prior map construction or
                    learning, unlike traditional methods like SLAM or reinforcement learning. It achieves this by
                    combining omnidirectional cameras with pre-trained vision-language models, enabling the robot to
                    understand and react to linguistic instructions in real-time.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">
                24:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11493" target="_blank">
                        @arXiv 2408.11493
                    </a>
                    <span class="tweet-title">
                        X-Ray Vision: Can AI Trained on One Lung Disease Spot Another?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Mohamed bin Zayed University of Artificial Intelligence, University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research explores cross-disease transferability (XDT) in medical imaging, specifically
                    focusing
                    on the ability of binary classifiers trained on one disease to perform zero-shot classification
                    on
                    another disease affecting the same organ. This differs from previous work by investigating the
                    potential of transferring knowledge from one disease to another within the same modality, rather
                    than relying on multi-label approaches or auxiliary data for unseen classes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">
                25:11
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11607" target="_blank">
                        @arXiv 2408.11607
                    </a>
                    <span class="tweet-title">
                        Mean-Field Games: When Bots Learn to Gossip and Outsmart Each Other
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces function approximation to the setting of decentralized agents learning
                    in
                    mean-field games. This allows for larger state spaces and the inclusion of population-dependent
                    policies, which are more realistic in real-world applications. Previous work focused on tabular
                    settings, which limited the scalability of the algorithms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">
                25:37
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11146" target="_blank">
                        @arXiv 2408.11146
                    </a>
                    <span class="tweet-title">
                        Game Over for Nash: A New Way to Compute the Meaning of a Game
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Columbia University, DeepMind
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a new way to understand the meaning of a game by mapping a prior
                    distribution
                    over mixed strategy profiles to a distribution over sink equilibria. This approach differs from
                    previous work that focused on the Nash equilibrium, which has been shown to be computationally
                    intractable.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">
                26:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11793" target="_blank">
                        @arXiv 2408.11793
                    </a>
                    <span class="tweet-title">
                        AI's New Recipe: Chemistry Models Spice Up Materials Design!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the use of chemistry foundation models for structure-focused retrieval
                    augmented generation (RAG) in multi-agent workflows for catalyst and materials design. Unlike
                    previous work that primarily focused on small molecules, this study demonstrates the model's
                    effectiveness for polymers and reactions, expanding its applicability to more complex materials.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">
                26:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11620" target="_blank">
                        @arXiv 2408.11620
                    </a>
                    <span class="tweet-title">
                        Annealed Sinkhorn: A Hot New Way to Solve Optimal Transport Problems!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        École Polytechnique Fédérale de Lausanne
                    </span>
                </div>
                <div class="primary-text">
                    This paper analyzes the convergence of Annealed Sinkhorn, a popular algorithm for solving
                    optimal
                    transport problems, and identifies a previously unknown "relaxation error" that arises from the
                    annealing process. It also proposes a simple modification to reduce this error, enabling faster
                    annealing schedules.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">
                26:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11288" target="_blank">
                        @arXiv 2408.11288
                    </a>
                    <span class="tweet-title">
                        AI Therapists: Are Chatbots Ready for the Couch?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University, Beth Israel Deaconess Medical Center, University of Technology Sydney...
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the use of large language models (LLMs) in mental health care,
                    specifically
                    examining studies where these models were tested with human participants in real-world
                    scenarios. It
                    differs from previous work by focusing on generative tasks, where LLMs create human-like
                    responses,
                    rather than simply analyzing data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">
                27:11
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.11211" target="_blank">
                        @arXiv 2408.11211
                    </a>
                    <span class="tweet-title">
                        Neural Networks: Sorting Out the ℓ∞ Norm's Proximal Operator
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Virginia, University of Maryland
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a neural network approach to approximate the proximal operator of the ℓ∞
                    norm, avoiding the need for sorting, which is typically required by traditional methods.
                </div>
            </div>
        </div>


        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202408221729_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>

</html>