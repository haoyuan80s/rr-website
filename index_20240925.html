
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY</div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">Fresh Picks: 
                    <span class="highlightNumber" style="font-size: 28px;">71</span> out of <span
                    class="highlightNumber">363</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-09-25"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>


        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">00:58</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16211" target="_blank">@arXiv 2409.16211</a>
                    <span class="tweet-title">MaskBit: Ditch the Embeddings, Generate Images with Just Bits!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ByteDance, Technical University Munich, Carnegie Mellon University...</span>
                </div>
                <div class="primary-text">
                    This research introduces MaskBit, an embedding-free image generation model that operates directly on bit tokens, a binary representation of image features. Unlike previous methods that rely on embedding tables to map tokens to image features, MaskBit eliminates this step, leading to a more efficient and compact model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">01:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15454" target="_blank">@arXiv 2409.15454</a>
                    <span class="tweet-title">LLMs: Smarter Than Babies? Not So Fast!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carleton College, Caltech</span>
                </div>
                <div class="primary-text">
                    This research investigates the ability of large language models (LLMs) to inhibit previously learned patterns, a cognitive skill known as inhibitory control. Unlike previous studies that focused on LLMs' performance on specific tasks, this paper examines how LLMs perform when presented with a simple, yet deliberately misleading, scenario designed to test their ability to adapt to changing contexts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">01:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16278" target="_blank">@arXiv 2409.16278</a>
                    <span class="tweet-title">Open-Vocabulary Panoptic Segmentation:  A Smart Way to See the World</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Bosch</span>
                </div>
                <div class="primary-text">
                    This research proposes Semantic Refocused Tuning (SMART), a novel framework that improves open-vocabulary panoptic segmentation by focusing on mask classification. Unlike previous methods that either train for a long time or sacrifice performance for efficiency, SMART achieves state-of-the-art results with significantly reduced training costs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">02:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15761" target="_blank">@arXiv 2409.15761</a>
                    <span class="tweet-title">Diffusion Models Get a Training-Free Makeover:  A Unified Framework for Guiding Generation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This paper introduces a unified framework called Training-Free Guidance (TFG) that encompasses existing methods as special cases. It provides a comprehensive design space for analyzing and comparing different training-free guidance techniques.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">02:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15359" target="_blank">@arXiv 2409.15359</a>
                    <span class="tweet-title">Chain of Thought Gets a Python Makeover:  LLMs Learn to Reason Step-by-Step</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces Program Trace Prompting (PTP), a new way to structure chain-of-thought (CoT) prompts. Unlike traditional CoT, PTP wraps explanations in a semi-formal syntax based on Python, making them more observable and analyzable.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">02:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15594" target="_blank">@arXiv 2409.15594</a>
                    <span class="tweet-title">LLMs Get Chatty:  New Model Makes AI Conversations Sound More Human</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, Meta</span>
                </div>
                <div class="primary-text">
                    This research introduces Synchronous LLMs (SyncLLMs), a new approach to spoken dialogue modeling that integrates time information into LLMs, allowing them to run synchronously with the real-world clock. This differs from previous work that primarily focused on turn-based interactions, where responses require explicit prompting or implicit tracking of pauses.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">03:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16277" target="_blank">@arXiv 2409.16277</a>
                    <span class="tweet-title">Depth Maps on a Diet:  Super-Resolution for Compressed Reality</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Würzburg, Sony PlayStation, Meta</span>
                </div>
                <div class="primary-text">
                    This research focuses on developing techniques to reconstruct high-quality depth maps from compressed data, a challenge not extensively explored in previous work.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">03:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15318" target="_blank">@arXiv 2409.15318</a>
                    <span class="tweet-title">Neural Networks:  Superposition's Not Just for Quantum Computers Anymore!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This paper focuses on the computational complexity of superposition in neural networks, specifically exploring the trade-off between the number of neurons and the number of features being computed. Unlike previous work that focused on feature representation, this research delves into the algorithmic aspects of computation in superposition.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">03:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16048" target="_blank">@arXiv 2409.16048</a>
                    <span class="tweet-title">Legged Robots Learn to Juggle: New AI Makes Them Master Manipulators!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research introduces a whole-body reinforcement learning (RL) approach for end-effector pose tracking in legged robots with an arm. Unlike previous RL methods that focused on position tracking or limited workspaces, this approach enables precise pose tracking across a large workspace and challenging terrains, including stairs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">04:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15394" target="_blank">@arXiv 2409.15394</a>
                    <span class="tweet-title">Neural Networks Learn to Integrate, Making Monte Carlo Simulations Faster!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Cornell University, Stanford University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for constructing control variates using arbitrary neural network architectures. Instead of directly approximating the integrand, the network learns to approximate the antiderivative of the integrand, allowing for the construction of pairs of networks where one is the analytical integral of the other. This approach tackles a key challenge in neural control variates methods, namely the difficulty of creating expressive parametric models with known analytical integrals.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">04:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15461" target="_blank">@arXiv 2409.15461</a>
                    <span class="tweet-title">Chatbots Go to School:  AI Teachers Get a Human Touch</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research focuses on creating a framework called RAM2C to generate high-quality educational dialogues for liberal arts subjects. Unlike previous work that relies solely on large language models (LLMs), RAM2C incorporates multiple experts with distinct roles, such as teachers, psychologists, and ethical safety experts, to ensure the generated dialogues are humanized, pedagogically sound, and ethically safe.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">05:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15451" target="_blank">@arXiv 2409.15451</a>
                    <span class="tweet-title">Robots Get Their Bearings: A Text-Based Map for Smarter Navigation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research proposes a text-based map, called a "tag map," for robots to understand their surroundings. Unlike previous methods that rely on complex embeddings, this map uses simple text tags to represent objects and locations, making it easier for language models to reason about the environment.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">05:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15869" target="_blank">@arXiv 2409.15869</a>
                    <span class="tweet-title">Whisper's New Trick:  Decoding Multiple Words at Once!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">aiOla Research</span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to speed up speech recognition models by modifying the decoder to predict multiple words simultaneously, rather than one at a time. This differs from previous work that focused on using separate "assistant" models to generate multiple predictions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">05:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16183" target="_blank">@arXiv 2409.16183</a>
                    <span class="tweet-title">Radiology's New AI Guru: RadFound Speaks the Language of X-rays!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sun Yat-Sen University, Beijing University of Posts and Telecommunications, Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces RadFound, a vision-language foundation model specifically trained on a massive dataset of radiology images and text. Unlike previous models, RadFound incorporates a novel vision encoder that captures both local and contextual information within medical images, enhancing its ability to understand and generate radiology reports.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">06:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15564" target="_blank">@arXiv 2409.15564</a>
                    <span class="tweet-title">Pain in the Joints? New AI Model Uncovers the Cause!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley, Columbia University, University College London...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel representation learning method that incorporates causal inference to analyze human joint dynamics and complex behaviors. Unlike previous work that primarily focused on statistical correlations, this study delves into the underlying causal relationships between joints, providing a deeper understanding of human movement.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">06:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15695" target="_blank">@arXiv 2409.15695</a>
                    <span class="tweet-title">Semantic Communication Gets a Trustworthy Makeover with MoE!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Guangdong University of Technology, Nanyang Technological University, Singapore University of Technology and Design...</span>
                </div>
                <div class="primary-text">
                    This research proposes a Mixture-of-Experts (MoE) based Semantic Communication (SemCom) system that can handle multiple security threats simultaneously, unlike previous work that focused on addressing individual threats.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">06:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15631" target="_blank">@arXiv 2409.15631</a>
                    <span class="tweet-title">AI Tutoring Gets a Data Makeover: Filling in the Gaps with Generative Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Memphis, CMU, Hong Kong Polytechnic University</span>
                </div>
                <div class="primary-text">
                    This research proposes a systematic framework for augmenting sparse learning performance data in Intelligent Tutoring Systems (ITSs) by combining tensor factorization for data imputation and generative AI models for data augmentation. This approach differs from previous work by integrating these two techniques to create a more comprehensive and robust dataset for training and evaluating ITSs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">07:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15732" target="_blank">@arXiv 2409.15732</a>
                    <span class="tweet-title">Speech Recognition Gets a Cluster-Busting Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sony, CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach called Hypothesis Clustering and Merging (HCM) for multi-talker speech recognition. Unlike previous methods like permutation invariant training (PIT) and serialized output training (SOT), HCM clusters speaker embeddings into discrete classes, allowing for more flexible and scalable recognition of overlapped speech.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">07:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15637" target="_blank">@arXiv 2409.15637</a>
                    <span class="tweet-title">Turning Tutorials into AI Agents: How to Train a Web-Surfing Bot with Just WikiHow!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach called Synatra, which synthesizes training data for digital agents by transforming indirect knowledge, such as online tutorials, into direct demonstrations. This differs from previous work that relies on costly human annotations or complex environment setups for data collection.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">08:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15647" target="_blank">@arXiv 2409.15647</a>
                    <span class="tweet-title">Transformers Go Loop-de-Loop:  Learning to Solve Problems of Any Size!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Wisconsin-Madison, Massachusetts Institute of Technology, UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new type of Transformer architecture called a "Looped Transformer" that can handle inputs of varying lengths. Unlike traditional Transformers, which have a fixed depth, Looped Transformers can adjust their computational complexity based on the difficulty of the task. This is achieved by iteratively applying the same decoder block multiple times, with the number of iterations determined by the input length.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">08:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15398" target="_blank">@arXiv 2409.15398</a>
                    <span class="tweet-title">AttackAtlas: Mapping the Wild West of GenAI Security</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM, University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research focuses on the practical challenges of red-teaming generative AI, offering a taxonomy of attack styles and highlighting the need for more realistic and scalable evaluation methods. It differs from previous work by emphasizing the importance of real-world attack vectors and the limitations of existing benchmarks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">09:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15486" target="_blank">@arXiv 2409.15486</a>
                    <span class="tweet-title">Vision Language Models: The New Data Miners in Town!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Cruise LLC, Meta Inc.</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel data mining approach called VLMine that leverages the knowledge embedded in large vision language models (VLMs) to identify rare examples within unlabeled data. Unlike previous methods that rely on model uncertainty, VLMine utilizes the semantic understanding of VLMs to extract keywords that describe the content of an image, and then uses the frequency of these keywords to identify long-tail examples.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">09:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15662" target="_blank">@arXiv 2409.15662</a>
                    <span class="tweet-title">Stock Market Forecasting:  Double the Fun with a Double-Path Transformer!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel Double-Path Adaptive-correlation Spatial-Temporal Inverted Transformer (DPA-STIFormer) for stock time series forecasting. Unlike previous methods that primarily focus on temporal relationships, DPA-STIFormer incorporates both temporal and feature perspectives to model correlations between different stocks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">09:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15727" target="_blank">@arXiv 2409.15727</a>
                    <span class="tweet-title">Shape Uncertainty:  How a New Model Tackles the Pose Problem with a Pinch of Probability</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Technical University of Munich, California Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel framework called LaPose that models object shape as a Laplacian Mixture Model (LMM) for RGB-based category-level object pose estimation. Unlike previous methods that rely on deterministic predictions, LaPose explicitly quantifies shape uncertainty by representing each point as a probabilistic distribution.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">10:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16283" target="_blank">@arXiv 2409.16283</a>
                    <span class="tweet-title">Robots Learn to Do Chores by Watching YouTube!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to robot manipulation by leveraging pre-trained video generation models to predict motion information from web data. Unlike previous methods that rely on expensive robot data collection, this approach uses readily available web videos to enable generalization to unseen tasks and objects.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">10:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15360" target="_blank">@arXiv 2409.15360</a>
                    <span class="tweet-title">Reward-Robust RLHF:  LLMs Get a Grip on Imperfect Feedback!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel reward-robust RLHF framework that addresses the inherent instability and imperfections of reward models in LLM training. Unlike previous work that focuses on single reward models, this approach utilizes Bayesian Reward Model Ensembles (BRME) to model the uncertainty set of reward functions, balancing performance and robustness.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">11:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15326" target="_blank">@arXiv 2409.15326</a>
                    <span class="tweet-title">Ask Avo:  LLM  for  Docs  Beats  ChatGPT  in  Trust  Test!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Missouri, Boston Children’s Hospital, Beth Israel Deaconess Medical Center...</span>
                </div>
                <div class="primary-text">
                    This study compares Ask Avo, an LLM specifically designed for clinical decision support, with ChatGPT-4, a general-purpose LLM. The key difference is Ask Avo's integration of a proprietary Language Model Augmented Retrieval (LMAR) system, in-built visual citation cues, and prompt engineering tailored for physician interactions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">11:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15724" target="_blank">@arXiv 2409.15724</a>
                    <span class="tweet-title">LLM-Cure:  Fixing Apps with AI and a Little Friendly Competition</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Queen’s University, University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research proposes LLM-Cure, a method that uses large language models to analyze user reviews of competing apps and suggest feature improvements for a target app. Unlike previous work, LLM-Cure focuses on generating concrete suggestions for specific complaints, rather than just identifying features or comparing ratings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">11:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16024" target="_blank">@arXiv 2409.16024</a>
                    <span class="tweet-title">Vision-Language Models:  Turning Text into Robot Actions, One View at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">NAVERLABSEurope, Sorbonne University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel decomposition of the problem of building language-conditioned agents (LCAs) by separating text-to-goal generation from goal-reaching. Unlike previous work that relies on large-scale annotated data or algorithmic annotations, this approach leverages vision-language models (VLMs) to find configurations of an environment that correspond well to a given text description.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">12:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15763" target="_blank">@arXiv 2409.15763</a>
                    <span class="tweet-title">IRSC:  A New Benchmark for  Retrieval-Augmented Generation,  It's  Not  Just  About  Finding  the  Right  Words!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">PengCheng Laboratory, Tsinghua University, Southern University of Science and Technology</span>
                </div>
                <div class="primary-text">
                    This research introduces the IRSC benchmark, which evaluates embedding models in Retrieval-Augmented Generation (RAG) tasks across five distinct retrieval scenarios, including query, title, part-of-paragraph, keyword, and summary retrieval. This differs from previous benchmarks that primarily focus on specific tasks like semantic textual similarity or clustering.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">12:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15939" target="_blank">@arXiv 2409.15939</a>
                    <span class="tweet-title">Shape Completion: A Self-Supervised Involutionary Journey!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, Google</span>
                </div>
                <div class="primary-text">
                    This research proposes a self-supervised approach for shape completion that leverages the concept of involution, a function that is its own inverse. Unlike previous methods that rely on adversarial learning or complete shape examples, this approach utilizes a consistency measure in a canonical space to guide the completion process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">13:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15980" target="_blank">@arXiv 2409.15980</a>
                    <span class="tweet-title">Tiny Computer, Big Anomaly:  Raspberry Pi Detects Defects with 95% Accuracy!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research focuses on deploying pre-trained anomaly detection models on low-cost hardware like the Raspberry Pi, aiming to make visual inspection more accessible to small and medium-sized enterprises (SMEs).  Previous work has primarily focused on using these models with more powerful computing resources.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">13:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15680" target="_blank">@arXiv 2409.15680</a>
                    <span class="tweet-title">One-Point Bandit:  A New Trick for Distributed Optimization</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IEEE</span>
                </div>
                <div class="primary-text">
                    This research proposes a distributed online bandit optimization algorithm that uses one-point residual feedback to estimate gradients. This approach differs from previous work by using only one function evaluation per iteration, making it more practical for online optimization where data is not all available a priori.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">14:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16202" target="_blank">@arXiv 2409.16202</a>
                    <span class="tweet-title">Chinese Exams:  LLMs Get Schooled on a New Benchmark!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tencent, Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces CJEval, a benchmark dataset for evaluating Large Language Models (LLMs) in educational contexts. Unlike previous benchmarks that focus on multiple-choice questions, CJEval includes a wider range of question types, such as fill-in-the-blank and analysis questions, and incorporates detailed annotations like knowledge concepts and answer explanations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">14:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15545" target="_blank">@arXiv 2409.15545</a>
                    <span class="tweet-title">Music's Emotional Rollercoaster: How Frechet Audio Distance Can Help Us Understand It Better</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Edinburgh, University of Toronto, Microsoft Research</span>
                </div>
                <div class="primary-text">
                    This research introduces the use of Frechet Audio Distance (FAD) as a reference-free metric to evaluate music emotion, addressing the inherent bias in traditional methods that rely on single audio encoders and subjective human annotations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">14:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15679" target="_blank">@arXiv 2409.15679</a>
                    <span class="tweet-title">Drone Detectives: New Dataset Helps AI Spot Tree Pests!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Qilu University of Technology  Shandong Academy of Sciences  Peking University  SHANDONGSCICOM Information and Economy Research Institute Co.  Ltd.</span>
                </div>
                <div class="primary-text">
                    This research introduces two new datasets, PDT and CWC, specifically designed for training AI models to detect pests and diseases in trees using drone imagery. Unlike previous datasets, these datasets are collected in real-world environments and include both high- and low-resolution images, making them more realistic for training models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">15:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15866" target="_blank">@arXiv 2409.15866</a>
                    <span class="tweet-title">UAVs on a Chase: Deep Learning Makes Drone Swarms Smarter!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research focuses on multi-UAV pursuit-evasion in 3D environments, incorporating UAV dynamics and physical constraints, unlike previous work that often simplified the problem. It introduces an evader prediction-enhanced network and an adaptive environment generator to address the challenges of large exploration space and policy generalization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">15:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15658" target="_blank">@arXiv 2409.15658</a>
                    <span class="tweet-title">Robot Brains Get Real: New Framework Makes Long-Horizon Planning a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Tongji University, Beihang University</span>
                </div>
                <div class="primary-text">
                    This research introduces ReLEP, a framework for real-world long-horizon embodied planning that uses a fine-tuned large vision language model to decompose tasks into a sequence of skills. Unlike previous work that relies on GPT-4V for task decomposition, ReLEP leverages a carefully designed skill library and a semi-automatic data generation pipeline to achieve greater task diversity and real-world applicability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">15:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15723" target="_blank">@arXiv 2409.15723</a>
                    <span class="tweet-title">Federated Learning for LLMs:  Sharing is Caring, But Not Your Data!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive overview of recent advancements in federated learning for large language models (LLMs), focusing on fine-tuning and prompt learning in a federated setting. It distinguishes itself from previous surveys by offering a more in-depth analysis of privacy and security concerns, as well as exploring potential future directions for federated LLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">16:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16040" target="_blank">@arXiv 2409.16040</a>
                    <span class="tweet-title">Time Series Forecasting Just Got a Billion Times Smarter!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Princeton University</span>
                </div>
                <div class="primary-text">
                    This research introduces TIME-MOE, a time series forecasting model that uses a sparse mixture-of-experts (MoE) architecture. This allows for scaling the model to billions of parameters while maintaining computational efficiency, unlike previous dense models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">16:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16143" target="_blank">@arXiv 2409.16143</a>
                    <span class="tweet-title">Seeing Faces in Coffee Stains: A New Model for Pareidolia</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Microsoft, Toyota Research Institute...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset of images containing pareidolic faces, which are faces perceived in random patterns. The study then uses this dataset to investigate how well a state-of-the-art face detector performs on these images, revealing a significant difference between human and machine perception.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">17:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15710" target="_blank">@arXiv 2409.15710</a>
                    <span class="tweet-title">Bipedal Robots Learn to Walk Like Humans, Thanks to a Neural Network!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, University of Southern California, University of Illinois</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to autotuning bipedal locomotion MPC parameters using a Ground Reaction Force-and-Moment Network (GRFM-Net). This method differs from previous work by leveraging a data-driven approach to enhance the fidelity of a simplified dynamics model, thereby reducing the sim-to-real gap.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">17:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16073" target="_blank">@arXiv 2409.16073</a>
                    <span class="tweet-title">Open-World Object Detection:  When Your AI Needs to Learn on the Fly!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a method for training object detectors to identify unknown objects in open-world scenarios while simultaneously learning semantically rich features for these objects. This approach leverages the knowledge of Vision Foundation Models (VFM) to enhance the detector's ability to capture relationships between detected objects, going beyond simply classifying them as known or unknown.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">17:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15342" target="_blank">@arXiv 2409.15342</a>
                    <span class="tweet-title">Multimodal Memories on the Go: How Your Phone Can Remember Everything You Do</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces Recall, a system that uses early exiting to generate coarse-grained embeddings for multimodal data on mobile devices. Unlike previous work, Recall focuses on optimizing for resource-limited environments, addressing the challenges of low throughput and high energy consumption associated with on-device multimodal embedding.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">18:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15734" target="_blank">@arXiv 2409.15734</a>
                    <span class="tweet-title">Constrained Optimization:  Saddle Point Escape Room!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of California  Berkeley, Georgia Institute of Technology, University of Southern California</span>
                </div>
                <div class="primary-text">
                    This research introduces the first stochastic sequential quadratic programming (SSQP) method with second-order convergence guarantees. Unlike previous SSQP methods that primarily focus on first-order convergence, this method can escape saddle points and converge to local minima.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">18:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15865" target="_blank">@arXiv 2409.15865</a>
                    <span class="tweet-title">Robots Get a Brain: LLMs Power Up Behavior Simulation!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National University of Defense Technology</span>
                </div>
                <div class="primary-text">
                    This research proposes a new approach to robot simulation called "Behavior Simulation" that focuses on the logic of robot actions rather than complex physical modeling. It uses large language models (LLMs) to simulate robot behavior in text-based environments, achieving long-horizon complex simulation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">18:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15790" target="_blank">@arXiv 2409.15790</a>
                    <span class="tweet-title">Tiny Brains, Big Ideas:  A Deep Dive into Small Language Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Beijing University of Posts and Telecommunications, Peng Cheng Laboratory, Helixon Research...</span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive survey and measurement of small language models (SLMs) with 100M–5B parameters, focusing on their capabilities, runtime cost on devices, and innovations. It differs from previous work by analyzing a larger and more diverse set of SLMs, including their performance on various tasks and their on-device runtime costs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">19:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15395" target="_blank">@arXiv 2409.15395</a>
                    <span class="tweet-title">LLMs Get a Trim: Parse Trees Help Squeeze Long Prompts</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel prompt compression method called PartPrompt that leverages parse trees to guide the selection of important tokens in a prompt. Unlike previous methods that rely solely on computational models, PartPrompt incorporates linguistic rules and considers the global structure of the prompt.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">19:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15373" target="_blank">@arXiv 2409.15373</a>
                    <span class="tweet-title">Say Goodbye to Padding: Jagged Flash Attention Makes Recommendation Systems Faster and Leaner</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta</span>
                </div>
                <div class="primary-text">
                    This research introduces Jagged Feature Interaction Kernels, a novel method for handling variable-length categorical features in recommendation systems. Unlike traditional padding methods, this approach avoids unnecessary memory usage and computational overhead by leveraging Jagged tensors, which efficiently store variable-length data without padding.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">20:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15615" target="_blank">@arXiv 2409.15615</a>
                    <span class="tweet-title">KISS-Matcher: Point Cloud Registration Gets a Speed Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Korea Advanced Institute of Science and Technology, Seoul National University</span>
                </div>
                <div class="primary-text">
                    This research introduces KISS-Matcher, a point cloud registration library that prioritizes speed and scalability. Unlike previous work that focused on specific components, KISS-Matcher takes a holistic approach, optimizing the entire registration pipeline. It achieves this by introducing Faster-PFH, a faster version of the FPFH feature descriptor, and using k-core-based graph-theoretic pruning to efficiently reject outlier correspondences.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">20:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15546" target="_blank">@arXiv 2409.15546</a>
                    <span class="tweet-title">Gram-Stained Slides Get a Transformer Makeover: AI Makes Bacteria ID a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Dartmouth College, Geisel School of Medicine at Dartmouth, Dartmouth-Hitchcock Medical Center...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework for automated Gram-stain analysis using a vision transformer model, which is trained on large regions of whole-slide images instead of smaller patches, eliminating the need for manual annotation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">20:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16118" target="_blank">@arXiv 2409.16118</a>
                    <span class="tweet-title">Tabular Data Augmentation:  EBMs Get Classy with Distinct Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research introduces TabEBM, a novel method for tabular data augmentation that uses distinct Energy-Based Models (EBMs) for each class. Unlike previous methods that use a single shared model for all classes, TabEBM learns the marginal distribution for each class separately, leading to more robust and accurate synthetic data generation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">21:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15582" target="_blank">@arXiv 2409.15582</a>
                    <span class="tweet-title">More Data, More Problems?  Concept Shift Makes Learning Backfire!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Princeton University, CUNY Graduate Center, University of Sydney</span>
                </div>
                <div class="primary-text">
                    This paper analyzes ridge regression under concept shift, a type of distribution shift where the relationship between input and output changes at test time. It derives an exact expression for prediction risk in the high-dimensional limit, revealing non-monotonic data dependence under concept shift.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">21:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15332" target="_blank">@arXiv 2409.15332</a>
                    <span class="tweet-title">Image Fusion:  GANs Go on a Diet,  Get  Faster  and  Smarter!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of California  Berkeley, New York University, Southwest University of Science and Technology</span>
                </div>
                <div class="primary-text">
                    This research introduces a lightweight image fusion algorithm that uses depthwise separable convolution and a convolutional block attention module (CBAM) to improve efficiency and performance. This approach differs from previous work by focusing on reducing the model's size and computational cost while maintaining or even enhancing the quality of the fused images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">22:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16057" target="_blank">@arXiv 2409.16057</a>
                    <span class="tweet-title">Object Detection's Secret Backdoor: How to Spot and Fix It!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research focuses on backdoor attacks in object detection models, a relatively unexplored area. It proposes a novel defense framework that leverages inconsistencies between different modules within the model to detect and remove backdoors. This approach differs from previous work that primarily focused on simpler image classification models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">22:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15299" target="_blank">@arXiv 2409.15299</a>
                    <span class="tweet-title">AI Recruiters:  Blind to Decoys,  But Not to Gender?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, The Adecco Group</span>
                </div>
                <div class="primary-text">
                    This research investigates the attraction effect, a well-known cognitive bias, in large language models (LLMs) specifically within the context of hiring decisions. Unlike previous studies that focused on product selection or human decision-making, this paper examines how LLMs are susceptible to this bias when tasked with choosing the best candidate.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">22:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15590" target="_blank">@arXiv 2409.15590</a>
                    <span class="tweet-title">Robot Explorers Get a Sixth Sense: Predicting Maps for Smarter Exploration</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces MapEx, a new exploration framework that uses predicted maps to calculate probabilistic information gain. Unlike previous approaches that rely solely on sensor coverage or map uncertainty, MapEx jointly reasons over both factors, leading to more efficient exploration.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">23:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15979" target="_blank">@arXiv 2409.15979</a>
                    <span class="tweet-title">LLMs Get a Soft Touch: Fine-Tuning for Better Text Comparisons</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research explores fine-tuning LLMs for comparative assessment tasks, focusing on using soft probabilities instead of hard decisions. This approach aims to align the model's output with the assumed distribution of pairwise probabilities, improving performance with fewer comparisons.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">23:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15491" target="_blank">@arXiv 2409.15491</a>
                    <span class="tweet-title">Deep Learning Predicts Breast Cancer Recurrence:  No More Expensive Tests!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Wake Forest University School of Medicine, The Ohio State University College of Medicine, The Ohio State University</span>
                </div>
                <div class="primary-text">
                    This study introduces Deep-BCR-Auto, a fully automatic pipeline that predicts breast cancer recurrence risk using deep learning models trained on whole slide images (WSIs). Unlike previous methods that rely on hand-crafted features and extensive annotations, Deep-BCR-Auto utilizes a weakly supervised learning approach, eliminating the need for pathologist annotations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">23:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15721" target="_blank">@arXiv 2409.15721</a>
                    <span class="tweet-title">Binary-State Networks:  Learning to Be Reliable, One Step at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National Tsing Hua University</span>
                </div>
                <div class="primary-text">
                    This research introduces incremental learning to the Binary-Addition-Tree (BAT) algorithm, enabling it to adapt to dynamic changes in binary-state networks without complete recalculation. This approach differs from traditional BAT, which is primarily designed for static networks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">24:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16001" target="_blank">@arXiv 2409.16001</a>
                    <span class="tweet-title">AI's New BFF: Humans Help Machines Get Smarter</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces the concept of "Artificial Human Intelligence" (AHI), which explores the interplay between human and machine intelligence, focusing on the role humans play in developing ethical and robust AI systems. It distinguishes between human-inspired, human-assisted, and human-independent AI, providing a taxonomy for understanding the different ways humans contribute to AI development.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">24:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15501" target="_blank">@arXiv 2409.15501</a>
                    <span class="tweet-title">Swin-UNet's Got Your Back:  A New AI Tool for Spotting Cancer Across Different Organs and Scanners</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Imperial College London, University College London, University of New South Wales</span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework for adenocarcinoma segmentation that uses a pre-trained Swin-UNet architecture enhanced with a parallel cross-attention module. This approach aims to improve the model's ability to generalize across different organs and scanners, addressing the challenge of domain shift in histopathology.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">25:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15600" target="_blank">@arXiv 2409.15600</a>
                    <span class="tweet-title">Polyatomic Complexes:  A New Way to Represent Atoms, It's Like Lego for Molecules!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley, Lawrence Berkeley National Laboratory</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new representation for atomistic systems called "Polyatomic Complexes." Unlike previous methods, this representation is designed to be generalizable, meaning it can encode any atomistic system, including complex molecules and materials. It also satisfies several important criteria, including invariance under coordinate transformations, uniqueness, continuity, and differentiability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">25:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15334" target="_blank">@arXiv 2409.15334</a>
                    <span class="tweet-title">Spanish for AI: Can LLMs Pass the Foreign Language Test?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UNED, Instituto Cervantes, SomosNLP...</span>
                </div>
                <div class="primary-text">
                    This research evaluates the performance of Large Language Models (LLMs) on a Spanish language proficiency test, TELEIA, which is designed to assess the language skills of foreign students. This is a novel approach to evaluating LLMs, as most benchmarks focus on English language tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon" title="Play from here">26:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15341" target="_blank">@arXiv 2409.15341</a>
                    <span class="tweet-title">Style Transfer for Videos:  No More Keyframe Fatigue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CTU in Prague, Google</span>
                </div>
                <div class="primary-text">
                    This research introduces StyleReiser, a method for video stylization that uses a single keyframe to transfer style to the entire video sequence while maintaining visual consistency, even when the scene structure changes significantly. Unlike previous keyframe-based methods, StyleReiser focuses on preserving the structure of the target video, reducing the need for additional keyframes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon" title="Play from here">26:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16016" target="_blank">@arXiv 2409.16016</a>
                    <span class="tweet-title">Retinal Vessel Detective: New AI Model Uncovers Hidden Clues in Eye Images</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Erasmus University Medical Center, Radboud University Medical Center, University of Basel...</span>
                </div>
                <div class="primary-text">
                    This research introduces VascX models, a set of model ensembles for analyzing retinal vasculature from color fundus images (CFIs). The models were trained on a combination of public datasets and new annotations from the Rotterdam Study, a large population-based cohort study. This approach allows for more robust and generalizable models compared to previous work, which often relied on smaller and more homogeneous datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon" title="Play from here">26:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15314" target="_blank">@arXiv 2409.15314</a>
                    <span class="tweet-title">Deep Learning's New Trick:  Fixing the Bias in Gradient Descent!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stevens Institute of Technology, Chinese University of Hong Kong, Illinois Institute of Technology...</span>
                </div>
                <div class="primary-text">
                    This paper proposes the RSGDM algorithm, which addresses the bias and lag in the SGDM algorithm by incorporating a differential correction term. This term dynamically adjusts based on the differences between consecutive gradients, improving the accuracy of gradient estimation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet67">
            <div class="start-time-icon" title="Play from here">27:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15355" target="_blank">@arXiv 2409.15355</a>
                    <span class="tweet-title">RAG's Speed Demon: Block-Attention Makes LLMs Zoom!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google, Google Research, Google AI...</span>
                </div>
                <div class="primary-text">
                    This paper introduces Block-Attention, a novel attention mechanism that divides the input sequence into blocks, allowing for independent computation of key-value states within each block. This differs from traditional self-attention, which calculates KV states for the entire sequence, leading to increased latency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet68">
            <div class="start-time-icon" title="Play from here">27:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15315" target="_blank">@arXiv 2409.15315</a>
                    <span class="tweet-title">Knowledge Graph Attention:  Boosting Recommendations with a Little Help from Friends!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research introduces a new recommendation model called KGAT-AX, which incorporates auxiliary information into entity embeddings using a holographic embedding approach. This differs from previous work by explicitly leveraging multi-source information and inferential relationships between entities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet69">
            <div class="start-time-icon" title="Play from here">28:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15378" target="_blank">@arXiv 2409.15378</a>
                    <span class="tweet-title">AI Docs:  Whisper-ing Secrets to Faster, Safer Medical Transcriptions</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Kentucky</span>
                </div>
                <div class="primary-text">
                    This research combines two open-source tools, PyAnnote and OpenAI's Whisper, to create a system that automatically transcribes and labels speakers in medical conversations. This approach differs from previous work by focusing on security and efficiency, aiming to reduce the burden of manual transcription on healthcare professionals.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet70">
            <div class="start-time-icon" title="Play from here">28:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16200" target="_blank">@arXiv 2409.16200</a>
                    <span class="tweet-title">MRI Fingerprinting:  Breathing Easy with Motion Correction!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Institute of Myology, Siemens Healthcare SAS</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to motion correction in MRI fingerprinting, specifically for the upper body. It uses a separate motion scan to estimate the deformation field, which is then applied to correct the MRF acquisition data before reconstructing parametric maps. This approach is modular, allowing for separate optimization of the motion scan and the MRF scan.
                </div>
            </div>
        </div>

        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202409251420_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>    
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>
</html>