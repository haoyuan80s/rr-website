
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY</div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">Fresh Picks: 
                    <span class="highlightNumber" style="font-size: 28px;">64</span> out of <span
                    class="highlightNumber">327</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-09-26"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>


        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">01:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17146" target="_blank">@arXiv 2409.17146</a>
                    <span class="tweet-title">Open-Weight VLMs:  No More Distilling, Just Describing!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Allen Institute for AI, University of Washington</span>
                </div>
                <div class="primary-text">
                    This research introduces a new family of vision-language models (VLMs) called Molmo, which are trained on a novel, high-quality image captioning dataset called PixMo. Unlike previous open-weight VLMs, Molmo does not rely on synthetic data generated by proprietary systems, making it truly open and reproducible.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">01:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16427" target="_blank">@arXiv 2409.16427</a>
                    <span class="tweet-title">AI Agents on Trial:  A Sandbox for Testing Their Safety in the Real World</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, Allen Institute for AI, University of Washington...</span>
                </div>
                <div class="primary-text">
                    This research introduces HAICOSYSTEM, a framework that simulates multi-turn interactions between AI agents and human users in a sandbox environment. Unlike previous work that focuses on single-turn interactions or isolated safety risks, HAICOSYSTEM examines the safety of AI agents across diverse scenarios and tool use, considering both benign and malicious user intents.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">01:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16517" target="_blank">@arXiv 2409.16517</a>
                    <span class="tweet-title">LLMs: Charting a New Course for Multi-Modality Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research focuses on building a large-scale chart dataset, SynChart, using LLMs to generate synthetic chart data, rather than relying on existing datasets or code-based approaches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">02:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17095" target="_blank">@arXiv 2409.17095</a>
                    <span class="tweet-title">Text Recognition:  A New Way to Crack the Code!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École des Ponts ParisTech</span>
                </div>
                <div class="primary-text">
                    This research introduces a detection-based approach to text line recognition, which differs from the prevalent autoregressive decoding methods. Instead of predicting characters one by one, it detects all characters in parallel, treating the entire line as a single unit.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">02:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16766" target="_blank">@arXiv 2409.16766</a>
                    <span class="tweet-title">Lensless Cameras:  Seeing Through the Noise of Everyday Light!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research focuses on the impact of external illumination on lensless imaging, a problem largely ignored in previous studies. It proposes techniques to address this issue by incorporating an estimate of the external illumination into the image recovery process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">03:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16434" target="_blank">@arXiv 2409.16434</a>
                    <span class="tweet-title">PETL:  The  Fine-Tuning  Revolution  That's  Not  So  Fine  After  All?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The Ohio State University, Google Research</span>
                </div>
                <div class="primary-text">
                    This research conducts a unifying empirical study of parameter-efficient transfer learning (PETL) approaches for visual recognition, systematically comparing their performance and exploring their complementary nature. Unlike previous work, it goes beyond low-shot scenarios and investigates PETL's effectiveness in many-shot regimes, as well as its ability to preserve robustness to distribution shifts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">03:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16560" target="_blank">@arXiv 2409.16560</a>
                    <span class="tweet-title">LLMs Get a Speed Boost: Beam Decoding Goes Dynamic!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of California  Los Angeles</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method called "dynamic-width speculative beam decoding" (DSBD) that combines speculative decoding with beam sampling for faster and more accurate LLM inference. Unlike previous work that focused on single-sequence generation, DSBD leverages multiple candidate sequences, dynamically adjusting their number based on the context.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">03:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16491" target="_blank">@arXiv 2409.16491</a>
                    <span class="tweet-title">Deepfakes, Be Gone! New Research Turns Adversarial Attacks into Social Good.</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Michigan State University, Meta</span>
                </div>
                <div class="primary-text">
                    This research explores "proactive schemes" for embedding templates into digital media, which are designed to enhance the performance of deep learning models and protect against adversarial attacks. This approach differs from previous work that focused on passive methods, which don't modify the input data distribution.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">04:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16686" target="_blank">@arXiv 2409.16686</a>
                    <span class="tweet-title">Embodied AI Gets a Memory Makeover: Multi-Scale Insights for Smarter Decisions</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Beijing University of Posts and Telecommunications, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces MSI-Agent, an embodied agent that improves planning and decision-making by summarizing and utilizing insights at multiple scales. Unlike previous work that focused on single-scale insights, MSI-Agent leverages a three-part pipeline to generate task-specific insights, store them in a database, and then use relevant insights to aid in decision-making.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">04:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17126" target="_blank">@arXiv 2409.17126</a>
                    <span class="tweet-title">From "Giraffe" to Giraffe: AI Designs & Robots Build It!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley, Cornell University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new problem called Generative Design-for-Robot-Assembly (GDfRA). Unlike previous work that focuses on designing for assembly with human input, GDfRA aims to automate the entire design process using AI. Blox-Net, the system presented in the paper, combines a vision language model (VLM) with a physical robot and physics simulation to generate designs and assemble them autonomously.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">05:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16663" target="_blank">@arXiv 2409.16663</a>
                    <span class="tweet-title">Driving AI That Dreams: World Models for Smarter Cars</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">NVIDIA</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to address the covariate shift problem in imitation learning for autonomous vehicles by leveraging latent space generative world models. Unlike previous methods that rely on collecting additional data or using heuristics, this approach trains a driving policy to recover from errors by aligning with states observed in human demonstrations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">05:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16301" target="_blank">@arXiv 2409.16301</a>
                    <span class="tweet-title">Deep Learning Gives Robots a Leg Up on Stability!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, UC Berkeley, University of Southern California</span>
                </div>
                <div class="primary-text">
                    This research uses deep learning to estimate the "region of attraction" (RoA) for walking robots, which is the set of states from which the robot can stabilize to a desired gait. This approach differs from previous work by leveraging neural networks to approximate solutions to the Hamilton-Jacobi (HJ) partial differential equations (PDEs), overcoming the computational limitations of traditional numerical methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">06:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16499" target="_blank">@arXiv 2409.16499</a>
                    <span class="tweet-title">Learning Linear Dynamics from Bilinear Observations: A New Twist on System Identification!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Cornell University, MIT</span>
                </div>
                <div class="primary-text">
                    This paper focuses on learning the dynamics of a system where the input affects both the state update and the observation of the state, a scenario not extensively studied before. It introduces a new approach to estimate the system's Markov parameters from bilinear observations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">06:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16572" target="_blank">@arXiv 2409.16572</a>
                    <span class="tweet-title">Deep Learning Goes Underground:  A Faster, Smarter Way to Store Carbon</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new deep learning framework called "nested Fourier-DeepONet" for simulating geological carbon sequestration (GCS). This framework combines the strengths of two existing techniques, Fourier neural operators (FNO) and deep operator networks (DeepONet), to improve computational efficiency and generalization capabilities compared to previous FNO-based approaches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">06:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16947" target="_blank">@arXiv 2409.16947</a>
                    <span class="tweet-title">Stereo Image Super-Resolution:  A Race to the Finish Line (Under 1MB!)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">LAVA, NTIRE, École Normale Supérieure...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new constraint on computational complexity, limiting the model size to 1MB and the number of MACs to 400G, making it more relevant for real-world applications on resource-limited devices.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">07:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17005" target="_blank">@arXiv 2409.17005</a>
                    <span class="tweet-title">Math Models: Not Just Symbols, But Conversations!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Texas at Austin, MIT</span>
                </div>
                <div class="primary-text">
                    This research argues that math models should consider the communicative aspects of human-generated math, going beyond purely symbolic representations. It suggests that language models can learn and represent the communicative intentions embedded in mathematical expressions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">07:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16997" target="_blank">@arXiv 2409.16997</a>
                    <span class="tweet-title">INT8 Attention: Making LLMs Faster and Smaller with a Pinch of Quantization</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This paper introduces INT-FlashAttention, a novel architecture that integrates INT8 quantization with FlashAttention, a technique for accelerating attention computation in large language models. Unlike previous work that focused on tensor-level quantization, INT-FlashAttention utilizes token-level quantization, which preserves more information and leads to higher accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">07:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16346" target="_blank">@arXiv 2409.16346</a>
                    <span class="tweet-title">Quantum Compilation:  Machine Learning Makes It a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, Los Alamos National Laboratory, University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research explores a variational quantum compilation (VQC) scheme that leverages out-of-distribution generalization results from quantum machine learning (QML). Unlike previous VQC methods, this approach trains a parameterized quantum circuit (PQC) on a small dataset of product states, enabling it to generalize to highly entangled states.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">08:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16618" target="_blank">@arXiv 2409.16618</a>
                    <span class="tweet-title">Backdoor Attacks:  Now They're Claiming Your Data!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Korea Advanced Institute of Science and Technology</span>
                </div>
                <div class="primary-text">
                    This research introduces a new type of backdoor attack that uses the inherent claims within a sentence as triggers, eliminating the need for input manipulation after model distribution. Unlike previous methods that rely on altering input queries, this approach leverages the model's understanding of claims to manipulate its decisions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">08:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16603" target="_blank">@arXiv 2409.16603</a>
                    <span class="tweet-title">AI Doctors:  New Study Tests Robots' Ability to Write Discharge Summaries!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces two new shared tasks, RRG24 and "Discharge Me!", focusing on the generation of radiology reports and discharge summaries, respectively.  These tasks are distinct from previous work by using a larger, more diverse dataset and incorporating clinician review in the evaluation process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">08:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16407" target="_blank">@arXiv 2409.16407</a>
                    <span class="tweet-title">Stop the Weight-Loss! New Research Finds the Perfect Representation for Causal Inference</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford, University of California  Berkeley</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new method for finding design-based weights in causal inference. Unlike previous approaches that rely on specific assumptions about the data generating process, this method learns a representation of the covariates that minimizes the information lost when using a weighted estimator.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">09:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16546" target="_blank">@arXiv 2409.16546</a>
                    <span class="tweet-title">KV-Cache Gets a Precision Makeover: Aligning Bits for Faster LLMs</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new criterion called "precision alignment" to quantitatively evaluate the importance of parameters in mixed-precision quantization. Unlike previous work that relied on qualitative analysis and manual experiments, this approach provides a systematic framework for determining the optimal bit-width for each parameter.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">09:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16986" target="_blank">@arXiv 2409.16986</a>
                    <span class="tweet-title">Data Selection for LLMs:  Quality AND Diversity?  Quad's Got Your Back!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Beijing Institute of Technology, SenseTime, Purple Mountain Observatory...</span>
                </div>
                <div class="primary-text">
                    This paper introduces Quad, a data selection approach for pre-training large language models (LLMs) that prioritizes both data quality and diversity. Unlike previous methods that focus solely on quality, Quad uses a multi-armed bandit (MAB) framework to balance the selection of high-influence data with the exploration of diverse data clusters.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">10:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16718" target="_blank">@arXiv 2409.16718</a>
                    <span class="tweet-title">Fine-Tuning Vision-Language Models:  A Tiny Tweak, A Big Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo, University of Southern California, Xiamen University</span>
                </div>
                <div class="primary-text">
                    This research explores a parameter-efficient fine-tuning method for Vision-Language Models (VLMs) called CLIPFit. Unlike previous methods that introduce external parameters, CLIPFit focuses on fine-tuning specific inherent parameters within the VLM, specifically bias terms in the text encoder and LayerNorm in the image encoder.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">10:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16541" target="_blank">@arXiv 2409.16541</a>
                    <span class="tweet-title">Smoothing Out the Wrinkles:  A New Way to Fit Data with Sobolev Budgets</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of British Columbia, University of California  Berkeley, Korea Advanced Institute of Science and Technology (KAIST)...</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new approach to approximating probability measures using a measure whose support is parametrized by a function. The key innovation is the use of a Sobolev norm as a constraint, which allows for the control of the complexity of the approximation by bounding the derivatives of the function. This differs from previous work that typically used length constraints or geometric constraints.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">11:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16471" target="_blank">@arXiv 2409.16471</a>
                    <span class="tweet-title">Neural ODEs:  Score-Based Flow for Mean Field Control Problems</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Los Angeles, University of South Carolina</span>
                </div>
                <div class="primary-text">
                    This paper proposes a system of neural ODEs representing first- and second-order score functions along trajectories, which is used to reformulate the mean field control (MFC) problem with individual noises into an unconstrained optimization problem. This approach differs from previous work by directly computing the score functions using neural ODEs, rather than relying on score-matching methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">11:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16644" target="_blank">@arXiv 2409.16644</a>
                    <span class="tweet-title">Speech Quality Evaluator:  LLMs Get a Voice!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research explores using auditory large language models (LLMs) for speech quality assessment, a task typically handled by smaller, task-specific models. The paper demonstrates that finetuned auditory LLMs can predict MOS and SIM scores, perform A/B testing, and even generate natural language descriptions of speech quality.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">11:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17113" target="_blank">@arXiv 2409.17113</a>
                    <span class="tweet-title">LLMs Have Secret Zones:  Where Tiny Tweaks Don't Matter (Much)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">LASR Labs, University of Oxford, Apollo Research</span>
                </div>
                <div class="primary-text">
                    This paper identifies "stable regions" in the residual stream of Transformers, where small changes in activations have minimal impact on the model's output. This differs from previous work on polytopes, which are smaller and don't correspond to semantic distinctions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">12:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17140" target="_blank">@arXiv 2409.17140</a>
                    <span class="tweet-title">Turning Apps into Agents:  LLMs Make Software Super Smart!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Nanjing University, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research proposes a new framework called AXIS that prioritizes using application programming interfaces (APIs) over user interface (UI) interactions for LLM-based agents. This approach differs from previous work that focused on UI-based agents, which often suffer from high latency and low reliability due to the need for numerous sequential interactions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">12:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16653" target="_blank">@arXiv 2409.16653</a>
                    <span class="tweet-title">Transformers Get a Credibility Boost:  New Model Predicts Insurance Claims Better Than Ever!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Parthenope University of Naples, ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel credibility mechanism to the Transformer architecture, which is typically used for time-series data. The authors apply this mechanism to tabular data, specifically for predicting insurance claims. This approach differs from previous work by incorporating a special token that encodes prior information and combines it with observation-based information in a credibility-weighted average.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">12:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16767" target="_blank">@arXiv 2409.16767</a>
                    <span class="tweet-title">Neural Collapse:  Entropy's Got a New Dance Move!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Science and Technology Beijing, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research uses information-theoretic metrics like matrix entropy and mutual information to analyze Neural Collapse, a phenomenon observed during supervised learning. Unlike previous work that focused on similarity, this study explores the information interplay between data representations and classification head weights.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">13:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16666" target="_blank">@arXiv 2409.16666</a>
                    <span class="tweet-title">TalkinNeRF:  Giving Digital Humans a Voice (and Hands!)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stony Brook University, Meta</span>
                </div>
                <div class="primary-text">
                    This research introduces TalkinNeRF, a novel framework that learns a dynamic neural radiance field (NeRF) for full-body talking humans from monocular videos. Unlike previous work that focuses on either body pose or facial expressions, TalkinNeRF combines both, along with hand articulation, to create more realistic and expressive digital humans.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">13:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17066" target="_blank">@arXiv 2409.17066</a>
                    <span class="tweet-title">LLMs Go on a Diet:  Extreme Low-Bit Quantization for Language Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft, University of Science and Technology of China</span>
                </div>
                <div class="primary-text">
                    This research introduces Vector Post-Training Quantization (VPTQ), a new method for compressing large language models (LLMs) to extremely low bit-widths. Unlike previous methods that quantize individual weights, VPTQ quantizes entire vectors of weights, which allows for greater compression and accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">14:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17020" target="_blank">@arXiv 2409.17020</a>
                    <span class="tweet-title">RIS on a Diet:  How to Slim Down Image Segmentation Models Without Losing Brains</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Shanghai University of Engineering Science, SenseTime, Carnegie Mellon University</span>
                </div>
                <div class="primary-text">
                    This paper focuses on post-training quantization (PTQ) for referring image segmentation (RIS) models, a task that involves understanding both images and text. Unlike previous PTQ methods designed for single-modal CNNs or transformers, this research proposes a tailored approach for RIS models, addressing the unique challenges of multi-modal data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">14:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16722" target="_blank">@arXiv 2409.16722</a>
                    <span class="tweet-title">LLMs Get a Skeleton Makeover:  Fine-Tuning with Less Fuss, More Brains!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Xiaomi</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new fine-tuning method called PMSS (Pre-trained Matrices Skeleton Selection) that leverages the inherent structure of pre-trained weights. Unlike LoRA, which relies on low-rank updates, PMSS enables high-rank updates while maintaining parameter efficiency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">14:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16408" target="_blank">@arXiv 2409.16408</a>
                    <span class="tweet-title">Hopfield Networks Get a Neural Makeover: Encoding Memories for Better Recall</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research introduces Hopfield Encoding Networks (HEN), which integrate encoded neural representations into Modern Hopfield Networks (MHN) to improve pattern separability and reduce meta-stable states. This differs from previous work by specifically addressing the issue of meta-stable states in MHNs through pre-encoding and post-decoding.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">15:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16294" target="_blank">@arXiv 2409.16294</a>
                    <span class="tweet-title">GenCAD:  Turning Images into 3D Designs, One Command at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces GenCAD, a generative model that creates editable 3D shapes from images by generating a sequence of CAD commands. Unlike previous work that focuses on unconditional generation or specific data structures, GenCAD leverages image conditioning to align CAD generation with user intent.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">15:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16298" target="_blank">@arXiv 2409.16298</a>
                    <span class="tweet-title">Antibody Design Gets a Boost from AI's "BetterBodies"</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Freiburg</span>
                </div>
                <div class="primary-text">
                    This research combines reinforcement learning (RL) with diffusion models to design antibody sequences, a novel approach that allows for optimization of specific properties like binding affinity. Unlike previous work that focuses on generating entire sequences at once, this method takes a stepwise approach, placing amino acids one by one, which allows for better control and potentially improved results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">16:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16998" target="_blank">@arXiv 2409.16998</a>
                    <span class="tweet-title">Predicting Surgery Time: A Neural Network Learns the Pituitary Dance!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research introduces PitRSDNet, a neural network model that predicts remaining surgery duration in endoscopic pituitary surgery. Unlike previous models, PitRSDNet incorporates workflow knowledge by predicting surgical steps concurrently with the remaining surgery duration and using prior step predictions as context in temporal learning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">16:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17025" target="_blank">@arXiv 2409.17025</a>
                    <span class="tweet-title">Robot Surgeons Get a New Training Ground:  Pituitary Surgery Simulation Gets Real!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London, NVIDIA, King’s College London...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new public dataset for automated surgical skill assessment in endoscopic pituitary surgery, focusing on a high-fidelity bench-top phantom. This differs from previous work which primarily focused on laparoscopic surgery, isolated tasks, and robotic surgery.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">16:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16974" target="_blank">@arXiv 2409.16974</a>
                    <span class="tweet-title">LLMs: The Good, the Bad, and the Data-Hungry</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">San Jose State University, Stanford University</span>
                </div>
                <div class="primary-text">
                    This research provides a systematic overview of the literature on large language models (LLMs), focusing on their development, impacts, and limitations. It goes beyond simply summarizing existing work by conducting a thematic analysis to identify key trends and emerging questions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">17:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16934" target="_blank">@arXiv 2409.16934</a>
                    <span class="tweet-title">OCR-Sensitive Neurons:  The Secret to Taming Noisy Historical Texts</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research investigates the presence of OCR-sensitive neurons within Transformer architectures and their influence on named entity recognition (NER) performance on historical documents. Unlike previous work that focused on improving transcription accuracy or model architecture, this study explores the sensitivity of individual neurons to OCR noise and proposes a method to neutralize these neurons to improve model performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">17:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16710" target="_blank">@arXiv 2409.16710</a>
                    <span class="tweet-title">Can GPT-4 Sway Experts? A New Turing Test for AI Persuasion</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research goes beyond the traditional Turing test, focusing on how LLM-generated text influences human decisions, particularly in the financial domain. It examines the impact of GPT-4-generated analyses on both amateur and expert investors, analyzing their decision-making processes and the factors that sway their choices.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">18:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16469" target="_blank">@arXiv 2409.16469</a>
                    <span class="tweet-title">Spelling Correction for Speech Recognition:  A Lattice-Based Rewrite!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google  Inc.</span>
                </div>
                <div class="primary-text">
                    This research extends a previous technique for spelling correction in speech recognition to handle non-autoregressive lattices, which are more common in modern systems. The key difference is that the new approach directly converts wordpieces into phonemes, avoiding the need for word representations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">18:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16578" target="_blank">@arXiv 2409.16578</a>
                    <span class="tweet-title">Robot Brains Get a Tune-Up:  Large-Scale Learning Makes Bots Smarter!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Allen Institute for Artificial Intelligence, University of Texas  Austin, University of Washington...</span>
                </div>
                <div class="primary-text">
                    This research proposes FLaRe, a framework for fine-tuning large-scale robot policies using reinforcement learning. Unlike previous work that focused on smaller networks and single-task domains, FLaRe leverages multi-task pre-trained models and large-scale simulations for more robust and generalizable results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">18:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16830" target="_blank">@arXiv 2409.16830</a>
                    <span class="tweet-title">Robot Pathfinding:  Learning from Past Mistakes, Offline!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Indian Institute of Technology Madras, Carnegie Mellon University, Georgia Institute of Technology...</span>
                </div>
                <div class="primary-text">
                    This research proposes an offline reinforcement learning (RL) framework for informative path planning (IPP). Unlike traditional RL approaches that require real-time interaction with the environment, this framework trains an agent using pre-collected datasets generated by arbitrary algorithms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">19:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16860" target="_blank">@arXiv 2409.16860</a>
                    <span class="tweet-title">AI Doctors:  Can Language Models  Heal Healthcare?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CHRISTUS SantaRosa Hospital, Riphah International University, Palo Alto  CA</span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive review of the use of large language models (LLMs) in healthcare, focusing on their strengths, challenges, and ethical considerations. It distinguishes itself from previous work by offering a detailed exploration of the various applications of LLMs in healthcare, including medical diagnostics, patient care, and drug discovery.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">19:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16770" target="_blank">@arXiv 2409.16770</a>
                    <span class="tweet-title">Sewage Surveillance Gets a Smart Upgrade: Evolutionary Algorithm Finds the Best Sensor Spots!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Hong Kong, National University of Singapore, University College London</span>
                </div>
                <div class="primary-text">
                    This research introduces an Evolutionary Greedy (EG) algorithm for optimal sensor placement in sewage surveillance networks. Unlike previous greedy algorithms that focus on single-objective optimization, EG tackles multi-objective optimization by incorporating an evolutionary mechanism to generate and evaluate a diverse set of solutions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">19:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16973" target="_blank">@arXiv 2409.16973</a>
                    <span class="tweet-title">LLMs Get Personal:  Self-Supervised Learning Makes AI More User-Friendly</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, National University of Singapore, University of Oxford...</span>
                </div>
                <div class="primary-text">
                    This research introduces Adaptive Self-Supervised Learning Strategies (ASLS) for on-device LLM personalization. Unlike traditional methods that rely heavily on labeled datasets, ASLS utilizes self-supervised learning techniques to dynamically adapt LLMs to individual user preferences without requiring extensive labeled data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">20:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16843" target="_blank">@arXiv 2409.16843</a>
                    <span class="tweet-title">Time Travel for Forecasts: Finding the Perfect Starting Point for Accurate Predictions</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Central University of Finance and Economics, Chinese Academy of Sciences, Peking University...</span>
                </div>
                <div class="primary-text">
                    This research focuses on optimizing the starting point of time series data for forecasting, rather than just improving the forecasting models themselves. It proposes a novel approach called OSP-TSP (Optimal Starting Point Time Series Forecast) that uses machine learning to identify the best starting point for a time series, leading to more accurate predictions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">20:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16399" target="_blank">@arXiv 2409.16399</a>
                    <span class="tweet-title">ASR's New Ears:  Bio-Inspired Features for Robust Speech Recognition</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research explores the impact of using acoustic features inspired by human auditory perception on the accuracy and robustness of Automatic Speech Recognition (ASR) systems. Unlike previous work that focused on improving ASR models through complex neural networks and large datasets, this study investigates the potential of biologically plausible features to enhance robustness, particularly against adversarial attacks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">21:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16495" target="_blank">@arXiv 2409.16495</a>
                    <span class="tweet-title">FL Goes Hierarchical:  Flight  Takes Federated Learning to New Heights!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Chicago, Argonne National Laboratory</span>
                </div>
                <div class="primary-text">
                    This research introduces Flight, a federated learning framework that supports complex hierarchical multi-tier topologies, unlike existing frameworks that typically assume simple two-tier structures.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">21:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16336" target="_blank">@arXiv 2409.16336</a>
                    <span class="tweet-title">Refereeing the Referees:  New Tests for Generative Models in High Dimensions</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Genova, INFN, MaLGa-DIBRIS</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel sliced Kolmogorov-Smirnov statistic for evaluating generative models, comparing its performance to existing metrics like the sliced Wasserstein distance and Maximum Mean Discrepancy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">21:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16380" target="_blank">@arXiv 2409.16380</a>
                    <span class="tweet-title">Forest Fires?  Deep Learning to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of West Florida, Florida Insitute for Human and Machine Cognition (IHMC), Google</span>
                </div>
                <div class="primary-text">
                    This research builds a large, labeled dataset of satellite images specifically for wildfire detection using deep learning.  Previous studies have used smaller datasets or focused on different types of forest disturbances.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">22:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17054" target="_blank">@arXiv 2409.17054</a>
                    <span class="tweet-title">AI Doctor's Notes:  Whisper & GPT-3  Team Up to  Streamline Indonesian Healthcare</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research focuses on using a localized large language model (LLM) to transcribe and summarize doctor-patient conversations in Bahasa Indonesia, specifically for the ePuskesmas system. This differs from previous work by focusing on a specific language and healthcare system, aiming to improve efficiency and accuracy of medical records.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">22:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16327" target="_blank">@arXiv 2409.16327</a>
                    <span class="tweet-title">GATher:  Gene-Disease Links,  Predicted with a  Graph's  Attention Span!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Exscientia, University of Dundee</span>
                </div>
                <div class="primary-text">
                    This research introduces a new graph attention network called GATv3, which incorporates a context-aware attention mechanism to better capture the complex interactions within biomedical graphs. This differs from previous work by considering shared interactors among biological nodes, identifying nodes that are similar if linked to common diseases, functions, or interactors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">22:58</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16900" target="_blank">@arXiv 2409.16900</a>
                    <span class="tweet-title">LLMs Need a Body, a Brain, and a Social Life: Grounding AI in Reality</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Italian Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This research proposes a roadmap for grounding LLMs in the real world, emphasizing the importance of embodiment, temporally structured experience, and social interaction, going beyond simply connecting LLMs to robots.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">23:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16872" target="_blank">@arXiv 2409.16872</a>
                    <span class="tweet-title">AI Ethics:  A Framework for Keeping Robots in Line!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research proposes a unified framework that integrates ethical AI governance with legal compliance, addressing the scalability of AI systems in diverse business contexts. This framework differs from previous work by explicitly addressing the interplay between ethical principles and legal requirements in the context of scalable AI automation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">23:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16422" target="_blank">@arXiv 2409.16422</a>
                    <span class="tweet-title">Learning's Secret Sauce: All About Natural Gradient Descent</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, IBM Research</span>
                </div>
                <div class="primary-text">
                    This paper demonstrates that a wide class of effective learning rules, those that improve a scalar performance measure over time, can be rewritten as natural gradient descent with respect to a suitably defined loss function and metric. This is a significant departure from previous work, which often focused on specific learning rules or assumed monotonic improvement in the performance measure.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">23:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16779" target="_blank">@arXiv 2409.16779</a>
                    <span class="tweet-title">LLaMa-SciQ:  The Chatbot That Makes Science MCQs a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research focuses on fine-tuning a large language model (LLM) specifically for answering science multiple-choice questions (MCQs). The novelty lies in the use of specialized datasets for training and alignment with human preferences, particularly in STEM fields.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">24:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16430" target="_blank">@arXiv 2409.16430</a>
                    <span class="tweet-title">LLMs:  Not Just  Smart,  But  Fair?  A  Survey  of  Bias  in  Language  Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, BIT Sindri</span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive survey of bias in LLMs, going beyond previous work by systematically categorizing bias types, sources, impacts, and mitigation strategies. It also critically assesses existing bias mitigation techniques and proposes future research directions to enhance fairness and equity in LLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">24:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17049" target="_blank">@arXiv 2409.17049</a>
                    <span class="tweet-title">ControlCity: Building Footprints with a Multimodal Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Zhejiang Agriculture and Forestry University, Xiaohongshu, Zhejiang University</span>
                </div>
                <div class="primary-text">
                    This research introduces ControlCity, a multimodal diffusion model that generates urban building footprints using OpenStreetMap data. Unlike previous methods that rely solely on road networks, ControlCity integrates image, text, and metadata inputs, leading to more accurate and detailed results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">25:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.17141" target="_blank">@arXiv 2409.17141</a>
                    <span class="tweet-title">FineZip:  LLMs Get a Speed Boost for Text Compression!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research introduces FineZip, a new text compression system that combines online memorization and dynamic context to significantly reduce compression time compared to previous LLM-based methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">25:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.16849" target="_blank">@arXiv 2409.16849</a>
                    <span class="tweet-title">AI Benchmarks:  Unmasking the Hidden Assumptions!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford, Aarhus University</span>
                </div>
                <div class="primary-text">
                    This research proposes using explicit cognitive models, operationalized through Structural Equation Modeling (SEM), to expose assumptions in how test batteries relate to theoretical constructs. This approach differs from previous work that primarily focused on applying human-oriented psychometric batteries to generative models.
                </div>
            </div>
        </div>

        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202409261549_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>    
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>
</html>