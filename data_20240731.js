
daily_data = {
    "date": "2024-07-31",
    "tweets": [
            {
                "startTime": "00:54",
                "arxivId": "2407.20471",
                "arxivLink": "https://arxiv.org/abs/2407.20471",
                "title": "Symmetry Breaking: When Neural Networks Get a Little Wild!",
                "institute": "MIT",
                "text": "This paper introduces a new type of graph neural network that can learn and represent symmetry breaking within continuous groups. It builds on the existing E(3)NN framework by introducing relaxed weights, which allow for controlled symmetry breaking.",
                "paper-title": "Relaxed Equivariant Graph Neural Networks",
                "image-path": ""
            },

            {
                "startTime": "01:29",
                "arxivId": "2407.20292",
                "arxivLink": "https://arxiv.org/abs/2407.20292",
                "title": "Generative Models Get a Makeover: Renormalization Group to the Rescue!",
                "institute": "University College London, Dresden University of Technology, University of Oxford",
                "text": "This paper introduces a new approach to generative modeling using the renormalization group (RG) to create scale-free models. Unlike previous work, it focuses on discrete state-space models, which are more efficient and easier to learn.",
                "paper-title": "From pixels to planning: scale-free active inference",
                "image-path": ""
            },

            {
                "startTime": "01:48",
                "arxivId": "2407.20722",
                "arxivLink": "https://arxiv.org/abs/2407.20722",
                "title": "SMC Gets a Memory Boost: Persistent Sampling for Bayesian Inference",
                "institute": "UC Berkeley",
                "text": "This paper introduces persistent sampling (PS), an extension of sequential Monte Carlo (SMC) that retains particles from previous iterations, creating a growing, weighted ensemble. This allows for more accurate posterior approximations and lower-variance marginal likelihood estimates.",
                "paper-title": "Persistent Sampling: Unleashing the Potential of Sequential Monte Carlo",
                "image-path": ""
            },

            {
                "startTime": "02:15",
                "arxivId": "2407.20336",
                "arxivLink": "https://arxiv.org/abs/2407.20336",
                "title": "Sun Off, Lights On: Turning Daytime Scenes Into Nighttime Dreams!",
                "institute": "ETH Zurich, University of Toronto, KU Leuven...",
                "text": "This research introduces a novel physically-based method for simulating photorealistic nighttime images from daytime counterparts. Unlike previous data-driven approaches, this method explicitly models the 3D geometry, materials, and light sources of the scene, enabling more accurate and realistic nighttime simulations.",
                "paper-title": "Sun Off, Lights On: Photorealistic Monocular Nighttime Simulation for Robust Semantic Perception",
                "image-path": ""
            },

            {
                "startTime": "02:41",
                "arxivId": "2407.20311",
                "arxivLink": "https://arxiv.org/abs/2407.20311",
                "title": "Language Models: Not Just Memorizing, They're Actually Thinking!",
                "institute": "CMU, Meta, Mohamed bin Zayed University of Artificial Intelligence",
                "text": "This research focuses on understanding how language models solve grade-school math problems by training them from scratch on a synthetic dataset. This approach allows the researchers to control the data and eliminate potential contamination from pre-trained models.",
                "paper-title": "Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process",
                "image-path": ""
            },

            {
                "startTime": "03:13",
                "arxivId": "2407.20371",
                "arxivLink": "https://arxiv.org/abs/2407.20371",
                "title": "AI Hiring Tools: Are They Biased Against Black Men?",
                "institute": "University of Washington",
                "text": "This research investigates the biases of Massive Text Embedding (MTE) models, a specific type of large language model (LLM), when used for resume screening. Unlike previous work that focused on general LLMs or AI hiring tools, this study specifically examines MTEs and their potential for discrimination.",
                "paper-title": "Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval",
                "image-path": ""
            },

            {
                "startTime": "03:32",
                "arxivId": "2407.20351",
                "arxivLink": "https://arxiv.org/abs/2407.20351",
                "title": "LiteEFG: Solving Games Faster Than You Can Say \"Checkmate!\"",
                "institute": "MIT",
                "text": "This research introduces LiteEFG, a Python library for solving extensive-form games (EFGs) that leverages a C++ backend for significant speedups compared to pure Python implementations. Unlike existing libraries, LiteEFG automatically handles the complex structure of imperfect-information games, simplifying the implementation process for researchers.",
                "paper-title": "LiteEFG: An Efficient Python Library for Solving Extensive-form Games",
                "image-path": ""
            },

            {
                "startTime": "03:51",
                "arxivId": "2407.20243",
                "arxivLink": "https://arxiv.org/abs/2407.20243",
                "title": "Shrinking Embeddings: How to Make LLMs More Efficient Without Sacrificing Performance",
                "institute": "Google",
                "text": "This research introduces Matryoshka-Adaptor, a novel tuning framework that customizes embeddings from Large Language Models (LLMs) to achieve substantial dimensionality reduction without compromising performance. Unlike previous work that focuses on Matryoshka properties during pre-training, this approach tunes embeddings after they are extracted from pre-trained LLMs.",
                "paper-title": "Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions",
                "image-path": ""
            },

            {
                "startTime": "04:10",
                "arxivId": "2407.20254",
                "arxivLink": "https://arxiv.org/abs/2407.20254",
                "title": "EEG Mamba Strikes: A Multi-Task Brainwave Classifier That's Both Smart and Speedy!",
                "institute": "Peking University",
                "text": "This research introduces EEGMamba, a novel EEG classification network that integrates Spatio-Temporal-Adaptive (ST-Adaptive) modules, Bidirectional Mamba, and Mixture of Experts (MoE) into a unified framework for multiple tasks. Unlike previous models that focus on single tasks, EEGMamba can handle EEG data from various tasks simultaneously, adapting to different signal lengths and channel counts.",
                "paper-title": "EEGMamba: Bidirectional State Space Models with Mixture of Experts for EEG Classification",
                "image-path": ""
            },

            {
                "startTime": "04:42",
                "arxivId": "2407.20584",
                "arxivLink": "https://arxiv.org/abs/2407.20584",
                "title": "Pruning LLMs: A New Trick to Make Big Models Tiny and Smart!",
                "institute": "Tsinghua University",
                "text": "This research proposes a novel training pipeline called Adaptive Sparse Trainer (AST) for semi-structured sparse models. Unlike previous methods that prune models after training, AST retrains dense pretrained LLMs into sparse ones, allowing the model to adaptively select better sparsity patterns during training.",
                "paper-title": "Pruning Large Language Models with Semi-Structural Adaptive Sparse Training",
                "image-path": ""
            },

            {
                "startTime": "05:07",
                "arxivId": "2407.21009",
                "arxivLink": "https://arxiv.org/abs/2407.21009",
                "title": "AI Makes Math Questions So Hard, Even AI Can't Solve Them!",
                "institute": "Mila \u2013 Quebec AI Institute, Universit\u00e9 de Montr\u00e9al, Princeton University...",
                "text": "This research proposes a novel framework for generating challenging math questions by combining the strengths of LLMs with human expertise. Unlike previous work that relies solely on LLMs or human experts, this approach leverages the metacognitive abilities of LLMs to extract core skills from existing datasets and then uses these skills to generate questions that require the application of multiple skills.",
                "paper-title": "AI-Assisted Generation of Difficult Math Questions",
                "image-path": ""
            },

            {
                "startTime": "05:31",
                "arxivId": "2407.20253",
                "arxivLink": "https://arxiv.org/abs/2407.20253",
                "title": "EEG Data Augmentation: A Diffusion Model's Random Reassembly Trick!",
                "institute": "Peking University",
                "text": "This research proposes a new data augmentation method for EEG classification networks that randomly reassembles original and generated EEG data to create \"vicinal\" data. This differs from previous methods that directly incorporated generated data into the training set, which often led to unstable performance.",
                "paper-title": "Improving EEG Classification Through Randomly Reassembling Original and Generated Data with Transformer-based Diffusion Models",
                "image-path": ""
            },

            {
                "startTime": "05:56",
                "arxivId": "2407.20756",
                "arxivLink": "https://arxiv.org/abs/2407.20756",
                "title": "Vision Models Get a Synthetic Makeover: 100k Fake Images, Real Results!",
                "institute": "Peking University",
                "text": "This paper introduces SynthVLM, a novel data synthesis pipeline for Vision Language Models (VLLMs). Unlike existing methods that generate captions from images, SynthVLM uses advanced diffusion models to generate images from captions, creating precisely aligned image-text pairs.",
                "paper-title": "SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models",
                "image-path": ""
            },

            {
                "startTime": "06:19",
                "arxivId": "2407.21011",
                "arxivLink": "https://arxiv.org/abs/2407.21011",
                "title": "Crafty CLEFT: A Language Model That's Smart, But Not Too Big for Medical Images",
                "institute": "Yale University",
                "text": "This research introduces CLEFT, a new method for contrastive language-image pre-training that uses a large language model (LLM) but focuses on fine-tuning only a small portion of the model's parameters. This approach aims to improve performance while reducing the computational resources needed for training, making it more suitable for medical applications where data is often limited.",
                "paper-title": "CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning",
                "image-path": ""
            },

            {
                "startTime": "06:42",
                "arxivId": "2407.20508",
                "arxivLink": "https://arxiv.org/abs/2407.20508",
                "title": "Spiking Neurons Go Graphing: A New Way to Learn from Networks",
                "institute": "Guangdong Institute of Intelligence Science and Technology, Tsinghua University, Hong Kong Polytechnic University...",
                "text": "This research introduces a novel framework for integrating spiking neural networks (SNNs) with graph representation learning, addressing the limitations of previous work in handling non-Euclidean data and exploring the impact of spiking dynamics on graph learning. The paper proposes a spatial-temporal feature normalization (STFN) technique to enhance training efficiency and model stability, offering a comprehensive spike-based modeling framework for graph tasks.",
                "paper-title": "Unveiling the Potential of Spiking Dynamics in Graph Representation Learning through Spatial-Temporal Normalization and Coding Strategies",
                "image-path": ""
            },

            {
                "startTime": "07:17",
                "arxivId": "2407.20273",
                "arxivLink": "https://arxiv.org/abs/2407.20273",
                "title": "Learning Material Behavior Without a Physics Textbook: AI Cracks the Code of Elasticity",
                "institute": "ETH Zurich",
                "text": "This research introduces a new machine learning approach called uLED, which learns the constitutive relations of hyperelastic materials solely from displacement data. Unlike previous methods, uLED does not require stress data or information about boundary forces, making it particularly suitable for in-situ applications.",
                "paper-title": "Learning Physics-Consistent Material Behavior Without Prior Knowledge",
                "image-path": ""
            },

            {
                "startTime": "07:44",
                "arxivId": "2407.20399",
                "arxivLink": "https://arxiv.org/abs/2407.20399",
                "title": "LiDAR's New Trick: Seeing in the Dark with a Neighborhood Watch!",
                "institute": "UC Berkeley, Purdue University",
                "text": "This research delves into the theoretical limitations of the rank-ordered mean (ROM) filter, a common technique for removing noise in single-photon LiDAR systems. It then proposes a new method, the neighborhood consensus filter, which leverages the temporal closeness of signal timestamps to improve depth estimation accuracy.",
                "paper-title": "Analysis and Improvement of Rank-Ordered Mean Algorithm in Single-Photon LiDAR",
                "image-path": ""
            },

            {
                "startTime": "08:05",
                "arxivId": "2407.20962",
                "arxivLink": "https://arxiv.org/abs/2407.20962",
                "title": "Music to My Eyes: A New Dataset for Training AI to Understand Videos with Soundtrack",
                "institute": "The Hong Kong University of Science and Technology, Peking University",
                "text": "This research introduces a new dataset, MMTrail, that focuses on trailer videos, incorporating both visual and audio information, including music descriptions. Unlike previous datasets that primarily rely on visual captions, MMTrail aims to capture the inherent relationship between visual and audio elements, particularly music.",
                "paper-title": "MMTrail: A Multimodal Trailer Video Dataset with Language and Music Descriptions",
                "image-path": ""
            },

            {
                "startTime": "08:30",
                "arxivId": "2407.20272",
                "arxivLink": "https://arxiv.org/abs/2407.20272",
                "title": "LLMs on a Diet: New Framework Makes Large Language Models Slim and Speedy!",
                "institute": "Peking University",
                "text": "This research focuses on building an efficient inference framework specifically for early-exit LLMs, a type of LLM that can skip layers during inference to save time and resources. This is different from previous work on LLM inference frameworks, which were designed for traditional LLMs that always run through all layers.",
                "paper-title": "An Efficient Inference Framework for Early-exit Large Language Models",
                "image-path": ""
            },

            {
                "startTime": "08:52",
                "arxivId": "2407.20447",
                "arxivLink": "https://arxiv.org/abs/2407.20447",
                "title": "AI Agent Makes Prescriptive Decisions, No Data Science Degree Required!",
                "institute": "MIT, IBM Research",
                "text": "This research focuses on making prescriptive AI accessible to users without data science expertise by developing a domain-adaptable conversational agent called PrecAIse. Unlike previous work that relied heavily on in-context learning, PrecAIse incorporates prompt tuning for improved accuracy and a fully automated pipeline for generalization to new datasets.",
                "paper-title": "Domain Adaptable Prescriptive AI Agent for Enterprise",
                "image-path": ""
            },

            {
                "startTime": "09:16",
                "arxivId": "2407.20455",
                "arxivLink": "https://arxiv.org/abs/2407.20455",
                "title": "Portrait Editing: From Fake Friends to Feature-Preserving Fun!",
                "institute": "University of Washington",
                "text": "This paper proposes a novel training-based method for portrait editing that leverages automatically generated paired data to learn desired editing while preserving subject features. Unlike previous training-free methods, this approach doesn't rely on inverting images into a model's latent space, which can lead to editability issues and feature loss.",
                "paper-title": "Learning Feature-Preserving Portrait Editing from Generated Pairs",
                "image-path": ""
            },

            {
                "startTime": "09:40",
                "arxivId": "2407.20859",
                "arxivLink": "https://arxiv.org/abs/2407.20859",
                "title": "LLM Agents: Not So Smart After All? New Attack Makes Them Go Bonkers!",
                "institute": "CISPA Helmholtz Center for Information Security, NetApp, Microsoft...",
                "text": "This research focuses on a new type of attack against LLM agents that aims to disrupt their normal functioning by inducing malfunctions, rather than focusing on overtly harmful or policy-violating actions.",
                "paper-title": "Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification",
                "image-path": ""
            },

            {
                "startTime": "10:05",
                "arxivId": "2407.20754",
                "arxivLink": "https://arxiv.org/abs/2407.20754",
                "title": "Querying Inconsistent Knowledge Bases: When Rules Break, Costs Take Over!",
                "institute": "University of Bordeaux, \u00c9cole Normale Sup\u00e9rieure",
                "text": "This research introduces a novel approach to querying inconsistent knowledge bases by assigning weights to both axioms and assertions, allowing for a cost-based evaluation of interpretations. This differs from previous work that primarily focused on repairing inconsistent ABoxes while leaving the TBox untouched.",
                "paper-title": "Cost-Based Semantics for Querying Inconsistent Weighted Knowledge Bases",
                "image-path": ""
            },

            {
                "startTime": "10:31",
                "arxivId": "2407.20798",
                "arxivLink": "https://arxiv.org/abs/2407.20798",
                "title": "AI Agents Get a Diffusion Makeover: Learning Faster with Synthetic Experiences!",
                "institute": "Imperial College London, Google DeepMind",
                "text": "This research introduces Diffusion Augmented Agents (DAAG), a framework that uses diffusion models to modify visual observations, creating synthetic experiences for training reinforcement learning agents. This differs from previous work by leveraging diffusion models for autonomous, geometrically and temporally consistent data augmentation, enabling more efficient learning and transfer.",
                "paper-title": "Diffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learning",
                "image-path": ""
            },

            {
                "startTime": "10:58",
                "arxivId": "2407.20266",
                "arxivLink": "https://arxiv.org/abs/2407.20266",
                "title": "AI Models on a Diet: Low-Rank Decomposition Gets a Speed Boost",
                "institute": "Huawei Technologies",
                "text": "This research focuses on improving the efficiency of low-rank decomposition (LRD) for compressing AI models. Unlike previous work that primarily focused on compression, this paper explores strategies to accelerate both training and inference by optimizing rank selection and introducing layer freezing and merging techniques.",
                "paper-title": "Accelerating the Low-Rank Decomposed Models",
                "image-path": ""
            },

            {
                "startTime": "11:27",
                "arxivId": "2407.20256",
                "arxivLink": "https://arxiv.org/abs/2407.20256",
                "title": "LLMs: From Web Wizards to Data Dwarfs?",
                "institute": "MIT, UW, Intel",
                "text": "This research investigates the performance of LLMs on enterprise data tasks, specifically text-to-SQL and semantic column type detection, highlighting the challenges and potential solutions for effectively utilizing LLMs in enterprise settings. This differs from previous work that primarily focused on LLMs' performance on public datasets.",
                "paper-title": "Making LLMs Work for Enterprise Data Tasks",
                "image-path": ""
            },

            {
                "startTime": "11:52",
                "arxivId": "2407.20635",
                "arxivLink": "https://arxiv.org/abs/2407.20635",
                "title": "Robots Learn From Their Mistakes, And It's Hilarious!",
                "institute": "UC Berkeley",
                "text": "This research introduces SOAR, a system that allows robots to autonomously improve their instruction-following skills by leveraging internet-scale knowledge from vision-language models (VLMs) and learning from their own experiences. This differs from previous work that often relies on costly human-provided demonstrations or hand-specified tasks.",
                "paper-title": "Autonomous Improvement of Instruction Following Skills via Foundation Models",
                "image-path": ""
            },

            {
                "startTime": "12:13",
                "arxivId": "2407.20990",
                "arxivLink": "https://arxiv.org/abs/2407.20990",
                "title": "AI Explains Itself: Chatting with Your Car's Brain",
                "institute": "University of Oxford",
                "text": "This research introduces a traceable question-answering approach for explaining AI model outputs using LLMs and an external knowledge repository. This differs from previous work by integrating feature importance and contrastive explanations into the LLM's responses.",
                "paper-title": "From Feature Importance to Natural Language Explanations Using LLMs with RAG",
                "image-path": ""
            },

            {
                "startTime": "12:40",
                "arxivId": "2407.20441",
                "arxivLink": "https://arxiv.org/abs/2407.20441",
                "title": "Multi-Agent Learning: Faster Than a Speeding Bullet (Even With Delays!)",
                "institute": "University of Padua, Princeton University, North Carolina State University...",
                "text": "This paper analyzes the convergence of an asynchronous multi-agent TD learning algorithm, AsyncMATD, which incorporates bounded delays in communication between agents. This differs from previous work by providing finite-time convergence guarantees for asynchronous MARL under Markovian sampling, a more realistic scenario than the i.i.d. sampling assumption used in prior studies.",
                "paper-title": "Finite-Time Analysis of Asynchronous Multi-Agent TD Learning",
                "image-path": ""
            },

            {
                "startTime": "13:05",
                "arxivId": "2407.20276",
                "arxivLink": "https://arxiv.org/abs/2407.20276",
                "title": "AI's Got a Gambling Problem: Random Guessers Beat Sophisticated Algorithms!",
                "institute": "IBM",
                "text": "This research introduces a \"random guesser test\" to evaluate the rationality of AI systems in sequential decision-making scenarios. Unlike previous work that focuses on regret analysis, this study emphasizes the importance of exploration and highlights the potential for AI systems to favor overly low-risk options.",
                "paper-title": "Assessing AI Rationality: The Random Guesser Test for Sequential Decision-Making Systems",
                "image-path": ""
            },

            {
                "startTime": "13:41",
                "arxivId": "2407.21001",
                "arxivLink": "https://arxiv.org/abs/2407.21001",
                "title": "AI's Got a Gender Bias: Can Robots Tell a Man From a Woman Doing the Dishes?",
                "institute": "Sharif University of Technology, \u00c9cole Polytechnique F\u00e9d\u00e9rale de lausanne (EPFL), IDIAP research institute...",
                "text": "This research focuses on a specific type of bias in vision-language models (VLMs) called \"Gender-Activity Binding (GAB) bias.\" It investigates how VLMs associate activities with specific genders, even when the activity is performed by someone of the opposite gender. This is different from previous work that has looked at gender bias in VLMs more generally.",
                "paper-title": "GABInsight: Exploring Gender-Activity Binding Bias in Vision-Language Models",
                "image-path": ""
            },

            {
                "startTime": "14:08",
                "arxivId": "2407.20893",
                "arxivLink": "https://arxiv.org/abs/2407.20893",
                "title": "ECG Diagnosis Gets a Brain Boost: MambaCapsule Makes Heart Health Transparent!",
                "institute": "Zhejiang University, Stanford University, Shanghai University",
                "text": "This research introduces MambaCapsule, a deep neural network for ECG arrhythmia classification that focuses on explainability. Unlike previous models that prioritize performance, MambaCapsule provides not only a confidence score but also signal features, making the diagnostic process more transparent.",
                "paper-title": "MambaCapsule: Towards Transparent Cardiac Disease Diagnosis with Electrocardiography Using Mamba Capsule Network",
                "image-path": ""
            },

            {
                "startTime": "14:32",
                "arxivId": "2407.20281",
                "arxivLink": "https://arxiv.org/abs/2407.20281",
                "title": "DNNs on a Diet: Semantic Slicing for Leaner, Meaner Models",
                "institute": "Nanyang Technological University, Huazhong University of Science and Technology",
                "text": "This research introduces a new technique called \"semantic slicing\" that identifies and manipulates individual neurons within a deep neural network (DNN) for model maintenance tasks. Unlike previous work that focused on layer-level manipulation, this approach allows for more precise control over the model's structure and behavior.",
                "paper-title": "NeuSemSlice: Towards Effective DNN Model Maintenance via Neuron-level Semantic Slicing",
                "image-path": ""
            },

            {
                "startTime": "15:07",
                "arxivId": "2407.20446",
                "arxivLink": "https://arxiv.org/abs/2407.20446",
                "title": "Event-Based Vision: A Dataset for Cars That See Like Humans!",
                "institute": "University of Michigan",
                "text": "This research introduces a new dataset, MEVDT, specifically designed for event-based vision, a technology inspired by the human retina. Unlike previous datasets, MEVDT provides synchronized streams of event data and grayscale images, along with detailed annotations for object detection and tracking.",
                "paper-title": "MEVDT: Multi-Modal Event-Based Vehicle Detection and Tracking Dataset",
                "image-path": ""
            },

            {
                "startTime": "15:31",
                "arxivId": "2407.20241",
                "arxivLink": "https://arxiv.org/abs/2407.20241",
                "title": "NudgeRank\u2122: AI Makes Your Steps Count (and Your Health Improve!)",
                "institute": "University of Washington",
                "text": "This research introduces NudgeRank\u2122, a recommender system that uses a novel combination of graph neural networks and a knowledge graph to deliver personalized health nudges. Unlike previous work that relies on rule-based systems or focuses on a single health goal, NudgeRank\u2122 dynamically generates nudges for multiple health outcomes, considering individual preferences and contextual states.",
                "paper-title": "NudgeRank: Digital Algorithmic Nudging for Personalized Health",
                "image-path": ""
            },

            {
                "startTime": "15:56",
                "arxivId": "2407.20257",
                "arxivLink": "https://arxiv.org/abs/2407.20257",
                "title": "Video Question Answering: It's Not Just About What, But Why and When!",
                "institute": "CMU",
                "text": "This research focuses on improving Video Question Answering (VQA) models by addressing the limitations of existing approaches. Unlike previous methods that rely on either single-frame or complete-video information, this paper proposes a novel approach that leverages a smart aggregation of sub-sampled information to enhance performance.",
                "paper-title": "Causal Understanding For Video Question Answering",
                "image-path": ""
            },

            {
                "startTime": "16:19",
                "arxivId": "2407.20535",
                "arxivLink": "https://arxiv.org/abs/2407.20535",
                "title": "DeepSpeech Models: Cochlear Implants Get a Brain Boost!",
                "institute": "Columbia University, DeepMind",
                "text": "This research uses a deep neural network model, DeepSpeech2, to simulate how cochlear implants process speech signals. Unlike previous work that focused on modeling the cochlea itself, this study investigates the entire auditory processing hierarchy, from sound to phonemes to words.",
                "paper-title": "DeepSpeech models show Human-like Performance and Processing of Cochlear Implant Inputs",
                "image-path": ""
            },

            {
                "startTime": "16:46",
                "arxivId": "2407.20466",
                "arxivLink": "https://arxiv.org/abs/2407.20466",
                "title": "Reinforcement Learning Gets a Speed Boost: Pre-trained Critics Make Agents Learn Faster!",
                "institute": "University of Washington, Western Washington University, Kennesaw State University...",
                "text": "This research introduces a novel approach to accelerate reinforcement learning by leveraging pre-trained critic value functions from multiple environments. Unlike traditional methods that require extensive retraining, this approach integrates existing knowledge to enable agents to adapt swiftly to new settings.",
                "paper-title": "A Method for Fast Autonomy Transfer in Reinforcement Learning",
                "image-path": ""
            },

            {
                "startTime": "17:12",
                "arxivId": "2407.20444",
                "arxivLink": "https://arxiv.org/abs/2407.20444",
                "title": "Sampling from Unnormalized Densities: A Neural JKO with a Rejection Twist!",
                "institute": "University College London, Free University of Berlin",
                "text": "This paper proposes a new sampling method that combines continuous normalizing flows (CNFs) with rejection-resampling steps based on importance weights. This approach aims to overcome local minima and slow convergence issues often encountered in Wasserstein gradient flows (WGFs) for multimodal distributions.",
                "paper-title": "Importance Corrected Neural JKO Sampling",
                "image-path": ""
            },

            {
                "startTime": "17:45",
                "arxivId": "2407.20529",
                "arxivLink": "https://arxiv.org/abs/2407.20529",
                "title": "LLMs: Not So Smart After All? New Research Uncovers Their Hidden Weaknesses!",
                "institute": "Microsoft",
                "text": "This research goes beyond simply identifying vulnerabilities in LLMs and proposes two novel mitigation strategies: \"Model Editing\" and \"Chroma Teaming.\" Model Editing focuses on modifying the LLM itself to improve its behavior, while Chroma Teaming brings together different teams (red, blue, green, and purple) to work collaboratively on LLM security.",
                "paper-title": "Can LLMs be Fooled? Investigating Vulnerabilities in LLMs",
                "image-path": ""
            },

            {
                "startTime": "18:11",
                "arxivId": "2407.20267",
                "arxivLink": "https://arxiv.org/abs/2407.20267",
                "title": "SMILES, But Make It Fashion: A New Foundation Model for Chemical Language",
                "institute": "IBM",
                "text": "This research introduces a new family of encoder-decoder foundation models for chemical language, pre-trained on a curated dataset of 91 million SMILES samples from PubChem. This differs from previous work by focusing on a larger, more carefully curated dataset and incorporating a Mixture-of-Experts approach for scalability.",
                "paper-title": "A Large Encoder-Decoder Family of Foundation Models For Chemical Language",
                "image-path": ""
            }
    ],
    "stats": {
        "num_pick": 41,
        "num_total": 234,
    },
    "audio": "https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202407312055_audio.mp3"
}