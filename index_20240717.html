<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                    Fresh Picks:
                    <span class="highlightNumber">121</span> out of <span class="highlightNumber">525</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-07-17"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>

        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">
                00:47
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10949" target="_blank">
                        @arXiv 2407.10949
                    </a>
                    <span class="tweet-title">
                        Eliza, the Chatbot, Gets a Neural Makeover!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Princeton University
                    </span>
                </div>
                <div class="primary-text">
                    This research constructs a Transformer model that implements the classic ELIZA chatbot
                    algorithm, a
                    departure from previous work that focused on single-sentence tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">
                01:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10031" target="_blank">
                        @arXiv 2407.10031
                    </a>
                    <span class="tweet-title">
                        Robots Get a Brain: Language Models Help Multi-Agent Teams Plan Long-Term Tasks
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT, TCS, USAF-MIT AI Accelerator...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces LLaMAR, a multi-agent planner that uses Language Models (LMs) to break
                    down
                    complex tasks into smaller steps. Unlike previous methods, LLaMAR doesn't rely on perfect
                    knowledge
                    of the environment or oracle feedback, making it more adaptable to real-world scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">
                01:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10329" target="_blank">
                        @arXiv 2407.10329
                    </a>
                    <span class="tweet-title">
                        AI Gone Wild: How Generative AI Is Discriminating, and What We Can Do About It
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Oxford University Press, European University Viadrina, European New School of Digital
                        Studies
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the unique ways generative AI (genAI) can perpetuate discrimination,
                    going
                    beyond traditional AI bias analysis. It examines how genAI's outputs, like text and images, can
                    create harmful stereotypes and inadequate representation, even when individual outputs aren't
                    overtly discriminatory.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">
                02:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10341" target="_blank">
                        @arXiv 2407.10341
                    </a>
                    <span class="tweet-title">
                        Robots Learn New Tricks with a Vision-Language Model's Help!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research uses a vision-language model (VLM) to generate dense rewards for reinforcement
                    learning (RL) in robotics, which helps robots learn more efficiently and with less reliance on
                    human
                    demonstrations. This differs from previous work that primarily used VLMs for generating sparse
                    rewards.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">
                02:27
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10817" target="_blank">
                        @arXiv 2407.10817
                    </a>
                    <span class="tweet-title">
                        LLMs as Judges: Training AI to Grade AI, and It's Getting Pretty Good!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google DeepMind, Google, UMass Amherst
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces FLAMe, a family of large language models (LLMs) trained on a curated
                    collection of over 100 quality assessment tasks, comprising over 5.3 million human judgments.
                    This
                    approach differs from previous work by relying solely on publicly available, permissively
                    licensed
                    human evaluations, rather than model-generated outputs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">
                02:54
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09975" target="_blank">
                        @arXiv 2407.09975
                    </a>
                    <span class="tweet-title">
                        GPT in Class: Coding Boost, Engagement Bust?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This study is unique because it examines the impact of a general-purpose LLM, GPT-4, on student
                    engagement and learning outcomes in a large-scale online coding course. Previous research has
                    focused on the use of LLMs in specific educational contexts or on the potential benefits of LLMs
                    for
                    learning, but this study provides a more nuanced picture of the potential risks and benefits of
                    integrating LLMs into classrooms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">
                03:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09690" target="_blank">
                        @arXiv 2407.09690
                    </a>
                    <span class="tweet-title">
                        Privacy-Preserving Federated Learning: A Trust-Free, Communication-Efficient Algorithm
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Wisconsin-Madison
                    </span>
                </div>
                <div class="primary-text">
                    This research extends previous work on private federated learning by addressing the challenge of
                    heterogeneous data, where each participant's data is unique. It proposes new algorithms that
                    achieve
                    optimal accuracy while requiring fewer communication rounds and less computation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">
                03:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09739" target="_blank">
                        @arXiv 2407.09739
                    </a>
                    <span class="tweet-title">
                        Active Learning for Sensitivity Analysis: A Derivative-Driven Approach
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, Meta, Washington State University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes novel active learning acquisition functions that directly target
                    derivative-based global sensitivity measures (DGSMs) under Gaussian process surrogate models.
                    This
                    is a departure from previous work that focused on general uncertainty reduction methods or
                    targeted
                    only the function itself.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">
                04:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10930" target="_blank">
                        @arXiv 2407.10930
                    </a>
                    <span class="tweet-title">
                        Prompt Engineering: It's Not Just About the Words, It's About the Weights Too!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research explores a novel approach to optimizing language model (LM) programs by
                    simultaneously
                    fine-tuning the LM weights and optimizing the prompts used to guide the model's behavior. This
                    differs from previous work that focused on optimizing either prompts or weights in isolation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">
                04:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09578" target="_blank">
                        @arXiv 2407.09578
                    </a>
                    <span class="tweet-title">
                        Anomaly Detection: Diffusion Models Get a Trend Makeover!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Samsung Display, Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to anomaly detection using diffusion models by analyzing
                    the
                    trend of reconstruction as the noise level increases. Unlike previous methods that rely on a
                    single
                    noise level, this method leverages the gradual change in the reconstructed image to identify
                    anomalies more effectively.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">
                05:05
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09835" target="_blank">
                        @arXiv 2407.09835
                    </a>
                    <span class="tweet-title">
                        Low-Rank Transformers: Making Big Models Slim and Speedy!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        École Polytechnique Fédérale de Lausanne, Google
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on applying low-rank parametrization to the feedforward networks (FFNs)
                    within
                    Transformer-based language models, specifically training these models from scratch. Previous
                    work
                    has explored low-rank techniques for compression or fine-tuning, but this study investigates
                    their
                    impact on training efficiency and scaling.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">
                05:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10315" target="_blank">
                        @arXiv 2407.10315
                    </a>
                    <span class="tweet-title">
                        Deep Learning's Memory: When Too Much Knowledge is a Bad Thing!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This research presents a statistical-mechanics theory of continual learning in deep, wide neural
                    networks, which characterizes the network's input-output mapping as it learns a sequence of
                    tasks.
                    This approach differs from previous work by not relying on data assumptions, allowing the
                    analysis
                    of continual learning in a broader range of tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">
                05:44
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10943" target="_blank">
                        @arXiv 2407.10943
                    </a>
                    <span class="tweet-title">
                        GRUtopia: Robots Learn to Live in a City, One Simulated Scene at a Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Shanghai AI Laboratory, Zhejiang University, Shanghai Jiao Tong University...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces GRUtopia, a simulated 3D city environment designed for training
                    embodied AI
                    robots. Unlike previous platforms that focus on home environments, GRUtopia includes a wider
                    range
                    of scene categories, including supermarkets, hospitals, and offices, to better prepare robots
                    for
                    real-world service applications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">
                06:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09590" target="_blank">
                        @arXiv 2407.09590
                    </a>
                    <span class="tweet-title">
                        Pruning Experts: How to Slim Down Your Giant Language Model Without Losing Its Smarts
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Rochester, Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on task-agnostic pruning of Mixture-of-Experts (MoE) models, which means
                    it
                    aims to reduce the model's size without relying on specific tasks for guidance. Unlike previous
                    work
                    that often uses task-specific information to prune experts, this study explores a more general
                    approach based on expert similarity in the feature space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">
                06:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10910" target="_blank">
                        @arXiv 2407.10910
                    </a>
                    <span class="tweet-title">
                        DataDream: AI's New BFF for Few-Shot Learning
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Tübingen, Helmholtz Munich, MCML...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes DataDream, a method that fine-tunes a text-to-image diffusion model using
                    a
                    small set of real images to generate synthetic data that better aligns with the real data
                    distribution. This differs from previous work that primarily focused on using class names or
                    pre-trained captioning models to guide the generation process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">
                07:04
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09512" target="_blank">
                        @arXiv 2407.09512
                    </a>
                    <span class="tweet-title">
                        AI Copilots: From Shopping Spree to Store Ops, They're Here to Help!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the design and evaluation of AI copilots, specifically highlighting the
                    importance of a systematic approach to building and testing these assistants. It goes beyond
                    simply
                    using LLMs and emphasizes the need for plugins, orchestration, and responsible AI guardrails.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">
                07:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10040" target="_blank">
                        @arXiv 2407.10040
                    </a>
                    <span class="tweet-title">
                        Thinking Out Loud: How AI Learned to Prove Theorems with a Chatty Sidekick
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to theorem proving by incorporating informal
                    "thoughts"
                    into the process. Unlike previous methods that solely rely on formal proofs, this study trains a
                    language model to generate natural language explanations before each step of a formal proof,
                    enhancing its theorem-proving capabilities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">
                07:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09905" target="_blank">
                        @arXiv 2407.09905
                    </a>
                    <span class="tweet-title">
                        Reinforcement Learning Goes Global: Beyond the Sum of Its Parts
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Global Reinforcement Learning (GRL), where rewards are defined over
                    entire
                    trajectories instead of individual states. This allows for modeling complex interactions between
                    states, which is crucial for tasks like experiment design and exploration.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">
                08:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10627" target="_blank">
                        @arXiv 2407.10627
                    </a>
                    <span class="tweet-title">
                        LLMs Get a Fight Club Makeover: Arena Learning Makes Them Stronger!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft, Tsinghua University, SIAT-UCAS
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Arena Learning, a novel approach to post-training LLMs that simulates
                    offline chatbot battles using AI-driven annotations. This differs from previous work that relied
                    on
                    human-based evaluations, which are costly and time-consuming.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">
                08:46
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09941" target="_blank">
                        @arXiv 2407.09941
                    </a>
                    <span class="tweet-title">
                        Hydra: The Double-Headed Mamba Slaying Sequence Modeling!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a new framework for understanding sequence models, called the Matrix Mixer
                    framework. It uses structured matrices to represent the sequence mixer, which allows for more
                    efficient and expressive models. The paper also introduces a new type of matrix mixer called the
                    quasiseparable matrix mixer, which is a bidirectional extension of the semiseparable matrix
                    mixer
                    used in state-space models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">
                09:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10725" target="_blank">
                        @arXiv 2407.10725
                    </a>
                    <span class="tweet-title">
                        LLMs: Value Judgments, But Can They Judge a Value?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces CLAVE, a framework that combines a large language model (LLM) with a
                    smaller, fine-tuned LLM to evaluate the values reflected in LLM-generated responses. This
                    approach
                    addresses the challenges of adaptability and generalizability in value assessment, which are
                    limitations of existing methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">
                09:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10960" target="_blank">
                        @arXiv 2407.10960
                    </a>
                    <span class="tweet-title">
                        LLMs on a Diet: New Kernel Makes Quantized Models Run Faster!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT, CMU
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces FLUTE, a kernel that optimizes matrix multiplications for lookup
                    table-quantized LLMs. Unlike previous kernels, FLUTE supports non-uniform quantization with odd
                    bit-widths, making it more flexible for various quantization methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">
                10:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10971" target="_blank">
                        @arXiv 2407.10971
                    </a>
                    <span class="tweet-title">
                        Q-Learning the Reward: A Bayesian IRL Algorithm That's Faster Than a Speeding Bullet!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford, University of Southampton
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new Bayesian inverse reinforcement learning (IRL) algorithm called
                    ValueWalk. Unlike previous methods that primarily focused on sampling in the space of reward
                    functions, ValueWalk focuses on sampling in the space of Q-values, which is computationally
                    cheaper.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">
                10:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09522" target="_blank">
                        @arXiv 2407.09522
                    </a>
                    <span class="tweet-title">
                        Querying the Unstructured: A New Engine for Data Insights
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel Universal Query Engine (UQE) that leverages Large Language Models
                    (LLMs) to perform analytics on unstructured databases. Unlike previous approaches that rely on
                    full
                    database scans or pre-processing, UQE utilizes statistically sound sampling techniques and a
                    compilation system to achieve efficient and accurate query execution.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">
                11:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10704" target="_blank">
                        @arXiv 2407.10704
                    </a>
                    <span class="tweet-title">
                        Tiny Tweaks, Big Gains: Quantizing Prompts for Vision-Language Model Generalization
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the use of quantization, a technique for compressing model parameters, to
                    improve the generalization ability of vision-language models. Unlike previous work that focuses
                    on
                    minimizing quantization error, this study proposes that a moderate level of quantization error
                    can
                    actually enhance generalization by acting as a form of regularization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">
                11:42
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10625" target="_blank">
                        @arXiv 2407.10625
                    </a>
                    <span class="tweet-title">
                        WildVidFit: Virtual Try-On Goes Wild, Ditching the Warping!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Sun Yat-sen University, University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to video virtual try-on that utilizes image-based
                    controlled
                    diffusion models, eliminating the need for explicit warping and temporal modules. This differs
                    from
                    previous methods that relied on warping techniques, which struggled with complex movements and
                    occlusions in real-world videos.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">
                12:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10633" target="_blank">
                        @arXiv 2407.10633
                    </a>
                    <span class="tweet-title">
                        Model Mistakes: Unmasking Bias with SKEWSIZE
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces SKEWSIZE, a new metric for evaluating model bias that focuses on
                    characterizing the distribution of prediction errors across subgroups, rather than simply
                    quantifying accuracy. This approach goes beyond traditional metrics like worst-group accuracy
                    and
                    accuracy gap, which often fail to capture subtle biases in how models make mistakes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">
                12:37
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10969" target="_blank">
                        @arXiv 2407.10969
                    </a>
                    <span class="tweet-title">
                        LLMs Go on a Diet: Sparsity Makes Them Slim and Smart!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft Research, University of Chinese Academy of Sciences
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces Q-Sparse, a method for training sparsely-activated large language models
                    (LLMs). Unlike previous approaches that focus on weight sparsity or achieve partial activation
                    sparsity, Q-Sparse enables full sparsity of activations, leading to significant efficiency gains
                    during inference.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">
                13:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09801" target="_blank">
                        @arXiv 2407.09801
                    </a>
                    <span class="tweet-title">
                        IOT-LM: Giving Your Smart Home a Brain (and a Mouth)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Carnegie Mellon University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces IOT-LM, a large multisensory language model specifically designed to
                    process data from the Internet of Things (IoT). Unlike previous work that focused on
                    single-modality
                    or single-task models, IOT-LM integrates multiple sensory modalities and can perform a range of
                    tasks, including question-answering and reasoning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">
                13:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10887" target="_blank">
                        @arXiv 2407.10887
                    </a>
                    <span class="tweet-title">
                        LLM Fingerprinting: Chain &amp; Hash - A New Way to Prove Your Model's the Real Deal
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new fingerprinting technique called Chain &amp; Hash, which uses a
                    cryptographic approach to link multiple fingerprints together. Unlike previous methods, Chain
                    &amp;
                    Hash operates in a black-box setting, meaning it only requires access to the model through an
                    API,
                    without needing to see the model's internal workings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">
                13:58
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10955" target="_blank">
                        @arXiv 2407.10955
                    </a>
                    <span class="tweet-title">
                        ROOT-SGD Goes on a Diet: Diminishing Stepsize for Optimal Optimization
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Illinois, University of California
                    </span>
                </div>
                <div class="primary-text">
                    This research revisits the ROOT-SGD algorithm, introducing a diminishing stepsize strategy that
                    improves its convergence rate and statistical efficiency. Unlike previous work, this approach
                    achieves optimal asymptotic covariance without requiring prior knowledge of the sample size.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">
                14:19
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10937" target="_blank">
                        @arXiv 2407.10937
                    </a>
                    <span class="tweet-title">
                        Depth-Charged Videos: A New Way to Make Videos Pop!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        State University of New York at Buffalo, Microsoft, AdvancedMicroDevices
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to generating videos and their corresponding depth
                    maps
                    simultaneously. Unlike previous methods that focus on either video or depth generation, this
                    paper
                    proposes a unified dual-modal U-Net that learns to denoise both video and depth features
                    jointly.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">
                14:42
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10468" target="_blank">
                        @arXiv 2407.10468
                    </a>
                    <span class="tweet-title">
                        Audio Diffusion Models Get a Speed Boost: LiteFocus Makes Long Audio Synthesis a Breeze!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        National University of Singapore
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces LiteFocus, a method that accelerates the inference of audio latent
                    diffusion models for long audio synthesis. Unlike previous work that focuses on reducing the
                    number
                    of inference steps, LiteFocus specifically targets the self-attention mechanism within the
                    model,
                    which is a major bottleneck for long audio generation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">
                15:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10264" target="_blank">
                        @arXiv 2407.10264
                    </a>
                    <span class="tweet-title">
                        Safety Fine-Tuning: LLMs Learn to Say "No" (But Jailbreaks Still Say "Yes")
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Five AI Ltd., University of Michigan, CBS Harvard University...
                    </span>
                </div>
                <div class="primary-text">
                    This research uses a synthetic data generation framework to study the mechanisms of safety
                    fine-tuning in LLMs. It goes beyond simply observing changes in model behavior and delves into
                    the
                    specific transformations learned by the model during safety fine-tuning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">
                15:30
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10318" target="_blank">
                        @arXiv 2407.10318
                    </a>
                    <span class="tweet-title">
                        Caustics Be Gone! 3D Gaussian Splatting Takes on Underwater Illusions
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method called Recurrent Gaussian Splatting (RecGS) for removing
                    water caustics from underwater imagery. Unlike previous methods that rely on 2D filtering or
                    supervised deep learning, RecGS leverages the 3D consistency of the scene to iteratively refine
                    the
                    3D representation and separate caustics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">
                15:58
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09709" target="_blank">
                        @arXiv 2407.09709
                    </a>
                    <span class="tweet-title">
                        Graph Language Modeling: LLMs Get a Graph Makeover!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Washington University in St. Louis, Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel generative graph language model called GOFA, which interleaves GNN
                    layers into a frozen pre-trained LLM. This approach allows the model to learn both semantic and
                    structural information from graph data, unlike previous methods that either focused on
                    language-based prediction or used LLMs as assistants for GNNs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">
                16:27
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10456" target="_blank">
                        @arXiv 2407.10456
                    </a>
                    <span class="tweet-title">
                        Don't Be a One-Trick Pony: Knowledge Distillation Gets a Multi-Sequence Makeover!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Melbourne, Google
                    </span>
                </div>
                <div class="primary-text">
                    This research explores using multiple high-scoring translations from a teacher model during
                    knowledge distillation, rather than just the single best one. This differs from previous work
                    that
                    primarily relied on a single sequence for training the student model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">
                16:49
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10179" target="_blank">
                        @arXiv 2407.10179
                    </a>
                    <span class="tweet-title">
                        AI's New Weapon: Text-Guided Attacks That Fool Even the Smartest Models
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Harbin Institute of Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for crafting adversarial attacks that can fool AI models
                    even
                    when they haven't seen the attack before. The key difference is that the attack uses text
                    descriptions of the target class to guide the generation of the attack, making it more effective
                    than previous methods that relied solely on visual information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">
                17:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10897" target="_blank">
                        @arXiv 2407.10897
                    </a>
                    <span class="tweet-title">
                        Light Up Your Images: Optical Diffusion Models for Faster, Greener Image Generation
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        École Polytechnique Fédérale de Lausanne, Google
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes using the propagation of light through specially designed transparent
                    layers
                    to implement denoising diffusion models for image generation. This approach differs from
                    traditional
                    methods that rely on digital electronic hardware like GPUs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">
                17:40
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09503" target="_blank">
                        @arXiv 2407.09503
                    </a>
                    <span class="tweet-title">
                        AI for AR: Giving Your Glasses a Brain with PARSE-Ego4D
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Groningen, University of Central Florida, Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces PARSE-Ego4D, a dataset specifically designed for training AI systems to
                    provide personalized action recommendations in egocentric videos, a task not addressed by
                    previous
                    datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">
                18:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10448" target="_blank">
                        @arXiv 2407.10448
                    </a>
                    <span class="tweet-title">
                        Causal Inference Gets a Spectral Makeover: Unveiling Hidden Confounders with a Low-Rank
                        Twist!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UT Austin, Georgia Tech, Universitat Pompeu Fabra...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to causal effect estimation in the presence of hidden
                    confounders. Unlike previous methods that rely on fixed feature dictionaries or deep neural
                    networks, this paper leverages a low-rank assumption on conditional densities to explicitly
                    characterize function classes within a saddle-point optimization problem.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">
                18:42
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10457" target="_blank">
                        @arXiv 2407.10457
                    </a>
                    <span class="tweet-title">
                        LLMs: Greedy or Random? The Decoding Debate Heats Up!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, Allen Institute for AI
                    </span>
                </div>
                <div class="primary-text">
                    This research delves into the non-determinism of LLMs, examining the performance differences
                    between
                    greedy decoding and sampling methods. Unlike previous work that often focuses on a single output
                    per
                    example, this study explores the variability of LLM performance across multiple generations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">
                19:07
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10973" target="_blank">
                        @arXiv 2407.10973
                    </a>
                    <span class="tweet-title">
                        Make-An-Agent: Turning Agent Behaviors into Policies with Diffusion Magic!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Maryland, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel method for generating control policies for agents using diffusion
                    models. Unlike previous work that focuses on learning policies from data, this approach directly
                    generates policy parameters from behavior embeddings, which encode trajectory information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">
                19:31
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10220" target="_blank">
                        @arXiv 2407.10220
                    </a>
                    <span class="tweet-title">
                        Pose-ing for the Future: Diffusion Models Get a Body Part Makeover!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Valeo.ai, LIGM, École des Ponts ParisTech...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel part-based approach for 3D whole-body pose estimation, where
                    the
                    model predicts poses for different body parts (body, hands, and face) separately, conditioned on
                    their respective root joints. This differs from previous methods that typically process all
                    keypoints in a single network.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">
                19:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10005" target="_blank">
                        @arXiv 2407.10005
                    </a>
                    <span class="tweet-title">
                        In-Context Learning: It's Not Just Attention, It's the Algorithm!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Michigan, Google
                    </span>
                </div>
                <div class="primary-text">
                    This research goes beyond the typical assumption that in-context learning (ICL) is solely driven
                    by
                    attention mechanisms. It explores the optimization landscape of ICL, considering both linear
                    attention and state-space models (SSMs), and demonstrates that both can implement gradient-based
                    algorithms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">
                20:23
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09694" target="_blank">
                        @arXiv 2407.09694
                    </a>
                    <span class="tweet-title">
                        Body Part Puzzle: Reconstructing Humans from Tiny Clues
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University at Buffalo, Peking University, Johns Hopkins University...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a "Divide and Fuse" approach for human body mesh reconstruction,
                    focusing
                    on scenarios where only a small portion of the body is visible. Unlike traditional methods that
                    rely
                    on whole-body models, this approach reconstructs individual body parts independently before
                    fusing
                    them together, making it more robust to occlusions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">
                20:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10918" target="_blank">
                        @arXiv 2407.10918
                    </a>
                    <span class="tweet-title">
                        PartImageNet++: Giving AI a Part-Time Job in Robust Recognition!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Harbin Institute of Technology, Beijing Institute of Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces PartImageNet++, a dataset with part-level annotations for all
                    categories in
                    ImageNet-1K. This differs from previous work by providing a large-scale, high-quality dataset
                    for
                    training part-based models, which are designed to improve the robustness of object recognition
                    systems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">
                21:16
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10916" target="_blank">
                        @arXiv 2407.10916
                    </a>
                    <span class="tweet-title">
                        Graph Learning Goes Heterophilic: A New Benchmark for Real-World Networks
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT, IBM, Virginia Tech
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new graph benchmark, H2GB, specifically designed to evaluate graph
                    learning methods on graphs that exhibit both heterophily and heterogeneity. This benchmark
                    addresses
                    the limitations of existing benchmarks that focus on either homogeneous graphs or heterogeneous
                    graphs with the homophily assumption.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">
                21:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09499" target="_blank">
                        @arXiv 2407.09499
                    </a>
                    <span class="tweet-title">
                        Curated AI: How Internet Filters Are Shaping Our Future
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Mila, Université de Montréal, Ecole Normale Supérieure de Paris...
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the impact of data curation on the training of generative models,
                    specifically focusing on how human preferences influence the models' output. Unlike previous
                    work
                    that focused on the stability of retraining models on synthetic data, this paper examines the
                    effects of curated synthetic data, where users select preferred outputs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">
                22:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10603" target="_blank">
                        @arXiv 2407.10603
                    </a>
                    <span class="tweet-title">
                        Speech Recognition: When Less is More (and Faster!)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        National Taiwan University, NVIDIA
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on knowledge distillation for code-switching automatic speech recognition
                    (CS-ASR) using unlabeled, realistic data. Unlike previous work that relies on labeled datasets,
                    this
                    study proposes a novel framework called K2D that leverages a small auxiliary model to filter out
                    inaccurate pseudo-labels generated from a large teacher model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">
                22:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10330" target="_blank">
                        @arXiv 2407.10330
                    </a>
                    <span class="tweet-title">
                        Tree-mendous 3D Trees: From Single Images to Simulation-Ready Models!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Purdue University, Massachusetts Institute of Technology, Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Tree-D Fusion, a dataset of 600,000 3D tree models generated from
                    single
                    images using diffusion priors. This approach differs from previous work by leveraging diffusion
                    models to reconstruct complete 3D volumes of trees, which are then refined by a
                    genus-conditioned
                    developmental model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">
                23:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10070" target="_blank">
                        @arXiv 2407.10070
                    </a>
                    <span class="tweet-title">
                        ASkotch: KRR's New Trick for Handling Big Data
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces ASkotch, a new method for solving kernel ridge regression (KRR) problems
                    that
                    uses block coordinate descent with Hessian preconditioning and acceleration. Unlike previous
                    methods, ASkotch scales linearly with the number of data points and does not scale with the
                    preconditioner rank, allowing it to run on a single inexpensive GPU.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">
                23:27
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10482" target="_blank">
                        @arXiv 2407.10482
                    </a>
                    <span class="tweet-title">
                        NeRFs Get a Speed Boost: Attention is All You Need (and a Little Occupancy Distance)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Horizon Robotics, Huawei Technologies...
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes NGP-RT, a real-time NeRF method that uses a lightweight attention mechanism
                    to
                    aggregate multi-level hash features. This approach differs from previous methods that rely on
                    computationally intensive MLPs for feature aggregation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">
                23:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10528" target="_blank">
                        @arXiv 2407.10528
                    </a>
                    <span class="tweet-title">
                        Motion Control: Text-to-Motion with Local Action Guidance!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to text-to-motion generation by using local actions as
                    control signals. Unlike previous methods that focus on directly synthesizing global motions,
                    this
                    method breaks down the motion into smaller, more manageable actions, making it easier to
                    generate
                    complex and diverse motions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">
                24:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09704" target="_blank">
                        @arXiv 2407.09704
                    </a>
                    <span class="tweet-title">
                        LLMs: Gender-Biased, But Surprisingly Consistent Across Languages!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research investigates the biases of multilingual LLMs through the lens of grammatical
                    gender.
                    Unlike previous work that focused on individual languages, this study examines how these biases
                    transfer across different languages.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">
                24:46
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10833" target="_blank">
                        @arXiv 2407.10833
                    </a>
                    <span class="tweet-title">
                        MoE-DiffIR: Image Restoration Gets a Prompt-Boost!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Science and Technology of China, Peking University, ByteDance
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to compressed image restoration (CIR) using a
                    Mixture-of-Experts (MoE) prompt module. Unlike previous methods that rely on single or multiple
                    weighted prompts, MoE-DiffIR dynamically selects and combines prompts based on the specific
                    compression task, enabling more effective extraction of task-customized diffusion priors from
                    Stable
                    Diffusion.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">
                25:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10583" target="_blank">
                        @arXiv 2407.10583
                    </a>
                    <span class="tweet-title">
                        Reinforcement Learning's Three Dogmas: Time to Shed Some Fur?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google, NYU
                    </span>
                </div>
                <div class="primary-text">
                    This paper challenges three common assumptions in reinforcement learning (RL), arguing that they
                    limit the field's potential. The authors propose shifting focus from modeling environments to
                    understanding agents, moving beyond finding solutions to embracing continuous adaptation, and
                    recognizing the limitations of using reward maximization as the sole goal-defining mechanism.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">
                25:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10328" target="_blank">
                        @arXiv 2407.10328
                    </a>
                    <span class="tweet-title">
                        AI Music: Can Robots Understand Our Musical Whispers?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Queen Mary University of London, Independent Researcher
                    </span>
                </div>
                <div class="primary-text">
                    This paper focuses on the "interpretation gap" in text-to-music generation models, arguing that
                    current models struggle to understand the nuances of human musical communication, unlike human
                    musicians who excel at interpreting ambiguous instructions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">
                25:58
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09646" target="_blank">
                        @arXiv 2407.09646
                    </a>
                    <span class="tweet-title">
                        Hand Reconstruction Gets a Graph-Guided Makeover: Mamba's New Trick!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to 3D hand reconstruction by integrating graph
                    learning
                    and state space modeling within a Mamba framework. Unlike previous transformer-based methods,
                    this
                    approach utilizes a graph-guided bidirectional scan, which effectively captures the spatial
                    relations between hand joints while using significantly fewer tokens.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">
                26:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10899" target="_blank">
                        @arXiv 2407.10899
                    </a>
                    <span class="tweet-title">
                        AI Students Ace Algebra: Can Robots Replace Humans in Testing?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research explores using Large Language Models (LLMs) to generate responses to assessment
                    questions, simulating human responses for psychometric analysis. Unlike previous work that
                    focused
                    on generating questions or hints, this study investigates the feasibility of using LLMs to
                    directly
                    mimic student abilities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">
                26:44
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09818" target="_blank">
                        @arXiv 2407.09818
                    </a>
                    <span class="tweet-title">
                        Arabic Banking Bots Get a Dialect Makeover: AraFinNLP2024 Shared Task
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Birzeit University, Lancaster University, King Saud University...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces the first Arabic Financial NLP shared task, focusing on multi-dialect
                    intent detection and cross-dialect translation within the banking domain. This is distinct from
                    previous work that primarily focused on English financial NLP or Arabic morphological and
                    semantic
                    annotations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">
                27:15
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09705" target="_blank">
                        @arXiv 2407.09705
                    </a>
                    <span class="tweet-title">
                        Multimodal Learning: Diagnosing &amp; Re-learning for a Balanced Model
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Renmin University of China, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for balancing multimodal learning by diagnosing the learning
                    state of each modality and then softly re-initializing the corresponding uni-modal encoder. This
                    approach differs from previous work by considering the intrinsic limitation of modality capacity
                    and
                    taking all modalities into account during balancing.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">
                27:44
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10956" target="_blank">
                        @arXiv 2407.10956
                    </a>
                    <span class="tweet-title">
                        Data Science Bots: Can They Handle the Real World?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        The University of Hong Kong, Shanghai Jiao Tong University, Google Cloud AI Research...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Spider2-V, a new benchmark for evaluating multimodal agents in data
                    science
                    and engineering workflows. Unlike previous benchmarks that focus on specific tasks like code
                    generation or data analysis, Spider2-V encompasses the entire data pipeline, including data
                    warehousing, ingestion, transformation, visualization, and orchestration. It also incorporates
                    real-world enterprise applications and intensive GUI operations, making it a more realistic and
                    challenging test for these agents.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">
                28:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09816" target="_blank">
                        @arXiv 2407.09816
                    </a>
                    <span class="tweet-title">
                        MaskMoE: Giving Infrequent Words a Voice in the Language Model Choir!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Chinese Academy of Sciences, Tsinghua University, Beihang University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces MaskMoE, a new routing strategy for Mixture-of-Experts (MoE) models.
                    Unlike
                    previous dynamic routing methods that can lead to underfitting for infrequent words, MaskMoE
                    uses a
                    masking technique to ensure that infrequent words are consistently routed to the same expert,
                    allowing for more thorough training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon" title="Play from here">
                28:46
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10267" target="_blank">
                        @arXiv 2407.10267
                    </a>
                    <span class="tweet-title">
                        NeRFs Go Rolling: How to Make 3D Models from Shaky Camera Footage
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Tokyo
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces RS-NeRF, a method that directly incorporates the physics of rolling
                    shutter
                    distortions into the NeRF model. Unlike previous approaches that correct distortions in 2D image
                    space, RS-NeRF jointly optimizes the NeRF parameters and camera poses for each image row,
                    leading to
                    more accurate 3D reconstructions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon" title="Play from here">
                29:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10125" target="_blank">
                        @arXiv 2407.10125
                    </a>
                    <span class="tweet-title">
                        Pedestrian Detection Goes Multi-Modal: A Model That Sees It All!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a generalist model, MMPedestron, that can process multiple sensor
                    modalities (like RGB, IR, Depth, LiDAR, and Event data) for pedestrian detection. Unlike
                    previous
                    models that focus on specific modality pairs, MMPedestron can handle diverse combinations,
                    making it
                    more flexible and adaptable.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon" title="Play from here">
                29:40
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09697" target="_blank">
                        @arXiv 2407.09697
                    </a>
                    <span class="tweet-title">
                        LiDAR and Camera: A Love Story of 3D Semantic Segmentation
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto, Huawei
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new method called LaCRange for 3D point cloud semantic segmentation. It
                    uses a distortion-compensating knowledge distillation (DCKD) strategy to address the information
                    loss caused by projecting RGB images onto the sparse LiDAR point cloud. Additionally, it
                    introduces
                    a context-based feature fusion module to combine camera and LiDAR features effectively.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet67">
            <div class="start-time-icon" title="Play from here">
                30:01
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10550" target="_blank">
                        @arXiv 2407.10550
                    </a>
                    <span class="tweet-title">
                        Fake Videos Can't Keep Up: New AI Detects Inconsistent Faces
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Chinese Academy of Sciences, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a self-supervised learning approach for face forgery detection that
                    leverages
                    the natural consistency of real face videos. Unlike previous methods that rely on specific
                    forgery
                    patterns or require additional modalities like audio, this method focuses on the inherent
                    spatiotemporal coherence of real faces, making it more robust to unseen forgery methods and
                    perturbations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet68">
            <div class="start-time-icon" title="Play from here">
                30:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10252" target="_blank">
                        @arXiv 2407.10252
                    </a>
                    <span class="tweet-title">
                        Multilingual Text Gets a Subjectivity Checkup: BERT's Got Your Back!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU, Northwestern University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on classifying text as subjective or objective across five languages,
                    including Arabic, Bulgarian, English, German, and Italian, using a fine-tuned BERT model and
                    addressing the data imbalance issue.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet69">
            <div class="start-time-icon" title="Play from here">
                31:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10954" target="_blank">
                        @arXiv 2407.10954
                    </a>
                    <span class="tweet-title">
                        Fuzzy Logic Makes CSG Shapes Smooth and Differentiable!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Roblox, University of British Columbia, Stanford University...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a unified differentiable boolean operator for CSG modeling, which
                    allows
                    continuous optimization of both the primitive shapes and the boolean operations. Unlike previous
                    methods that rely on discrete choices for boolean operations, this approach uses fuzzy logic to
                    create a continuous function that can be differentiated with respect to the type of operation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet70">
            <div class="start-time-icon" title="Play from here">
                31:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09533" target="_blank">
                        @arXiv 2407.09533
                    </a>
                    <span class="tweet-title">
                        Predicting the Future, One Frame at a Time: Video Occupancy Models for Control
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Alberta, UT Austin, Microsoft Research...
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces Video Occupancy Models (VOCs), which directly predict the discounted
                    distribution of future states in a single step, unlike prior latent-space world models that
                    require
                    multistep rollouts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet71">
            <div class="start-time-icon" title="Play from here">
                31:59
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10207" target="_blank">
                        @arXiv 2407.10207
                    </a>
                    <span class="tweet-title">
                        Steering Agents: A Game of Incentives and Uncertainty
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich, University of Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on designing incentives to steer multi-agent systems towards desired
                    outcomes
                    without knowing the agents' learning dynamics. It introduces a model-based non-episodic
                    reinforcement learning formulation for this steering problem, emphasizing the learning of
                    history-dependent steering strategies to handle model uncertainty. This approach differs from
                    previous work by explicitly tackling the challenge of learning in a single, finite-horizon
                    episode,
                    where the mediator cannot simply learn from repeated trial-and-error.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet72">
            <div class="start-time-icon" title="Play from here">
                32:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10794" target="_blank">
                        @arXiv 2407.10794
                    </a>
                    <span class="tweet-title">
                        LLMs Go Global: Building Knowledge Graphs with a Fusion Twist!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Duke-NUS Medical School, University of Tokyo, Smartor Inc....
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Graphusion, a framework for constructing knowledge graphs (KGs) from
                    free
                    text using large language models (LLMs). Unlike previous approaches that focus on extracting
                    knowledge from individual sentences or documents, Graphusion takes a global perspective, merging
                    and
                    resolving conflicts across multiple sources to create a more comprehensive and accurate KG.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet73">
            <div class="start-time-icon" title="Play from here">
                32:54
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09580" target="_blank">
                        @arXiv 2407.09580
                    </a>
                    <span class="tweet-title">
                        Don't Fear the Funky Functions: New Activation Function Makes Deep Learning More Powerful
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        The Chinese University of Hong Kong, The Hong Kong Polytechnic University, Southern Medical
                        University...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new activation function called PEUAF, which is a parameterized
                    version of
                    the Elementary Universal Activation Function (EUAF). Unlike previous super-expressive activation
                    functions, PEUAF can adaptively learn the frequency of stationary signals, making it more
                    versatile
                    and effective in real-world applications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet74">
            <div class="start-time-icon" title="Play from here">
                33:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09661" target="_blank">
                        @arXiv 2407.09661
                    </a>
                    <span class="tweet-title">
                        Bridging the Divide: A Dictionary for Politically Charged Words
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces the Bridging Dictionary, an interactive tool that uses a large language
                    model (LLM) to analyze how words are used differently by Republicans and Democrats. Unlike
                    previous
                    work that focused on measuring and debiasing NLP models, this study aims to help humans
                    understand
                    and write less biased content.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet75">
            <div class="start-time-icon" title="Play from here">
                33:54
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10844" target="_blank">
                        @arXiv 2407.10844
                    </a>
                    <span class="tweet-title">
                        Predicting Molecular Uncertainty: When to Trust Your AI Chemist!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Meta, CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on uncertainty quantification for relaxed energy calculations using graph
                    neural networks (GNNs), a task that is more complex than other molecular property predictions
                    due to
                    the impact of structure optimizations on the error distribution. The paper proposes using
                    distribution-free techniques for assessing calibration and developing uncertainty prediction
                    methods
                    for GNNs performing relaxed energy calculations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet76">
            <div class="start-time-icon" title="Play from here">
                34:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09853" target="_blank">
                        @arXiv 2407.09853
                    </a>
                    <span class="tweet-title">
                        Image Compression: A Tune-Up for Machine Vision!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Shanghai Jiao Tong University, Tsinghua University, Chinese University of Hong Kong
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel Spatial-Frequency Modulation Adapter (SFMA) to fine-tune
                    pre-trained
                    image compression models for machine vision tasks. Unlike previous methods that rely on
                    task-specific networks, SFMA adapts the existing model by modulating features in both spatial
                    and
                    frequency domains, leading to more efficient fine-tuning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet77">
            <div class="start-time-icon" title="Play from here">
                34:52
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09602" target="_blank">
                        @arXiv 2407.09602
                    </a>
                    <span class="tweet-title">
                        Neutron Star Mergers: Machine Learning Makes Inference a Breeze!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Max Planck Society, University of Nottingham
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new machine learning method called "prior-conditioning" that allows
                    for
                    rapid and accurate inference of binary neutron star (BNS) parameters from gravitational wave
                    data.
                    This method differs from previous work by enabling the neural network to be instantly tuned to
                    an
                    event-specific prior, which significantly improves the speed and accuracy of inference.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet78">
            <div class="start-time-icon" title="Play from here">
                35:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09897" target="_blank">
                        @arXiv 2407.09897
                    </a>
                    <span class="tweet-title">
                        Chatbots Gone Wild: New Framework Tames LLM-Powered Conversations
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Tokyo
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the quality of multi-agent dialogues in simulations powered by LLMs,
                    specifically examining how these dialogues evolve over multiple sessions. Unlike previous work
                    that
                    primarily evaluated single sessions or interactions between two agents, this study analyzes the
                    long-term impact of LLM-generated conversations on the consistency and factual accuracy of the
                    simulated world.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet79">
            <div class="start-time-icon" title="Play from here">
                35:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10244" target="_blank">
                        @arXiv 2407.10244
                    </a>
                    <span class="tweet-title">
                        Social Workers Say "No" to AI Decision-Making: A New Study Reveals Why
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the perspectives of UK social work practitioners on AI, specifically
                    exploring their experiences, attitudes, and needs, rather than focusing on the impacts of
                    specific
                    AI systems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet80">
            <div class="start-time-icon" title="Play from here">
                36:05
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10639" target="_blank">
                        @arXiv 2407.10639
                    </a>
                    <span class="tweet-title">
                        Traffic Trouble? New AI Predicts Risky Driving, One Intersection at a Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on improving trajectory prediction by analyzing the spatial and temporal
                    patterns of risky interactions within a dataset. Unlike previous work that uses risk metrics
                    based
                    on current information, this study leverages historical data to identify high-risk locations and
                    incorporate this knowledge into the prediction model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet81">
            <div class="start-time-icon" title="Play from here">
                36:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09679" target="_blank">
                        @arXiv 2407.09679
                    </a>
                    <span class="tweet-title">
                        Smoke and Mirrors: Neural Trajectories for Realistic Fluid Reconstruction
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich, Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Neural Characteristic Trajectory Fields, a novel representation that
                    implicitly models Lagrangian fluid trajectories using Eulerian neural fields. This differs from
                    previous work by enabling efficient flow map calculations between arbitrary frames and
                    facilitating
                    end-to-end supervision covering long-term conservation and short-term physics priors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet82">
            <div class="start-time-icon" title="Play from here">
                37:04
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10382" target="_blank">
                        @arXiv 2407.10382
                    </a>
                    <span class="tweet-title">
                        Robot Teams: Less Talk, More Action!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Michigan
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new distributed coordination algorithm for multi-robot networks that
                    prioritizes speed over absolute optimality. Unlike previous methods that either sacrifice
                    real-time
                    performance for near-optimal solutions or rely on heuristics without guarantees, this algorithm
                    balances the trade-off by enabling robots to choose their communication partners strategically.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet83">
            <div class="start-time-icon" title="Play from here">
                37:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09879" target="_blank">
                        @arXiv 2407.09879
                    </a>
                    <span class="tweet-title">
                        Multilingual LLMs: A Recipe for Success with a Dash of N-Shot Prompting!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces SPHINX, a novel multilingual instruction tuning dataset created by
                    selectively translating English instruction-response pairs into 50 languages using GPT-4. This
                    approach aims to preserve semantic information and linguistic diversity, unlike previous methods
                    that relied on direct translation APIs or self-instruct techniques.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet84">
            <div class="start-time-icon" title="Play from here">
                37:59
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10275" target="_blank">
                        @arXiv 2407.10275
                    </a>
                    <span class="tweet-title">
                        Knowledge Editing Goes Global: LLMs Learn New Facts in Any Language!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft, UNC Chapel Hill, Massachusetts Institute of Technology...
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on cross-lingual multi-hop knowledge editing, a new paradigm that
                    addresses
                    the limitations of existing monolingual knowledge editing techniques. Unlike previous work, this
                    study explores how to efficiently update LLMs with new information from various languages,
                    considering the ripple effects of these edits on related facts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet85">
            <div class="start-time-icon" title="Play from here">
                38:21
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10279" target="_blank">
                        @arXiv 2407.10279
                    </a>
                    <span class="tweet-title">
                        Doudizhu AI Gets a Bidding Boost: AlphaDou Learns to Bluff!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Shanghai Jiao Tong University, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces AlphaDou, a Doudizhu AI that integrates bidding into its training and
                    testing phases, unlike previous models that focused solely on the card-playing phase.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet86">
            <div class="start-time-icon" title="Play from here">
                38:47
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10886" target="_blank">
                        @arXiv 2407.10886
                    </a>
                    <span class="tweet-title">
                        LLMs on the Edge? No Problem! New Method Keeps Your AI Secrets Safe
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel hybrid inference algorithm called SLIP, which aims to protect the
                    intellectual property of large language models (LLMs) deployed on edge devices. Unlike previous
                    methods that focus on cost, performance, or privacy, SLIP prioritizes model security while
                    maintaining accuracy and minimal latency. It achieves this by strategically decomposing the
                    model's
                    weight matrices, ensuring that the most sensitive information is stored on a secure resource,
                    while
                    the less valuable parts are offloaded to a vulnerable edge device.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet87">
            <div class="start-time-icon" title="Play from here">
                39:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09486" target="_blank">
                        @arXiv 2407.09486
                    </a>
                    <span class="tweet-title">
                        LLM Serving: From Costly to Cost-Effective with ENOVA's Autoscaling Magic
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Sun Yat-sen University, University of Toronto
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on building a service called ENOVA that automates the deployment,
                    monitoring,
                    and autoscaling of large language models (LLMs) on multi-GPU clusters. Unlike previous work that
                    primarily focuses on optimizing LLM inference, ENOVA addresses the challenges of providing
                    stable
                    and scalable LLM services in real-world environments.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet88">
            <div class="start-time-icon" title="Play from here">
                39:47
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10649" target="_blank">
                        @arXiv 2407.10649
                    </a>
                    <span class="tweet-title">
                        Patch It Up: A New Way to Segment Images with Less Data
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Xi’an Jiaotong-Liverpool University, University of Aberdeen, Microsoft...
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a new method for weakly supervised semantic segmentation that uses Vision
                    Transformers (ViT) without relying on Class Activation Maps (CAM). It introduces Adaptive-K
                    Pooling
                    to select the most relevant patches for prediction and Patch Contrastive Learning to improve the
                    quality of patch embeddings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet89">
            <div class="start-time-icon" title="Play from here">
                40:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10967" target="_blank">
                        @arXiv 2407.10967
                    </a>
                    <span class="tweet-title">
                        Reinforcement Learning's New Trick: Unmasking the Hidden Causes for Better Decisions
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research tackles the objective mismatch problem in offline model-based reinforcement
                    learning
                    (MBRL) by introducing a novel approach called BilinEar CAUSal rEpresentation (BECAUSE). Unlike
                    previous methods that focus on reweighting the entire model or jointly training the model and
                    policy, BECAUSE identifies and models the underlying causal structures in the environment,
                    specifically addressing the spurious correlations introduced by confounders in offline data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet90">
            <div class="start-time-icon" title="Play from here">
                41:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10061" target="_blank">
                        @arXiv 2407.10061
                    </a>
                    <span class="tweet-title">
                        InfiniMotion: Making AI Dance for Hours, Not Seconds!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Monash University, The Australian National University, The University of Adelaide...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces InfiniMotion, a method that uses a memory-enhanced transformer to
                    generate
                    long motion sequences. Unlike previous methods that struggle with long sequences, InfiniMotion
                    leverages the Bidirectional Mamba Memory block to improve the transformer's memory capacity,
                    enabling the generation of continuous motion sequences of arbitrary length.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet91">
            <div class="start-time-icon" title="Play from here">
                41:33
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09541" target="_blank">
                        @arXiv 2407.09541
                    </a>
                    <span class="tweet-title">
                        VLMs Meet LLMs: A Match Made in Embedding Heaven!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Meta AI, Georgia Institute of Technology, University of Wisconsin-Madison...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach called MATE (Meet At The Embedding) that connects
                    images
                    with long texts, such as lengthy captions or documents, by aligning embeddings from Vision
                    Language
                    Models (VLMs) with those from Large Language Models (LLMs). Unlike previous work that primarily
                    focused on aligning images with short captions, MATE leverages the strengths of both VLMs and
                    LLMs
                    to handle complex textual interactions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet92">
            <div class="start-time-icon" title="Play from here">
                42:05
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10795" target="_blank">
                        @arXiv 2407.10795
                    </a>
                    <span class="tweet-title">
                        Decoding Decoding: Skipping Layers for Multilingual Language Models
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nanjing University, École Polytechnique Fédérale de Lausanne
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new contrastive decoding algorithm that addresses the language mismatch
                    issue in previous methods, specifically by skipping language-agnostic layers in the model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet93">
            <div class="start-time-icon" title="Play from here">
                42:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10062" target="_blank">
                        @arXiv 2407.10062
                    </a>
                    <span class="tweet-title">
                        Spike Cameras: Seeing the Unseen, Blur-Free, and in 3D!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces SpikeGS, a novel approach for 3D scene reconstruction using spike
                    cameras.
                    Unlike previous methods that rely on traditional cameras and suffer from motion blur, SpikeGS
                    leverages the ultra-high temporal resolution of spike cameras to capture dense and continuous
                    views,
                    enabling accurate 3D modeling even in high-speed scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet94">
            <div class="start-time-icon" title="Play from here">
                42:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09545" target="_blank">
                        @arXiv 2407.09545
                    </a>
                    <span class="tweet-title">
                        Chaos on Command: Scientists Design Chaotic Attractors with a Semi-Supervised Twist
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Tokyo
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel semi-supervised method for designing chaotic attractors using
                    reservoir computing. Unlike previous methods that rely solely on supervised learning, this
                    approach
                    leverages the intrinsic dynamics of the reservoir to generate chaos with a desired shape.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet95">
            <div class="start-time-icon" title="Play from here">
                43:21
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10454" target="_blank">
                        @arXiv 2407.10454
                    </a>
                    <span class="tweet-title">
                        Deflating Value Iteration: A Faster Way to Learn from Experience
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Seoul National University, University of Toronto, Vector Institute...
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel approach to accelerate Value Iteration (VI), a fundamental
                    algorithm
                    in reinforcement learning, by modifying the eigenvalues of the transition dynamics. This method,
                    called Deflated Dynamics Value Iteration (DDVI), effectively removes the top dominant
                    eigenvalues of
                    the transition matrix, leading to a faster convergence rate.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet96">
            <div class="start-time-icon" title="Play from here">
                43:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09648" target="_blank">
                        @arXiv 2407.09648
                    </a>
                    <span class="tweet-title">
                        2D to 3D: Part Segmentation Without Training!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Georgia Institute of Technology, Meta AI, University of Illinois Urbana-Champaign
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a training-free method for 3D object part segmentation that leverages
                    semantic
                    correspondences from 2D images. Unlike previous methods that rely on language inputs or 3D
                    segmentation priors, this approach uses features from pretrained image diffusion models to
                    transfer
                    part labels from a 2D database to 3D objects.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet97">
            <div class="start-time-icon" title="Play from here">
                44:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10118" target="_blank">
                        @arXiv 2407.10118
                    </a>
                    <span class="tweet-title">
                        Parsing Speech Without Words: A Textless Approach to Dependency Parsing
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Tokyo, Keio University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a textless method for dependency parsing, directly predicting a
                    dependency
                    tree from speech representations without relying on an automatic speech recognition (ASR)
                    system.
                    This differs from previous work like Wav2tree, which cascades an ASR system into a dependency
                    parser.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet98">
            <div class="start-time-icon" title="Play from here">
                44:40
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09497" target="_blank">
                        @arXiv 2407.09497
                    </a>
                    <span class="tweet-title">
                        Simulating Squishy Things: A Neural Network That Doesn't Need a Mesh!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nvidia, Texas A&amp;M University, University of Toronto
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a mesh-free, geometry-agnostic approach for elastic simulation. Unlike
                    previous methods that rely on specific geometric representations like meshes or grids, this
                    technique uses neural fields to represent the object's deformation, making it adaptable to
                    various
                    3D representations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet99">
            <div class="start-time-icon" title="Play from here">
                45:03
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09555" target="_blank">
                        @arXiv 2407.09555
                    </a>
                    <span class="tweet-title">
                        DMMs on Steroids: Parallel Evolution for Faster Memory Management
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Universidad Complutense de Madrid, Ecole Polytechnique Fédérale de Lausanne
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a parallel evolutionary algorithm for optimizing dynamic memory
                    managers
                    (DMMs) in embedded systems. Unlike previous approaches that rely on sequential exploration, this
                    method leverages a master-worker scheme to distribute the simulation and evaluation tasks across
                    multiple processors, significantly reducing the optimization time.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet100">
            <div class="start-time-icon" title="Play from here">
                45:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10078" target="_blank">
                        @arXiv 2407.10078
                    </a>
                    <span class="tweet-title">
                        Missing Data? No Problem! LLMs to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Columbia University, Georgia Institute of Technology, University of Texas at Austin...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes using fine-tuned Large Language Models (LLMs) to impute missing data in
                    recommendation systems, a novel approach compared to traditional statistical methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet101">
            <div class="start-time-icon" title="Play from here">
                45:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10195" target="_blank">
                        @arXiv 2407.10195
                    </a>
                    <span class="tweet-title">
                        LiDAR Love: How to Make Vehicle and Infrastructure Sensors See Eye-to-Eye
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel calibration method for cooperative vehicle and infrastructure
                    LiDAR
                    systems that leverages spatial association information between detection boxes, enabling
                    real-time
                    monitoring of calibration results. Unlike previous methods that rely on high-precision maps or
                    complex perception algorithms, this approach utilizes a novel Overall IoU metric to identify
                    common
                    targets detected by both vehicle and infrastructure LiDARs, simplifying the calibration process
                    and
                    reducing computational complexity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet102">
            <div class="start-time-icon" title="Play from here">
                46:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10369" target="_blank">
                        @arXiv 2407.10369
                    </a>
                    <span class="tweet-title">
                        AI Act's Governance: A Bureaucratic Circus, But With Rules!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Bologna, European University Viadrina, Yale University...
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the institutional design of the EU's AI Act, analyzing the roles and
                    responsibilities of various supranational and national bodies involved in its implementation and
                    enforcement. It differs from previous work by providing a detailed analysis of the governance
                    framework and offering recommendations for a more robust and coordinated approach.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet103">
            <div class="start-time-icon" title="Play from here">
                47:03
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10487" target="_blank">
                        @arXiv 2407.10487
                    </a>
                    <span class="tweet-title">
                        Relighting Portraits: From Flat to 3D with a Single Click!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Max Planck Society, Google, Friedrich-Alexander University Erlangen-Nürnberg...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Lite2Relight, a method that can relight a portrait image from a single
                    input, achieving 3D-consistent head poses and physically plausible lighting at interactive
                    speeds.
                    Unlike previous methods that rely on multi-view images or slow optimization processes,
                    Lite2Relight
                    leverages a pre-trained 3D generative model and a lightstage dataset to implicitly disentangle
                    face
                    reflectance and perform relighting under target HDRI environment maps.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet104">
            <div class="start-time-icon" title="Play from here">
                47:31
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10657" target="_blank">
                        @arXiv 2407.10657
                    </a>
                    <span class="tweet-title">
                        Stop Feeding LLMs Junk Data: How to Validate Synthetic Formulas for Better Spreadsheet
                        Predictions
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on validating synthetic natural language (NL) data generated by LLMs for
                    fine-tuning models in the NL-to-Formula task. Unlike previous work that focuses on manual
                    annotation
                    or data selection based on alignment, this study proposes and empirically evaluates three
                    surrogate
                    objectives for validating the accuracy of synthetic NL.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet105">
            <div class="start-time-icon" title="Play from here">
                48:00
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10645" target="_blank">
                        @arXiv 2407.10645
                    </a>
                    <span class="tweet-title">
                        ChatGPT's Got a New Trick: Prompt Engineering for Social Science Research
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Université Paris 1 Panthéon-Sorbonne, Université Paris-Saclay, CNRS...
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the impact of prompt selection on the accuracy of text annotation tasks
                    using Large Language Models (LLMs) in social sciences. Unlike previous studies that used simple,
                    hand-crafted prompts, this paper investigates the effectiveness of automatic prompt
                    optimization, a
                    method that systematically crafts high-quality prompts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet106">
            <div class="start-time-icon" title="Play from here">
                48:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09719" target="_blank">
                        @arXiv 2407.09719
                    </a>
                    <span class="tweet-title">
                        AI Gets a Material Makeover: New Dataset Tests AI's Design Smarts
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU, Autodesk
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset, MSEval, specifically designed to evaluate the
                    performance of
                    large language models (LLMs) in the context of material selection for conceptual design. Unlike
                    existing benchmarks that focus on general language tasks, MSEval focuses on a niche application
                    with
                    real-world relevance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet107">
            <div class="start-time-icon" title="Play from here">
                49:00
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10188" target="_blank">
                        @arXiv 2407.10188
                    </a>
                    <span class="tweet-title">
                        Self-Modeling: The Secret to Making Your Brain (and AI) More Predictable?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Princeton University
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the impact of self-modeling on neural networks, proposing that learning
                    to
                    predict one's internal states leads to a reduction in network complexity. Unlike previous work
                    focusing on performance improvements, this study investigates the underlying mechanism of
                    self-modeling, specifically its effect on network structure.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet108">
            <div class="start-time-icon" title="Play from here">
                49:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10247" target="_blank">
                        @arXiv 2407.10247
                    </a>
                    <span class="tweet-title">
                        AI Boss: Why Every Company Needs a Chief AI Officer
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford, Siemens AG
                    </span>
                </div>
                <div class="primary-text">
                    This research examines the emergence of the Chief AI Officer (CAIO) role within the C-suite,
                    focusing on the unique pressures and opportunities presented by AI-driven transformations. It
                    distinguishes the CAIO from the traditional CIO role, highlighting the need for specialized AI
                    leadership in navigating the complexities of AI integration and governance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet109">
            <div class="start-time-icon" title="Play from here">
                49:41
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10086" target="_blank">
                        @arXiv 2407.10086
                    </a>
                    <span class="tweet-title">
                        AI Doctor Diagnoses Research Gaps: Pandemic PACT's New Tool
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a fine-tuned language model called PPACE, which automatically
                    classifies
                    biomedical research abstracts based on WHO-aligned research priorities. Unlike previous work,
                    PPACE
                    leverages human-annotated data and incorporates rationales generated by a larger language model
                    to
                    explain its classifications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet110">
            <div class="start-time-icon" title="Play from here">
                50:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10331" target="_blank">
                        @arXiv 2407.10331
                    </a>
                    <span class="tweet-title">
                        Robots Get a Grip: 3D Models Help Robots Understand Objects They Hold
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for jointly estimating the geometry and pose of objects
                    grasped by a robot using 3D foundation models. Unlike previous methods that rely on calibrated
                    cameras or pre-existing object models, this approach leverages uncalibrated external cameras and
                    does not require prior knowledge of the object's geometry.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet111">
            <div class="start-time-icon" title="Play from here">
                50:30
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09994" target="_blank">
                        @arXiv 2407.09994
                    </a>
                    <span class="tweet-title">
                        Rocket Science Just Got a Speed Boost: Distributed Computing Makes Model Reduction Fly!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Jacobs Engineering Group Inc., Air Force Research Laboratory, The University of Texas at
                        Austin
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a distributed algorithm for constructing reduced-order models (ROMs)
                    that
                    can handle extremely large datasets generated by high-performance computing (HPC) simulations.
                    This
                    approach differs from previous work by incorporating HPC into the data-driven learning process,
                    enabling faster and more scalable learning of structured ROMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet112">
            <div class="start-time-icon" title="Play from here">
                50:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10510" target="_blank">
                        @arXiv 2407.10510
                    </a>
                    <span class="tweet-title">
                        AI Doctor's Prescription: Fine-Tuning LLMs for Herbal Remedies
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Hong Kong University of Science and Technology, Beijing Jiaotong University, Nvidia
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces TCM-FTP, a method that fine-tunes large language models (LLMs) on a new
                    dataset called DigestDS, which contains clinical records from TCM experts specializing in
                    digestive
                    system diseases. This approach differs from previous work by utilizing a low-rank adaptation
                    technique for efficient fine-tuning and incorporating data augmentation through herb
                    permutation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet113">
            <div class="start-time-icon" title="Play from here">
                51:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10022" target="_blank">
                        @arXiv 2407.10022
                    </a>
                    <span class="tweet-title">
                        AI Agents: Building Better Alloys, One Atom at a Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces AtomAgents, a multi-agent AI system that combines large language models
                    (LLMs) with physics-based simulations to design alloys. Unlike previous data-driven models,
                    AtomAgents can integrate knowledge from diverse sources and adapt to new challenges.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet114">
            <div class="start-time-icon" title="Play from here">
                51:47
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10481" target="_blank">
                        @arXiv 2407.10481
                    </a>
                    <span class="tweet-title">
                        SuperPADL: Teaching AI to Dance (and Do Kung Fu!) with Text Commands
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nvidia
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces SuperPADL, a framework that combines reinforcement learning (RL) and
                    supervised learning to train physics-based animation models on large datasets. Unlike previous
                    work
                    that primarily relied on RL, SuperPADL uses a progressive distillation approach, starting with
                    smaller, specialized controllers and gradually merging them into a single, versatile controller
                    capable of performing thousands of motions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet115">
            <div class="start-time-icon" title="Play from here">
                52:20
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10935" target="_blank">
                        @arXiv 2407.10935
                    </a>
                    <span class="tweet-title">
                        Skeleton-Based Action Recognition: Masked Prediction Gets a Contrastive Tune-Up!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a sequential approach called STARS that combines masked prediction with
                    contrastive learning to improve the performance of skeleton-based action recognition models.
                    Unlike
                    previous work, STARS uses a contrastive tuning stage to enhance the encoder's output
                    representation,
                    leading to better-separated clusters for different actions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet116">
            <div class="start-time-icon" title="Play from here">
                52:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10371" target="_blank">
                        @arXiv 2407.10371
                    </a>
                    <span class="tweet-title">
                        AI's Silent Curriculum: How LLMs Are Shaping Our Kids' Minds
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the concept of "LLM monoculture," where the convergence of training data
                    and
                    techniques across different LLMs leads to a uniform perspective, potentially shaping the
                    cultural
                    lens of future generations. This differs from previous work by focusing on the implicit
                    influence of
                    LLMs on children's understanding of the world.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet117">
            <div class="start-time-icon" title="Play from here">
                53:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10237" target="_blank">
                        @arXiv 2407.10237
                    </a>
                    <span class="tweet-title">
                        AI's Carbon Footprint: How Big Is Too Big?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        The University of Tokyo, Fraunhofer Institute for Reliability and Microintegration IZM
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a life-cycle-based system thinking approach to assess the environmental
                    impact of AI systems, considering both software and hardware components. This differs from
                    previous
                    work that primarily focused on energy efficiency during the model training phase.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet118">
            <div class="start-time-icon" title="Play from here">
                53:41
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.10543" target="_blank">
                        @arXiv 2407.10543
                    </a>
                    <span class="tweet-title">
                        AI's Got a Blind Spot: New Research Reveals How to See What Deep Learning Misses
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on explaining why a deep learning model might lack confidence in its
                    predictions, going beyond simply identifying uncertainty. It explores five novel methods to
                    pinpoint
                    specific image regions that contribute to this lack of confidence.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet119">
            <div class="start-time-icon" title="Play from here">
                54:11
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09642" target="_blank">
                        @arXiv 2407.09642
                    </a>
                    <span class="tweet-title">
                        Time Travel for AI: A New Benchmark for Learning from the Past
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT, UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark for evaluating how well machine learning models can
                    learn
                    from a sequence of datasets that change over time. The benchmark allows researchers to construct
                    synthetic sequences of shifts, which can be used to study different types of temporal
                    distribution
                    shift.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet120">
            <div class="start-time-icon" title="Play from here">
                54:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.09926" target="_blank">
                        @arXiv 2407.09926
                    </a>
                    <span class="tweet-title">
                        CGENNs Get a Makeover: Learning Metrics for Geometric Deep Learning
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a method for learning the metric within Clifford Group Equivariant
                    Neural
                    Networks (CGENNs). Unlike previous work that relied on fixed, diagonal metrics, this approach
                    allows
                    the network to dynamically adapt its internal geometric representations based on the data.
                </div>
            </div>
        </div>


        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202407172112_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>

</html>