
daily_data = {
    "date": "2024-10-24",
    "tweets": [
        
        {
            "startTime": "01:11",
            "arxivId": "2410.18077",
            "arxivLink": "https://arxiv.org/abs/2410.18077",
            "title": "Transformers Get a Programming Language: ALTA Makes Them Think Like Us!",
            "institute": "Google, DeepMind",
            "text": "This research introduces ALTA, a new programming language that can be compiled into Transformer weights. Unlike previous work, ALTA supports dynamic control flow operations like loops and can compile programs to Universal Transformers.",
            "paper-title": "ALTA: Compiler-Based Analysis of Transformers",
            "image-path": "flux_paper_image/2410.18077_1729798487.png"
        },

        {
            "startTime": "01:34",
            "arxivId": "2410.18082",
            "arxivLink": "https://arxiv.org/abs/2410.18082",
            "title": "AI's Memory Gets a Makeover: Generative Replay Makes Learning More Efficient!",
            "institute": "University of California Berkeley",
            "text": "This research proposes a new approach to online reinforcement learning called Prioritized Generative Replay (PGR). Unlike traditional methods that rely on uniform replay buffers, PGR uses a generative model to create new, relevant training data, guided by a \"relevance function\" that prioritizes the most useful transitions.",
            "paper-title": "Prioritized Generative Replay",
            "image-path": "flux_paper_image/2410.18082_1729797872.png"
        },

        {
            "startTime": "02:03",
            "arxivId": "2410.18084",
            "arxivLink": "https://arxiv.org/abs/2410.18084",
            "title": "DynamicCity: Building 4D LiDAR Scenes, One HexPlane at a Time!",
            "institute": "Shanghai AI Laboratory, Carnegie Mellon University, National University of Singapore...",
            "text": "This research introduces DynamicCity, a framework for generating large-scale, dynamic LiDAR scenes. Unlike previous work that focused on static or single-frame scenes, DynamicCity captures the temporal evolution of environments by leveraging a novel 4D representation called HexPlane.",
            "paper-title": "DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes",
            "image-path": "flux_paper_image/2410.18084_1729799861.png"
        },

        {
            "startTime": "02:28",
            "arxivId": "2410.17482",
            "arxivLink": "https://arxiv.org/abs/2410.17482",
            "title": "AI Still Smart? LLMs Pass the Fake Watch Test, But Not the Homemade Cat One!",
            "institute": "Harvard University, Boston University",
            "text": "This research evaluates the ability of large language models (LLMs) to understand the meaning of adjective-noun combinations, focusing on \"privative\" adjectives like \"fake\" and \"counterfeit.\" Unlike previous work that focused on single correct answers, this study examines how well LLMs can match the full distribution of human judgments, which often vary depending on context and individual interpretation.",
            "paper-title": "Is artificial intelligence still intelligence? LLMs generalize to novel adjective-noun pairs, but don't mimic the full human distribution",
            "image-path": "flux_paper_image/2410.17482_1729797007.png"
        },

        {
            "startTime": "02:52",
            "arxivId": "2410.17498",
            "arxivLink": "https://arxiv.org/abs/2410.17498",
            "title": "Transformers: Not Just for Language, They Can Do Math Too!",
            "institute": "Microsoft, Johns Hopkins University, Yale University...",
            "text": "This research explores the mechanisms of symbol processing in transformers, focusing on in-context learning (ICL) for tasks that involve manipulating meaningless symbols. It differs from previous work by explicitly demonstrating how transformers can implement fundamental aspects of symbolic computation, such as variable binding and hierarchical structure, through a novel framework called the Transformer Production Framework (TPF).",
            "paper-title": "Mechanisms of Symbol Processing for In-Context Learning in Transformer Networks",
            "image-path": "flux_paper_image/2410.17498_1729797404.png"
        },

        {
            "startTime": "03:12",
            "arxivId": "2410.17336",
            "arxivLink": "https://arxiv.org/abs/2410.17336",
            "title": "Finding the Perfect Regularizer: A Recipe for Optimal Online Learning",
            "institute": "MIT, Google",
            "text": "This research presents an algorithm that computes an optimal regularizer for online linear optimization, guaranteeing regret within a constant factor of the best possible bound. This differs from previous work by providing a constructive method for finding the optimal regularizer, rather than simply proving its existence.",
            "paper-title": "Computing Optimal Regularizers for Online Linear Optimization",
            "image-path": "flux_paper_image/2410.17336_1729797526.png"
        },

        {
            "startTime": "03:32",
            "arxivId": "2410.17413",
            "arxivLink": "https://arxiv.org/abs/2410.17413",
            "title": "LLMs: Fact-Checking the Fact-Checkers!",
            "institute": "Google",
            "text": "This research scales up training data attribution (TDA) methods to work with large language models (LLMs) trained on massive datasets like C4, which has over 160 billion tokens. Previous work either focused on smaller models or used extremely limited queries.",
            "paper-title": "Scalable Influence and Fact Tracing for Large Language Model Pretraining",
            "image-path": "flux_paper_image/2410.17413_1729797460.png"
        },

        {
            "startTime": "03:57",
            "arxivId": "2410.17584",
            "arxivLink": "https://arxiv.org/abs/2410.17584",
            "title": "Sheet Music's New Tune: Tokenizing for Better AI Compositions",
            "institute": "University of Rochester, Tsinghua University",
            "text": "This research introduces two new tokenization methods, bar-stream patching and line-stream patching, for multitrack sheet music in ABC notation. These methods aim to improve computational efficiency and musicality compared to existing techniques like bar patching, byte patching, and Byte Pair Encoding (BPE).",
            "paper-title": "Exploring Tokenization Methods for Multitrack Sheet Music Generation",
            "image-path": "flux_paper_image/2410.17584_1729799077.png"
        },

        {
            "startTime": "04:17",
            "arxivId": "2410.18050",
            "arxivLink": "https://arxiv.org/abs/2410.18050",
            "title": "Long-Context Question Answering: When LLMs Get Lost in the Middle, LongRAG Comes to the Rescue!",
            "institute": "Chinese Academy of Sciences, University of Chinese Academy of Sciences, Tsinghua University...",
            "text": "This research introduces LongRAG, a novel retrieval-augmented generation (RAG) system paradigm for long-context question answering (LCQA). Unlike previous RAG systems that rely on chunking strategies, LongRAG employs a dual-perspective approach to extract global information and identify factual details from long documents.",
            "paper-title": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering",
            "image-path": "flux_paper_image/2410.18050_1729797559.png"
        },

        {
            "startTime": "04:42",
            "arxivId": "2410.17971",
            "arxivLink": "https://arxiv.org/abs/2410.17971",
            "title": "Quantum Leap for D2D: Backscattering Data with a Quantum Twist!",
            "institute": "University of Liverpool, University of Warwick, University of Luxembourg...",
            "text": "This research integrates ambient backscatter communication (AmBC) into device-to-device (D2D) systems and proposes a novel quantum reinforcement learning (QRL) algorithm for dynamic spectrum access. This approach differs from previous work by utilizing a parametrized quantum circuit to approximate the optimal policy, leading to faster convergence and reduced training complexity.",
            "paper-title": "Dynamic Spectrum Access for Ambient Backscatter Communication-assisted D2D Systems with Quantum Reinforcement Learning",
            "image-path": "flux_paper_image/2410.17971_1729799829.png"
        },

        {
            "startTime": "05:06",
            "arxivId": "2410.17485",
            "arxivLink": "https://arxiv.org/abs/2410.17485",
            "title": "Speech-Enabled LLMs: Talking to the AI, One Word at a Time!",
            "institute": "Carnegie Mellon University, NVIDIA",
            "text": "This research proposes a single-stage joint speech-text supervised fine-tuning (SFT) approach for training SpeechLMs, which simplifies the training process and preserves the original text-only performance of the LLM backbone. This differs from previous work that often used multi-stage training or froze the LLM to maintain text capabilities.",
            "paper-title": "VoiceTextBlender: Augmenting Large Language Models with Speech Capabilities via Single-Stage Joint Speech-Text Supervised Fine-Tuning",
            "image-path": "flux_paper_image/2410.17485_1729799679.png"
        },

        {
            "startTime": "05:26",
            "arxivId": "2410.17661",
            "arxivLink": "https://arxiv.org/abs/2410.17661",
            "title": "Hybrid Transformers Get a Makeover: PETAH Makes Them More Efficient and Adaptable!",
            "institute": "University of T\u00fcbingen, Meta, FAIR...",
            "text": "This research introduces PETAH, a parameter-efficient task adaptation framework specifically designed for hybrid transformer architectures. Unlike previous work that primarily focused on adapting attention layers in vision transformers, PETAH also adapts convolutional layers, leading to improved performance on various vision tasks.",
            "paper-title": "PETAH: Parameter Efficient Task Adaptation for Hybrid Transformers in a resource-limited Context",
            "image-path": "flux_paper_image/2410.17661_1729796515.png"
        },

        {
            "startTime": "05:44",
            "arxivId": "2410.17309",
            "arxivLink": "https://arxiv.org/abs/2410.17309",
            "title": "AI Gets a Brain: Literature + Data = Smarter Hypotheses",
            "institute": "University of Chicago, Tsinghua University",
            "text": "This research combines literature-based and data-driven approaches to hypothesis generation, unlike previous work that focused on only one method.",
            "paper-title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
            "image-path": "flux_paper_image/2410.17309_1729798300.png"
        },

        {
            "startTime": "06:00",
            "arxivId": "2410.18065",
            "arxivLink": "https://arxiv.org/abs/2410.18065",
            "title": "Robot Learning Gets a Helping Hand: SPIRE Combines Planning, Imitation, and Reinforcement for Long-Horizon Tasks",
            "institute": "NVIDIA, University of Toronto, Vector Institute...",
            "text": "This research introduces SPIRE, a system that combines Task and Motion Planning (TAMP) with imitation and reinforcement learning to tackle long-horizon manipulation tasks. Unlike previous approaches that rely solely on imitation or reinforcement learning, SPIRE leverages the strengths of both methods by decomposing tasks into smaller subproblems and training agents on specific segments.",
            "paper-title": "SPIRE: Synergistic Planning, Imitation, and Reinforcement Learning for Long-Horizon Manipulation",
            "image-path": "flux_paper_image/2410.18065_1729800054.png"
        },

        {
            "startTime": "06:22",
            "arxivId": "2410.17263",
            "arxivLink": "https://arxiv.org/abs/2410.17263",
            "title": "Bias Amplification: When AI Goes Rogue and Gets Even More Biased!",
            "institute": "UC Los Angeles, Meta",
            "text": "This paper presents a new theoretical framework for understanding how machine learning models amplify biases present in training data. Unlike previous work, it uses operator-valued free probability theory to analyze the test error disparities between groups with different data distributions.",
            "paper-title": "An Effective Theory of Bias Amplification",
            "image-path": "flux_paper_image/2410.17263_1729799469.png"
        },

        {
            "startTime": "06:38",
            "arxivId": "2410.17906",
            "arxivLink": "https://arxiv.org/abs/2410.17906",
            "title": "Deep Learning Predicts Star Metallicity: A Stellar Recipe for Success!",
            "institute": "National Institute for Astrophysics",
            "text": "This research uses deep learning models, specifically GRU networks, to predict the metallicity of RR Lyrae stars directly from their light curves. This approach differs from previous methods that relied on intermediate steps like calibrating spectral indices.",
            "paper-title": "Leveraging Deep Learning for Time-Series Extrinsic Regression in Predicting the Photometric Metallicity of Fundamental-Mode RR Lyrae Stars",
            "image-path": "flux_paper_image/2410.17906_1729799237.png"
        },

        {
            "startTime": "07:02",
            "arxivId": "2410.17980",
            "arxivLink": "https://arxiv.org/abs/2410.17980",
            "title": "Attention, Please! Stick-Breaking Makes Transformers Smarter",
            "institute": "IBM, MIT, Montreal Institute for Learning Algorithms",
            "text": "This paper proposes a new attention mechanism called \"stick-breaking attention\" that replaces the traditional softmax operator. Unlike softmax, stick-breaking naturally incorporates a recency bias, which is beneficial for tasks like language modeling.",
            "paper-title": "Stick-breaking Attention",
            "image-path": "flux_paper_image/2410.17980_1729796917.png"
        },

        {
            "startTime": "07:34",
            "arxivId": "2410.17331",
            "arxivLink": "https://arxiv.org/abs/2410.17331",
            "title": "Tired of FID? New Metrics Judge Text-to-Image Sets Like Humans Do!",
            "institute": "University of Waterloo, Google",
            "text": "This research introduces a new set of evaluation metrics for text-to-image (TTI) systems that explicitly model how users browse and interact with sets of generated images. Unlike previous work that focused on population-level metrics like FID, these new metrics consider factors like image relevance, diversity, and novelty within a set, making them more aligned with human preferences.",
            "paper-title": "Offline Evaluation of Set-Based Text-to-Image Generation",
            "image-path": "flux_paper_image/2410.17331_1729798481.png"
        },

        {
            "startTime": "07:58",
            "arxivId": "2410.18076",
            "arxivLink": "https://arxiv.org/abs/2410.18076",
            "title": "Unlabeled Data: The Secret Sauce for Super-Smart Robots",
            "institute": "UC Berkeley",
            "text": "This research explores how unlabeled data can be used to train robots to explore their environment more efficiently. Unlike previous methods that only use unlabeled data for skill learning, this paper proposes a method that also uses it to guide online exploration, effectively \"double-dipping\" the data for faster learning.",
            "paper-title": "Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration",
            "image-path": "flux_paper_image/2410.18076_1729797481.png"
        },

        {
            "startTime": "08:24",
            "arxivId": "2410.17434",
            "arxivLink": "https://arxiv.org/abs/2410.17434",
            "title": "Long Videos, Short Attention Spans? LongVU to the Rescue!",
            "institute": "King Abdullah University of Science and Technology, Meta",
            "text": "This paper proposes LongVU, a spatiotemporal adaptive compression mechanism that reduces the number of video tokens while preserving visual details of long videos. Unlike previous methods that rely on uniform sampling, LongVU leverages cross-modal queries and inter-frame dependencies to adaptively compress video content.",
            "paper-title": "LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding",
            "image-path": "flux_paper_image/2410.17434_1729798633.png"
        },

        {
            "startTime": "08:49",
            "arxivId": "2410.17883",
            "arxivLink": "https://arxiv.org/abs/2410.17883",
            "title": "Tiny AI, Big Control: How a Lightweight Model Makes Your Phone Smarter",
            "institute": "Huawei, University College London",
            "text": "This research introduces a new architecture for app agents called LiMAC, which combines a lightweight transformer network with a fine-tuned vision-language model (VLM). This approach aims to improve efficiency and responsiveness compared to using large foundation models for every action.",
            "paper-title": "Lightweight Neural App Control",
            "image-path": "flux_paper_image/2410.17883_1729798799.png"
        },

        {
            "startTime": "09:07",
            "arxivId": "2410.17538",
            "arxivLink": "https://arxiv.org/abs/2410.17538",
            "title": "Off-Policy Evaluation Gets a Spectral Makeover: DICE is Now Linear!",
            "institute": "Harvard University, Georgia Institute of Technology",
            "text": "This paper introduces a novel primal-dual spectral representation for off-policy evaluation (OPE) that leverages linear representations of both the Q-function and the stationary distribution correction ratio. This approach differs from previous work by bypassing the non-convex non-concave saddle-point optimization typically encountered in DICE estimators.",
            "paper-title": "Primal-Dual Spectral Representation for Off-policy Evaluation",
            "image-path": "flux_paper_image/2410.17538_1729798084.png"
        },

        {
            "startTime": "09:26",
            "arxivId": "2410.17273",
            "arxivLink": "https://arxiv.org/abs/2410.17273",
            "title": "Data Scientists, It's Time to Get Behavioral!",
            "institute": "Emory University, University of Washington",
            "text": "This paper proposes a new approach to responsible data science by focusing on the behaviors of data scientists, not just the technical aspects of algorithms and datasets.",
            "paper-title": "Behavior Matters: An Alternative Perspective on Promoting Responsible Data Science",
            "image-path": "flux_paper_image/2410.17273_1729799222.png"
        },

        {
            "startTime": "09:43",
            "arxivId": "2410.17509",
            "arxivLink": "https://arxiv.org/abs/2410.17509",
            "title": "LLMs on a Diet: How to Make AI Forget What It Shouldn't Know",
            "institute": "Michigan State University, IBM Research",
            "text": "This research introduces a novel weight attribution framework for LLM unlearning. Unlike previous methods that focus solely on algorithm design, this approach identifies the specific weights within the model that are most influential for forgetting undesirable information.",
            "paper-title": "WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models",
            "image-path": "flux_paper_image/2410.17509_1729796441.png"
        },

        {
            "startTime": "10:09",
            "arxivId": "2410.17856",
            "arxivLink": "https://arxiv.org/abs/2410.17856",
            "title": "Minecraft's New AI: Seeing the World, One Pixel at a Time!",
            "institute": "Peking University, UC Los Angeles",
            "text": "This research introduces a novel communication protocol called \"visual-temporal context prompting\" for embodied AI agents. Unlike previous approaches that rely on language or imagined images, this method uses object segmentation from past and present observations to guide the agent's actions.",
            "paper-title": "ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting",
            "image-path": "flux_paper_image/2410.17856_1729799622.png"
        },

        {
            "startTime": "10:39",
            "arxivId": "2410.18070",
            "arxivLink": "https://arxiv.org/abs/2410.18070",
            "title": "Flowing with Control: A Training-Free Guide to Guiding Generative Models",
            "institute": "University of Cambridge, University of Illinois",
            "text": "This research introduces OC-Flow, a training-free framework for guiding pre-trained generative models using optimal control. Unlike previous methods that rely on specialized training routines or strong assumptions about the denoising process, OC-Flow leverages optimal control theory to guide the model's trajectory towards a desired target distribution.",
            "paper-title": "Training Free Guided Flow Matching with Optimal Control",
            "image-path": "flux_paper_image/2410.18070_1729798403.png"
        },

        {
            "startTime": "11:05",
            "arxivId": "2410.17676",
            "arxivLink": "https://arxiv.org/abs/2410.17676",
            "title": "Surprisal Gets a Makeover: Words Are More Than Just Words!",
            "institute": "ETH Zurich",
            "text": "This research extends surprisal theory, a model of how we understand language, by considering the similarity between words. Previous work treated words as distinct entities, but this paper introduces a new metric, similarity-adjusted surprisal, that accounts for how similar words are in meaning.",
            "paper-title": "Towards a Similarity-adjusted Surprisal Theory",
            "image-path": "flux_paper_image/2410.17676_1729799482.png"
        },

        {
            "startTime": "11:27",
            "arxivId": "2410.17600",
            "arxivLink": "https://arxiv.org/abs/2410.17600",
            "title": "Knowledge Graph Construction: From Local to Global with Graphusion!",
            "institute": "Duke-NUS Medical School, The University of Tokyo, Yale University...",
            "text": "This research introduces Graphusion, a framework for constructing knowledge graphs (KGs) from free text using large language models (LLMs). Unlike previous approaches that focus on extracting knowledge from individual sentences or documents, Graphusion incorporates a global perspective by fusing knowledge from multiple local sub-graphs into a single, connected KG.",
            "paper-title": "Graphusion: A RAG Framework for Knowledge Graph Construction with a Global Perspective",
            "image-path": "flux_paper_image/2410.17600_1729797750.png"
        },

        {
            "startTime": "11:54",
            "arxivId": "2410.17935",
            "arxivLink": "https://arxiv.org/abs/2410.17935",
            "title": "Particle-Based Inference Gets a Noise Boost: Semi-Implicit Functional Gradient Flow",
            "institute": "Peking University, UC Berkeley",
            "text": "This paper introduces a new particle-based variational inference method called Semi-Implicit Functional Gradient Flow (SIFG). Unlike previous methods that rely on deterministic updates, SIFG injects Gaussian noise into the particles, enhancing exploration and sample diversity.",
            "paper-title": "Semi-Implicit Functional Gradient Flow",
            "image-path": "flux_paper_image/2410.17935_1729799973.png"
        },

        {
            "startTime": "12:21",
            "arxivId": "2410.18083",
            "arxivLink": "https://arxiv.org/abs/2410.18083",
            "title": "Super-Resolution Meets Compression: A Unified Field for Sharper, Smaller Images!",
            "institute": "National Yang Ming Chiao Tung University, Johns Hopkins University, Academia Sinica",
            "text": "This research proposes a unified representation called \"Factorized Fields\" for both image super-resolution and image compression. Unlike previous methods that focus on network architecture, this approach explicitly captures multi-scale visual features and structural components in images through a basis-coefficient decomposition.",
            "paper-title": "FIPER: Generalizable Factorized Fields for Joint Image Compression and Super-Resolution",
            "image-path": "flux_paper_image/2410.18083_1729797888.png"
        },

        {
            "startTime": "12:45",
            "arxivId": "2410.17551",
            "arxivLink": "https://arxiv.org/abs/2410.17551",
            "title": "Multimodal Data: Don't Just Throw It All In, Squeeze It!",
            "institute": "Tsinghua University",
            "text": "This paper proposes a multimodal information bottleneck model (MIB) for deep reinforcement learning. Unlike previous methods that focus on reconstruction or mutual information maximization, MIB compresses information in the learned joint representations, filtering out irrelevant information and retaining only task-relevant data.",
            "paper-title": "Multimodal information bottleneck for deep reinforcement learning with multiple sensors",
            "image-path": "flux_paper_image/2410.17551_1729800290.png"
        },

        {
            "startTime": "13:11",
            "arxivId": "2410.17878",
            "arxivLink": "https://arxiv.org/abs/2410.17878",
            "title": "Equivariant Models: Train 'em Soft, Not Hard!",
            "institute": "University of Oxford, MIT, Valence Labs...",
            "text": "This paper introduces REMUL, a training procedure that approximates equivariance in unconstrained models using multitask learning. Unlike previous work that focuses on building equivariance into the architecture, REMUL achieves approximate equivariance by adding a simple equivariance loss to the training objective.",
            "paper-title": "Relaxed Equivariance via Multitask Learning",
            "image-path": "flux_paper_image/2410.17878_1729798367.png"
        },

        {
            "startTime": "13:38",
            "arxivId": "2410.17904",
            "arxivLink": "https://arxiv.org/abs/2410.17904",
            "title": "Reinforcement Learning's Hidden Talent: Unmasking Latent Dynamics!",
            "institute": "University of Illinois, Microsoft, Google",
            "text": "This paper explores reinforcement learning in environments where the agent observes complex data, but the underlying dynamics are simpler and hidden. It investigates the statistical and algorithmic requirements for learning in such scenarios, going beyond restrictive settings like small latent spaces.",
            "paper-title": "Reinforcement Learning under Latent Dynamics: Toward Statistical and Algorithmic Modularity",
            "image-path": "flux_paper_image/2410.17904_1729798586.png"
        },

        {
            "startTime": "14:04",
            "arxivId": "2410.17261",
            "arxivLink": "https://arxiv.org/abs/2410.17261",
            "title": "Masked Autoencoders: The New Superheroes of sEMG Gesture Recognition!",
            "institute": "Concordia University, Nanyang Technological University",
            "text": "This research proposes a novel Masked Autoencoder with Swin Transformer (MAST) framework for mitigating electrode shift in HD-sEMG-based gesture recognition. Unlike previous work that uses a fixed subset of HD-sEMG channels, MAST utilizes a combination of four masking strategies to learn robust representations that can handle incomplete data.",
            "paper-title": "Masked Autoencoder with Swin Transformer Network for Mitigating Electrode Shift in HD-EMG-based Gesture Recognition",
            "image-path": "flux_paper_image/2410.17261_1729796833.png"
        },

        {
            "startTime": "14:28",
            "arxivId": "2410.17957",
            "arxivLink": "https://arxiv.org/abs/2410.17957",
            "title": "BERT on a Budget: Tiny Language Models for Tiny Devices",
            "institute": "Peking University",
            "text": "This research proposes MCUBERT, a framework that enables BERT models to run on microcontrollers (MCUs) by compressing the embedding table and optimizing the scheduling of computations. This differs from previous work that focused on deploying CNNs or smaller Transformer models on MCUs.",
            "paper-title": "MCUBERT: Memory-Efficient BERT Inference on Commodity Microcontrollers",
            "image-path": "flux_paper_image/2410.17957_1729799327.png"
        },

        {
            "startTime": "14:54",
            "arxivId": "2410.17599",
            "arxivLink": "https://arxiv.org/abs/2410.17599",
            "title": "Tiny Model, Big Impact: One-Time Training for Multiple LLMs",
            "institute": "East China Normal University, Peking University, Chinese Academy of Sciences...",
            "text": "This research introduces Cross-model Control (CMC), a method that uses a small, portable language model to improve the performance of multiple larger LLMs in a single training session. Unlike previous methods that require fine-tuning each model individually, CMC leverages the similarities in fine-tuning effects across different models to achieve efficient optimization.",
            "paper-title": "Cross-model Control: Improving Multiple Large Language Models in One-time Training",
            "image-path": "flux_paper_image/2410.17599_1729798865.png"
        },

        {
            "startTime": "15:27",
            "arxivId": "2410.17545",
            "arxivLink": "https://arxiv.org/abs/2410.17545",
            "title": "Predicting Hospital Readmissions: LSTM's Got Your Back!",
            "institute": "Georgia Institute of Technology, University of Miami, UC Berkeley...",
            "text": "This research utilizes an LSTM deep learning model to predict 30-day hospital readmissions in Medicare patients, focusing on admission-level data and temporal dynamics, unlike previous studies that often rely on the LACE index or target specific subpopulations.",
            "paper-title": "Predicting 30-Day Hospital Readmission in Medicare Patients: Insights from an LSTM Deep Learning Model",
            "image-path": "flux_paper_image/2410.17545_1729797001.png"
        },

        {
            "startTime": "15:48",
            "arxivId": "2410.17816",
            "arxivLink": "https://arxiv.org/abs/2410.17816",
            "title": "Sunspot Sleuths: Deep Learning Cracks the Code of Solar Activity!",
            "institute": "University of Genova, Istituto Nazionale di Astrofisica",
            "text": "This research compares the performance of convolutional neural networks (CNNs) and vision transformers (ViTs) for classifying solar active regions, focusing on the impact of training data and augmentation techniques. It builds upon previous work by exploring a wider range of architectures and incorporating on-the-fly data augmentation.",
            "paper-title": "Deep Learning for Active Region Classification: A Systematic Study from Convolutional Neural Networks to Vision Transformers",
            "image-path": "flux_paper_image/2410.17816_1729796743.png"
        },

        {
            "startTime": "16:08",
            "arxivId": "2410.17933",
            "arxivLink": "https://arxiv.org/abs/2410.17933",
            "title": "Sharing Healthcare Data? No Problem! Blockchain-Powered Federated Learning to the Rescue!",
            "institute": "Tongji University, University College London",
            "text": "This research proposes a framework for global healthcare modeling using blockchain-enabled federated learning. Unlike previous work, this approach allows multiple institutions to contribute to model training without sharing sensitive patient data directly.",
            "paper-title": "Multi-Continental Healthcare Modelling Using Blockchain-Enabled Federated Learning",
            "image-path": "flux_paper_image/2410.17933_1729796934.png"
        },

        {
            "startTime": "16:34",
            "arxivId": "2410.17557",
            "arxivLink": "https://arxiv.org/abs/2410.17557",
            "title": "Blurry Images, Clear Diagnosis: AI-Powered Microscope Makes HER2 Scoring a Breeze!",
            "institute": "University of California Los Angeles",
            "text": "This research introduces BlurryScope, a compact and cost-effective scanning microscope that leverages deep learning to analyze motion-blurred images for automated HER2 scoring. Unlike traditional digital pathology scanners, BlurryScope prioritizes speed and affordability over high resolution, making it suitable for resource-limited settings.",
            "paper-title": "BlurryScope: a cost-effective and compact scanning microscope for automated HER2 scoring using deep learning on blurry image data",
            "image-path": "flux_paper_image/2410.17557_1729797217.png"
        },

        {
            "startTime": "16:53",
            "arxivId": "2410.17465",
            "arxivLink": "https://arxiv.org/abs/2410.17465",
            "title": "Bauplan: Serverless Data Pipelines Get a Speed Boost!",
            "institute": "Bauplan Labs, University of Wisconsin-Madison",
            "text": "This research introduces Bauplan, a novel FaaS runtime specifically designed for data pipelines. Unlike existing FaaS platforms, Bauplan prioritizes data awareness and optimizes for large data workloads, addressing limitations in handling intermediate dataframes and scaling up.",
            "paper-title": "Bauplan: zero-copy, scale-up FaaS for data pipelines",
            "image-path": "flux_paper_image/2410.17465_1729798249.png"
        },

        {
            "startTime": "17:11",
            "arxivId": "2410.17629",
            "arxivLink": "https://arxiv.org/abs/2410.17629",
            "title": "Graph Signals Get a Message: Adaptive Filtering for Time-Varying Data",
            "institute": "Tsinghua University",
            "text": "This research introduces Graph Signal Adaptive Message Passing (GSAMP), a novel method for processing time-varying graph signals. Unlike conventional methods that apply the same filter to the entire graph, GSAMP utilizes localized computations at each node, allowing for more flexible and adaptive filtering.",
            "paper-title": "Graph Signal Adaptive Message Passing",
            "image-path": "flux_paper_image/2410.17629_1729800303.png"
        },

        {
            "startTime": "17:36",
            "arxivId": "2410.17566",
            "arxivLink": "https://arxiv.org/abs/2410.17566",
            "title": "Privacy-Preserving Language Models: A Recipe for Better Text, Less Noise!",
            "institute": "University of Vermont, University of Washington",
            "text": "This research introduces DPRefine, a three-phase method that uses data synthesis, differentially private fine-tuning, and self-distillation to improve the linguistic quality of differentially private language models. This approach differs from previous work by focusing on enhancing the quality of outputs generated by privacy-preserving models, rather than solely focusing on privacy guarantees.",
            "paper-title": "Differentially Private Learning Needs Better Model Initialization and Self-Distillation",
            "image-path": "flux_paper_image/2410.17566_1729799139.png"
        },

        {
            "startTime": "17:57",
            "arxivId": "2410.17409",
            "arxivLink": "https://arxiv.org/abs/2410.17409",
            "title": "Crowd Surfing: A Geometric GNN Predicts Pedestrian Paths with a 180-Degree View!",
            "institute": "University of Maryland",
            "text": "This research proposes a geometric graph neural network (GNN) architecture that incorporates domain knowledge from psychological studies to model pedestrian interactions and predict future trajectories. Unlike prior studies using complete graphs, this paper defines interaction neighborhoods using pedestrians\u2019 field of view, motion direction, and distance-based kernel functions to construct graph representations of crowds.",
            "paper-title": "Geometric Graph Neural Network Modeling of Human Interactions in Crowded Environments",
            "image-path": "flux_paper_image/2410.17409_1729799848.png"
        },

        {
            "startTime": "18:19",
            "arxivId": "2410.17976",
            "arxivLink": "https://arxiv.org/abs/2410.17976",
            "title": "Metaclustering for SNF: Finding the Best Cluster, Not Just the Closest!",
            "institute": "Hospital for Sick Children, University of Toronto, Centre for Addiction and Mental Health...",
            "text": "This research introduces the \"metasnf\" R package, which implements metaclustering for similarity network fusion (SNF) workflows. Unlike traditional SNF, which focuses on finding the most numerically compact cluster solution, metaclustering explores a broader space of possible solutions and identifies those that are most useful for a specific context.",
            "paper-title": "metasnf: Meta Clustering with Similarity Network Fusion in R",
            "image-path": "flux_paper_image/2410.17976_1729797040.png"
        },

        {
            "startTime": "18:42",
            "arxivId": "2410.17840",
            "arxivLink": "https://arxiv.org/abs/2410.17840",
            "title": "LLM Scheduling: Is Your GPU Half-Empty or Half-Full?",
            "institute": "MIT",
            "text": "This research introduces two new scheduling techniques, LARRY and SAL, designed specifically for LLM serving systems. These techniques are easy to implement and deploy, requiring minimal changes to existing systems. They outperform current techniques by prioritizing requests based on their anticipated memory consumption and the current system load.",
            "paper-title": "Is the GPU Half-Empty or Half-Full? Practical Scheduling Techniques for LLMs",
            "image-path": "flux_paper_image/2410.17840_1729799841.png"
        },

        {
            "startTime": "18:59",
            "arxivId": "2410.17787",
            "arxivLink": "https://arxiv.org/abs/2410.17787",
            "title": "LLMs: Feature Engineering's New BFF or Just a Fancy Calculator?",
            "institute": "University of Freiburg",
            "text": "This research investigates whether large language models (LLMs) exhibit a bias when used for feature engineering in tabular data. Unlike previous work that focuses on code generation, this study examines the frequency of operators used by LLMs to engineer new features.",
            "paper-title": "Large Language Models Engineer Too Many Simple Features For Tabular Data",
            "image-path": "flux_paper_image/2410.17787_1729797131.png"
        },

        {
            "startTime": "19:26",
            "arxivId": "2410.17268",
            "arxivLink": "https://arxiv.org/abs/2410.17268",
            "title": "Spiking Neurons Go Long: A New Model for Efficient Sequence Learning",
            "institute": "Peking University, Tsinghua University, Huawei...",
            "text": "This research proposes a new spiking state space model (SPikE-SSM) that incorporates a refractory mechanism into the LIF neuron model. This allows for more biologically realistic and efficient long-sequence learning compared to previous spiking SSMs.",
            "paper-title": "SPikE-SSM: A Sparse, Precise, and Efficient Spiking State Space Model for Long Sequences Learning",
            "image-path": "flux_paper_image/2410.17268_1729799965.png"
        },

        {
            "startTime": "20:00",
            "arxivId": "2410.17690",
            "arxivLink": "https://arxiv.org/abs/2410.17690",
            "title": "Traffic Jam? Not in My Sky! New Game Theory Solves Air Traffic Control",
            "institute": "ETH Zurich",
            "text": "This paper introduces a new approach to multi-player trajectory planning that focuses on maximizing the probability of all players reaching their destinations while avoiding collisions. Unlike previous work that relies on centralized computation and global policies, this research utilizes local state feedback policies, making it more practical for real-world applications.",
            "paper-title": "Markov Potential Game with Final-time Reach-Avoid Objectives",
            "image-path": "flux_paper_image/2410.17690_1729799004.png"
        },

        {
            "startTime": "20:30",
            "arxivId": "2410.18038",
            "arxivLink": "https://arxiv.org/abs/2410.18038",
            "title": "LLM Inference Gets a Speed Boost: POD-Attention Makes Hybrid Batching a Breeze!",
            "institute": "Microsoft, University of Washington",
            "text": "This research introduces POD-Attention, a novel GPU kernel designed to efficiently compute attention for hybrid batches in LLM inference. Unlike previous approaches that optimize prefill and decode operations separately, POD-Attention concurrently computes both phases, maximizing the utilization of both compute and memory bandwidth.",
            "paper-title": "POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference",
            "image-path": "flux_paper_image/2410.18038_1729799794.png"
        },

        {
            "startTime": "20:51",
            "arxivId": "2410.17589",
            "arxivLink": "https://arxiv.org/abs/2410.17589",
            "title": "Sound Scene Synthesis: A Challenge to Make AI Music More Realistic!",
            "institute": "KAIST, Nantes Universit\u00e9, CMU...",
            "text": "This research introduces a new evaluation protocol for text-to-audio generation, combining objective metrics with human perception assessments. This approach aims to address the limitations of previous evaluation methods, which often relied solely on subjective ratings.",
            "paper-title": "Challenge on Sound Scene Synthesis: Evaluating Text-to-Audio Generation",
            "image-path": "flux_paper_image/2410.17589_1729798746.png"
        },

        {
            "startTime": "21:23",
            "arxivId": "2410.17389",
            "arxivLink": "https://arxiv.org/abs/2410.17389",
            "title": "LLMs Gone Wild? Taming Noisy Feedback in Reinforcement Learning!",
            "institute": "CMU",
            "text": "This research proposes a novel approach to reinforcement learning from large language model (LLM) feedback by utilizing a potential-based scoring function. Unlike previous methods that directly use the LLM's score as a reward, this approach issues uninformative rewards when the LLM is uncertain, mitigating the impact of noisy feedback.",
            "paper-title": "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models",
            "image-path": "flux_paper_image/2410.17389_1729796490.png"
        },

        {
            "startTime": "21:51",
            "arxivId": "2410.17397",
            "arxivLink": "https://arxiv.org/abs/2410.17397",
            "title": "Quantum LLMs: Giving Language Models a Quantum Boost!",
            "institute": "Multiverse Computing, Tecnun - University of Navarra, Donostia International Physics Center...",
            "text": "This research proposes a method to enhance the performance of Large Language Models (LLMs) by integrating quantum computing and quantum-inspired techniques. The key idea is to replace the weight matrices in the Self-Attention and Multi-layer Perceptron layers with a combination of two variational quantum circuits and a quantum-inspired tensor network, such as a Matrix Product Operator (MPO). This approach differs from previous work by incorporating quantum circuits into the deep layers of the model, enabling the capture of more complex correlations and leading to improved accuracy beyond classical models.",
            "paper-title": "Quantum Large Language Models via Tensor Network Disentanglers",
            "image-path": "flux_paper_image/2410.17397_1729799256.png"
        },

        {
            "startTime": "22:16",
            "arxivId": "2410.17732",
            "arxivLink": "https://arxiv.org/abs/2410.17732",
            "title": "Fuzzing Hardware: A Faster, More Efficient Way to Find Bugs",
            "institute": "Infineon Technologies, Google",
            "text": "This research proposes an automated fuzzing framework called FuzzWiz that leverages metamodeling and Python to achieve faster hardware coverage compared to traditional simulation-based approaches. Unlike previous work, FuzzWiz is design-agnostic and compatible with various open-source fuzzing engines.",
            "paper-title": "FuzzWiz -- Fuzzing Framework for Efficient Hardware Coverage",
            "image-path": "flux_paper_image/2410.17732_1729799454.png"
        },

        {
            "startTime": "22:37",
            "arxivId": "2410.17423",
            "arxivLink": "https://arxiv.org/abs/2410.17423",
            "title": "AI in Brazil: More Products, Less Problems?",
            "institute": "University of Cambridge",
            "text": "This research analyzes AI coverage in Brazilian news, a region often overlooked in AI media studies, providing insights into how AI is perceived in a non-anglophone context.",
            "paper-title": "Artificial Intelligence in Brazilian News: A Mixed-Methods Analysis",
            "image-path": "flux_paper_image/2410.17423_1729800077.png"
        },

        {
            "startTime": "22:58",
            "arxivId": "2410.18074",
            "arxivLink": "https://arxiv.org/abs/2410.18074",
            "title": "Depth Completion's Got a Memory Problem: New Benchmark Tests How AI Forgets!",
            "institute": "Yale University",
            "text": "This research introduces a standardized benchmark, UnCLe, for evaluating unsupervised continual learning in depth completion. Unlike previous work, which typically focuses on single datasets, UnCLe simulates real-world scenarios by training models on sequences of datasets with diverse scenes and sensor types.",
            "paper-title": "UnCLe: Unsupervised Continual Learning of Depth Completion",
            "image-path": "flux_paper_image/2410.18074_1729797492.png"
        },

        {
            "startTime": "23:18",
            "arxivId": "2410.17433",
            "arxivLink": "https://arxiv.org/abs/2410.17433",
            "title": "AI Fairness: It's Not Just About the Algorithm, It's About the People!",
            "institute": "Stanford University",
            "text": "This research focuses on the practical limitations of technical bias mitigation strategies in healthcare settings, providing a structured analysis across five key dimensions affecting their real-world implementation.",
            "paper-title": "Revisiting Technical Bias Mitigation Strategies",
            "image-path": "flux_paper_image/2410.17433_1729799746.png"
        },

        {
            "startTime": "23:39",
            "arxivId": "2410.17382",
            "arxivLink": "https://arxiv.org/abs/2410.17382",
            "title": "Bandits Unite! New Algorithm Makes Agents Cooperate to Conquer Constraints",
            "institute": "Stanford University, UC Santa Cruz, UC Santa Barbara",
            "text": "This research explores a multi-agent setting where agents collaborate to solve constrained linear bandit problems. Unlike previous work, this paper focuses on agents communicating only with their immediate neighbors, making it more realistic for decentralized applications.",
            "paper-title": "Cooperative Multi-Agent Constrained Stochastic Linear Bandits",
            "image-path": "flux_paper_image/2410.17382_1729797552.png"
        },

        {
            "startTime": "24:08",
            "arxivId": "2410.17445",
            "arxivLink": "https://arxiv.org/abs/2410.17445",
            "title": "PINN-Proj: A Neural Network That Knows Its Limits (and Conserves Momentum!)",
            "institute": "MIT, IBM Research",
            "text": "This research introduces a novel projection method that guarantees adherence to conservation laws in physics-informed neural networks (PINNs). Unlike previous work that relied on soft constraints, this method imposes a hard constraint, ensuring the PINN's predictions always obey conservation laws.",
            "paper-title": "Guaranteeing Conservation Laws with Projection in Physics-Informed Neural Networks",
            "image-path": "flux_paper_image/2410.17445_1729797729.png"
        },

        {
            "startTime": "24:33",
            "arxivId": "2410.17288",
            "arxivLink": "https://arxiv.org/abs/2410.17288",
            "title": "Poo-licious AI: Detecting Colorectal Cancer with a Smartphone Snap!",
            "institute": "CMU",
            "text": "This research introduces a novel approach to colorectal cancer detection using a stool recognition neural network trained on a dataset augmented with synthetic images generated by a GAN. This differs from previous methods like the faecal occult blood test (FOBT) which relies on laboratory analysis and has limitations in terms of convenience and cost.",
            "paper-title": "Stool Recognition for Colorectal Cancer Detection through Deep Learning",
            "image-path": "flux_paper_image/2410.17288_1729799687.png"
        },

        {
            "startTime": "24:56",
            "arxivId": "2410.17271",
            "arxivLink": "https://arxiv.org/abs/2410.17271",
            "title": "AI Alignment: Can Law Help Us Avoid a Robot Rebellion?",
            "institute": "Harvard University",
            "text": "This paper proposes using legal theory to improve AI alignment, specifically by incorporating the interplay between rules and cases. It argues that this approach can help address the problems of specifying vague principles and ensuring inclusivity of diverse perspectives.",
            "paper-title": "Legal Theory for Pluralistic Alignment",
            "image-path": "flux_paper_image/2410.17271_1729798207.png"
        },

        {
            "startTime": "25:19",
            "arxivId": "2410.17628",
            "arxivLink": "https://arxiv.org/abs/2410.17628",
            "title": "Attention Wins: Feature Learning Is More Stable and Compact Than Convolution",
            "institute": "University of Tokyo",
            "text": "This research directly compares the feature learning dynamics of attention and convolution by analyzing their Lipschitz continuity and covering numbers. It goes beyond previous work by examining the global behavior of these models, not just their local properties.",
            "paper-title": "Feature Learning in Attention Mechanisms Is More Compact and Stable Than in Convolution",
            "image-path": "flux_paper_image/2410.17628_1729799896.png"
        }
    ],
    "stats": {
        "num_pick": 62,
        "num_total": 299,
    },
    "audio": "https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202410241356_audio.mp3"
}
