
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY</div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">Fresh Picks: 
                    <span class="highlightNumber" style="font-size: 28px;">110</span> out of <span
                    class="highlightNumber">487</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-09-24"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>


        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">01:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13707" target="_blank">@arXiv 2409.13707</a>
                    <span class="tweet-title">IT Support Gets a Brain:  How Retrieval Augmented Generation is Solving Tickets Faster</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research explores the use of Retrieval Augmented Generation (RAG) for IT support incident resolution, a novel application of RAG in this domain. The paper presents a system that combines retrieval, classification, and generation components to recommend solutions based on support documents.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">01:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14254" target="_blank">@arXiv 2409.14254</a>
                    <span class="tweet-title">Instruction Following: It's Easier Than You Think!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research explores the idea that instruction following in language models can be achieved implicitly, meaning through methods not explicitly designed for that purpose. The authors demonstrate that training on responses alone, without corresponding instructions, or even finetuning on narrow-domain data like poetry generation, can still lead to instruction-following behavior.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">01:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13773" target="_blank">@arXiv 2409.13773</a>
                    <span class="tweet-title">Reasoning Models: Code Wizards or Code Fiascos?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ONEKQLab, OpenAI</span>
                </div>
                <div class="primary-text">
                    This research evaluates the performance of OpenAI's reasoning models in a web app coding context, introducing a new benchmark (WebApp1K-Duo) that doubles the number of tasks and test cases compared to the single-task benchmark (WebApp1K).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">02:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14634" target="_blank">@arXiv 2409.14634</a>
                    <span class="tweet-title">AI Helps Scientists Brainstorm Better Research Ideas</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, Allen Institute for AI</span>
                </div>
                <div class="primary-text">
                    This research introduces Scideator, a tool that helps scientists generate new research ideas by combining key aspects of existing research papers. Unlike previous work, Scideator uses large language models (LLMs) to extract these aspects, called facets, and then automatically assesses the novelty of the generated ideas.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">02:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13931" target="_blank">@arXiv 2409.13931</a>
                    <span class="tweet-title">LLMs Get a Social Life:  Generalists and Specialists Team Up for On-Device Learning!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach for on-device collaborative fine-tuning of LLMs by combining generalist and specialist experts within a Mixture of Experts (MoE) architecture. Unlike previous work that focuses on aggregating parameters across users, this method leverages a learnable routing network to balance personalization and collaboration at the token level.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">02:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14623" target="_blank">@arXiv 2409.14623</a>
                    <span class="tweet-title">Deep Learning's Lazy Rich: How Initialization Makes or Breaks Your Network</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London, Imperial College London</span>
                </div>
                <div class="primary-text">
                    This research derives exact solutions for the learning dynamics in deep linear networks, specifically focusing on the impact of "lambda-balanced" initializations, which vary the relative scale of weights across layers. This extends previous work by analyzing the full spectrum of learning dynamics from the lazy to the rich regime, going beyond just the absolute scale of initialization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">03:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15273" target="_blank">@arXiv 2409.15273</a>
                    <span class="tweet-title">Material Fusion:  Giving Objects a Material Makeover with AI!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, Tel Aviv University</span>
                </div>
                <div class="primary-text">
                    This research introduces MaterialFusion, a 3D inverse rendering approach that incorporates a 2D diffusion model prior to enhance material reconstruction. Unlike previous methods, MaterialFusion leverages a large-scale dataset of synthetic objects with high-quality PBR assets to train its prior, enabling it to accurately predict materials from images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">03:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15156" target="_blank">@arXiv 2409.15156</a>
                    <span class="tweet-title">Scaling Up, Scaling Down: When Bigger Isn't Always Better in Machine Learning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research explores the limitations of traditional regularization techniques in the context of large language models (LLMs) trained on massive datasets. It introduces the concept of "scaling law crossover," where methods that perform well at smaller scales may not generalize to larger ones.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">03:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15278" target="_blank">@arXiv 2409.15278</a>
                    <span class="tweet-title">PixWizard:  The Image-to-Image AI That Speaks Your Language!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CUHK, Peking University, Shanghai AI Laboratory</span>
                </div>
                <div class="primary-text">
                    This research introduces PixWizard, a visual assistant that can perform a wide range of image generation, manipulation, and translation tasks based on open-ended language instructions. Unlike previous approaches that rely on fixed prompts or pixel-based prompts, PixWizard leverages a unified image-text-to-image generation framework and a large, diverse dataset to understand and execute complex visual tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">04:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14981" target="_blank">@arXiv 2409.14981</a>
                    <span class="tweet-title">Neural Modules:  Specializing for Systematic Generalization, But It's Tricky!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of the Witwatersrand, University College London</span>
                </div>
                <div class="primary-text">
                    This paper introduces a formal definition of systematicity and uses it to analyze the learning dynamics of linear neural modules in a specific space of datasets. It differs from previous work by providing a mathematical framework for studying the interplay between dataset structure and architectural biases in the context of systematic generalization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">04:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14778" target="_blank">@arXiv 2409.14778</a>
                    <span class="tweet-title">Hair Today, Gone Tomorrow: 3D Gaussians Give Hair a New Look!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, Max Planck Institute for Intelligent Systems, Meta...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel dual representation of hair strands using 3D Gaussians, which allows for more accurate and realistic hair reconstruction compared to previous methods that relied solely on 3D polylines.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">05:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15087" target="_blank">@arXiv 2409.15087</a>
                    <span class="tweet-title">AI Eye Doctor:  Helping Humans See Better, Faster!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University, National Institutes of Health</span>
                </div>
                <div class="primary-text">
                    This research goes beyond just showing AI can diagnose eye disease accurately. It actually tests how AI *assists* real doctors in a real-world setting, measuring both accuracy and time saved.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">05:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14464" target="_blank">@arXiv 2409.14464</a>
                    <span class="tweet-title">Hate-Mongers Unmasked:  Social Network Sleuthing for Online Bigots</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Ben-Gurion University of the Negev, University of Michigan</span>
                </div>
                <div class="primary-text">
                    This research focuses on identifying hate-mongers on social platforms by analyzing not just their individual posts, but also their social network connections and the overall distribution of their hateful content. This approach goes beyond simply detecting hateful utterances and aims to understand the broader context of hate speech.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">05:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15241" target="_blank">@arXiv 2409.15241</a>
                    <span class="tweet-title">Domino:  LLM Training Gets a Speed Boost by Hiding Communication!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research proposes Domino, a novel approach that breaks down the data dependency of LLM training into smaller, independent pieces. This allows for pipelined execution, enabling communication to be overlapped with computation, unlike previous methods that primarily focused on kernel fusion.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">06:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14740" target="_blank">@arXiv 2409.14740</a>
                    <span class="tweet-title">Toxicraft:  Turning Toxic Tweets into Training Data with GPT-4!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel framework called Toxicraft, which uses GPT-4 to generate synthetic data for harmful content detection. Unlike previous work that relies on zero-shot prompting, Toxicraft incorporates attribute extraction and context-based enhancements to create more diverse and realistic synthetic data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">06:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14980" target="_blank">@arXiv 2409.14980</a>
                    <span class="tweet-title">MMD Flow Gets a Tune-Up:  (De)-regularization for Faster Sampling</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London, Pennsylvania State University, Gatsby Unit...</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new divergence measure called DrMMD, which is a (de)-regularized version of the Maximum Mean Discrepancy (MMD).  DrMMD interpolates between MMD and the χ²-divergence, allowing it to inherit the computational advantages of MMD while achieving near-global convergence properties similar to the χ²-divergence. This is a significant departure from previous work on MMD-based gradient flows, which often struggle with local optima and lack strong convergence guarantees.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">06:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14379" target="_blank">@arXiv 2409.14379</a>
                    <span class="tweet-title">Group Photo Editing Gets a Diffusion Makeover:  Say Goodbye to Awkward Cropping!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanyang Technological University, Adobe Research, Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces GroupDiff, a novel approach to group photo editing that utilizes diffusion models to generate realistic interactions between individuals. Unlike previous methods that focus on single-person editing, GroupDiff tackles the complex dynamics of human interactions in group photos.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">07:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14273" target="_blank">@arXiv 2409.14273</a>
                    <span class="tweet-title">Lidar Panoptic Segmentation:  Seeing the Unseen in Open Worlds!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, Technical University of Munich, University of Macau</span>
                </div>
                <div class="primary-text">
                    This research tackles the challenge of Lidar Panoptic Segmentation (LPS) in open-world settings, where the model encounters previously unseen objects. Unlike prior work that assumes a fixed vocabulary of classes, this paper proposes a method that can segment both known and unknown objects.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">07:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15126" target="_blank">@arXiv 2409.15126</a>
                    <span class="tweet-title">Data Poisoning? Not on My Watch! UTrace Tracks Down the Bad Guys in Private Learning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Northeastern University, ETH Zurich, University of Toronto</span>
                </div>
                <div class="primary-text">
                    This paper introduces UTrace, a framework for user-level traceback of poisoning attacks in privacy-preserving machine learning (PPML). Unlike previous work that focused on sample-level traceback, UTrace identifies the specific data owners responsible for introducing malicious data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">08:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13768" target="_blank">@arXiv 2409.13768</a>
                    <span class="tweet-title">Magika: AI's Magic Trick for Unmasking File Types</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces MAGIKA, a deep learning model for content-type detection, which differs from previous signature-based approaches by automatically inferring content types without relying on human-crafted rules.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">08:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15107" target="_blank">@arXiv 2409.15107</a>
                    <span class="tweet-title">Semantic Segmentation Under Stress:  BRAVO Challenge Tests AI's Limits!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">valeo.ai, Eindhoven University of Technology, Chinese Academy of Sciences...</span>
                </div>
                <div class="primary-text">
                    This research introduces the BRAVO challenge, a new benchmark for evaluating the reliability of semantic segmentation models under real-world conditions. Unlike previous benchmarks, BRAVO focuses on assessing model robustness to perturbations and out-of-distribution scenarios, which are crucial for safety-critical applications like autonomous driving.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">08:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15254" target="_blank">@arXiv 2409.15254</a>
                    <span class="tweet-title">LLMs Need a Tune-Up: Archon Automates Inference-Time Architecture Search</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces Archon, a framework for automatically designing inference-time architectures for LLMs. Unlike previous work that focuses on specific techniques, Archon explores a broader design space, encompassing methods like generation ensembling, multi-sampling, ranking, fusion, critiquing, verification, and unit testing.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">09:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13940" target="_blank">@arXiv 2409.13940</a>
                    <span class="tweet-title">Stop Asking Users Dumb Questions: New Method Learns Feature Costs from Pairwise Comparisons</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel technique for inferring feature modification costs using the Bradley-Terry model. Unlike previous methods that require exhaustive surveys, this approach leverages pairwise comparisons of entire recourses, making it more practical for real-world applications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">09:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15269" target="_blank">@arXiv 2409.15269</a>
                    <span class="tweet-title">Dressed to Impress: AI Reconstructs Humans in Loose Clothing from Videos</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces a layered neural human representation that separates the body and clothing, allowing for more accurate reconstruction of loose garments. Unlike previous methods that rely solely on skeletal deformations, this approach incorporates a virtual bone deformation module to capture the dynamic movements of loose clothing.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">09:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14066" target="_blank">@arXiv 2409.14066</a>
                    <span class="tweet-title">Robots Learn to Grasp Without Grasping: New AI Model Uses Imagination to Master Manipulation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of California  Berkeley, Cornell University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach called KALIE, which fine-tunes pre-trained Vision-Language Models (VLMs) for robotic manipulation without requiring robot data. Instead of directly controlling the robot, KALIE predicts point-based affordance representations based on language instructions and visual observations. This approach leverages the vast knowledge embedded in VLMs and bypasses the need for extensive robot data collection.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">10:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14296" target="_blank">@arXiv 2409.14296</a>
                    <span class="tweet-title">Open-Vocabulary Object Navigation:  Finding Your Keys (Even If You Don't Know What They Look Like!)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Georgia Institute of Technology, Meta</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset and benchmark called HM3D-OVON for open-vocabulary object goal navigation. Unlike previous datasets that limit goal objects to a predefined set of categories, HM3D-OVON allows for the training and evaluation of models with an open set of goals defined through free-form language at test time.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">10:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13730" target="_blank">@arXiv 2409.13730</a>
                    <span class="tweet-title">Science Got a New Test: Can AI Ace K12 Exams?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces VisScience, a benchmark dataset for evaluating multi-modal large language models (MLLMs) in scientific reasoning across mathematics, physics, and chemistry. Unlike previous benchmarks that primarily focus on mathematics, VisScience expands the scope to include other key scientific disciplines.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">11:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15012" target="_blank">@arXiv 2409.15012</a>
                    <span class="tweet-title">MixAttention:  Making LLMs  Faster  and  Smaller  Without  Losing  Their  Brains!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Databricks, Stanford University</span>
                </div>
                <div class="primary-text">
                    This research explores a novel architecture called MixAttention, which combines sliding window attention with KV cache sharing across layers. This approach differs from previous work by focusing on optimizing the KV cache, a key component of Transformer models, for faster and more memory-efficient inference.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">11:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14179" target="_blank">@arXiv 2409.14179</a>
                    <span class="tweet-title">Hashing Out Invariance: When Data's Identity Doesn't Matter</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This paper explores the concept of lexical invariance in multisets and graphs, focusing on functions that remain unchanged even when the input elements are transformed through injective hashing. It differs from previous work by providing formal definitions and theorems characterizing the necessary and sufficient conditions for most expressive lexical invariant functions in these data structures.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">11:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14925" target="_blank">@arXiv 2409.14925</a>
                    <span class="tweet-title">DanceCamAnimator:  AI Choreographs the Perfect Camera Moves for Your Next Dance Video!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, University of Rochester</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel three-stage framework for synthesizing 3D dance camera movements that incorporates knowledge from the animation industry. Unlike previous methods that treat all frames equally, this approach leverages keyframes to better capture the dynamic nature of dance camera movements.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">12:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14113" target="_blank">@arXiv 2409.14113</a>
                    <span class="tweet-title">MRI Reconstruction Gets a Speed Boost with Frequency-Spatial Mutual Learning!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method called FSMNet for Multi-Contrast MR Reconstruction (MCMR). Unlike previous methods that rely solely on convolutional neural networks (CNNs) or transformers, FSMNet leverages the global properties of the Fourier transform to capture long-range dependencies within each modality. This approach allows for more efficient feature extraction and integration across different modalities, leading to improved reconstruction accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">12:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13761" target="_blank">@arXiv 2409.13761</a>
                    <span class="tweet-title">LLMs Need a CDN for Knowledge?  KDN to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The University of Chicago</span>
                </div>
                <div class="primary-text">
                    This paper proposes a Knowledge Delivery Network (KDN) to manage and deliver knowledge in the form of KV caches to LLMs, improving both modularity and efficiency compared to traditional methods like fine-tuning and in-context learning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">13:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14000" target="_blank">@arXiv 2409.14000</a>
                    <span class="tweet-title">Sentiment Analysis Gets a Graph Makeover:  Syntactic Structure Makes the Difference!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Columbia University, Southwest Jiaotong University, San Francisco State University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel hybrid graph neural network model that incorporates syntactic structure into sentiment analysis. Unlike previous approaches that rely solely on word embeddings, this model leverages the relationships between words within a sentence, represented as a graph, to improve sentiment classification accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">13:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13992" target="_blank">@arXiv 2409.13992</a>
                    <span class="tweet-title">Stop the Lies! New AI Tool Fights Fake News with Matrix Magic</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces SMART, a novel unsupervised method for context selection in Retrieval-Augmented Generation (RAG) systems. Unlike previous methods, SMART leverages Determinantal Point Processes (DPPs) to model not only relevance and diversity but also conflict relations between retrieved contexts. This allows SMART to select contexts that are not only relevant and diverse but also free from contradictions, improving the factual accuracy and coherence of generated answers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">13:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14206" target="_blank">@arXiv 2409.14206</a>
                    <span class="tweet-title">AI Astronaut Assistant:  Chatting with a Knowledge Graph in Space!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">German Aerospace Center, European Space Agency, University of Washington</span>
                </div>
                <div class="primary-text">
                    This research proposes a new type of AI assistant for astronauts that combines a generative pre-trained transformer (GPT) with a retrieval-augmented generation (RAG) system and a knowledge graph (KG). This approach aims to improve the reliability and flexibility of the assistant by providing access to a structured knowledge base and allowing for natural language interaction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">14:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13851" target="_blank">@arXiv 2409.13851</a>
                    <span class="tweet-title">Symmetry-Savvy AI:  Unlocking the Secrets of Crystalline Order!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research explores the ability of graph convolutional neural networks (GCNNs) to differentiate between atomic orderings in crystalline materials. Unlike previous work that focused on composition-dependent properties, this study benchmarks both symmetry-invariant and symmetry-equivariant GCNNs to understand their capability in capturing ordering-dependent energetics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">14:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14165" target="_blank">@arXiv 2409.14165</a>
                    <span class="tweet-title">Can ChatGPT Drive? LLMs Take the Wheel in Autonomous Driving!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research explores the potential of large language models (LLMs) in autonomous driving (AD), focusing on how LLMs can address challenges in both modular and end-to-end AD approaches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">14:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14177" target="_blank">@arXiv 2409.14177</a>
                    <span class="tweet-title">LLMs:  Escape the Security Maze with PathSeeker!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Beihang University, Monash University, Huazhong University of Science and Technology...</span>
                </div>
                <div class="primary-text">
                    PathSeeker uses multi-agent reinforcement learning to guide smaller models in attacking a target LLM, unlike previous methods that rely on reference answers from proxy models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">15:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15095" target="_blank">@arXiv 2409.15095</a>
                    <span class="tweet-title">Teleoperating Robots:  No Fancy Gear, Just a Joystick!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Freiburg, Toyota Motor Europe, NVIDIA...</span>
                </div>
                <div class="primary-text">
                    This research introduces MoMa-Teleop, a teleoperation method that simplifies controlling mobile manipulators by delegating base motion control to a reinforcement learning agent, allowing the operator to focus solely on end-effector motions. This approach differs from previous methods that require specialized hardware or restrict the operator's movement.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">15:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13959" target="_blank">@arXiv 2409.13959</a>
                    <span class="tweet-title">One Model to Rule Them All: GNNs Conquer Complex Knowledge Graph Queries</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford, Intel</span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to query answering over knowledge graphs, focusing on classification rather than ranking. It proposes ANYCQ, a graph neural network model that can handle any conjunctive query, unlike previous methods that often struggle with complex structures.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">16:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14119" target="_blank">@arXiv 2409.14119</a>
                    <span class="tweet-title">Backdoor Buster:  New Defense Makes LLMs Forget Their Bad Habits</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Korea Advanced Institute of Science and Technology</span>
                </div>
                <div class="primary-text">
                    This research focuses on defending parameter-efficient fine-tuning (PEFT) against task-agnostic backdoors, a type of attack that manipulates the output representations of large language models (LLMs) to compromise downstream tasks. Unlike previous defenses, which often require extra predictions or auxiliary models, this method integrates directly into the PEFT training process, adding two loss terms to amplify benign neurons and penalize trigger tokens.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">16:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14454" target="_blank">@arXiv 2409.14454</a>
                    <span class="tweet-title">Power System Dynamics:  A Unified Approach to Learning the Ins and Outs of Generators and Inverters</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Texas at Austin</span>
                </div>
                <div class="primary-text">
                    This research proposes a unified approach for learning the dynamics of power system components, including both synchronous generators (SGs) and inverter-based resources (IBRs).  The key innovation lies in the development of a Stable Integral Recurrent Neural Network (SI-RNN) model that mimics high-order numerical integration methods, enabling more accurate and stable predictions of fast transients, particularly those associated with IBRs. This approach differs from previous work by focusing on learning individual component dynamics rather than modeling the entire system, allowing for greater flexibility and scalability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">16:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14204" target="_blank">@arXiv 2409.14204</a>
                    <span class="tweet-title">Motion Correction Without Retraining: A Deep Learning Miracle for Medical Images!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This research introduces a motion correction framework called UniMo that can be trained on a single image modality and then applied to multiple unseen modalities without requiring retraining. This is different from previous methods that typically require retraining for each new modality.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">17:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14993" target="_blank">@arXiv 2409.14993</a>
                    <span class="tweet-title">Multi-Modal AI: Can One Model Rule Them All?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research explores the possibility of creating a unified multi-modal generative AI model that can both understand and generate visual content, unlike previous models that specialize in either understanding or generation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">17:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13714" target="_blank">@arXiv 2409.13714</a>
                    <span class="tweet-title">LLMs:  The New Code Chefs,  Serving Up Interpretability Testbeds!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Independent, Apollo Research</span>
                </div>
                <div class="primary-text">
                    This research introduces TracrBench, a dataset of RASP programs and their corresponding transformer weights, generated using LLMs. This differs from previous work by automating the creation of interpretability testbeds, which was previously a manual and time-consuming process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">17:58</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14972" target="_blank">@arXiv 2409.14972</a>
                    <span class="tweet-title">Warehouse Robots Learn to Avoid People (and Not Be Creepy)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">NYU, CMU, UC Berkeley...</span>
                </div>
                <div class="primary-text">
                    This research introduces a deep reinforcement learning-based obstacle avoidance algorithm for warehouse robots that incorporates pedestrian interaction information and comfort requirements. Unlike previous methods that rely on trajectory prediction or social force models, this approach uses an angular pedestrian grid and an attention mechanism to learn pedestrian behavior more accurately.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">18:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14705" target="_blank">@arXiv 2409.14705</a>
                    <span class="tweet-title">Language Models on a Diet:  Sampling Smarter, Not Harder!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta, Virginia Tech, Iowa State University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for selecting training data for language models by using multi-granular tokens (subwords, words, and multi-words) as features in importance sampling. This approach aims to reduce domain biases and improve model performance in specific tasks while maintaining generalizability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">18:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14019" target="_blank">@arXiv 2409.14019</a>
                    <span class="tweet-title">Monocular Vision:  Seeing the World in 3D, One Eye at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National University of Defense Technology</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new method called MOSE for reconstructing 3D scenes with semantic labels using only monocular images and noisy 2D priors. Unlike previous methods that rely on dense 3D scans or coarse semantic classes, MOSE leverages generic image segments to improve the local consistency of semantic predictions and uses a semantically-weighted geometric regularization term to enhance the accuracy of surface reconstruction in texture-less regions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">19:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13902" target="_blank">@arXiv 2409.13902</a>
                    <span class="tweet-title">Ophthalmology AI Gets a Reality Check:  Can LLMs See Past Their Own Hallucinations?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National University of Singapore, Yale University</span>
                </div>
                <div class="primary-text">
                    This research focuses on evaluating the effectiveness of Retrieval Augmented Generation (RAG) in improving the accuracy of Large Language Models (LLMs) for long-form consumer health question answering in ophthalmology. Unlike previous studies that primarily assessed content accuracy, this study delves into the factuality of evidence, selection and ranking of evidence, and evidence attribution.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">19:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13941" target="_blank">@arXiv 2409.13941</a>
                    <span class="tweet-title">Photomosaics Get Chatty: AI Turns Images into Interactive Q&A Experiences</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel interactive photomosaic system that combines image composition with a multimodal custom GPT for Q&A interactions. Unlike previous photomosaic work, this approach allows users to click on tiles within the mosaic to view the original images and ask questions about them.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">19:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13913" target="_blank">@arXiv 2409.13913</a>
                    <span class="tweet-title">Word Timing Without the Lexicon: A New Approach to ASR Word Boundaries</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for estimating word boundaries in Automatic Speech Recognition (ASR) systems without relying on lexicons, which are traditionally used to map words to their corresponding phonetic representations. This approach leverages word embeddings derived from sub-word token units and a pretrained ASR model, requiring only word alignment information during training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">20:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14175" target="_blank">@arXiv 2409.14175</a>
                    <span class="tweet-title">Telecom Talk: LLMs Get a Shuffle for Better Answers</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research focuses on enhancing the performance of smaller, open-source language models for answering multiple-choice questions in the telecommunications domain. It introduces a novel technique called "option batch-shuffling" to mitigate selection bias, a common issue in LLMs when dealing with MCQs. This approach differs from previous work by focusing on improving the accuracy of smaller models rather than relying solely on large, proprietary models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">20:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14433" target="_blank">@arXiv 2409.14433</a>
                    <span class="tweet-title">DARTS' Degeneration Problem? Operation Strength to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Xi’an Jiaotong University, Alibaba, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research focuses on improving the architecture selection process in Differentiable Architecture Search (DARTS) by introducing a new criterion called "operation strength." Unlike previous work that primarily focused on improving the supernet optimization, this paper proposes a novel method for selecting the most impactful operations based on their effect on the final loss.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">21:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14655" target="_blank">@arXiv 2409.14655</a>
                    <span class="tweet-title">FedGCN's New Trick:  Smart Sampling for Speedy Graph Learning!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University, Nanyang Technological University, University of Alberta...</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new approach called Federated Adaptive Importance-based Sampling (FedAIS) for training graph convolutional networks (GCNs) in a federated learning (FL) setting. FedAIS addresses the challenges of high computation and communication costs in FedGCN by leveraging historical embeddings and dynamically selecting important nodes for training. This differs from previous methods that either ignore graph structural information or rely on periodic embedding synchronization, which can lead to inaccurate node embeddings and suboptimal performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">21:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13697" target="_blank">@arXiv 2409.13697</a>
                    <span class="tweet-title">Baking Prompts into LLMs: A Recipe for Smarter AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Caltech</span>
                </div>
                <div class="primary-text">
                    This paper introduces "Prompt Baking," a technique that converts natural language prompts into permanent weight updates within a large language model (LLM). Unlike traditional prompting, which relies on temporary instructions, Prompt Baking embeds the prompt's influence directly into the model's weights, ensuring consistent behavior without repeated prompting.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">22:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13884" target="_blank">@arXiv 2409.13884</a>
                    <span class="tweet-title">LLMs Chatting It Up:  How Talking to Each Other Makes Them Less Biased</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Adobe Research</span>
                </div>
                <div class="primary-text">
                    This research explores a novel multi-LLM debiasing framework that leverages multiple LLMs in a conversational setting to reduce bias in their outputs. Unlike previous work that focuses on single-LLM approaches, this study investigates both centralized and decentralized communication structures among LLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">22:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13786" target="_blank">@arXiv 2409.13786</a>
                    <span class="tweet-title">Kernel Learning Gets a Physics Makeover:  Solving PDEs with a Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sorbonne University, French Institute for Research in Computer Science and Automation, Paris-Saclay University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method called Physics-Informed Kernel Learning (PIKL) that uses Fourier methods to approximate the kernel associated with a physics-informed machine learning problem. This approach differs from previous work by providing theoretical guarantees and enabling the quantification of the physical prior's impact on convergence speed.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">23:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14305" target="_blank">@arXiv 2409.14305</a>
                    <span class="tweet-title">UU-Mamba Strikes Again:  A Heart-Smart Segmentation Model with Uncertainty Awareness</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University at Albany SUNY, Purdue University, Harvard University...</span>
                </div>
                <div class="primary-text">
                    This research introduces UU-Mamba, an extension of the U-Mamba architecture, which incorporates a novel uncertainty-aware loss function and the Sharpness-Aware Minimization (SAM) optimizer. This approach aims to improve the model's generalization and robustness, particularly in scenarios with limited annotated data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">23:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14891" target="_blank">@arXiv 2409.14891</a>
                    <span class="tweet-title">Robots Get Eyes:  A New Model for Smarter Manipulation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Zhengzhou University, Beijing University of Posts and Telecommunications, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a task-driven asynchronous active vision-action model for robotic manipulation. Unlike previous models that rely on fixed cameras, this model allows the robot to actively adjust its viewpoint to better observe the environment, similar to how humans use their eyes to gather information before taking action.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">24:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14063" target="_blank">@arXiv 2409.14063</a>
                    <span class="tweet-title">Federated Learning's Label Imbalance?  Let's Fake It 'Til We Make It!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research tackles label imbalance in federated learning by using generative models to create synthetic data that complements the missing or minority classes at each client. Unlike previous work that focuses on optimizing local updates or global aggregation, this approach directly addresses the underlying issue of misaligned data distributions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">24:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15054" target="_blank">@arXiv 2409.15054</a>
                    <span class="tweet-title">Fisheye Depth:  Seeing the World in a Whole New Way (and Scale)!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Hong Kong University of Science and Technology</span>
                </div>
                <div class="primary-text">
                    This research introduces a self-supervised depth estimation model specifically designed for fisheye cameras. Unlike previous methods, it incorporates real-scale pose information from sensor fusion, eliminating the need for a separate pose network and improving depth prediction accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">24:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13728" target="_blank">@arXiv 2409.13728</a>
                    <span class="tweet-title">Language Models: Rule Extrapolators or Rule Breakers?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research introduces a new type of out-of-distribution (OOD) generalization called "rule extrapolation" specifically for formal languages. It focuses on how language models trained on a set of rules behave when presented with prompts that violate one or more of those rules. This differs from previous work on OOD generalization, which often focuses on data sampled from a different distribution than the training data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">25:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14866" target="_blank">@arXiv 2409.14866</a>
                    <span class="tweet-title">LLMs on Lockdown: Fuzz Testing Cracks AI's Safety Walls</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanyang Technological University, Zhejiang University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel jailbreaking attack framework that utilizes fuzz testing, a technique typically used for software security, to automatically generate malicious prompts for Large Language Models (LLMs). Unlike previous methods that rely on manually crafted templates or existing prompts, this approach starts with an empty seed pool and employs question-dependent mutation strategies to create semantically coherent and concise prompts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">25:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14057" target="_blank">@arXiv 2409.14057</a>
                    <span class="tweet-title">Language Models:  Fact-Checkers or Word-Counters?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research investigates the difference between how language models learn word co-occurrence statistics and true factual associations. It shows that models trained on text with explicit co-occurrence primarily learn statistical patterns, which don't generalize well to reasoning tasks. In contrast, models trained on text with implicit associations learn factual knowledge that can be applied to various reasoning scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon" title="Play from here">25:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14912" target="_blank">@arXiv 2409.14912</a>
                    <span class="tweet-title">Tabular Data Preprocessing:  FPGA to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research proposes Piper, a hardware accelerator specifically designed for tabular data preprocessing, which is often a bottleneck in machine learning pipelines. Unlike previous work that focused on CPU-based solutions, Piper leverages the parallel processing capabilities of FPGAs to achieve significant speedups.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon" title="Play from here">26:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13953" target="_blank">@arXiv 2409.13953</a>
                    <span class="tweet-title">Speech Recognition Gets a Privacy Makeover:  How Google's New Method Keeps Your Voice Safe</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research applies differential privacy (DP) to the pre-training stage of large speech recognition (ASR) models, a novel approach compared to previous work that focused on DP during fine-tuning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon" title="Play from here">26:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14617" target="_blank">@arXiv 2409.14617</a>
                    <span class="tweet-title">Protein-Mamba:  A Two-Step Dance to Predict Protein Function!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Rensselaer Polytechnic Institute, Stanford University, University of Minnesota Twin Cities...</span>
                </div>
                <div class="primary-text">
                    This research introduces Protein-Mamba, a two-stage model that combines pre-training on unlabeled data with fine-tuning on labeled data to improve protein function prediction. This approach differs from previous work by leveraging both unlabeled and labeled data, potentially reducing the need for extensive labeled datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet67">
            <div class="start-time-icon" title="Play from here">26:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13741" target="_blank">@arXiv 2409.13741</a>
                    <span class="tweet-title">LLMs Learn to Ask: Data Commons Makes AI Smarter</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research focuses on bridging the gap between LLMs and real-world data by integrating them with Data Commons, a vast repository of public statistics. Unlike previous work that primarily focused on tool-use or retrieval augmented generation, this paper explores two novel methods: Retrieval Interleaved Generation (RIG) and Retrieval Augmented Generation (RAG).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet68">
            <div class="start-time-icon" title="Play from here">27:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14012" target="_blank">@arXiv 2409.14012</a>
                    <span class="tweet-title">Time Series Forecasting:  A Test-Time Training Revolution!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">NYU, IBM</span>
                </div>
                <div class="primary-text">
                    This research introduces Test-Time Training (TTT) modules into a time series forecasting model, specifically the TimeMachine architecture. TTT modules are linear RNNs that dynamically update their weights during testing, allowing them to capture long-range dependencies more effectively than traditional methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet69">
            <div class="start-time-icon" title="Play from here">27:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13997" target="_blank">@arXiv 2409.13997</a>
                    <span class="tweet-title">AI Learns Like a Brain: Drifting to Remember Everything!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, University of Minnesota</span>
                </div>
                <div class="primary-text">
                    This research introduces DriftNet, a lifelong learning framework that leverages representational drift, a phenomenon observed in biological brains, to mitigate catastrophic forgetting in artificial neural networks. Unlike previous methods that rely on regularization, replay, or architectural changes, DriftNet introduces noise to the network's weights, encouraging exploration of diverse local minima in the loss landscape. This continuous exploration helps prevent overwriting previously learned knowledge and enables efficient retrieval of relevant information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet70">
            <div class="start-time-icon" title="Play from here">27:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13919" target="_blank">@arXiv 2409.13919</a>
                    <span class="tweet-title">AI's Got a Bad Case of the "Oops": New Metrics Measure How AI Systems Make Mistakes</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research introduces two new metrics, Misclassification Agreement (MA) and Class-Level Error Similarity (CLES), to measure how similar two AI systems are in their error patterns. Unlike previous work that focused on whether systems make errors together, these new metrics delve into the *type* of errors made.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet71">
            <div class="start-time-icon" title="Play from here">28:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14026" target="_blank">@arXiv 2409.14026</a>
                    <span class="tweet-title">Steering Language Models Towards Thought: A Vector-Based Approach</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research explores the use of "steering vectors" to guide language models towards Chain-of-Thought (CoT) reasoning, an approach that differs from traditional methods like fine-tuning or prompt engineering.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet72">
            <div class="start-time-icon" title="Play from here">28:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15172" target="_blank">@arXiv 2409.15172</a>
                    <span class="tweet-title">Robot Chefs Get a Recipe for Success: Internet Data Makes Cooking Skills a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research explores using internet data to select pre-programmed robot behaviors, or "templates," for complex tasks like cooking. Unlike previous work that focused on learning skills from scratch, this study leverages existing templates and uses internet data to choose the best one for the job.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet73">
            <div class="start-time-icon" title="Play from here">29:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14552" target="_blank">@arXiv 2409.14552</a>
                    <span class="tweet-title">Emojis Get Smart: Graph Pre-training Makes Them Understand Us Better</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Fudan University, University of Chicago</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel graph pre-training framework for text and emoji co-modeling. Unlike previous methods that either ignore emojis or treat them as regular characters, this approach constructs a heterogeneous graph to capture the relationships between posts, words, and emojis. This allows the model to learn richer semantic information about emojis and their interactions with text.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet74">
            <div class="start-time-icon" title="Play from here">29:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13972" target="_blank">@arXiv 2409.13972</a>
                    <span class="tweet-title">Chatbots Got Brains, But Can They Use 'Em? New Study Uncovers Language Model's Hidden Knowledge Gap</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, University of Waterloo</span>
                </div>
                <div class="primary-text">
                    This research delves into the discrepancy between how language models understand word semantics internally (based on their hidden representations) and how they express that understanding externally through prompts and queries. Unlike previous work focusing on sentence-level understanding, this study specifically examines word-level semantics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet75">
            <div class="start-time-icon" title="Play from here">29:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14316" target="_blank">@arXiv 2409.14316</a>
                    <span class="tweet-title">3D Gaussian Splatting:  A Multi-View Feast for Sparse Scenes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Peking University Shenzhen Graduate School</span>
                </div>
                <div class="primary-text">
                    This paper proposes MVPGS, a novel method for few-shot novel view synthesis that leverages multi-view stereo (MVS) to enhance the quality of geometric initialization for 3D Gaussian Splatting. Unlike previous methods that rely on implicit representations, MVPGS utilizes an explicit point-based representation, enabling real-time rendering.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet76">
            <div class="start-time-icon" title="Play from here">30:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14327" target="_blank">@arXiv 2409.14327</a>
                    <span class="tweet-title">Time Series Data:  Unraveling the Secrets of Multidimensional Motion!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Trine University, Carnegie Mellon University, Emory University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for representing the spatiotemporal structure of multidimensional time series data, focusing on extracting non-redundant, variable-length tuples as spatiotemporal features. This approach differs from previous methods by converting multidimensional time series into one-dimensional sequences of spatially evolving events, thereby preserving the intricate original structure and richness of the data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet77">
            <div class="start-time-icon" title="Play from here">30:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14293" target="_blank">@arXiv 2409.14293</a>
                    <span class="tweet-title">EVs on the Move:  A New Way to Distribute Power in Smart Grids</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Technical University of Munich, Nanyang Technological University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel load distribution strategy for aggregators that leverages the mobility of devices like electric vehicles (EVs). Unlike previous work that focused on time-shifting loads, this study considers both time and location flexibility, allowing EVs to move between charging stations to optimize load balancing.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet78">
            <div class="start-time-icon" title="Play from here">31:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14572" target="_blank">@arXiv 2409.14572</a>
                    <span class="tweet-title">LLMs in Materials Science:  Can They Handle the Heat?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research goes beyond simply evaluating LLMs in materials science. It delves into their robustness, testing how they handle various types of "noise" in prompts, including intentional misinformation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet79">
            <div class="start-time-icon" title="Play from here">31:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14302" target="_blank">@arXiv 2409.14302</a>
                    <span class="tweet-title">LLMs:  Med School Dropouts?  New Test Shows They're Not So Smart After All!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new evaluation method called Predicate-text Dual Transformation (PretextTrans) to assess LLMs' mastery of medical factual knowledge. Unlike previous methods that rely on static benchmarks, PretextTrans dynamically generates multiple test samples for each knowledge point, ensuring greater diversity and reliability in the evaluation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet80">
            <div class="start-time-icon" title="Play from here">31:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14474" target="_blank">@arXiv 2409.14474</a>
                    <span class="tweet-title">SynBench:  A Point Cloud Registration Playground for Deformable Objects!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This research introduces SynBench, a new dataset for non-rigid point cloud registration that includes various challenges like deformation levels, noise, outliers, and incompleteness, which are not comprehensively addressed in existing datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet81">
            <div class="start-time-icon" title="Play from here">32:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14893" target="_blank">@arXiv 2409.14893</a>
                    <span class="tweet-title">Sparsifying Gradients:  A Bayesian Twist on TOP-k!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, Ericsson</span>
                </div>
                <div class="primary-text">
                    This research proposes a new gradient sparsification algorithm called REGTOP-k, which controls the learning rate scaling of error accumulation. Unlike previous work, it utilizes a Bayesian framework to determine the optimal sparsification mask.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet82">
            <div class="start-time-icon" title="Play from here">32:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14371" target="_blank">@arXiv 2409.14371</a>
                    <span class="tweet-title">Can AI Grade Your Homework? New Benchmark Tests LLMs' Ability to Evaluate Constraints</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel dataset called ACS (Arithmetic Constraint-Satisfaction) to benchmark LLMs on their ability to evaluate constraint satisfaction in agent responses to open-ended requests. Unlike previous datasets that focus on specific capabilities like arithmetic reasoning or question-answering, ACS specifically tests LLMs' ability to reason, extract data, perform calculations, and count within the context of a complex, multi-step task.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet83">
            <div class="start-time-icon" title="Play from here">33:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15267" target="_blank">@arXiv 2409.15267</a>
                    <span class="tweet-title">Wide Neural Networks:  A Peek Inside the Distributed Learning Brain</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, Georgia Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This research uses Neural Tangent Kernel (NTK) theory to analyze the training dynamics of wide neural networks in peer-to-peer learning settings. Unlike previous work that focused on linear models, this study provides an explicit characterization of the learning dynamics for nonconvex neural networks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet84">
            <div class="start-time-icon" title="Play from here">33:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13926" target="_blank">@arXiv 2409.13926</a>
                    <span class="tweet-title">VR Telepresence Gets a Makeover: Blending Your Real World into Virtual Meetings!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft, University College London, University of Michigan</span>
                </div>
                <div class="primary-text">
                    This research introduces SpaceBlender, a pipeline that blends users' physical surroundings into unified virtual environments for VR telepresence. Unlike previous work that focuses on generating entirely synthetic environments or grounding in a single space, SpaceBlender leverages multiple user-provided images to create a cohesive, context-rich space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet85">
            <div class="start-time-icon" title="Play from here">33:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14633" target="_blank">@arXiv 2409.14633</a>
                    <span class="tweet-title">Robot Navigation:  A Few-Shot Learning Trick for Smarter Cars!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of California  Berkeley, University of California  San Diego, University of California  Los Angeles...</span>
                </div>
                <div class="primary-text">
                    This research proposes a hierarchical navigation system that uses few-shot learning for waypoint detection. Unlike previous methods that rely on extensive training data or precise localization sensors, this approach enables a robot to navigate in a new environment with only a few example images of landmarks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet86">
            <div class="start-time-icon" title="Play from here">34:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15261" target="_blank">@arXiv 2409.15261</a>
                    <span class="tweet-title">Comet Hunting with AI:  A New Way to Spot Fuzzy Space Rocks</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Goddard Space Flight Center, University of Minnesota</span>
                </div>
                <div class="primary-text">
                    This research focuses on using convolutional neural networks (CNNs) to identify comets in astronomical survey data, a departure from traditional methods that rely on analyzing individual detections or linking multiple images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet87">
            <div class="start-time-icon" title="Play from here">34:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14368" target="_blank">@arXiv 2409.14368</a>
                    <span class="tweet-title">AI Tutors Code: Can LLMs Teach Novices to Program?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Illinois, University of Pittsburgh, CMU</span>
                </div>
                <div class="primary-text">
                    This research focuses on evaluating the instructional quality of code comments generated by LLMs for novice programmers, specifically comparing their effectiveness to expert-developed comments. Previous studies have explored the feasibility of using LLMs for code comment generation, but this study delves deeper into the educational value of these comments for beginners.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet88">
            <div class="start-time-icon" title="Play from here">35:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13758" target="_blank">@arXiv 2409.13758</a>
                    <span class="tweet-title">AI Songwriters:  Can Machines Write Hits?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research focuses on generating lyrics that fit specific music genres using deep learning models. Unlike previous work that primarily focused on rap lyrics, this study explores a broader range of genres, including pop, rock, and R&B.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet89">
            <div class="start-time-icon" title="Play from here">35:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13881" target="_blank">@arXiv 2409.13881</a>
                    <span class="tweet-title">Deep Learning for Underwater Sounds:  Mixing It Up for Better Ship ID!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Texas A&M University, MIT</span>
                </div>
                <div class="primary-text">
                    This research explores the impact of combining different types of acoustic features on the performance of a Histogram Layer Time Delay Neural Network (HLTDNN) for underwater acoustic target recognition (UATR). Unlike previous work that focused on single features, this study investigates the synergistic effects of combining multiple features, specifically six time-frequency features, to improve classification accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet90">
            <div class="start-time-icon" title="Play from here">35:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14677" target="_blank">@arXiv 2409.14677</a>
                    <span class="tweet-title">Mirror, Mirror, on the Wall, Who's Got the Most Realistic Reflection of All?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IISc Bangalore, Samsung R & D Institute India, University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research tackles the challenge of generating realistic mirror reflections using diffusion models by framing it as an image inpainting task. Unlike previous methods that struggle with geometric consistency, this approach leverages depth maps to guide the model, resulting in more accurate and controlled reflections.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet91">
            <div class="start-time-icon" title="Play from here">36:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14545" target="_blank">@arXiv 2409.14545</a>
                    <span class="tweet-title">Zombies Hate This New Theory of Consciousness!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Australian National University, University College London</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new mathematical formalism for consciousness that starts with the embodied organism and its interactions with the environment, rather than abstract mental states. It argues that phenomenal consciousness precedes access consciousness, making zombies impossible.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet92">
            <div class="start-time-icon" title="Play from here">36:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13852" target="_blank">@arXiv 2409.13852</a>
                    <span class="tweet-title">LLMs: Preaching Inclusivity, Practicing Bias?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research examines the language ideologies encoded in LLMs by studying their use of gendered language reform, specifically focusing on the use of gender-neutral variants like "congressperson" and singular "they." It goes beyond simply assessing the models' output and investigates their metalinguistic preferences, which are statements about language itself.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet93">
            <div class="start-time-icon" title="Play from here">36:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15005" target="_blank">@arXiv 2409.15005</a>
                    <span class="tweet-title">Budgeting for the People: A New Voting Rule That's Fairer Than Fair</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Warsaw, University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research introduces the Method of Equal Shares with Bounded Overspending (BOS), a new voting rule for participatory budgeting (PB) elections. BOS aims to balance proportionality and efficiency by allowing voters to occasionally overspend their initial budget allocation. This approach addresses the inefficiencies inherent in strict proportionality guarantees while still providing good proportionality similar to the original Method of Equal Shares.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet94">
            <div class="start-time-icon" title="Play from here">37:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13964" target="_blank">@arXiv 2409.13964</a>
                    <span class="tweet-title">Robots Get Opinionated: New Algorithm Makes Them Team Players!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Cornell University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new task allocation model that combines nonlinear opinion dynamics (NOD) with an evolutionary game framework. Unlike previous work where the bias parameter in NOD was static or heuristically tuned, this paper introduces an adaptive bias that can be controlled to achieve a desired allocation of agents.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet95">
            <div class="start-time-icon" title="Play from here">37:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15250" target="_blank">@arXiv 2409.15250</a>
                    <span class="tweet-title">Robot Vision:  Forget What You Learned, Remember What You Knew!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">INSAIT, Sofia University</span>
                </div>
                <div class="primary-text">
                    This research focuses on the visual generalization capabilities of robotic foundation models, specifically addressing the issue of catastrophic forgetting in vision encoders during training. The paper proposes a novel approach called ReVLA, which reverts the vision encoders back to their original pretrained states, enabling the models to regain their ability to generalize to unseen objects and environments.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet96">
            <div class="start-time-icon" title="Play from here">38:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14553" target="_blank">@arXiv 2409.14553</a>
                    <span class="tweet-title">Virtual Try-On for Watches:  GlamTry Makes Your Wrist Dreams Come True!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research focuses on extending virtual try-on technology to accessories, specifically watches, by customizing and retraining a model using accessory-specific data and network architecture modifications. This differs from previous work that primarily focused on clothing items.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet97">
            <div class="start-time-icon" title="Play from here">38:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13756" target="_blank">@arXiv 2409.13756</a>
                    <span class="tweet-title">"Party's Over, Metadata Wins: Simple Model, Big Results in Political Stance Detection"</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research explores the effectiveness of incorporating metadata, specifically party affiliation and policy information, into political stance detection models. Unlike previous work that focused on complex models and feature engineering, this study demonstrates that a simple Bayesian model using only metadata outperforms existing state-of-the-art models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet98">
            <div class="start-time-icon" title="Play from here">38:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13705" target="_blank">@arXiv 2409.13705</a>
                    <span class="tweet-title">Bias Busters: How to Make AI Safety Classifiers Fairer</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel post-processing method for mitigating counterfactual fairness in closed-source text safety classifiers. Unlike previous work that focuses on data reweighting during training, this approach builds an ensemble model on top of existing classifiers, acting as a debiasing regularizer.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet99">
            <div class="start-time-icon" title="Play from here">39:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14827" target="_blank">@arXiv 2409.14827</a>
                    <span class="tweet-title">Mouse-Tracking Madness: A New Dataset for Video Saliency Prediction</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Moscow State University, University of Würzburg</span>
                </div>
                <div class="primary-text">
                    This research introduces a new large-scale dataset called AViMoS, which uses crowdsourced mouse tracking to collect saliency data. This differs from previous work that relied on eye-tracking data, which is more expensive and time-consuming to collect.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet100">
            <div class="start-time-icon" title="Play from here">39:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14575" target="_blank">@arXiv 2409.14575</a>
                    <span class="tweet-title">Battery Health Check:  New Tricks for Predicting EV Battery Life</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Polytechnic University of Turin</span>
                </div>
                <div class="primary-text">
                    This research proposes five new health indicators for lithium-ion batteries, derived from real-world electric vehicle operation data. Unlike previous studies that focused on idealized charging profiles, these indicators are designed to be more accurate and reliable in real-world scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet101">
            <div class="start-time-icon" title="Play from here">39:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14556" target="_blank">@arXiv 2409.14556</a>
                    <span class="tweet-title">RACOON:  Giving LLMs a Knowledge Graph Boost for Column Type Annotation!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington</span>
                </div>
                <div class="primary-text">
                    This research focuses on improving Column Type Annotation (CTA) by integrating a Knowledge Graph (KG) with Large Language Models (LLMs). Unlike previous work that fine-tunes LLMs for CTA, this approach leverages LLMs' zero-shot capabilities and augments their prompts with information retrieved from a KG.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet102">
            <div class="start-time-icon" title="Play from here">40:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14363" target="_blank">@arXiv 2409.14363</a>
                    <span class="tweet-title">MANTA:  AI Art's New BFF for Budget-Friendly, Diverse Images</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research introduces MANTA, a system that addresses the model-adapter composition problem by factoring in hardware and affordability constraints. Unlike previous work that focuses solely on adapter selection, MANTA considers both checkpoint and adapter selection, aiming to improve image diversity and quality while minimizing token usage.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet103">
            <div class="start-time-icon" title="Play from here">40:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14689" target="_blank">@arXiv 2409.14689</a>
                    <span class="tweet-title">Recommending Movies with Diffusion: A Matrix of Ratings, Not Just Likes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to recommender systems by applying diffusion models directly to the weighted interaction matrix of user-item ratings, incorporating user and item features to guide the denoising process. This differs from previous work that focused on one-hot encoded interaction vectors or node embeddings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet104">
            <div class="start-time-icon" title="Play from here">40:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14599" target="_blank">@arXiv 2409.14599</a>
                    <span class="tweet-title">Flow Fusion:  Generative Models Get a Speed Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This paper introduces Implicit Dynamical Flow Fusion (IDFF), a new method for generative modeling that incorporates a momentum term into the vector field of conditional flow models. This allows for longer sampling steps during generation, reducing the number of function evaluations (NFEs) needed by a factor of ten compared to traditional conditional flow matching (CFM) models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet105">
            <div class="start-time-icon" title="Play from here">41:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13951" target="_blank">@arXiv 2409.13951</a>
                    <span class="tweet-title">Deep Learning:  Seeing Tiny Details in AR/VR Devices</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta</span>
                </div>
                <div class="primary-text">
                    This research fine-tunes a pre-trained deep learning model, Segment Anything Model (SAM), using a diverse dataset of electron microscopy images. This approach allows for faster and more accurate segmentation of regions of interest (ROIs) compared to traditional methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet106">
            <div class="start-time-icon" title="Play from here">41:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.14791" target="_blank">@arXiv 2409.14791</a>
                    <span class="tweet-title">Scattered Data Interpolation:  A Multiscale Approach with Samplet Compression</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, University of Göttingen, University of Innsbruck...</span>
                </div>
                <div class="primary-text">
                    This research introduces a multiscale interpolation scheme for globally supported radial basis functions (RBFs) using samplet compression. This approach differs from previous work by leveraging samplets to compress the generalized Vandermonde matrices, resulting in a quasi-sparse representation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet107">
            <div class="start-time-icon" title="Play from here">42:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.15119" target="_blank">@arXiv 2409.15119</a>
                    <span class="tweet-title">Fake Images:  Log-Normal Mutations Are the New Black!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta, Leiden University, ESIEE Paris</span>
                </div>
                <div class="primary-text">
                    This research explores the use of log-normal mutations, a technique typically used in discrete optimization, for attacking fake image detectors. This approach differs from previous work that focused on specialized algorithms designed specifically for attacking image classifiers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet108">
            <div class="start-time-icon" title="Play from here">42:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13706" target="_blank">@arXiv 2409.13706</a>
                    <span class="tweet-title">Romanizing Names:  A Tonal Tune for Better Data Linkage</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London, University of Washington</span>
                </div>
                <div class="primary-text">
                    This research explores the impact of tonal representation in romanized Chinese names on data linkage accuracy, highlighting the limitations of non-tonal systems and proposing the use of Jyutping and Pinyin for improved results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet109">
            <div class="start-time-icon" title="Play from here">42:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13929" target="_blank">@arXiv 2409.13929</a>
                    <span class="tweet-title">AI's Got a Blind Spot: Can Multimodal Models See Like We Do?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington</span>
                </div>
                <div class="primary-text">
                    This research applies established cognitive science techniques to assess the perspective-taking abilities of multimodal AI systems, specifically GPT-4o. Unlike previous studies that relied on broader benchmarks, this study uses a targeted set of tasks designed to isolate specific spatial reasoning abilities, allowing for a more precise evaluation of AI performance.
                </div>
            </div>
        </div>

        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202409241531_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>    
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>
</html>