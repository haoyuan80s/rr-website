<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - AI Paper Picks of the Day</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 80px;"></div>
            <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                Freshest
                Top Picks:
                <span class="highlightNumber" style="font-size: 28px;">70</span> out of <span
                    class="highlightNumber">397</span> arXiv AI
                papers
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="line-height: 1.2;">just released on</div>
                <div id="data-default-date" data-date="2024-08-06"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 10px;">
            </div>
        </div>

        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">
                00:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01654" target="_blank">
                        @arXiv 2408.01654
                    </a>
                    <span class="tweet-title">
                        SLAM-tastic! Deep Patch Visual SLAM: One GPU, Two Loop Closures, and No More Backend
                        Bottlenecks!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Princeton University
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces Deep Patch Visual SLAM (DPV-SLAM), a new approach to monocular visual SLAM
                    that uses a single GPU and two loop closure mechanisms to achieve high accuracy and real-time
                    performance. Unlike previous deep SLAM systems, DPV-SLAM doesn't require two GPUs to run the backend
                    and frontend in parallel, making it more efficient and cost-effective.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">
                01:19
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01875" target="_blank">
                        @arXiv 2408.01875
                    </a>
                    <span class="tweet-title">
                        Tool Retrieval: LLMs Get a Helping Hand (and a Brain Boost)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces Re-Invoke, an unsupervised tool retrieval method that leverages LLMs to enrich
                    tool documents and extract user intents, improving retrieval performance without requiring any
                    labeled data. This differs from previous work that often relies on supervised methods or in-context
                    learning, which can be challenging to scale to large toolsets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">
                01:41
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01431" target="_blank">
                        @arXiv 2408.01431
                    </a>
                    <span class="tweet-title">
                        Building a Trustworthy AI Ecosystem for Biomedical Breakthroughs: It's Not Just About the Data,
                        It's About the People!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on building an ethical and trustworthy AI ecosystem for biomedical
                    applications, specifically addressing the challenges of integrating foundational models (FMs) into
                    clinical settings. It goes beyond simply discussing data bias and privacy, delving into the
                    importance of stakeholder engagement, co-design, and continuous AI stewardship throughout the AI
                    lifecycle.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">
                02:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01505" target="_blank">
                        @arXiv 2408.01505
                    </a>
                    <span class="tweet-title">
                        LoRA-MoE Gets a Makeover: MoDE's Dyadic Experts Steal the Show!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a new multi-task parameter-efficient fine-tuning method called MoDE. Unlike
                    previous LoRA-based MoE architectures, MoDE shares the down-projection matrix across experts,
                    reducing parameter redundancy. It also introduces atomic rank-one adapters, enabling more
                    fine-grained task-specific adaptation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">
                02:31
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02582" target="_blank">
                        @arXiv 2408.02582
                    </a>
                    <span class="tweet-title">
                        Accents, Algorithms, and ASR: How to Make Speech Recognition Fairer for Everyone!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on improving accent recognition in speech recognition systems by addressing
                    the issue of imbalanced datasets and unreliable accent labels. It introduces three schemes:
                    pre-training, distributionally robust optimization (DRO), and unsupervised clustering, to enhance
                    the performance of accent recognition models, particularly for under-represented accents.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">
                02:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02110" target="_blank">
                        @arXiv 2408.02110
                    </a>
                    <span class="tweet-title">
                        AvatarPose: Giving AI a Virtual Body to See Human Interaction Better!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich, Max Planck Institute for Intelligent Systems
                    </span>
                </div>
                <div class="primary-text">
                    This research uses personalized 3D avatars as priors to improve 3D pose estimation of people in
                    close interaction, unlike previous methods that rely heavily on 2D joint estimations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">
                03:16
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02666" target="_blank">
                        @arXiv 2408.02666
                    </a>
                    <span class="tweet-title">
                        LLMs Learn to Judge Themselves: No Humans Needed!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Meta
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to training LLM evaluators without relying on
                    human-annotated preference data. Instead, it uses synthetically generated data to iteratively
                    improve the model's ability to judge the quality of responses.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">
                03:46
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01748" target="_blank">
                        @arXiv 2408.01748
                    </a>
                    <span class="tweet-title">
                        Uncovering Hidden Gems: How AI Finds Rare Causal Knowledge in Financial Statements
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        The University of Tokyo, Seikei University, Sumitomo Mitsui Asset Management Company Limited
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a method for extracting rare causal knowledge from Japanese financial
                    statement summaries. Unlike previous work that focuses on extracting general causal knowledge, this
                    method specifically targets rare cause-effect pairs that are less likely to be known or readily
                    available.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">
                04:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01605" target="_blank">
                        @arXiv 2408.01605
                    </a>
                    <span class="tweet-title">
                        LLMs: Phishing for Fun and Profit? Meta's New Cybersecurity Benchmarks Put AI to the Test!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Meta
                    </span>
                </div>
                <div class="primary-text">
                    This research expands upon previous work by introducing new areas focused on offensive security
                    capabilities, including automated social engineering, scaling manual offensive cyber operations, and
                    autonomous offensive cyber operations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">
                04:34
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01633" target="_blank">
                        @arXiv 2408.01633
                    </a>
                    <span class="tweet-title">
                        Self-Emotion: The Secret Sauce for More Human-Like Chatbots?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Tokyo
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the impact of "self-emotion" on dialogue generation, a concept often
                    overlooked in previous work. It investigates how simulated agents' emotional states, influenced by
                    events outside the conversation, affect their dialogue strategies and decision-making.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">
                04:55
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01839" target="_blank">
                        @arXiv 2408.01839
                    </a>
                    <span class="tweet-title">
                        Optimizing for the "α" Factor: A New Twist on Gradient Dominance
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        EPFL, Leiden University, ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces the (α,τ,X)-projected-gradient-dominance property, which extends the
                    gradient-dominance concept to constrained optimization problems. It analyzes the performance of
                    stochastic first-order methods under this property, deriving both lower and upper bounds on their
                    complexity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">
                05:30
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02555" target="_blank">
                        @arXiv 2408.02555
                    </a>
                    <span class="tweet-title">
                        Meshing Around: New Tokenization Makes 3D Models More Efficient!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nanyang Technological University, Tsinghua University, Imperial College London...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new method called Adjacent Mesh Tokenization (AMT) for representing 3D
                    meshes. Unlike previous methods that use three vertices to represent a face, AMT uses only one
                    vertex whenever possible, resulting in a more compact and efficient representation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">
                05:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02496" target="_blank">
                        @arXiv 2408.02496
                    </a>
                    <span class="tweet-title">
                        Hippocampus Flip-Flop: AI Rates Brain Quirks in a Flash!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Sorbonne Université, CNRS, Inria...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method for automatically rating Incomplete Hippocampal Inversion
                    (IHI), a brain structure variation, using deep learning models. Unlike previous studies relying on
                    manual assessment, this approach enables large-scale analysis of IHI across multiple cohorts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">
                06:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02373" target="_blank">
                        @arXiv 2408.02373
                    </a>
                    <span class="tweet-title">
                        AI Assistants: Privacy-Conscious and Form-Filling Fantastic!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This research operationalizes contextual integrity (CI) for AI assistants, focusing on form-filling
                    tasks. Unlike previous work that primarily focused on conversational AI, this paper explores the
                    application of CI in assistive tasks, specifically form-filling.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">
                06:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02408" target="_blank">
                        @arXiv 2408.02408
                    </a>
                    <span class="tweet-title">
                        Weather Woes? No Problem! New AI Denoises Images for Better Geo-Localization
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework called MCGF that uses denoising diffusion models to improve
                    geo-localization in varying weather conditions. Unlike previous methods that focus on specific
                    weather types, MCGF dynamically adapts to unseen weather conditions by jointly optimizing image
                    restoration and geo-localization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">
                07:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01970" target="_blank">
                        @arXiv 2408.01970
                    </a>
                    <span class="tweet-title">
                        Forgetful AI? Not This Time! New System Mimics Human Memory to Learn Continuously.
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harbin Institute of Technology, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new system called SR-CIS that combines fast and slow reasoning with
                    decoupled memory modules, allowing it to learn new tasks without forgetting old ones. Unlike
                    previous methods that rely on data replay or parameter expansion, SR-CIS uses a scenario description
                    pool and memory restructuring to achieve this.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">
                07:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02297" target="_blank">
                        @arXiv 2408.02297
                    </a>
                    <span class="tweet-title">
                        Embodied AI Gets a Reality Check: Uncertainty-Aware Perception for Smarter Robots
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Freiburg, University of Technology Nuremberg
                    </span>
                </div>
                <div class="primary-text">
                    This research addresses the gap between training embodied AI agents with perfect perception and
                    deploying them in real-world scenarios with noisy perception. It introduces uncertainty-aware
                    semantic segmentation, which incorporates calibrated probabilities and uncertainties into the
                    agent's map aggregation and decision-making processes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">
                08:07
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01582" target="_blank">
                        @arXiv 2408.01582
                    </a>
                    <span class="tweet-title">
                        Diffusion Models: Conformal Inference for Individual Treatment Effects
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of California Irvine, University of California San Francisco, University of
                        California Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel approach for estimating individual treatment effects (ITE) using
                    conformal diffusion models. Unlike previous methods that rely on simpler models or strong
                    distributional assumptions, this approach leverages the flexibility of diffusion modeling and the
                    model-free inference paradigm of conformal inference to address the challenges of ITE estimation and
                    inference in observational studies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">
                08:34
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01458" target="_blank">
                        @arXiv 2408.01458
                    </a>
                    <span class="tweet-title">
                        Surveys: The New AI Ethics Minefield?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley, Arizona State University, IBM...
                    </span>
                </div>
                <div class="primary-text">
                    This paper takes a critical look at the use of surveys in AI research, development, and governance.
                    It goes beyond simply highlighting the limitations of surveys, focusing on how they can perpetuate
                    existing power dynamics and potentially harm marginalized communities. The authors argue that
                    surveys should be designed with, not just about, the people they aim to understand.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">
                09:00
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01800" target="_blank">
                        @arXiv 2408.01800
                    </a>
                    <span class="tweet-title">
                        GPT-4 on Your Phone? MiniCPM-V Makes It Possible!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        OpenBMB
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces MiniCPM-V, a series of efficient multimodal large language models (MLLMs)
                    designed for deployment on end-side devices like smartphones. Unlike previous MLLMs that require
                    powerful cloud servers, MiniCPM-V achieves comparable performance with significantly smaller model
                    sizes, making it suitable for resource-constrained environments.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">
                09:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01880" target="_blank">
                        @arXiv 2408.01880
                    </a>
                    <span class="tweet-title">
                        Knowledge Graph Reasoning: Two Agents, One Smart Walk!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        China University of Petroleum, Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces FULORA, a dual-agent framework for knowledge graph reasoning. Unlike
                    previous multi-agent models, FULORA balances guidance from one agent with the other agent's
                    self-exploration, improving long-distance reasoning ability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">
                10:04
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01959" target="_blank">
                        @arXiv 2408.01959
                    </a>
                    <span class="tweet-title">
                        AI's Got a Face for Bias: How Big Data Makes Machines Mirror Our Prejudices
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research investigates whether vision-language AI models, like CLIP, learn human-like facial
                    impression biases. Unlike previous work that focused on supervised learning, this study examines
                    biases in pretrained models, highlighting the impact of dataset scale on bias acquisition.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">
                10:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01966" target="_blank">
                        @arXiv 2408.01966
                    </a>
                    <span class="tweet-title">
                        Bias Unmasked: New Test Reveals Hidden Layers of Prejudice in AI
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces the Multilevel Embedding Association Test (ML-EAT), a method that goes
                    beyond traditional bias measurement by quantifying bias at three levels of granularity. This allows
                    for a more nuanced understanding of how biases manifest in language technologies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">
                11:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01575" target="_blank">
                        @arXiv 2408.01575
                    </a>
                    <span class="tweet-title">
                        Deep Learning Helps Us Match History (and CO2 Storage)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research uses two separate deep learning models to predict different types of data, making the
                    process more efficient than using a single model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">
                11:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02407" target="_blank">
                        @arXiv 2408.02407
                    </a>
                    <span class="tweet-title">
                        Tiny Tech, Big Impact: How a Tiny Device is Revolutionizing Biodiversity Monitoring
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Imperial College London, University of Cambridge
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel on-device reinforcement learning scheduler for event-driven sensor
                    networks, specifically designed for biodiversity monitoring. Unlike previous approaches that rely on
                    centralized coordination or always-on devices, this scheduler operates autonomously on each device,
                    minimizing power consumption and maximizing event detection.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">
                11:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02320" target="_blank">
                        @arXiv 2408.02320
                    </a>
                    <span class="tweet-title">
                        Diffusion Models: A New Theory That's Almost Linear!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        The Chinese University of Hong Kong, University of Pennsylvania, Carnegie Mellon University
                    </span>
                </div>
                <div class="primary-text">
                    This paper improves upon previous work by establishing a nearly linear dimension dependency for the
                    probability flow ODE sampler, a popular deterministic method for generating new data instances. This
                    is achieved by directly analyzing the discrete-time processes without relying on continuous-time
                    limits.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">
                12:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01437" target="_blank">
                        @arXiv 2408.01437
                    </a>
                    <span class="tweet-title">
                        Turning Images into 3D Blueprints: A New AI Trick for CAD Design
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, Peking University, United States Army
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a two-stage approach for reverse engineering 3D CAD models from images.
                    Unlike previous methods that rely on 3D point clouds, this work leverages large language models
                    (LLMs) to predict the discrete structure of a CAD model from a single image.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">
                13:03
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01953" target="_blank">
                        @arXiv 2408.01953
                    </a>
                    <span class="tweet-title">
                        Robots Learn to Open Drawers, No Matter How They're Turned!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework called EqvAfford that leverages SE(3) equivariance for
                    point-level affordance learning in robotic manipulation. Unlike previous methods, EqvAfford ensures
                    that the model's predictions are consistent even when the object is rotated or translated.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">
                13:41
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01679" target="_blank">
                        @arXiv 2408.01679
                    </a>
                    <span class="tweet-title">
                        MMPKUBase: A Chinese Multi-Modal Knowledge Graph That's Got It All!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on building a multi-modal knowledge graph in Chinese, a language with limited
                    high-quality knowledge graphs. It uses a novel image filtering method to ensure the quality of the
                    images incorporated into the graph.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">
                14:00
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02484" target="_blank">
                        @arXiv 2408.02484
                    </a>
                    <span class="tweet-title">
                        Zero-Shot HOI Detection: Prompts Make It Happen!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel framework for zero-shot HOI detection using Conditional Multi-Modal
                    Prompts (CMMP). Unlike traditional prompt-learning methods, CMMP learns decoupled vision and
                    language prompts for interactiveness-aware visual feature extraction and generalizable interaction
                    classification, respectively.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">
                14:27
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02006" target="_blank">
                        @arXiv 2408.02006
                    </a>
                    <span class="tweet-title">
                        Shopping Assistant Gets a Brain: LLMs Make E-Commerce Smarter
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Zhejiang University, Peking University, Harbin Institute of Technology...
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on using Large Language Models (LLMs) to create a multi-task e-commerce
                    shopping assistant. Unlike previous assistants that were task-specific, this one can handle a
                    variety of shopping tasks, including understanding concepts, reasoning about products, and aligning
                    with user behavior. The researchers also created a new dataset called EshopInstruct, which contains
                    65,000 samples specifically designed for training LLMs in e-commerce scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">
                14:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01651" target="_blank">
                        @arXiv 2408.01651
                    </a>
                    <span class="tweet-title">
                        AI Album Art: From Music to Masterpiece, No Design Degree Needed!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU, Ulsan National Institute of Science and Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Music2P, a multi-modal AI system for album cover generation that utilizes
                    various input modalities, including text, image, and audio, unlike previous systems that often rely
                    solely on textual input.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">
                15:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01961" target="_blank">
                        @arXiv 2408.01961
                    </a>
                    <span class="tweet-title">
                        AI Thinks Teens Are Trouble: A Bilingual Study Uncovers Bias in Language Models
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research compares how static word embeddings (SWEs) and generative language models (GLMs)
                    depict teenagers in English and Nepali, contrasting these representations with the perspectives of
                    teenagers themselves. This bilingual, bicultural approach distinguishes it from previous work that
                    primarily focused on English-language models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">
                15:37
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01962" target="_blank">
                        @arXiv 2408.01962
                    </a>
                    <span class="tweet-title">
                        Fact-Checking's Open Secret: Why Open AI Models Are a Mixed Bag
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research delves into the practical implications of open generative models within organizations,
                    specifically focusing on fact-checking organizations. It examines their motivations for using open
                    models, the limitations they face, and the impact on their data science pipelines. This approach
                    differs from previous work that primarily focused on the scientific integrity of open models in
                    academic research.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">
                15:59
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01916" target="_blank">
                        @arXiv 2408.01916
                    </a>
                    <span class="tweet-title">
                        AI Agents Team Up to Build Software Blueprints: A New Era of Process Modeling!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Capital Normal University, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a framework called MAO that uses multiple AI agents to automatically
                    generate process models from textual descriptions. Unlike previous approaches that rely on
                    data-driven methods or human intervention, MAO leverages the collaborative capabilities of AI agents
                    to address the challenges of traditional process modeling.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">
                16:21
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01942" target="_blank">
                        @arXiv 2408.01942
                    </a>
                    <span class="tweet-title">
                        Minecraft's New AI Can Hunt Anything, Even If It's Never Seen It Before!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for training AI agents in Minecraft that uses visual grounding
                    to enable zero-shot generalization to unseen objects. The key difference is that instead of relying
                    solely on language instructions, the agent learns to understand the target object through a visual
                    confidence map generated by a modified vision-language model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">
                16:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02487" target="_blank">
                        @arXiv 2408.02487
                    </a>
                    <span class="tweet-title">
                        AI Code Wizards: Can They Pass the License Test?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on evaluating the license compliance capabilities of LLMs in code generation,
                    a critical yet underexplored area. Unlike previous work that primarily focused on code completion
                    accuracy, this study establishes a benchmark to assess LLMs' ability to provide accurate license
                    information for their generated code.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">
                17:10
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01732" target="_blank">
                        @arXiv 2408.01732
                    </a>
                    <span class="tweet-title">
                        Talking Heads, No More Jitter: Landmark-Guided Diffusion Makes Videos Smooth!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        South China University of Technology, Zhejiang University, Peking University...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a two-stage diffusion model for talking head generation. Unlike previous
                    GAN-based models that prioritize lip synchronization but struggle with visual quality, this approach
                    leverages the strengths of diffusion models to generate high-fidelity frames while using facial
                    landmarks to ensure temporal coherence.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">
                17:33
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02019" target="_blank">
                        @arXiv 2408.02019
                    </a>
                    <span class="tweet-title">
                        Long-Tailed Data? No Problem! Expert Collaboration Saves the Day in Personalized Federated
                        Learning
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Xiamen University, University College London, A*STAR...
                    </span>
                </div>
                <div class="primary-text">
                    This research tackles the challenge of long-tailed data in personalized federated learning (PFL) by
                    introducing a novel approach called Expert Collaborative Learning (ECL). Unlike previous methods
                    that rely on global data distribution or adversarial feature augmentation, ECL assigns multiple
                    expert models to each client, each trained on a different subset of the data. This ensures that even
                    minority classes receive sufficient training, leading to improved performance for personalized
                    models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">
                18:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01655" target="_blank">
                        @arXiv 2408.01655
                    </a>
                    <span class="tweet-title">
                        Robots Get Imaginative: New AI Can Rearrange Objects Like a Human!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Tokyo
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a framework called SPORT that leverages pre-trained large vision models for
                    object localization and a diffusion-based 3D pose estimator for physically-realistic object
                    rearrangement. This approach differs from previous work by decoupling the rearrangement process into
                    object localization, goal imagination, and robot control, allowing for greater generalization and
                    reducing the need for extensive training data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">
                18:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01963" target="_blank">
                        @arXiv 2408.01963
                    </a>
                    <span class="tweet-title">
                        LLMs: Not So Smart When You Misspell!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new metric, Cohen's h, to measure the robustness of large language models
                    (LLMs) in non-adversarial scenarios. Unlike previous metrics like PDR, Cohen's h is symmetric and
                    defined for all score values, making it more suitable for evaluating LLMs' sensitivity to naturally
                    occurring variations in input.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">
                18:58
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02509" target="_blank">
                        @arXiv 2408.02509
                    </a>
                    <span class="tweet-title">
                        Code Completion Engines: Vulnerable to a Comment's Charm?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on practical attacks against black-box code completion engines, unlike
                    previous work that focused on manipulating training data or model weights. The attack, called INSEC,
                    works by inserting a malicious comment into the code, effectively tricking the engine into
                    generating vulnerable code.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">
                19:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01993" target="_blank">
                        @arXiv 2408.01993
                    </a>
                    <span class="tweet-title">
                        LLMs: The New Detectives on the Cybersecurity Beat!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to detecting Hands-on-Keyboard (HOK) cyberattacks by
                    transforming endpoint activity data into narratives that Large Language Models (LLMs) can analyze.
                    This differs from previous work by utilizing LLMs to interpret the context and relationships within
                    security events, rather than relying solely on pattern matching or anomaly detection.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">
                19:49
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01455" target="_blank">
                        @arXiv 2408.01455
                    </a>
                    <span class="tweet-title">
                        Beliefs: Not Just a List, It's an Epistemological Tree!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google, Google Research, Google Deepmind
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes an ontology of belief systems based on epistemological justification, a novel
                    approach that focuses on how beliefs are validated rather than their content. This differs from
                    previous work that often relied on categorization based on geographical, historical, or cultural
                    factors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">
                20:11
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01622" target="_blank">
                        @arXiv 2408.01622
                    </a>
                    <span class="tweet-title">
                        Robots Learn to Avoid Trouble: New AI Method Infers Constraints from Demonstrations
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        École Polytechnique Fédérale de Lausanne
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel Positive-Unlabeled Constraint Learning (PUCL) algorithm that infers
                    continuous, nonlinear constraint functions from expert demonstrations. Unlike previous methods, PUCL
                    does not require prior knowledge of the true constraint parameterization or environmental model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">
                20:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02544" target="_blank">
                        @arXiv 2408.02544
                    </a>
                    <span class="tweet-title">
                        AI Assistants: Distracted by Shiny Objects?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Shanghai Jiao Tong University, Meta
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the faithfulness of multimodal GUI agents in distracting environments, a
                    topic not extensively explored in previous studies. It investigates whether these agents can stay
                    focused on user goals despite the presence of irrelevant content in the environment.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">
                21:10
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01596" target="_blank">
                        @arXiv 2408.01596
                    </a>
                    <span class="tweet-title">
                        Machine Learning Goes to Therapy: How to Make AI Trustworthy in a World of Strategic Users
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Toyota Technological Institute at Chicago
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the impact of strategic and adversarial data sources on machine learning,
                    focusing on scenarios where agents can manipulate their data or defect from collaborative learning
                    processes. Unlike previous work, it considers personalized and unknown manipulation abilities,
                    making the learning problem more challenging.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">
                21:42
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01620" target="_blank">
                        @arXiv 2408.01620
                    </a>
                    <span class="tweet-title">
                        AI Gets a Second Opinion: New Model Leverages Human Feedback for Medical Image Segmentation
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces MedUHIP, a model that combines uncertainty-aware segmentation with
                    human-in-the-loop interaction. Unlike previous methods that either generate multiple segmentations
                    or rely on fixed interaction strategies, MedUHIP adapts to individual clinician preferences through
                    iterative feedback.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">
                22:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02421" target="_blank">
                        @arXiv 2408.02421
                    </a>
                    <span class="tweet-title">
                        Emotion Recognition on a Budget: Tiny AI Model, Big Results!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford, Oxford Suzhou Centre for Advanced Research
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel cross-modality transfer learning approach, specifically adapting
                    image-based emotion classifiers to video analysis. Unlike previous methods that focused on
                    fine-tuning entire models, this approach utilizes "adapters" – small, efficient modules that are
                    added to pre-trained image models to enable them to process video data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">
                22:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02533" target="_blank">
                        @arXiv 2408.02533
                    </a>
                    <span class="tweet-title">
                        Hyperparameter Tuning: Beyond the Average, LMEMs to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Freiburg
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to analyzing hyperparameter optimization (HPO)
                    benchmarking results using Linear Mixed-Effect Models (LMEMs). Unlike traditional methods that rely
                    on averaging performance across datasets, LMEMs allow for a more nuanced analysis by considering
                    hierarchical relationships within the data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">
                22:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01774" target="_blank">
                        @arXiv 2408.01774
                    </a>
                    <span class="tweet-title">
                        Driver Attention: The Secret Sauce for Safer Self-Driving Cars
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research incorporates driver attention into a spatio-temporal dual-encoder network, a novel
                    approach for predicting driver behavior in safety-critical scenarios. Unlike previous work, this
                    model explicitly considers where drivers focus their attention, aiming to improve both prediction
                    accuracy and interpretability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">
                23:20
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02357" target="_blank">
                        @arXiv 2408.02357
                    </a>
                    <span class="tweet-title">
                        AI's Got a Secret: The Power of "I Don't Know"
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        King's College London, University of Cambridge, Simon Fraser University
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces the Consistent Reasoning Paradox (CRP), which argues that any AI striving for
                    human-like intelligence through consistent reasoning will inevitably make mistakes, even on simple
                    arithmetic problems. This is different from previous work that focused on the limitations of AI due
                    to non-computable problems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">
                23:48
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01541" target="_blank">
                        @arXiv 2408.01541
                    </a>
                    <span class="tweet-title">
                        Image Quality Metrics: Defending Against the Sneaky Attacks!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Moscow State University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a comprehensive benchmark for evaluating defense mechanisms against
                    adversarial attacks on image quality assessment (IQA) metrics. Unlike previous work that focused on
                    IQA metric robustness, this study systematically evaluates the effectiveness of various defense
                    strategies, including adversarial purification, adversarial training, and certified robustness
                    methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">
                24:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01857" target="_blank">
                        @arXiv 2408.01857
                    </a>
                    <span class="tweet-title">
                        Predicting Particle Chaos: A Linearized Optimal Transport Approach
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of California San Diego, University of Michigan, Johns Hopkins University...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel algorithm that uses linearized optimal transport to predict the
                    evolution of discrete particle systems without explicitly learning the governing operator. This
                    approach differs from previous work by directly using the measure-valued data to make predictions,
                    avoiding the need for summary statistics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">
                24:45
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01744" target="_blank">
                        @arXiv 2408.01744
                    </a>
                    <span class="tweet-title">
                        AI Makes Investment Reports Less of a Headache!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        The University of Tokyo, Daiwa Institute of Research Ltd., Daiwa Asset Management Co.Ltd.
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the use of transformer-based models for summarizing investment reports,
                    specifically comparing extractive and abstractive summarization methods. Unlike previous work, it
                    focuses on the application of these methods to the financial domain, analyzing their effectiveness
                    in summarizing investment reports from monthly reports.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">
                25:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01831" target="_blank">
                        @arXiv 2408.01831
                    </a>
                    <span class="tweet-title">
                        Deep Learning Dings Ringing in Vibroseis Data!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research uses a deep convolutional neural network (CNN) to attenuate the "ringing effect" in
                    vibroseis data, a common problem in seismic exploration. Unlike previous deconvolution methods, this
                    approach directly learns the mapping between ringing and deringed data, potentially improving the
                    accuracy and efficiency of the process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">
                25:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01639" target="_blank">
                        @arXiv 2408.01639
                    </a>
                    <span class="tweet-title">
                        Dual Network: The Secret Sauce for Smarter Robots
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Pennsylvania
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to coordinating trajectory planning and tracking in layered
                    control architectures by introducing a dual network that learns to preemptively perturb the
                    reference trajectory, accounting for the low-level controller's inaccuracies. This differs from
                    previous work that directly feeds the reference trajectory to the controller, often leading to
                    suboptimal performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">
                26:03
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01579" target="_blank">
                        @arXiv 2408.01579
                    </a>
                    <span class="tweet-title">
                        Robot Vision Gets a Color Makeover: New Algorithm Mimics Human Perception
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new descriptor, TOPS2, which combines 3D shape and color information for
                    object recognition. Unlike previous methods that rely on specific colors, TOPS2 uses a network of
                    coarse color regions, similar to how humans perceive color.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">
                26:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02604" target="_blank">
                        @arXiv 2408.02604
                    </a>
                    <span class="tweet-title">
                        Flow-MRI: Seeing Through Blood's Secrets, One Jet at a Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge
                    </span>
                </div>
                <div class="primary-text">
                    This research uses flow-MRI data to simultaneously reconstruct the flow field and learn the
                    rheological parameters of a non-Newtonian fluid, specifically a Carreau fluid, without requiring
                    pressure drop measurements. This approach differs from previous methods that relied on invasive
                    techniques or assumed known rheological properties.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">
                26:46
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02284" target="_blank">
                        @arXiv 2408.02284
                    </a>
                    <span class="tweet-title">
                        Video Denoising Gets a Tune-Up: Uncertainty Makes It Smarter!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a cascading refinement video denoising method that iteratively refines both
                    the alignment and the restored image. It also incorporates an uncertainty estimation module to adapt
                    to multi-level noise, reducing computational cost. This approach differs from previous methods by
                    combining iterative refinement with uncertainty adaptivity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">
                27:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02280" target="_blank">
                        @arXiv 2408.02280
                    </a>
                    <span class="tweet-title">
                        AutoML's New Trick: Balancing Accuracy and Cost with Hardware-Aware Ensembles
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Helmholtz-Zentrum Berlin, Humboldt University Berlin, University of Freiburg
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a hardware-aware ensemble selection approach that considers inference time
                    alongside predictive accuracy, allowing for a balanced trade-off between performance and operational
                    efficiency. This differs from previous work that primarily focused on maximizing accuracy without
                    considering hardware constraints.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">
                27:41
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02462" target="_blank">
                        @arXiv 2408.02462
                    </a>
                    <span class="tweet-title">
                        AI Heart Scan Bias: It's Not Just the Heart, It's the Fat!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        King's College London, Guy’s and St Thomas’ Hospital, Tongji University...
                    </span>
                </div>
                <div class="primary-text">
                    This research delves into the source of race bias in AI-based CMR segmentation, specifically
                    investigating whether the bias originates from the images themselves or the ground truth
                    segmentations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">
                28:03
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02272" target="_blank">
                        @arXiv 2408.02272
                    </a>
                    <span class="tweet-title">
                        Cooking Up a Storm: A New Dataset for Vision-Language Understanding in the Kitchen
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        OMRON SINICX Corp., Tokyo Institute of Technology, Tokyo Metropolitan University...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset, COM Kitchens, which focuses on unedited overhead-view
                    cooking videos. Unlike previous datasets that often rely on web videos or ego-centric perspectives,
                    COM Kitchens provides a unique fixed-viewpoint perspective, capturing the entire cooking process
                    from above.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">
                28:34
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02525" target="_blank">
                        @arXiv 2408.02525
                    </a>
                    <span class="tweet-title">
                        Say Goodbye to Single-Tap Lag: New Tech Predicts Your Double Taps!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Tokyo, Yahoo Japan Corporation
                    </span>
                </div>
                <div class="primary-text">
                    This research extends previous work on tap prediction by evaluating its performance in real-world
                    conditions, including daily laptop use and various smartphone scenarios. It also investigates the
                    impact of inaccurate tap classification on usability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon" title="Play from here">
                28:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01581" target="_blank">
                        @arXiv 2408.01581
                    </a>
                    <span class="tweet-title">
                        Weather Forecasting Goes Big: 7,424-Member Ensemble Predicts Extreme Heatwaves
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research expands on a previous study that used Spherical Fourier Neural Operators (SFNO) to
                    create a weather forecasting ensemble. This paper introduces a "huge ensemble" (HENS) with 7,424
                    members, significantly larger than previous ensembles.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon" title="Play from here">
                29:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01736" target="_blank">
                        @arXiv 2408.01736
                    </a>
                    <span class="tweet-title">
                        Can LLMs Predict Where Your AI Will End Up?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Huawei, ENS Paris-Saclay, EURECOM
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the ability of LLMs to understand the dynamics of stochastic gradient descent
                    (SGD), a core algorithm in machine learning. Unlike previous work that focused on learning
                    transition probabilities of known dynamical systems, this paper proposes a method to estimate the
                    transition kernel of SGD itself, enabling predictions about its convergence behavior.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon" title="Play from here">
                29:49
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02125" target="_blank">
                        @arXiv 2408.02125
                    </a>
                    <span class="tweet-title">
                        Brain Networks: From Chaos to Clarity with Abstraction Levels
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research extends previous work by applying the concept of abstraction levels to arbitrary
                    neural network digraphs, not just layered networks representing hierarchical concepts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet67">
            <div class="start-time-icon" title="Play from here">
                30:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02217" target="_blank">
                        @arXiv 2408.02217
                    </a>
                    <span class="tweet-title">
                        Climate Change is Doubling the Odds of Maize Insurance Claims: A Neural Network Predicts a
                        Stormy Future for Farmers
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of California Berkeley, University of Arkansas
                    </span>
                </div>
                <div class="primary-text">
                    This research uses an artificial neural network to predict future maize yields at the level of
                    insured units, providing a more granular analysis of insurer-relevant outcomes under climate change
                    than previous studies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet68">
            <div class="start-time-icon" title="Play from here">
                30:31
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.02581" target="_blank">
                        @arXiv 2408.02581
                    </a>
                    <span class="tweet-title">
                        Exoplanet Spectra: Don't Let Your Models Go Off the Rails!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ML Analytics, OPIT, University College London...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to operational range bounding for spectroscopy models by
                    utilizing anomaly detection algorithms. Unlike traditional monitoring systems that operate
                    asynchronously, this method runs in parallel with the prediction model, providing real-time feedback
                    on model performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet69">
            <div class="start-time-icon" title="Play from here">
                30:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.01452" target="_blank">
                        @arXiv 2408.01452
                    </a>
                    <span class="tweet-title">
                        AI in the Classroom: Building a Guardrail for Safe and Sound Learning
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Emergence AI, Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on building a domain-specific guardrail model for educational AI systems,
                    specifically addressing the need for safety and appropriateness in K-12 education. Unlike previous
                    work that primarily focused on general-purpose chatbots or safety in other domains, this paper
                    tackles the unique challenges of ensuring safe and appropriate content generation in a learning
                    environment.
                </div>
            </div>
        </div>


        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202408061909_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>

</html>