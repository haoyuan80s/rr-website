
daily_data = {
    "date": "2024-10-17",
    "tweets": [
        
        {
            "startTime": "01:02",
            "arxivId": "2410.12023",
            "arxivLink": "https://arxiv.org/abs/2410.12023",
            "title": "Simulating Humans: Neural Networks Learn Physics for 3D Pose Reconstruction",
            "institute": "Google DeepMind, Anthropic",
            "text": "This research introduces a novel neural network architecture called LARP (Learned Articulated Rigidbody Physics) that simulates the dynamics of articulated human motion with contact. Unlike traditional physics simulators, LARP is differentiable and significantly faster, especially when running multiple simulations in parallel.",
            "paper-title": "Learned Neural Physics Simulation for Articulated 3D Human Pose Reconstruction",
            "image-path": "flux_paper_image/2410.12023_1729195906.png"
        },

        {
            "startTime": "01:22",
            "arxivId": "2410.12109",
            "arxivLink": "https://arxiv.org/abs/2410.12109",
            "title": "Time-Traveling Transformers: New Model Makes Sense of Audio-Visual Events",
            "institute": "Nvidia",
            "text": "This research introduces a new dataset called OCTAV and a model called OMCAT. OCTAV is designed to capture event transitions across audio and video, while OMCAT uses a novel time embedding technique called RoTE to improve temporal grounding and computational efficiency. This approach differs from previous work by focusing on fine-grained, cross-modal temporal understanding, which is crucial for tasks like audio-visual question answering.",
            "paper-title": "OMCAT: Omni Context Aware Transformer",
            "image-path": "flux_paper_image/2410.12109_1729195571.png"
        },

        {
            "startTime": "01:44",
            "arxivId": "2410.12608",
            "arxivLink": "https://arxiv.org/abs/2410.12608",
            "title": "Math Models Get a Reality Check: Programs as Verifiers Boost Reasoning Accuracy",
            "institute": "Singapore University of Technology and Design, DeepMind",
            "text": "This research introduces PROVE, a framework that uses program-based verification to filter out potentially incorrect reasoning paths before aggregating final answers. Unlike previous work that relies on majority voting, PROVE only aggregates solutions validated by Python programs.",
            "paper-title": "Not All Votes Count! Programs as Verifiers Improve Self-Consistency of Language Models for Math Reasoning",
            "image-path": "flux_paper_image/2410.12608_1729195743.png"
        },

        {
            "startTime": "02:09",
            "arxivId": "2410.12076",
            "arxivLink": "https://arxiv.org/abs/2410.12076",
            "title": "AI Security: Taking Off the Rose-Tinted Glasses",
            "institute": "IBM, Stony Brook University",
            "text": "This research critiques the common assumptions made in adversarial machine learning research, arguing that overly permissive attack models and overly restrictive defensive models have hindered the development of practical defenses. It emphasizes the need to consider the entire system security lifecycle, not just the model itself, and proposes a more realistic threat model for evaluating adversarial attacks.",
            "paper-title": "Taking off the Rose-Tinted Glasses: A Critical Look at Adversarial ML Through the Lens of Evasion Attacks",
            "image-path": "flux_paper_image/2410.12076_1729194782.png"
        },

        {
            "startTime": "02:37",
            "arxivId": "2410.12074",
            "arxivLink": "https://arxiv.org/abs/2410.12074",
            "title": "Camera-Agnostic Deep Learning: One Algorithm to Rule Them All!",
            "institute": "University of Maryland, NVIDIA",
            "text": "This research introduces nvTorchCam, an open-source library that allows deep learning algorithms to work with various camera models without requiring model-specific implementations. This is different from previous work, which often required separate implementations for each camera type.",
            "paper-title": "nvTorchCam: An Open-source Library for Camera-Agnostic Differentiable Geometric Vision",
            "image-path": "flux_paper_image/2410.12074_1729195231.png"
        },

        {
            "startTime": "02:58",
            "arxivId": "2410.12219",
            "arxivLink": "https://arxiv.org/abs/2410.12219",
            "title": "AI's Got Multi-Modal Madness: New Test Makes Models Sweat!",
            "institute": "University of Maryland, Google",
            "text": "This research introduces Omni\u00d7R, a benchmark specifically designed to evaluate the reasoning capabilities of omni-modality language models (OLMs) across multiple modalities, including text, vision, audio, and video. Unlike existing benchmarks that are limited to single or dual-modality tasks, Omni\u00d7R provides a comprehensive testbed that includes complex modality combinations, offering a more rigorous and holistic evaluation of these models' capabilities.",
            "paper-title": "OmnixR: Evaluating Omni-modality Language Models on Reasoning across Modalities",
            "image-path": "flux_paper_image/2410.12219_1729193765.png"
        },

        {
            "startTime": "03:21",
            "arxivId": "2410.12766",
            "arxivLink": "https://arxiv.org/abs/2410.12766",
            "title": "Model Merging: When Permutations Aren't Enough, TACT Saves the Day!",
            "institute": "University of Toronto, Google",
            "text": "This paper tackles the \"non-local\" model merging problem, where expert models are trained on different tasks and don't share a common foundation model. Unlike previous work that focused on \"local\" merging, this research explores the challenges of merging models from diverse origins and proposes a new technique called TACT to address the issue of variance collapse.",
            "paper-title": "The Non-Local Model Merging Problem: Permutation Symmetries and Variance Collapse",
            "image-path": "flux_paper_image/2410.12766_1729194215.png"
        },

        {
            "startTime": "03:42",
            "arxivId": "2410.12592",
            "arxivLink": "https://arxiv.org/abs/2410.12592",
            "title": "Cocoon: Sensor Fusion Gets a Confidence Boost!",
            "institute": "University of Michigan, NVIDIA Research, Stanford University...",
            "text": "This research introduces Cocoon, a framework that quantifies uncertainty at both the object and feature levels for multi-modal perception. Unlike previous methods that rely on uniform weights or separate detection pipelines, Cocoon dynamically adjusts the weights of modality-specific features based on their uncertainty.",
            "paper-title": "Cocoon: Robust Multi-Modal Perception with Uncertainty-Aware Sensor Fusion",
            "image-path": "flux_paper_image/2410.12592_1729194737.png"
        },

        {
            "startTime": "04:16",
            "arxivId": "2410.12045",
            "arxivLink": "https://arxiv.org/abs/2410.12045",
            "title": "Trust Graphs: When Your Friends Know Your Secrets, But Privacy Still Reigns!",
            "institute": "Google Research",
            "text": "This research introduces a new model of differential privacy (DP) that considers trust relationships between users, represented as a trust graph. Unlike traditional local DP where each user trusts no other, this model allows users to share their data with trusted neighbors, potentially leading to better privacy-utility trade-offs.",
            "paper-title": "Differential Privacy on Trust Graphs",
            "image-path": "flux_paper_image/2410.12045_1729195619.png"
        },

        {
            "startTime": "04:35",
            "arxivId": "2410.12360",
            "arxivLink": "https://arxiv.org/abs/2410.12360",
            "title": "Time Series Models: Bigger is Better, But How Much?",
            "institute": "Griffith University, The Hong Kong University of Science and Technology (Guangzhou), NVIDIA Research...",
            "text": "This research investigates the scaling laws of time series foundation models (TSFMs) in out-of-distribution (OOD) scenarios, a topic largely unexplored in previous work.",
            "paper-title": "Towards Neural Scaling Laws for Time Series Foundation Models",
            "image-path": "flux_paper_image/2410.12360_1729194623.png"
        },

        {
            "startTime": "05:04",
            "arxivId": "2410.12284",
            "arxivLink": "https://arxiv.org/abs/2410.12284",
            "title": "AI Explanations: Fool Me Once, Shame on You, Fool Me Twice...",
            "institute": "University of Oxford",
            "text": "This research stands out by evaluating the usefulness of AI explanations in a real-world clinical setting, considering both the accuracy of the AI advice and the correctness of the explanations provided.",
            "paper-title": "Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical Decision-Support Setting",
            "image-path": "flux_paper_image/2410.12284_1729192961.png"
        },

        {
            "startTime": "05:26",
            "arxivId": "2410.12361",
            "arxivLink": "https://arxiv.org/abs/2410.12361",
            "title": "LLMs Go Proactive: From \"Do This\" to \"Let Me Help!\"",
            "institute": "Tsinghua University",
            "text": "This research focuses on developing proactive LLM agents that can anticipate user needs and offer assistance without explicit instructions, unlike traditional reactive agents that only respond to prompts.",
            "paper-title": "Proactive Agent: Shifting LLM Agents from Reactive Responses to Active Assistance",
            "image-path": "flux_paper_image/2410.12361_1729194246.png"
        },

        {
            "startTime": "05:51",
            "arxivId": "2410.12694",
            "arxivLink": "https://arxiv.org/abs/2410.12694",
            "title": "VividMed: Giving Medical Images a Voice (and a Highlight!)",
            "institute": "Tsinghua University",
            "text": "This research introduces VividMed, a vision-language model that goes beyond generating text descriptions of medical images. It also provides visual grounding, highlighting specific anatomical structures and abnormalities within the image using bounding boxes and segmentation masks. This sets it apart from previous work that primarily focused on text generation or used a single method of visual grounding.",
            "paper-title": "VividMed: Vision Language Model with Versatile Visual Grounding for Medicine",
            "image-path": "flux_paper_image/2410.12694_1729194905.png"
        },

        {
            "startTime": "06:12",
            "arxivId": "2410.12665",
            "arxivLink": "https://arxiv.org/abs/2410.12665",
            "title": "Building Bridges: How Physics Can Help Us Design Patterns",
            "institute": "Harvard University",
            "text": "This research introduces a new framework called the \"Hamiltonian bridge\" for controlling patterns in non-equilibrium systems. It combines the power of stochastic optimal control with the principles of pattern formation, allowing for the generation of complex patterns while respecting physical constraints. This approach differs from previous work by explicitly incorporating physical priors into the generative process, going beyond simple interpolation techniques.",
            "paper-title": "Hamiltonian bridge: A physics-driven generative framework for targeted pattern control",
            "image-path": "flux_paper_image/2410.12665_1729193777.png"
        },

        {
            "startTime": "06:42",
            "arxivId": "2410.12138",
            "arxivLink": "https://arxiv.org/abs/2410.12138",
            "title": "Generative Models Get a Group Therapy Session: Multi-Sample Preference Optimization for Better AI!",
            "institute": "University of Chicago",
            "text": "This research introduces a new approach to fine-tuning generative models by using multi-sample comparisons instead of single-sample comparisons. This allows the models to learn from the overall distribution of outputs, rather than just individual examples, leading to better optimization of characteristics like diversity and bias.",
            "paper-title": "Preference Optimization with Multi-Sample Comparisons",
            "image-path": "flux_paper_image/2410.12138_1729193682.png"
        },

        {
            "startTime": "07:07",
            "arxivId": "2410.12784",
            "arxivLink": "https://arxiv.org/abs/2410.12784",
            "title": "AI Judges Get a Reality Check: New Benchmark Tests Their Reasoning Skills",
            "institute": "UC Berkeley, Washington University in St. Louis",
            "text": "This research introduces JudgeBench, a benchmark specifically designed to evaluate the reasoning abilities of LLM-based judges. Unlike previous benchmarks that focus on alignment with human preferences, JudgeBench prioritizes factual and logical correctness, creating a more objective and challenging evaluation.",
            "paper-title": "JudgeBench: A Benchmark for Evaluating LLM-based Judges",
            "image-path": "flux_paper_image/2410.12784_1729192778.png"
        },

        {
            "startTime": "07:26",
            "arxivId": "2410.12774",
            "arxivLink": "https://arxiv.org/abs/2410.12774",
            "title": "Task Grouping for MTL: When Easier is Better!",
            "institute": "Harvard University, University of Arizona",
            "text": "This research proposes a new metric, Pointwise V-Usable Information (PVI), to measure task difficulty and identify optimal task groupings for multi-task learning (MTL). Unlike previous methods that focus on task similarity, PVI directly assesses how much information a model can extract from a dataset, grouping tasks with similar PVI distributions for joint training.",
            "paper-title": "Identifying Task Groupings for Multi-Task Learning Using Pointwise V-Usable Information",
            "image-path": "flux_paper_image/2410.12774_1729195819.png"
        },

        {
            "startTime": "07:51",
            "arxivId": "2410.12029",
            "arxivLink": "https://arxiv.org/abs/2410.12029",
            "title": "LLMs: The New Sherlock Holmes of Cultural Analytics?",
            "institute": "UC Berkeley",
            "text": "This research compares the performance of large language models (LLMs) to traditional supervised methods for classification tasks in cultural analytics. It focuses on the tradeoffs between accuracy and sensemaking, particularly in the context of de novo tasks.",
            "paper-title": "On Classification with Large Language Models in Cultural Analytics",
            "image-path": "flux_paper_image/2410.12029_1729195347.png"
        },

        {
            "startTime": "08:16",
            "arxivId": "2410.12189",
            "arxivLink": "https://arxiv.org/abs/2410.12189",
            "title": "LLMs Gone Wild: Taming the Beast of Complex Document Processing",
            "institute": "UC Berkeley, Columbia University",
            "text": "This research introduces DocETL, a system that optimizes complex document processing pipelines by leveraging LLMs to automatically rewrite and evaluate them. Unlike previous work that focuses on reducing cost while maintaining accuracy, DocETL prioritizes improving accuracy by decomposing complex operations into simpler ones.",
            "paper-title": "DocETL: Agentic Query Rewriting and Evaluation for Complex Document Processing",
            "image-path": "flux_paper_image/2410.12189_1729195900.png"
        },

        {
            "startTime": "08:42",
            "arxivId": "2410.12104",
            "arxivLink": "https://arxiv.org/abs/2410.12104",
            "title": "Hackers vs. AI: A Bug Bounty for Safer Language Models",
            "institute": "UL Research Institutes, Allen Institute for AI, Massachusetts Institute of Technology...",
            "text": "This research details a large-scale bug bounty program specifically designed for large language models (LLMs), focusing on identifying and reporting \"flaws\" in their behavior and documentation. This approach differs from traditional security bug bounties by considering the unique challenges of probabilistic systems and the broader range of potential harms associated with AI.",
            "paper-title": "To Err is AI : A Case Study Informing LLM Flaw Reporting Practices",
            "image-path": "flux_paper_image/2410.12104_1729194444.png"
        },

        {
            "startTime": "09:07",
            "arxivId": "2410.12771",
            "arxivLink": "https://arxiv.org/abs/2410.12771",
            "title": "AI's New Material Recipe: 110 Million DFT Calculations and a Dash of Open Source!",
            "institute": "Meta",
            "text": "This research introduces the OpenMaterials2024 (OMat24) dataset, a massive collection of density functional theory (DFT) calculations for inorganic materials. Unlike previous datasets, OMat24 focuses on non-equilibrium structures, which are more relevant for studying dynamic properties and materials discovery.",
            "paper-title": "Open Materials 2024 (OMat24) Inorganic Materials Dataset and Models",
            "image-path": "flux_paper_image/2410.12771_1729193713.png"
        },

        {
            "startTime": "09:27",
            "arxivId": "2410.12428",
            "arxivLink": "https://arxiv.org/abs/2410.12428",
            "title": "LLMs: Conformity is Contagious, Even When Wrong!",
            "institute": "University of Cambridge, University of Sheffield",
            "text": "This research goes beyond simply identifying conformity in LLMs, exploring the factors that influence it, such as the tone of the majority and the model's initial confidence. It also proposes two novel methods to mitigate conformity.",
            "paper-title": "Conformity in Large Language Models",
            "image-path": "flux_paper_image/2410.12428_1729195706.png"
        },

        {
            "startTime": "09:50",
            "arxivId": "2410.12388",
            "arxivLink": "https://arxiv.org/abs/2410.12388",
            "title": "Prompt Compression: Making LLMs Slim & Trim!",
            "institute": "University of Cambridge",
            "text": "This survey categorizes prompt compression techniques into hard and soft prompt methods, providing a comprehensive overview of their technical approaches, mechanisms, and downstream adaptations. It also highlights the limitations of current methods and proposes potential future research directions.",
            "paper-title": "Prompt Compression for Large Language Models: A Survey",
            "image-path": "flux_paper_image/2410.12388_1729194383.png"
        },

        {
            "startTime": "10:08",
            "arxivId": "2410.12790",
            "arxivLink": "https://arxiv.org/abs/2410.12790",
            "title": "VLMs Get a Multi-Modal Makeover: Dual Prototypes Evolve for Test-Time Adaptation",
            "institute": "CMU",
            "text": "This research introduces Dual Prototype Evolving (DPE), a test-time adaptation approach for vision-language models (VLMs) that leverages both textual and visual modalities to learn more accurate representations of target classes. Unlike previous methods that focus on a single modality, DPE evolves two sets of prototypes\u2014textual and visual\u2014progressively capturing more accurate multi-modal representations during test time.",
            "paper-title": "Dual Prototype Evolving for Test-Time Generalization of Vision-Language Models",
            "image-path": "flux_paper_image/2410.12790_1729194108.png"
        },

        {
            "startTime": "10:33",
            "arxivId": "2410.11963",
            "arxivLink": "https://arxiv.org/abs/2410.11963",
            "title": "CtrlSynth: Building Better AI Models with a Little Help from Imagination!",
            "institute": "Apple, Meta",
            "text": "This research introduces CtrlSynth, a controllable image-text synthesis pipeline that differs from previous work by decomposing image semantics into basic elements and using pretrained foundation models to recompose them based on user-defined control policies.",
            "paper-title": "CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning",
            "image-path": "flux_paper_image/2410.11963_1729193086.png"
        },

        {
            "startTime": "10:59",
            "arxivId": "2410.12779",
            "arxivLink": "https://arxiv.org/abs/2410.12779",
            "title": "Data's Got Curves: New AI Learns to Navigate Data Manifolds",
            "institute": "Yale University",
            "text": "This research introduces a novel framework called Geometry-Aware Generative Autoencoder (GAGA) that combines manifold learning with generative modeling. Unlike previous methods that focus on matching data distributions, GAGA learns a warped Riemannian metric on the data space, allowing it to generate points uniformly across the manifold, interpolate along geodesics, and transport populations along geodesic paths.",
            "paper-title": "Geometry-Aware Generative Autoencoders for Warped Riemannian Metric Learning and Generative Modeling on Data Manifolds",
            "image-path": "flux_paper_image/2410.12779_1729195639.png"
        },

        {
            "startTime": "11:26",
            "arxivId": "2410.12782",
            "arxivLink": "https://arxiv.org/abs/2410.12782",
            "title": "Robots Learn to Act Like Humans, Thanks to a Chatty AI!",
            "institute": "University of California Berkeley",
            "text": "This research explores using large language models (LLMs) for robot action prediction through in-context learning (ICL) without any additional training. Unlike previous work that relies on vision-based approaches or separate perception modules, RoboPrompt leverages the text-only capabilities of LLMs to directly predict robot actions based on textual descriptions of object poses and robot actions.",
            "paper-title": "In-Context Learning Enables Robot Action Prediction in LLMs",
            "image-path": "flux_paper_image/2410.12782_1729192701.png"
        },

        {
            "startTime": "11:49",
            "arxivId": "2410.12713",
            "arxivLink": "https://arxiv.org/abs/2410.12713",
            "title": "Bandits on a Budget: How Low Variance Can Save Your Regret",
            "institute": "MIT, University of Virginia",
            "text": "This paper explores how the variance of rewards in contextual bandit problems affects regret, going beyond the typical minimax regret bounds. It shows that the eluder dimension, a measure of function class complexity, plays a crucial role in variance-dependent regret bounds, particularly in the strong adversary setting.",
            "paper-title": "How Does Variance Shape the Regret in Contextual Bandits?",
            "image-path": "flux_paper_image/2410.12713_1729194170.png"
        },

        {
            "startTime": "12:12",
            "arxivId": "2410.12217",
            "arxivLink": "https://arxiv.org/abs/2410.12217",
            "title": "Annotator Disagreements? Let's Predict Their Ratings!",
            "institute": "UC Berkeley, USC",
            "text": "This research explores predicting individual annotator ratings for toxicity detection, going beyond traditional majority voting methods. It investigates the effectiveness of incorporating annotator-specific information like demographics, survey responses, and rating history.",
            "paper-title": "Accurate and Data-Efficient Toxicity Prediction when Annotators Disagree",
            "image-path": "flux_paper_image/2410.12217_1729194120.png"
        },

        {
            "startTime": "12:39",
            "arxivId": "2410.12176",
            "arxivLink": "https://arxiv.org/abs/2410.12176",
            "title": "Sliced and Diced: A New Way to Measure Distance Between Data Clouds",
            "institute": "Vanderbilt University, Tufts University, University of Warwick",
            "text": "This paper introduces a new method for calculating distances between probability measures, called Expected Sliced Transport (EST). Unlike previous sliced-Wasserstein approaches, EST constructs a transportation plan between the measures, allowing for explicit coupling.",
            "paper-title": "Expected Sliced Transport Plans",
            "image-path": "flux_paper_image/2410.12176_1729193310.png"
        },

        {
            "startTime": "13:03",
            "arxivId": "2410.12464",
            "arxivLink": "https://arxiv.org/abs/2410.12464",
            "title": "LLMs: Smarter Isn't Always Better, Especially When Trading Crypto!",
            "institute": "National University of Singapore, The Hong Kong University of Science and Technology",
            "text": "This research explores the counterintuitive finding that stronger LLMs don't always outperform weaker ones in cryptocurrency trading. It proposes a novel multi-agent framework, FS-ReasoningAgent, that separates factual and subjective information to improve trading performance.",
            "paper-title": "Enhancing LLM Trading Performance with Fact-Subjectivity Aware Reasoning",
            "image-path": "flux_paper_image/2410.12464_1729195612.png"
        },

        {
            "startTime": "13:33",
            "arxivId": "2410.12557",
            "arxivLink": "https://arxiv.org/abs/2410.12557",
            "title": "Diffusion Models Just Got a Speed Boost: Shortcut Models Skip the Noise!",
            "institute": "UC Berkeley",
            "text": "This paper introduces shortcut models, a new type of diffusion model that can generate high-quality images in a single step or a few steps, unlike traditional diffusion models that require many steps. The key difference is that shortcut models are conditioned on the desired step size, allowing them to \"jump ahead\" in the denoising process.",
            "paper-title": "One Step Diffusion via Shortcut Models",
            "image-path": "flux_paper_image/2410.12557_1729195520.png"
        },

        {
            "startTime": "13:56",
            "arxivId": "2410.12476",
            "arxivLink": "https://arxiv.org/abs/2410.12476",
            "title": "AI Doctor's Prescription: Fake Clinical Trials for Real Research!",
            "institute": "Stanford University, Rensselaer, University of Southern California",
            "text": "This research proposes a novel retrieval-reasoning framework that uses LLMs to generate synthetic clinical trials with binary success/failure labels. This approach differs from previous work by incorporating a retrieval module to select relevant clinical trials based on drug interventions and a reasoning module to generate explanations for the trial outcomes.",
            "paper-title": "Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial Generation",
            "image-path": "flux_paper_image/2410.12476_1729194376.png"
        },

        {
            "startTime": "14:20",
            "arxivId": "2410.12443",
            "arxivLink": "https://arxiv.org/abs/2410.12443",
            "title": "LLMs: The New Privacy Crackers? They Can Unmask Your Data!",
            "institute": "Nanjing University of Science and Technology, Western Sydney University, Institute of Information Engineering...",
            "text": "This research explores a new privacy risk associated with using differential privacy (DP) in large language models (LLMs). Unlike previous work that focused on privacy issues during LLM training, this paper investigates the potential for LLMs to reconstruct private information from DP-sanitized prompts, even after training is complete.",
            "paper-title": "Reconstruction of Differentially Private Text Sanitization via Large Language Models",
            "image-path": "flux_paper_image/2410.12443_1729193322.png"
        },

        {
            "startTime": "14:41",
            "arxivId": "2410.12178",
            "arxivLink": "https://arxiv.org/abs/2410.12178",
            "title": "Low-Data Learning: When Your Model's Layers Get a Little Too \"Heavy\"",
            "institute": "Dartmouth College, University of California Berkeley, University of California San Diego...",
            "text": "This research focuses on the problem of training machine learning models with limited data. It uses a theory called Heavy-Tailed Self-Regularization (HT-SR) to analyze the performance of different layers within a model. The study finds that layers trained with insufficient data can become imbalanced, leading to poor overall performance. To address this, the researchers propose a new method called TempBalance, which dynamically adjusts the learning rate for each layer based on its training quality. This approach aims to balance the training quality across all layers, improving the model's performance in low-data scenarios.",
            "paper-title": "Model Balancing Helps Low-data Training and Fine-tuning",
            "image-path": "flux_paper_image/2410.12178_1729194610.png"
        },

        {
            "startTime": "15:05",
            "arxivId": "2410.12265",
            "arxivLink": "https://arxiv.org/abs/2410.12265",
            "title": "LLMs as Reviewers: A Cost-Effective, Automated Peer-Review System for Language Models",
            "institute": "Tsinghua University",
            "text": "This research introduces Auto-PRE, an automated framework for evaluating language models (LLMs) that uses other LLMs as reviewers. Unlike previous methods that rely on human annotations or homogeneous LLMs, Auto-PRE selects evaluator LLMs based on their consistency, self-confidence, and pertinence, making it more objective and cost-efficient.",
            "paper-title": "An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation",
            "image-path": "flux_paper_image/2410.12265_1729194344.png"
        },

        {
            "startTime": "15:32",
            "arxivId": "2410.12201",
            "arxivLink": "https://arxiv.org/abs/2410.12201",
            "title": "Merging Uncertainty Sets: A Data-Light Approach to Combining Confidence Intervals",
            "institute": "Fudan University, Yale University",
            "text": "This research introduces a novel method called SAT (Synthetic, Aggregation, and Test inversion) for merging uncertainty sets. Unlike previous methods that require access to raw data, SAT only relies on the initial sets and their control levels, making it a data-light approach.",
            "paper-title": "SAT: Data-light Uncertainty Set Merging via Synthetics, Aggregation, and Test Inversion",
            "image-path": "flux_paper_image/2410.12201_1729195247.png"
        },

        {
            "startTime": "15:57",
            "arxivId": "2410.12555",
            "arxivLink": "https://arxiv.org/abs/2410.12555",
            "title": "GPT-2's Hidden Levers: Unmasking the Secrets of Sparse Autoencoders",
            "institute": "Harvard University",
            "text": "This research introduces a new baseline for measuring the impact of perturbations on language models, addressing a flaw in previous work. It also compares different types of Sparse Autoencoders (SAEs) and their influence on model outputs.",
            "paper-title": "Investigating Sensitive Directions in GPT-2: An Improved Baseline and Comparative Analysis of SAEs",
            "image-path": "flux_paper_image/2410.12555_1729193733.png"
        },

        {
            "startTime": "16:18",
            "arxivId": "2410.12292",
            "arxivLink": "https://arxiv.org/abs/2410.12292",
            "title": "Long-Range Context: Do LLMs Really \"Get It\"?",
            "institute": "Nvidia",
            "text": "This research focuses on how well contextualized representations in language models capture long-range context, which spans thousands of tokens. Unlike previous work that primarily analyzed short sequences, this study investigates the ability of models to encode information from distant parts of a text.",
            "paper-title": "How much do contextualized representations encode long-range context?",
            "image-path": "flux_paper_image/2410.12292_1729195538.png"
        },

        {
            "startTime": "16:38",
            "arxivId": "2410.12343",
            "arxivLink": "https://arxiv.org/abs/2410.12343",
            "title": "Graph Clustering Goes Decentralized: Federated Learning for Dynamic Networks",
            "institute": "Chinese Academy of Science, Xi\u2019an Jiaotong University, Tsinghua University...",
            "text": "This research introduces a Federated Temporal Graph Clustering (FTGC) framework that allows for decentralized training of graph neural networks (GNNs) across multiple clients, ensuring data privacy while capturing the evolution of graph structures over time. This approach differs from previous work by incorporating a temporal aggregation mechanism and a federated optimization strategy to address the challenges of communication efficiency and data privacy in dynamic graph clustering.",
            "paper-title": "Federated Temporal Graph Clustering",
            "image-path": "flux_paper_image/2410.12343_1729193206.png"
        },

        {
            "startTime": "17:18",
            "arxivId": "2410.12289",
            "arxivLink": "https://arxiv.org/abs/2410.12289",
            "title": "Kalman Filters Get a Deep Learning Makeover: AI-Aided State Estimation",
            "institute": "Ben-Gurion University of the Negev, ETH Zurich, University of Massachusetts Boston...",
            "text": "This research explores the fusion of AI techniques, specifically deep neural networks (DNNs), with classic Kalman filtering algorithms. The paper focuses on designing AI-aided Kalman filters that leverage partial knowledge of the underlying system dynamics, rather than replacing the Kalman filter entirely.",
            "paper-title": "AI-Aided Kalman Filters",
            "image-path": "flux_paper_image/2410.12289_1729193914.png"
        },

        {
            "startTime": "17:44",
            "arxivId": "2410.12241",
            "arxivLink": "https://arxiv.org/abs/2410.12241",
            "title": "Taming the Curse: A New Trick for Training AI Models on Multi-Dimensional Data",
            "institute": "Stanford University",
            "text": "This research proposes a novel training strategy for neural network-based surrogate models of PDEs that dramatically reduces the cost of data generation. The authors leverage transfer learning to train a CNN surrogate on a mixture of numerical solutions to the original d-dimensional problem and its (d-1)-dimensional approximation.",
            "paper-title": "Transfer Learning on Multi-Dimensional Data: A Novel Approach to Neural Network-Based Surrogate Modeling",
            "image-path": "flux_paper_image/2410.12241_1729192968.png"
        },

        {
            "startTime": "18:02",
            "arxivId": "2410.12700",
            "arxivLink": "https://arxiv.org/abs/2410.12700",
            "title": "AI Gets a Moral Compass: New Method Makes Text-to-Image Models More Ethical",
            "institute": "Microsoft, Tsinghua University",
            "text": "This research focuses on aligning text-to-image (T2I) models with human values, a challenge largely unexplored in previous work. Unlike existing methods that primarily address image-to-text (I2T) generation or focus on specific issues like debiasing, this paper proposes a lightweight value optimization method (LiVO) that can handle diverse ethical risks.",
            "paper-title": "Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via Lightweight Value Optimization",
            "image-path": "flux_paper_image/2410.12700_1729192905.png"
        },

        {
            "startTime": "18:23",
            "arxivId": "2410.12294",
            "arxivLink": "https://arxiv.org/abs/2410.12294",
            "title": "AI Tutors Learn to Make the Same Mistakes as Students!",
            "institute": "Rice University, ETH Zurich",
            "text": "This research explores the potential of Large Language Models (LLMs) to accurately model student misconceptions in algebra, going beyond simply replicating correct solutions. It introduces a novel Python library, MalAlgoPy, to generate datasets reflecting authentic student solution patterns, including both correct and incorrect steps.",
            "paper-title": "Towards LLM-based Cognitive Models of Students with Misconceptions",
            "image-path": "flux_paper_image/2410.12294_1729193561.png"
        },

        {
            "startTime": "18:46",
            "arxivId": "2410.12119",
            "arxivLink": "https://arxiv.org/abs/2410.12119",
            "title": "LLMs on a Diet: Scaling Laws for Quantized Language Models",
            "institute": "Yale University",
            "text": "This research investigates scaling laws for post-training quantization of large language models (LLMs), a process that compresses models for efficient inference. Unlike previous work focusing on pre-training scaling laws, this study explores how factors like the local loss landscape, quantization data type, and optimization algorithms impact the performance of quantized LLMs.",
            "paper-title": "Scaling laws for post-training quantized large language models",
            "image-path": "flux_paper_image/2410.12119_1729193127.png"
        },

        {
            "startTime": "19:08",
            "arxivId": "2410.12413",
            "arxivLink": "https://arxiv.org/abs/2410.12413",
            "title": "Transformers Without Positional Encoding: A Deep Dive into Hierarchical Language Recognition and Generation",
            "institute": "University of Tokyo",
            "text": "This research provides constructive proofs that Transformers can recognize and generate hierarchical languages without relying on specific positional encoding. Unlike previous work, which often required specific positional encoding or larger network widths, this study demonstrates that causal masking and a starting token can enable Transformers to compute positional information and depth within hierarchical structures.",
            "paper-title": "Theoretical Analysis of Hierarchical Language Recognition and Generation by Transformers without Positional Encoding",
            "image-path": "flux_paper_image/2410.12413_1729194674.png"
        },

        {
            "startTime": "19:33",
            "arxivId": "2410.12712",
            "arxivLink": "https://arxiv.org/abs/2410.12712",
            "title": "Quantum Communication: A Memory Game for Purity Estimation",
            "institute": "Harvard University, Tsinghua University",
            "text": "This research explores the sample complexity of quantum purity estimation and inner product estimation, focusing on protocols with limited quantum communication and memory. It establishes a strong connection between the sample complexity of these tasks, showing that the same scaling applies to both.",
            "paper-title": "On the sample complexity of purity and inner product estimation",
            "image-path": "flux_paper_image/2410.12712_1729192935.png"
        },

        {
            "startTime": "19:56",
            "arxivId": "2410.12439",
            "arxivLink": "https://arxiv.org/abs/2410.12439",
            "title": "Explaining AI: From Pixels to Punching Bags!",
            "institute": "Peking University",
            "text": "This research proposes ConLUX, a framework that elevates existing model-agnostic explanation techniques from feature-level to concept-level. Unlike previous work, ConLUX automatically extracts high-level concepts from large pre-trained models, enabling it to explain various machine learning models without requiring model-specific knowledge.",
            "paper-title": "ConLUX: Concept-Based Local Unified Explanations",
            "image-path": "flux_paper_image/2410.12439_1729192897.png"
        },

        {
            "startTime": "20:17",
            "arxivId": "2410.12483",
            "arxivLink": "https://arxiv.org/abs/2410.12483",
            "title": "Robot Placement Planner: It's All About the Contacts!",
            "institute": "University of Toronto",
            "text": "This research introduces a novel approach to object placement planning by prioritizing the selection of contact points before determining the object's pose. This contrasts with traditional methods that typically sample poses and then evaluate their stability.",
            "paper-title": "Stable Object Placement Planning From Contact Point Robustness",
            "image-path": "flux_paper_image/2410.12483_1729194725.png"
        },

        {
            "startTime": "20:34",
            "arxivId": "2410.12501",
            "arxivLink": "https://arxiv.org/abs/2410.12501",
            "title": "Virtual Try-On Gets a Text Makeover: Deep Learning Meets Fashion Sense!",
            "institute": "Beijing Institute of Technology, Tsinghua University",
            "text": "This research introduces a new approach to virtual try-on that leverages a deep text-driven model, specifically InternViT-6B, to extract fine-grained garment semantics. This differs from previous methods that relied on CLIP, which primarily focuses on coarse textual captions.",
            "paper-title": "DH-VTON: Deep Text-Driven Virtual Try-On via Hybrid Attention Learning",
            "image-path": "flux_paper_image/2410.12501_1729193810.png"
        },

        {
            "startTime": "20:55",
            "arxivId": "2410.11985",
            "arxivLink": "https://arxiv.org/abs/2410.11985",
            "title": "Weight Decay's Secret Bias: How LLMs Forget Rare Words",
            "institute": "MIT, Texas A&M University, Brown University",
            "text": "This research investigates the impact of weight decay on the token-level performance of large language models (LLMs), revealing that increasing weight decay disproportionately harms the performance of low-frequency tokens. This finding is unique because previous research has focused on the overall impact of weight decay on model performance, overlooking its effects on individual tokens.",
            "paper-title": "The Fair Language Model Paradox",
            "image-path": "flux_paper_image/2410.11985_1729193100.png"
        },

        {
            "startTime": "21:21",
            "arxivId": "2410.12101",
            "arxivLink": "https://arxiv.org/abs/2410.12101",
            "title": "Unraveling the Persian Rug: How Large-Scale Symmetries Solve Superposition in Neural Networks",
            "institute": "Stanford University",
            "text": "This research delves into a toy model of superposition, focusing on the regime of permutation symmetric data and large input dimensions. It goes beyond empirical observations to analytically derive the loss function and identify the optimal algorithm, which is shown to be statistically permutation symmetric.",
            "paper-title": "The Persian Rug: solving toy models of superposition using large-scale symmetries",
            "image-path": "flux_paper_image/2410.12101_1729195023.png"
        },

        {
            "startTime": "21:43",
            "arxivId": "2410.12409",
            "arxivLink": "https://arxiv.org/abs/2410.12409",
            "title": "Language Agents: Planning is Hard, Constraints are Harder!",
            "institute": "Fudan University, CMU, ByteDance...",
            "text": "This research delves into the limitations of language agents in planning tasks, specifically focusing on the role of constraints and questions in the planning process. Unlike previous studies that mainly observed weak performance, this paper uses feature attribution analysis to uncover the deeper reasons behind these limitations.",
            "paper-title": "Revealing the Barriers of Language Agents in Planning",
            "image-path": "flux_paper_image/2410.12409_1729194128.png"
        },

        {
            "startTime": "22:11",
            "arxivId": "2410.12112",
            "arxivLink": "https://arxiv.org/abs/2410.12112",
            "title": "LLMs Go From Chatty to Code-Savvy: Planning Problems Solved with Optimization!",
            "institute": "MIT, IBM",
            "text": "This research proposes a novel approach to planning problems by leveraging LLMs to formally encode them as optimization problems, eliminating the need for task-specific examples.",
            "paper-title": "Planning Anything with Rigor: General-Purpose Zero-Shot Planning with LLM-based Formalized Programming",
            "image-path": "flux_paper_image/2410.12112_1729194389.png"
        },

        {
            "startTime": "22:38",
            "arxivId": "2410.12530",
            "arxivLink": "https://arxiv.org/abs/2410.12530",
            "title": "Federated Learning: Unentangling Data for Faster, More Efficient Models",
            "institute": "Tsinghua University, WeBank",
            "text": "This paper proposes a new algorithm, FedDistr, that leverages stable diffusion models to disentangle data distributions across clients in federated learning. This approach allows for a single round of communication, significantly improving efficiency compared to traditional methods that require multiple rounds.",
            "paper-title": "Disentangling data distribution for Federated Learning",
            "image-path": "flux_paper_image/2410.12530_1729194135.png"
        },

        {
            "startTime": "23:07",
            "arxivId": "2410.11934",
            "arxivLink": "https://arxiv.org/abs/2410.11934",
            "title": "Fluid Flow's New Trick: Self-Supervised Learning with a Splat of Zero-Divergence!",
            "institute": "Chinese Academy of Sciences, Tsinghua University, Beijing Institute of Technology",
            "text": "This research introduces a self-supervised method for dual-frame fluid motion estimation, which differs from previous supervised methods by requiring only 1% of the training data.",
            "paper-title": "Dual-frame Fluid Motion Estimation with Test-time Optimization and Zero-divergence Loss",
            "image-path": "flux_paper_image/2410.11934_1729192843.png"
        },

        {
            "startTime": "23:31",
            "arxivId": "2410.11883",
            "arxivLink": "https://arxiv.org/abs/2410.11883",
            "title": "Scattering is All You Need: Cosmologists Find a New Way to Crunch Data",
            "institute": "University College London",
            "text": "This research demonstrates the successful use of scattering representations without further compression for simulation-based inference (SBI) with images, specifically in a cosmological context. This approach differs from previous work by avoiding the need for additional simulations for training or computing derivatives, making it more efficient and interpretable.",
            "paper-title": "Simulation-based inference with scattering representations: scattering is all you need",
            "image-path": "flux_paper_image/2410.11883_1729193361.png"
        },

        {
            "startTime": "23:55",
            "arxivId": "2410.12164",
            "arxivLink": "https://arxiv.org/abs/2410.12164",
            "title": "Table-LLM-Specialist: Fine-Tuning Tables Without the Tears (of Labeling)",
            "institute": "University of Michigan, Microsoft",
            "text": "This research proposes a new fine-tuning paradigm called \"Table-LLM-Specialist\" that leverages the duality of table tasks to iteratively generate and validate training data, eliminating the need for expensive manual labeling. This approach differs from previous methods like \"dataset-specific fine-tuning\" which often leads to overfitting and poor generalization.",
            "paper-title": "Table-LLM-Specialist: Language Model Specialists for Tables using Iterative Generator-Validator Fine-tuning",
            "image-path": "flux_paper_image/2410.12164_1729193921.png"
        },

        {
            "startTime": "24:17",
            "arxivId": "2410.12411",
            "arxivLink": "https://arxiv.org/abs/2410.12411",
            "title": "Robot Farmers Get a Vision Makeover: Self-Supervised Learning for Under-Canopy Navigation",
            "institute": "University of Illinois Urbana-Champaign, University of Bonn, University of Oxford...",
            "text": "This research introduces a self-supervised online adaptation method for improving the accuracy of under-canopy navigation in agricultural robots. Unlike previous methods that rely on large labeled datasets, this approach uses minimal data and adapts the perception model directly on the robot during deployment.",
            "paper-title": "AdaCropFollow: Self-Supervised Online Adaptation for Visual Under-Canopy Navigation",
            "image-path": "flux_paper_image/2410.12411_1729194085.png"
        },

        {
            "startTime": "24:44",
            "arxivId": "2410.12075",
            "arxivLink": "https://arxiv.org/abs/2410.12075",
            "title": "Weathering the Domain Shift: How AI Learned to Drive in Any Weather",
            "institute": "Carnegie Mellon University, University of Leeds",
            "text": "This research proposes a novel data augmentation approach called WeatherDG, which leverages Stable Diffusion and a Large Language Model (LLM) to generate realistic, weather-diverse images for training semantic segmentation models. Unlike previous methods that rely on simulators or image translation models, WeatherDG generates images that are more diverse and authentic, particularly in adverse weather conditions.",
            "paper-title": "WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation",
            "image-path": "flux_paper_image/2410.12075_1729193608.png"
        },

        {
            "startTime": "25:05",
            "arxivId": "2410.12783",
            "arxivLink": "https://arxiv.org/abs/2410.12783",
            "title": "Transformers: They're Not Just for Language Anymore!",
            "institute": "UC San Diego, Broad Institute of MIT and Harvard, Harvard University...",
            "text": "This paper introduces two distinct aspects of generalization in In-Context Learning (ICL): context-scaling and task-scaling. It then demonstrates that a simplified transformer architecture, SGPT, can achieve both context-scaling and task-scaling, unlike standard Multi-Layer Perceptrons (MLPs).",
            "paper-title": "Context-Scaling versus Task-Scaling in In-Context Learning",
            "image-path": "flux_paper_image/2410.12783_1729194942.png"
        },

        {
            "startTime": "25:38",
            "arxivId": "2410.11908",
            "arxivLink": "https://arxiv.org/abs/2410.11908",
            "title": "ChatHouseDiffusion: Building Your Dream Home, One Text Prompt at a Time!",
            "institute": "Tsinghua University, Southwest Jiaotong University",
            "text": "This research introduces ChatHouseDiffusion, a model that uses large language models (LLMs) to interpret natural language input and generate floor plans. Unlike previous methods that rely on pre-defined room layouts or bubble diagrams, ChatHouseDiffusion allows for more flexible and interactive design adjustments based on user input.",
            "paper-title": "ChatHouseDiffusion: Prompt-Guided Generation and Editing of Floor Plans",
            "image-path": "flux_paper_image/2410.11908_1729193951.png"
        },

        {
            "startTime": "26:04",
            "arxivId": "2410.12071",
            "arxivLink": "https://arxiv.org/abs/2410.12071",
            "title": "LLMs in Software: Beyond the Bug Hunt, Into the Metric Maze!",
            "institute": "CMU",
            "text": "This research focuses on the emerging solutions software developers are adopting to integrate LLMs into products, going beyond simply identifying challenges. It analyzes 19 solutions across various Microsoft teams, providing a real-world perspective on their adoption and effectiveness.",
            "paper-title": "Beyond the Comfort Zone: Emerging Solutions to Overcome Challenges in Integrating LLMs into Software Products",
            "image-path": "flux_paper_image/2410.12071_1729195712.png"
        },

        {
            "startTime": "26:30",
            "arxivId": "2410.11905",
            "arxivLink": "https://arxiv.org/abs/2410.11905",
            "title": "LLMs Chatting: A New Protocol for AI Teamwork",
            "institute": "University of Oxford",
            "text": "This paper introduces Agora, a meta-protocol that uses a combination of structured data and natural language to enable efficient communication between heterogeneous LLMs. This approach differs from previous work by addressing the Agent Communication Trilemma, which highlights the trade-offs between versatility, efficiency, and portability in communication protocols.",
            "paper-title": "A Scalable Communication Protocol for Networks of Large Language Models",
            "image-path": "flux_paper_image/2410.11905_1729192889.png"
        },

        {
            "startTime": "26:55",
            "arxivId": "2410.12692",
            "arxivLink": "https://arxiv.org/abs/2410.12692",
            "title": "Brain Tumor Detection: CNNs Outsmart the Docs (Almost)!",
            "institute": "MIT",
            "text": "This study compares various statistical and machine learning models for brain tumor detection and classification, finding that CNNs outperform other models in both binary and multi-class classification tasks.",
            "paper-title": "Machine Learning Approach to Brain Tumor Detection and Classification",
            "image-path": "flux_paper_image/2410.12692_1729193025.png"
        },

        {
            "startTime": "27:20",
            "arxivId": "2410.12655",
            "arxivLink": "https://arxiv.org/abs/2410.12655",
            "title": "Protein Classification: A Weighted PSS Kernel Matrix Takes the Cake!",
            "institute": "Georgia State University, Lahore University of Management Sciences, IBM",
            "text": "This research proposes a novel kernel function called the weighted position-specific scoring kernel matrix (W-PSSKM) for protein sequence classification. Unlike previous methods, W-PSSKM incorporates both the frequency information of amino acids and their relative positions within the sequence.",
            "paper-title": "Position Specific Scoring Is All You Need? Revisiting Protein Sequence Classification Tasks",
            "image-path": "flux_paper_image/2410.12655_1729194260.png"
        },

        {
            "startTime": "27:44",
            "arxivId": "2410.12677",
            "arxivLink": "https://arxiv.org/abs/2410.12677",
            "title": "Linear Adversarial Training: Faster Than a Speeding Bullet (and More Accurate Too!)",
            "institute": "Uppsala University, PSL Research University, INRIA",
            "text": "This paper proposes new optimization algorithms specifically tailored for linear adversarial training, which is a method for making machine learning models more robust against malicious data manipulation. Unlike previous approaches that rely on generic convex solvers, these algorithms leverage the structure of the problem to achieve significantly faster convergence rates, especially in high-dimensional settings.",
            "paper-title": "Efficient Optimization Algorithms for Linear Adversarial Training",
            "image-path": "flux_paper_image/2410.12677_1729194397.png"
        },

        {
            "startTime": "28:06",
            "arxivId": "2410.12456",
            "arxivLink": "https://arxiv.org/abs/2410.12456",
            "title": "Neural Samplers: Diffusing Away Mode Collapse!",
            "institute": "University of Cambridge, University College London",
            "text": "This paper introduces a new training objective called \"diffusive KL divergence\" (DiKL) for neural samplers. Unlike traditional reverse KL divergence, which often leads to \"mode collapse,\" DiKL encourages the model to capture multiple modes in the target distribution by convolving both the model and target densities with Gaussian diffusion kernels.",
            "paper-title": "Training Neural Samplers with Reverse Diffusive KL Divergence",
            "image-path": "flux_paper_image/2410.12456_1729194553.png"
        },

        {
            "startTime": "28:36",
            "arxivId": "2410.12375",
            "arxivLink": "https://arxiv.org/abs/2410.12375",
            "title": "LLMs Learn to Think: New Framework Makes AI Reason Like a Scientist",
            "institute": "MIT",
            "text": "This research introduces PRefLexOR, a framework that combines preference optimization with recursive reasoning inspired by Reinforcement Learning (RL) principles. Unlike previous approaches, PRefLexOR enables models to self-teach by iterating over thought processes, refining reasoning, and continuously learning from both preferred and rejected outputs.",
            "paper-title": "PRefLexOR: Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning and Agentic Thinking",
            "image-path": "flux_paper_image/2410.12375_1729195454.png"
        },

        {
            "startTime": "28:56",
            "arxivId": "2410.12730",
            "arxivLink": "https://arxiv.org/abs/2410.12730",
            "title": "Counterfactual Modeling: A New Way to Predict What Might Have Been",
            "institute": "UC Berkeley, Genentech",
            "text": "This research proposes a novel variational Bayesian causal inference framework for counterfactual generative modeling. Unlike previous work that focused on conditional variational autoencoders, this framework directly optimizes the individual-specific outcome likelihood, leading to more accurate and robust counterfactual predictions.",
            "paper-title": "Counterfactual Generative Modeling with Variational Causal Inference",
            "image-path": "flux_paper_image/2410.12730_1729194209.png"
        },

        {
            "startTime": "29:19",
            "arxivId": "2410.12722",
            "arxivLink": "https://arxiv.org/abs/2410.12722",
            "title": "WorldMedQA-V: AI Exams Get a Multimodal Makeover!",
            "institute": "Harvard University",
            "text": "This research introduces WorldMedQA-V, a multilingual, multimodal dataset for evaluating vision-language models (VLMs) in healthcare. Unlike previous datasets, WorldMedQA-V includes medical images alongside multiple-choice questions, making it more realistic for evaluating AI systems in real-world healthcare settings.",
            "paper-title": "WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation",
            "image-path": "flux_paper_image/2410.12722_1729194715.png"
        },

        {
            "startTime": "29:44",
            "arxivId": "2410.11911",
            "arxivLink": "https://arxiv.org/abs/2410.11911",
            "title": "AI Learns to Hear Gravitational Waves in Noisy Space!",
            "institute": "CMU",
            "text": "This research introduces a novel training methodology for a simplified multi-layer perceptron (MLP) model, enabling it to accurately detect gravitational waves amidst highly complex noise. The key innovation lies in the use of transfer learning, where the model is first trained on clean data and then fine-tuned on noisy data, allowing it to adapt to changing noise conditions. This approach differs from previous work that relied on more complex architectures or unsupervised learning methods.",
            "paper-title": "Transfer Learning Adapts to Changing PSD in Gravitational Wave Data",
            "image-path": "flux_paper_image/2410.11911_1729195798.png"
        },

        {
            "startTime": "30:03",
            "arxivId": "2410.12053",
            "arxivLink": "https://arxiv.org/abs/2410.12053",
            "title": "Brain MRI's New Trick: Rotating Images to Learn More!",
            "institute": "Stanford University",
            "text": "This research introduces a new method for encoding 3D MRIs that enforces SO(3)-equivariance, meaning the model learns to understand rotations in 3D space. This is different from previous work that often ignores or removes geometric information.",
            "paper-title": "SOE: SO(3)-Equivariant 3D MRI Encoding",
            "image-path": "flux_paper_image/2410.12053_1729192692.png"
        },

        {
            "startTime": "30:29",
            "arxivId": "2410.12032",
            "arxivLink": "https://arxiv.org/abs/2410.12032",
            "title": "AI's Energy Diet: From Tiny Chips to Megawatt Monsters, This Benchmark Measures It All!",
            "institute": "Harvard University, Meta, NVIDIA...",
            "text": "This research introduces MLPerf Power, a standardized methodology for benchmarking the energy efficiency of machine learning systems across a wide range of scales, from tiny IoT devices to massive datacenter clusters. This differs from previous work by focusing on full system power consumption, including auxiliary components, and by providing a comprehensive analysis of energy efficiency trends across multiple generations of MLPerf submissions.",
            "paper-title": "MLPerf Power: Benchmarking the Energy Efficiency of Machine Learning Systems from {\\mu}Watts to MWatts for Sustainable AI",
            "image-path": "flux_paper_image/2410.12032_1729195323.png"
        },

        {
            "startTime": "30:51",
            "arxivId": "2410.12591",
            "arxivLink": "https://arxiv.org/abs/2410.12591",
            "title": "Explaining AI Decisions: Zooming In on the \"Why\" with Region-Constrained Counterfactuals",
            "institute": "University of Warsaw, Warsaw University of Technology, Harvard University",
            "text": "This research introduces region-constrained visual counterfactual explanations (RVCEs), which differ from previous methods by focusing on modifying only a predefined region of an image to understand how an AI model's decision changes.",
            "paper-title": "Rethinking Visual Counterfactual Explanations Through Region Constraint",
            "image-path": "flux_paper_image/2410.12591_1729195029.png"
        },

        {
            "startTime": "31:20",
            "arxivId": "2410.12544",
            "arxivLink": "https://arxiv.org/abs/2410.12544",
            "title": "LQ Games: Unveiling the Hidden Nash Equilibrium Trio!",
            "institute": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne",
            "text": "This research focuses on finding all Nash equilibria in a specific type of game called a scalar discrete-time infinite-horizon linear quadratic (LQ) game. Unlike previous work, which often focused on proving the existence or uniqueness of Nash equilibria, this paper provides a method to compute all of them.",
            "paper-title": "Nash equilibria in scalar discrete-time linear quadratic games",
            "image-path": "flux_paper_image/2410.12544_1729195497.png"
        },

        {
            "startTime": "31:45",
            "arxivId": "2410.12763",
            "arxivLink": "https://arxiv.org/abs/2410.12763",
            "title": "Gravity-Guided Rotation Averaging: When Your Phone Knows Which Way is Up!",
            "institute": "ETH Zurich",
            "text": "This research introduces a novel approach to rotation averaging in Structure-from-Motion (SfM) by incorporating gravity direction as an additional constraint. Unlike previous methods that rely on 3-DoF rotations, this method leverages the known gravity direction to simplify the problem to a 1-DoF optimization, resulting in faster and more accurate camera orientation estimation.",
            "paper-title": "Gravity-aligned Rotation Averaging with Circular Regression",
            "image-path": "flux_paper_image/2410.12763_1729193260.png"
        },

        {
            "startTime": "32:09",
            "arxivId": "2410.12424",
            "arxivLink": "https://arxiv.org/abs/2410.12424",
            "title": "Plasma Tomography Gets a Bayesian Makeover: Ion Temperature and Velocity Revealed!",
            "institute": "National Institute of Fusion Science, University of Tokyo",
            "text": "This research introduces a novel Bayesian tomography approach for Coherence Imaging Spectroscopy (CIS) that simultaneously reconstructs ion temperature and velocity distributions in plasmas. Unlike conventional CIS tomography, this method incorporates nonlinear effects and temperature dependencies, enabling robust reconstruction even in regions of high temperature and velocity.",
            "paper-title": "Nonlinear bayesian tomography of ion temperature and velocity for Doppler coherence imaging spectroscopy in RT-1",
            "image-path": "flux_paper_image/2410.12424_1729193746.png"
        },

        {
            "startTime": "32:33",
            "arxivId": "2410.12001",
            "arxivLink": "https://arxiv.org/abs/2410.12001",
            "title": "Legal AI's Attention Span: Does More Training Mean Better Understanding?",
            "institute": "CMU",
            "text": "This research investigates the impact of continued pre-training and instruction fine-tuning (IFT) on large language models (LLMs) trained on legal corpora. It analyzes how these training methods affect the models' attention to human-defined legal concepts, going beyond simply measuring performance on benchmarks.",
            "paper-title": "Impacts of Continued Legal Pre-Training and IFT on LLMs' Latent Representations of Human-Defined Legal Concepts",
            "image-path": "flux_paper_image/2410.12001_1729192764.png"
        }
    ],
    "stats": {
        "num_pick": 79,
        "num_total": 381,
    },
    "audio": "https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202410171324_audio.mp3"
}
