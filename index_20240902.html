<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - AI Paper Picks of the Day</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 80px;"></div>
            <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                Fresh Picks:
                <span class="highlightNumber" style="font-size: 28px;">34</span> out of <span
                    class="highlightNumber">205</span> arXiv AI
                papers
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="line-height: 1.2;">just released on</div>
                <div id="data-default-date" data-date="2024-09-02"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 10px;">
            </div>
        </div>

        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">
                00:55
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16916" target="_blank">
                        @arXiv 2408.16916
                    </a>
                    <span class="tweet-title">
                        Brain's Color Vision: A Computational Mystery Solved?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel computational framework for modeling the emergence of human color
                    vision by simulating both the eye and the cortex. Unlike previous work, it does not assume a priori
                    knowledge of color dimensionality but instead infers it from the optic nerve signals.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">
                01:19
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.17355" target="_blank">
                        @arXiv 2408.17355
                    </a>
                    <span class="tweet-title">
                        Action Chunking: A Decoding Algorithm That's Smarter Than Your Average Robot
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new decoding algorithm called Bidirectional Decoding (BID) that improves
                    action chunking by considering both past decisions and future plans. Unlike previous methods that
                    rely on random sampling or averaging, BID searches for the optimal action from a batch of sampled
                    plans, enhancing temporal consistency and reactivity in stochastic environments.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">
                01:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16944" target="_blank">
                        @arXiv 2408.16944
                    </a>
                    <span class="tweet-title">
                        Robots Learn New Tricks by Watching Old Movies (of Themselves)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for few-shot imitation learning that leverages motion similarity
                    extracted from optical flow representations, rather than relying solely on visual or semantic
                    similarity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">
                02:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.17016" target="_blank">
                        @arXiv 2408.17016
                    </a>
                    <span class="tweet-title">
                        Machine Learning's New Party Trick: Finding Hidden Interactions with Controlled Error!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Waterloo, University of Michigan, University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Diamond, a method for discovering feature interactions in machine learning
                    models while controlling the false discovery rate (FDR). Unlike previous approaches, Diamond
                    leverages the model-X knockoffs framework to generate dummy features that mimic the original data's
                    structure, enabling a more robust and reliable estimation of FDR.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">
                02:37
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.17005" target="_blank">
                        @arXiv 2408.17005
                    </a>
                    <span class="tweet-title">
                        Camera Exposure Control: DRL Makes VO See the Light!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Hong Kong University of Science and Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a deep reinforcement learning (DRL) framework for camera exposure control in
                    visual odometry (VO) systems. Unlike previous DRL-based methods that require online training, this
                    approach utilizes an offline training scheme with a lightweight image simulator, enabling efficient
                    and robust exposure control.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">
                03:05
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16987" target="_blank">
                        @arXiv 2408.16987
                    </a>
                    <span class="tweet-title">
                        AI Explains Models, Not Data: The Misinterpretation Trap
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Iowa, Yale University, University of Electronic Science and Technology of China...
                    </span>
                </div>
                <div class="primary-text">
                    This research investigates the validity of using post hoc explainers, like SHAP and LIME, to draw
                    inferences about data relationships. It goes beyond simply explaining the model's behavior and
                    examines whether these explanations accurately reflect the true marginal effects of features in the
                    data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">
                03:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.17424" target="_blank">
                        @arXiv 2408.17424
                    </a>
                    <span class="tweet-title">
                        Camera Controllable Video Previsualization: AI Meets the Movie Set
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Purple Mountain Observatory, Stanford University, Hong Kong University of Science and
                        Technology...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces CinePreGen, a system that combines game engines with diffusion models for
                    video previsualization. Unlike previous methods that primarily focused on object motion control,
                    CinePreGen offers dynamic camera control, allowing users to create cinematic camera movements like
                    pan, tilt, push, and dolly zoom.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">
                03:59
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16981" target="_blank">
                        @arXiv 2408.16981
                    </a>
                    <span class="tweet-title">
                        Q-Learning's Communication Diet: How to Slim Down Without Losing Brains!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research establishes a lower bound on the communication complexity of federated Q-learning
                    algorithms, demonstrating that any algorithm achieving a speedup in convergence rate must incur a
                    communication cost proportional to the effective horizon of the MDP. Additionally, the paper
                    proposes a new algorithm, Fed-DVR-Q, which achieves both order-optimal sample and communication
                    complexities, bridging the gap between existing upper and lower bounds.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">
                04:25
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16852" target="_blank">
                        @arXiv 2408.16852
                    </a>
                    <span class="tweet-title">
                        Star-Shaped Regularizers: A Geometric Approach to Unsupervised Learning
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Los Angeles, Johns Hopkins University, National University of Singapore
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the structure of regularizers learned through a critic-based loss function,
                    focusing on a specific family of regularizers called gauges of star-shaped bodies. It leverages
                    tools from star geometry and dual Brunn-Minkowski theory to analyze the optimal regularizer in
                    certain cases.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">
                04:49
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.17027" target="_blank">
                        @arXiv 2408.17027
                    </a>
                    <span class="tweet-title">
                        2D &amp; 3D: A Love Story of Feature Fusion
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UCSanDiego, Stanford University, StabilityAI...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel 2D-3D joint training scheme called ConDense, which extracts
                    co-embedded 2D and 3D features by enforcing consistency through a ray-marching process inspired by
                    Neural Radiance Fields (NeRFs). This approach differs from previous work by leveraging pre-trained
                    2D networks and multi-view datasets for 3D pre-training, rather than relying solely on 3D data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">
                05:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.17065" target="_blank">
                        @arXiv 2408.17065
                    </a>
                    <span class="tweet-title">
                        Deepfakes: Facial Feature Drift is the New Tell!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, Tencent Youtu Lab
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel video-level blending technique to simulate a previously
                    underexplored temporal forgery artifact called Facial Feature Drift (FFD). Unlike prior work that
                    focuses on spatial artifacts, this method specifically targets temporal inconsistencies in facial
                    features, enhancing the generalization ability of deepfake detectors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">
                05:58
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16966" target="_blank">
                        @arXiv 2408.16966
                    </a>
                    <span class="tweet-title">
                        User Summaries: Predicting Your Next Move, One Click at a Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google DeepMind
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark framework called USERSUMBENCH, which focuses on evaluating
                    user summarization approaches by measuring how accurately generated summaries predict future user
                    activities. This differs from previous work that often relies on simplistic heuristics or models
                    that struggle with the inherent subjectivity of user summaries.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">
                06:19
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16961" target="_blank">
                        @arXiv 2408.16961
                    </a>
                    <span class="tweet-title">
                        Open Feedback: AI's New Best Friend?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford, IBM
                    </span>
                </div>
                <div class="primary-text">
                    This paper explores the potential for an open ecosystem of human feedback for AI, focusing on the
                    challenges and opportunities of creating a sustainable system for collecting and sharing this data.
                    It differs from previous work by proposing a framework for a dynamic and collaborative approach to
                    human feedback, rather than relying on closed, proprietary datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">
                06:45
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.17432" target="_blank">
                        @arXiv 2408.17432
                    </a>
                    <span class="tweet-title">
                        Voice Cloning Made Easy: New TTS Method Uses Frame Selection to Mimic Anyone's Voice
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Texas at Dallas, National University of Singapore
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes SelectTTS, a multi-speaker text-to-speech (TTS) framework that directly
                    utilizes frames from the target speaker's speech for decoding, rather than relying on speaker
                    conditioning or prompts. This approach simplifies the task and reduces model complexity compared to
                    previous methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">
                07:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16982" target="_blank">
                        @arXiv 2408.16982
                    </a>
                    <span class="tweet-title">
                        Splatting with Style: Hermite Polynomials Give 3D Rendering a Boost!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes using Gaussian-Hermite polynomials as the kernel for Gaussian Splatting, a
                    technique used for 3D reconstruction and rendering. This approach differs from previous work by
                    introducing higher-rank terms in the kernel, allowing for more complex and anisotropic shapes to be
                    represented.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">
                07:40
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.17062" target="_blank">
                        @arXiv 2408.17062
                    </a>
                    <span class="tweet-title">
                        Vision Transformers: Token Pruning Gets a Vote!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, ByteDance
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces Vote&amp;Mix (VoMix), a parameter-free token reduction method for Vision
                    Transformers (ViTs). Unlike previous methods that focus on discarding unimportant tokens, VoMix aims
                    to reduce token homogeneity by identifying and mixing tokens with high similarity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">
                08:05
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.17433" target="_blank">
                        @arXiv 2408.17433
                    </a>
                    <span class="tweet-title">
                        DARES: Depth Estimation in Surgery Gets a Vector-Powered Boost!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University College London, Politecnico di Milano
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Vector-LoRA, a new adaptation technique for foundation models that
                    allocates more parameters to earlier layers, addressing the inherent feature hierarchy in deep
                    networks. This differs from previous LoRA methods that distribute parameters uniformly.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">
                08:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16913" target="_blank">
                        @arXiv 2408.16913
                    </a>
                    <span class="tweet-title">
                        Gradients Got Game: Unmasking Privacy Risks in Distributed Learning
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Vanderbilt University, University of Wisconsin-Madison, Mitsubishi Electric Research
                        Laboratories
                    </span>
                </div>
                <div class="primary-text">
                    This research provides a unified framework for analyzing inference attacks from gradients in
                    distributed learning settings, encompassing a broader range of attacks than previous work. It also
                    investigates the effectiveness of various defenses against both static and adaptive adversaries.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">
                08:58
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16984" target="_blank">
                        @arXiv 2408.16984
                    </a>
                    <span class="tweet-title">
                        AI Alignment: Beyond the "I Want" and Into the "We Should"
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT, UC Berkeley, University College London...
                    </span>
                </div>
                <div class="primary-text">
                    This paper challenges the dominant "preferentist" approach to AI alignment, which assumes that human
                    values can be adequately represented by preferences and that AI systems should maximize those
                    preferences. It proposes alternative frameworks that consider the limitations of rational choice
                    theory and expected utility theory, emphasizing the importance of aligning AI with normative
                    standards appropriate to their social roles.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">
                09:25
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.17175" target="_blank">
                        @arXiv 2408.17175
                    </a>
                    <span class="tweet-title">
                        Codec's Got Talent: Giving Audio LLMs a Semantic Makeover
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Hong Kong University of Science and Technology, Microsoft, University of Science and Technology
                        Beijing...
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on improving the semantic understanding of audio codecs used in audio LLMs.
                    Unlike previous work that primarily focused on compression and acoustic reconstruction, this paper
                    introduces X-Codec, which integrates semantic features into the codec's architecture.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">
                10:01
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16978" target="_blank">
                        @arXiv 2408.16978
                    </a>
                    <span class="tweet-title">
                        Training LLMs on Long Texts: A Memory-Saving Masterclass
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Ohio State University, Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new method called Fully Pipelined Distributed Transformer (FPDT) for
                    training LLMs on extremely long sequences. Unlike previous approaches that rely on downstream
                    finetuning or adaptations, FPDT directly trains LLMs on long contexts, achieving significant
                    hardware efficiency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">
                10:25
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.17422" target="_blank">
                        @arXiv 2408.17422
                    </a>
                    <span class="tweet-title">
                        Open-Vocabulary Action Localization: VLMs Get a Time Machine!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a learning-free approach for open-vocabulary temporal action localization
                    using vision-language models (VLMs). Unlike previous methods that require training on labeled
                    datasets, this approach leverages an iterative visual prompting technique to identify actions
                    without any prior training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">
                10:45
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16829" target="_blank">
                        @arXiv 2408.16829
                    </a>
                    <span class="tweet-title">
                        Supernova Science Gets a Multimodal Makeover: Maven's the New Rosetta Stone!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University, Stony Brook University, Massachusetts Institute of Technology...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Maven, a foundation model for supernova science that uses both photometry
                    and spectroscopy simultaneously. Unlike previous models that focused on one modality or the other,
                    Maven leverages the combined information to improve performance on classification and redshift
                    estimation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">
                11:07
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.17207" target="_blank">
                        @arXiv 2408.17207
                    </a>
                    <span class="tweet-title">
                        NanoMVG: A Tiny Brain for Big Waterway Vision!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Hong Kong University of Science and Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new multi-task visual grounding model called NanoMVG, designed for
                    low-power edge devices. Unlike previous models, NanoMVG combines image and radar data with textual
                    prompts, enabling it to locate objects based on both visual and quantitative features.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">
                11:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16871" target="_blank">
                        @arXiv 2408.16871
                    </a>
                    <span class="tweet-title">
                        Graph Distillation Gets a Brain: Attention Matching Makes Condensing Datasets a Breeze!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        National University of Singapore, University of Toronto
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new method called GSTAM for condensing graph classification datasets.
                    Unlike previous methods that focus on node classification or rely on computationally intensive
                    processes, GSTAM leverages the attention maps of GNNs to distill structural information from the
                    original dataset into synthetic graphs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">
                12:01
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.17443" target="_blank">
                        @arXiv 2408.17443
                    </a>
                    <span class="tweet-title">
                        Long Videos, Short Attention Spans? This Model Gets It!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        National Taiwan University, NVIDIA, Mobile Drive Technology...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel framework called BREASE that leverages episodic memory and semantic
                    knowledge to understand long-form videos. Unlike previous approaches that often treat long videos as
                    extended short videos, BREASE simulates how humans accumulate information over time, capturing
                    action sequences and reinforcing them with semantic context.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">
                12:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.17280" target="_blank">
                        @arXiv 2408.17280
                    </a>
                    <span class="tweet-title">
                        Building a Super-Brain: Mixing LLMs for Domain Expertise
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a toolkit for creating Mixture-of-Domain-Experts (MOE) models by combining
                    pre-trained, fine-tuned models. Unlike previous work that focuses on training a single MOE model
                    from scratch, this approach leverages existing models, making it significantly more efficient.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">
                12:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16875" target="_blank">
                        @arXiv 2408.16875
                    </a>
                    <span class="tweet-title">
                        Robots Learn to Tend Machines: A Multi-Agent, Multi-Machine Tending Framework
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETS Montreal, Mila, Polytechnique MontrÂ´eal...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel multi-agent reinforcement learning (MARL) framework for mobile
                    robots to learn how to tend multiple machines simultaneously. Unlike previous work that focused on
                    single-machine tending or assumed infinite storage capacity, this study addresses the complexities
                    of real-world manufacturing scenarios where robots need to navigate between machines, collect parts,
                    and deliver them to storage.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">
                13:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.17337" target="_blank">
                        @arXiv 2408.17337
                    </a>
                    <span class="tweet-title">
                        OOD Detection: When AI Gets Tricked by Rulers and Annotations!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research challenges the common assumption that out-of-distribution (OOD) artefacts always lead
                    to uncertain model outputs. It demonstrates that OOD artefacts can actually boost a model's
                    confidence in its predictions, contradicting the foundation of many confidence-based OOD detection
                    methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">
                14:05
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.17428" target="_blank">
                        @arXiv 2408.17428
                    </a>
                    <span class="tweet-title">
                        AI's New Glasses: Fixing OCR Errors with a Dash of Context!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University College London
                    </span>
                </div>
                <div class="primary-text">
                    This research explores using pre-trained language models (LLMs) to correct errors in Optical
                    Character Recognition (OCR) outputs, specifically focusing on historical newspaper archives. Unlike
                    previous work that primarily relied on crowd-sourcing or specific algorithms, this study leverages
                    the contextual understanding and infilling capabilities of LLMs to improve OCR accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">
                14:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16877" target="_blank">
                        @arXiv 2408.16877
                    </a>
                    <span class="tweet-title">
                        Modularity Goes Temporal: A New Way to Find Communities in Ever-Changing Networks
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CNRS, UCBL, INSA Lyon...
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces Longitudinal Modularity (L-Modularity), a new way to measure community
                    structure in dynamic networks represented as link streams. Unlike previous methods, L-Modularity is
                    independent of the time scale of analysis and can handle communities that evolve over time.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">
                14:47
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.17221" target="_blank">
                        @arXiv 2408.17221
                    </a>
                    <span class="tweet-title">
                        Lightning Strikes: Unmasking the Geometry of Self-Attention Networks
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto, KTH Royal Institute of Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research delves into the geometry of function spaces defined by self-attention networks,
                    specifically focusing on the "lightning" variant where attention weights are unnormalized. It uses
                    tools from algebraic geometry to analyze the identifiability of deep attention and compute the
                    dimension of the function space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">
                15:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16890" target="_blank">
                        @arXiv 2408.16890
                    </a>
                    <span class="tweet-title">
                        Robots in Warehouses: Learning to Optimize for Maximum Efficiency!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel "learn-then-optimize" approach to large-scale neighborhood search,
                    which uses machine learning to predict objective improvements based on subproblem features and an
                    online optimization model to generate new subproblems at each iteration. This differs from previous
                    work that typically relies on pre-defined candidate neighborhoods or sequential decomposition
                    methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">
                15:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.17118" target="_blank">
                        @arXiv 2408.17118
                    </a>
                    <span class="tweet-title">
                        ICA Gets a Matrix Makeover: Faster, Unique, and Ready for EEG!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Seikei University, The University of Tokyo
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel algorithm for the ordering ICA by reformulating it in matrix
                    representation. This approach leverages the efficiency of matrix manipulations, particularly matrix
                    multiplication, to accelerate the process of estimating unique components in ICA.
                </div>
            </div>
        </div>


        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202409021136_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>

</html>