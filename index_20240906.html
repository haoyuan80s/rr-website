
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Fresh AI Paper Top Picks</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 80px;"></div>
            <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                Freshest
                Top Picks:
                <span class="highlightNumber" style="font-size: 28px;">52</span> out of <span
                    class="highlightNumber">226</span> arXiv AI
                papers
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="line-height: 1.2;">just released on</div>
                <div id="data-default-date" data-date="2024-09-06"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 10px;">
            </div>
        </div>


        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">01:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03753" target="_blank">@arXiv 2409.03753</a>
                    <span class="tweet-title">Million-Scale Chat Logs:  A Visual Feast for Researchers!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Waterloo, Cornell University, University of Southern California...</span>
                </div>
                <div class="primary-text">
                    This research introduces WILDVIS, a tool that allows researchers to analyze large-scale chat logs by combining traditional search functionalities with embedding-based visualization. This approach differs from previous work by offering a more intuitive and interactive way to explore the vast amounts of data generated by user-chatbot interactions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">01:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03021" target="_blank">@arXiv 2409.03021</a>
                    <span class="tweet-title">LLMs:  Not Just  Word  Spitters,  They  Think  in  Concepts!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National Taiwan University, CMU, UC Los Angeles</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new framework called CLUE that measures uncertainty in LLMs at the concept level, breaking down generated text into individual concepts and assessing their uncertainty separately. This differs from previous methods that focused on sequence-level uncertainty, treating the entire output as a single unit.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">01:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03682" target="_blank">@arXiv 2409.03682</a>
                    <span class="tweet-title">MAML's New Trick: First-Order Meta-Learning Gets a Smooth Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new first-order variant of MAML, called FO-B-MAML, that avoids the computational and memory burdens of second-order methods. Unlike previous first-order approaches, FO-B-MAML has a bias that can be made arbitrarily small, allowing for convergence to any desired precision. The paper also shows that the MAML objective satisfies a generalized smoothness assumption, suggesting that clipped gradient descent is better suited for this problem.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">02:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03149" target="_blank">@arXiv 2409.03149</a>
                    <span class="tweet-title">MGP Gets a Time Machine: New Model Tracks Dynamic Correlations!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research proposes a non-stationary multi-output Gaussian process (MGP) model that can capture both dynamic and sparse correlations among outputs. Unlike previous MGP models, this one uses a spike-and-slab prior to automatically decide which sources are informative to the target output during the training process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">02:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03215" target="_blank">@arXiv 2409.03215</a>
                    <span class="tweet-title">AI Agents Get a New Set of Wheels: xLAM Models Drive Function-Calling Performance</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Salesforce AI Research</span>
                </div>
                <div class="primary-text">
                    This research introduces xLAM, a series of large action models specifically designed for AI agent tasks. Unlike previous work that often relies on proprietary models, xLAM is open-source and trained on a unified, augmented, and synthesized dataset, addressing the scarcity of high-quality agent datasets and the lack of standard protocols in this area.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">02:58</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03685" target="_blank">@arXiv 2409.03685</a>
                    <span class="tweet-title">Robot Vision:  Learning to See Like a Human, One View at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Toyota Research Institute</span>
                </div>
                <div class="primary-text">
                    This research explores using single-image novel view synthesis models to augment training data for robotic policies, making them more robust to changes in camera viewpoint. Unlike previous methods that rely on multi-view data or explicit 3D representations, this approach leverages the power of generative models to synthesize novel views from a single image.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">03:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03512" target="_blank">@arXiv 2409.03512</a>
                    <span class="tweet-title">MOOCs Got an Upgrade: AI Agents Take Over the Classroom!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes MAIC (Massive AI-empowered Course), a new online learning model that uses LLM-driven multi-agent systems to create an AI-augmented classroom. Unlike traditional MOOCs, MAIC aims to balance scalability with adaptivity by providing personalized learning experiences for each student.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">03:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03302" target="_blank">@arXiv 2409.03302</a>
                    <span class="tweet-title">Quantum Spin Systems:  FNOs Learn the Dance of Tiny Magnets!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">California Institute of Technology, Ahmedabad University, NVIDIA</span>
                </div>
                <div class="primary-text">
                    This research uses Fourier Neural Operators (FNOs) to model the time evolution of quantum spin systems, a task that's usually computationally expensive.  Unlike traditional neural networks, FNOs are resolution-invariant, meaning they can be trained at lower resolutions and still accurately predict at higher resolutions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">04:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02977" target="_blank">@arXiv 2409.02977</a>
                    <span class="tweet-title">LLMs Go Agent:  Software Engineering Gets a New AI Buddy!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Fudan University, Nanyang Technological University, University of Illinois</span>
                </div>
                <div class="primary-text">
                    This research focuses on LLM-based agents, which are AI systems that use LLMs as their core but also have the ability to interact with the environment and use external tools. This sets it apart from previous work that primarily focused on standalone LLMs for software engineering tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">04:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02965" target="_blank">@arXiv 2409.02965</a>
                    <span class="tweet-title">Social Media Users: Do We Trust What They Say or What They Do?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Los Angeles, University of Washington</span>
                </div>
                <div class="primary-text">
                    This research proposes a framework called Contribution-Aware Multimodal User Embedding (CAMUE) that can identify and remove misleading information from specific social network users during text-graph fusion. This is different from previous work because it provides personalized explanations for downstream analysis and recommendations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">05:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03755" target="_blank">@arXiv 2409.03755</a>
                    <span class="tweet-title">Diffusion Models:  A New Trick to Speed Up Image Generation!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new technique called "dynamic compensation" to improve the accuracy of predictor-corrector diffusion samplers. This method uses a learned compensation ratio to adjust the model's output at each sampling step, mitigating the misalignment issue that arises from the corrector step.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">05:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03270" target="_blank">@arXiv 2409.03270</a>
                    <span class="tweet-title">Talking Heads Get a Personality Makeover: New AI Model Injects Style into Videos</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Fudan University, Tencent</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel framework called SVP  that incorporates intrinsic style into talking head generation. Unlike previous methods that focus on lip-sync and head movements, SVP  learns style-related information from audio and visual cues, enabling the generation of more diverse and expressive videos.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">05:58</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03731" target="_blank">@arXiv 2409.03731</a>
                    <span class="tweet-title">AI-Powered Planning:  Deep Learning Makes Robust Optimization Less Conservative</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach for adaptive robust optimization (ARO) that uses a variational autoencoder (VAE) to learn a tighter uncertainty set, improving the accuracy of planning decisions. Unlike previous methods that rely on simpler uncertainty sets or approximate recourse costs, this approach leverages deep generative learning to capture complex, high-dimensional uncertainty more effectively.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">06:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03684" target="_blank">@arXiv 2409.03684</a>
                    <span class="tweet-title">Predicting Quantum Channels: It's Not All Classical!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, ETH Zurich, CMU...</span>
                </div>
                <div class="primary-text">
                    This research extends previous work on predicting quantum channels by demonstrating that accurate prediction is possible for a broader class of input distributions, specifically those that are not "classical" in nature. The key difference lies in the use of a "biased Pauli analysis" technique, which overcomes limitations of previous methods that relied on low-degree approximations in the Pauli basis.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">06:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03757" target="_blank">@arXiv 2409.03757</a>
                    <span class="tweet-title">Vision Models for 3D Scenes:  Who's Got the Best View?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Illinois Urbana-Champaign, Carnegie Mellon University</span>
                </div>
                <div class="primary-text">
                    This research systematically compares different visual foundation models for 3D scene understanding, evaluating their strengths and weaknesses across various tasks. Unlike previous work that focused on 2D image-based tasks, this study explores the use of video and 3D point cloud encoders for complex 3D scenes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">07:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03632" target="_blank">@arXiv 2409.03632</a>
                    <span class="tweet-title">Beyond the Model:  Unmasking AI Bias with Social Structures</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research proposes a new type of explanation for machine learning outputs called "socio-structural explanations." Unlike traditional model-centric interpretations, this approach considers the social and structural context in which the model operates, highlighting how societal biases can influence its predictions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">07:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03161" target="_blank">@arXiv 2409.03161</a>
                    <span class="tweet-title">AI Goes to College: Can ChatGPT Ace Materials Science?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">OMRON SINIC X, Osaka University, University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research introduces MaterialBENCH, a new benchmark dataset specifically designed to evaluate the problem-solving abilities of large language models (LLMs) in materials science. Unlike previous benchmarks, MaterialBENCH focuses on college-level materials science problems, including those requiring complex calculations and reasoning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">08:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02980" target="_blank">@arXiv 2409.02980</a>
                    <span class="tweet-title">Painting Galaxies with AI: A New Way to Model the Universe</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces NeHOD, a generative framework that uses diffusion models and point clouds to model the distribution of galaxies within dark matter halos. Unlike traditional methods like Halo Occupation Distribution (HOD), NeHOD can capture more complex relationships between galaxies and their host halos, achieving accuracy comparable to computationally expensive hydrodynamic simulations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">08:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03129" target="_blank">@arXiv 2409.03129</a>
                    <span class="tweet-title">Subsidies: The Secret Weapon to Tame Selfish Agents?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, Toyota Technological Institute at Chicago</span>
                </div>
                <div class="primary-text">
                    This research explores the use of subsidies to address information avoidance behavior in multi-agent systems, a phenomenon not previously addressed in subsidy design literature. It also establishes formal hardness results for designing optimal subsidy schemes and proposes a data-driven approach to overcome these computational challenges.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">08:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03142" target="_blank">@arXiv 2409.03142</a>
                    <span class="tweet-title">Unmasking Hidden Actions: New AI Learns to Spot Shifts in Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, Mohamed bin Zayed University of Artificial Intelligence</span>
                </div>
                <div class="primary-text">
                    This research tackles the challenge of identifying causal relationships in non-stationary time series data without relying on prior knowledge of domain variables. Unlike previous methods that assume a Markov structure or require observing domain indices, this work introduces a novel approach based on sparse transitions and conditional independence constraints.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">09:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03060" target="_blank">@arXiv 2409.03060</a>
                    <span class="tweet-title">Explaining AI:  From "Why?" to "How Much?" with VERIX+</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research builds upon previous work on verified explainability by introducing new techniques to improve the size and generation time of explanations. It utilizes bound propagation-based sensitivity analysis to obtain more fine-grained feature-level information, leading to smaller explanations. Additionally, it proposes a binary search-inspired traversal approach to process features in batches, significantly reducing the time required to generate explanations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">09:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03733" target="_blank">@arXiv 2409.03733</a>
                    <span class="tweet-title">LLMs:  Thinking Big, Planning Small for Code Generation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Scale AI, California Institute of Technology, Northeastern University...</span>
                </div>
                <div class="primary-text">
                    This research proposes PLANSEARCH, a novel search algorithm that improves code generation by searching over plans in natural language, rather than directly over code solutions. This approach increases the diversity of generated ideas, leading to more efficient search and better performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">10:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03385" target="_blank">@arXiv 2409.03385</a>
                    <span class="tweet-title">Graph-Based Referring Expression Comprehension:  Making It Great Again!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Academia Sinica, National Taiwan University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel "dynamic gate constraint" (DGC) module that adaptively disables irrelevant objects during reasoning in graph-based referring expression comprehension (REC) models. This approach differs from previous methods by focusing on sub-expressions of the referring expression, rather than the entire expression, to identify relevant objects.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">10:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03402" target="_blank">@arXiv 2409.03402</a>
                    <span class="tweet-title">AI Goes From Player to Coach: Language Models Take the Reins in RL Experiments</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research proposes a system that uses a large vision language model (VLM) to automate most of the steps in a reinforcement learning (RL) experiment, including task proposition, decomposition, and performance analysis. This differs from previous work that typically focuses on automating individual steps in isolation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">11:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03034" target="_blank">@arXiv 2409.03034</a>
                    <span class="tweet-title">Meshing Up Neural Fields: A Multi-Resolution Revolution!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Technion - Israel Institute of Technology, Broad Institute of MIT and Harvard</span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel framework for representing neural fields on triangle meshes that is multi-resolution across both spatial and frequency domains. Unlike previous work that focuses on Euclidean spaces, this approach leverages the geometry-aware DiffusionNet architecture to decompose the spatial and frequency domains using multiple DiffusionNet components representing different spatial resolutions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">11:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03412" target="_blank">@arXiv 2409.03412</a>
                    <span class="tweet-title">Doctors' Notes, AI's Sight:  Text Boosts Medical Image Segmentation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sun Yat-sen University, National Cancer Center, Chinese Academy of Medical Sciences...</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach called TG-LMM (Text-Guided Large Multi-Modal Model) that incorporates textual descriptions of organs into the segmentation process. Unlike previous text-visual models that focus on identifying the target, TG-LMM aims to improve segmentation accuracy by leveraging expert descriptions of organ locations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">12:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03274" target="_blank">@arXiv 2409.03274</a>
                    <span class="tweet-title">LLMs:  A  Whack-a-Mole  Game  of  Safety  and  Security</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Chinese Academy of Sciences, Xidian University, Peking University</span>
                </div>
                <div class="primary-text">
                    This research paper provides a comprehensive overview of recent advancements in attack and defense approaches for Large Language Models (LLMs), focusing on studies published in 2023 and beyond. It distinguishes itself from previous surveys by highlighting the unique vulnerabilities of LLMs, analyzing the evolving threat landscape, and examining the effectiveness of contemporary defense mechanisms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">12:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03444" target="_blank">@arXiv 2409.03444</a>
                    <span class="tweet-title">LLMs Get a Makeover:  Merging Models for Super-Smart AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research explores the effects of merging multiple fine-tuned LLMs, demonstrating that this process can lead to the emergence of capabilities that surpass the individual contributions of the parent models. This is different from previous work that primarily focused on single model fine-tuning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">12:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03277" target="_blank">@arXiv 2409.03277</a>
                    <span class="tweet-title">ChartMoE:  A Multi-Expert Chart Whisperer for Smarter Data Understanding</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Peking University, Hong Kong University of Science and Technology  Guangzhou...</span>
                </div>
                <div class="primary-text">
                    This research introduces ChartMoE, a new approach to chart understanding that utilizes a Mixture of Experts (MoE) architecture to bridge the gap between visual and language models. Unlike previous work that relies on a single connector, ChartMoE trains multiple connectors with distinct alignment tasks (chart-table, chart-JSON, chart-code), allowing for more specialized and effective chart interpretation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">13:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03012" target="_blank">@arXiv 2409.03012</a>
                    <span class="tweet-title">More Labels, More Love:  Crowdsourcing Apps Get a Boost from User Effort!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Saint Louis University, IBM, Temple University</span>
                </div>
                <div class="primary-text">
                    This research explores the impact of different levels of user labeling effort on the quantity and quality of data collected in camera-centric mobile crowdsourcing applications. Unlike previous work that focused on minimizing user effort, this study suggests that requesting more effort from users does not necessarily lead to lower engagement or satisfaction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">13:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03634" target="_blank">@arXiv 2409.03634</a>
                    <span class="tweet-title">Surface-Centric Modeling:  A Neural Network That's Got Your Back (and Your Surface)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Peking University Shenzhen Graduate School</span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to neural surface reconstruction called "surface-centric modeling." Unlike previous methods that focus on dense volumes, this approach prioritizes regions near the surface, leading to more efficient and accurate reconstructions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">14:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03368" target="_blank">@arXiv 2409.03368</a>
                    <span class="tweet-title">Training-Free SNNs:  No Sweat, Just Spikes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Hunan University</span>
                </div>
                <div class="primary-text">
                    This research proposes a training-free method for converting pre-trained Artificial Neural Networks (ANNs) into Spiking Neural Networks (SNNs). Unlike previous methods that require retraining or fine-tuning, this approach directly converts the ANN model without any additional training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">14:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03583" target="_blank">@arXiv 2409.03583</a>
                    <span class="tweet-title">Text-Guided Mixup:  When Images Learn to Chat with Words!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, Alibaba</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel text-guided mixup technique that leverages the semantic relationships between classes recognized by a pre-trained text encoder to improve the performance of long-tailed image categorization tasks. This approach differs from previous work by incorporating textual information into the mixup process, which helps to alleviate the long-tailed problem by boosting the performance of tail classes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">15:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03272" target="_blank">@arXiv 2409.03272</a>
                    <span class="tweet-title">Self-Driving Cars Get a Brain Upgrade:  Occupancy, Language, and Action, Oh My!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Fudan University, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new type of world model for autonomous driving that combines visual, language, and action information. Unlike previous models that focus on predicting sensor data, this model aims to understand and reason about the world in a more human-like way.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">15:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03365" target="_blank">@arXiv 2409.03365</a>
                    <span class="tweet-title">Training Giant AI Models:  A Juggling Act of Data and Resources!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Purdue University, Alibaba</span>
                </div>
                <div class="primary-text">
                    This research introduces Spindle, a training system for multi-task, multi-modal large models that optimizes resource allocation and scheduling by considering the heterogeneous workloads of different tasks and data modalities. Unlike previous systems designed for single-task models, Spindle decomposes the model into stages and addresses the joint optimization problem sequentially, achieving significant speedups.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">15:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03187" target="_blank">@arXiv 2409.03187</a>
                    <span class="tweet-title">Noise Can Actually Boost Your Brain's Memory! 🤯</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research delves into the effects of correlated noise on memory in linear recurrent neural networks (RNNs), a departure from previous studies that primarily focused on uncorrelated noise.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">16:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03505" target="_blank">@arXiv 2409.03505</a>
                    <span class="tweet-title">Newsvendor's Dilemma: From Slow to Fast, a Spectrum of Regret</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Columbia University</span>
                </div>
                <div class="primary-text">
                    This research introduces the concept of "clustered distributions" to analyze the data-driven Newsvendor problem. This framework unifies previous work by considering a spectrum of regret rates, ranging from 1/√n to 1/n, depending on the clustering parameter β.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">16:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03282" target="_blank">@arXiv 2409.03282</a>
                    <span class="tweet-title">Traffic Jams?  Not So Fast!  New AI Model Predicts Speed with Incident Awareness.</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a Mixture of Experts (MoE) model for traffic speed prediction that explicitly considers incidents, unlike previous models that are incident-agnostic. The MoE model leverages separate recurrent and non-recurrent expert models to capture distinct traffic patterns under different conditions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">17:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03103" target="_blank">@arXiv 2409.03103</a>
                    <span class="tweet-title">Cloud Scaling Gets Smart: AI Predicts Latency, Interprets the Why!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research introduces an interpretable approach to predict end-to-end latency in microservice-based applications, using the Temporal Fusion Transformer (TFT) and Kernel Ridge Regression (KRR). Unlike previous work, this method not only predicts latency but also explains the factors contributing to it, enabling informed, fine-grained autoscaling.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">17:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03614" target="_blank">@arXiv 2409.03614</a>
                    <span class="tweet-title">Soft Robots Get a Grip on Long-Term Learning!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a modular parallel robotic manipulation platform specifically designed for long-term data collection in soft robotics. Unlike previous work that often relies on high-fidelity simulations, this platform focuses on collecting real-world data from a robust hardware system.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">17:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03734" target="_blank">@arXiv 2409.03734</a>
                    <span class="tweet-title">Safety First, Data Second: How Multi-Objective Learning Levels the AI Playing Field</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of California  Berkeley</span>
                </div>
                <div class="primary-text">
                    This research explores how the need to prioritize safety in large language models can actually create opportunities for new companies to enter the market, even when incumbents have massive datasets. It differs from previous work by focusing on the impact of multi-objective learning on market entry barriers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">18:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03563" target="_blank">@arXiv 2409.03563</a>
                    <span class="tweet-title">100 Instances is All You Need: Predicting LLM Success with a Tiny Test</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge, Polytechnic University of Valencia</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel framework for predicting the performance of a new LLM on unseen data by evaluating it on a small set of reference instances, rather than requiring a full evaluation on a large dataset. This approach leverages the evaluation results of previously tested LLMs to reduce the cost and time required for evaluating new models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">18:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03299" target="_blank">@arXiv 2409.03299</a>
                    <span class="tweet-title">Can Robots Learn New Tricks?  RT-1-X Takes on a 40-Year-Old SCARA Robot!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Universiteit van Amsterdam</span>
                </div>
                <div class="primary-text">
                    This research investigates the ability of a robotic foundation model, RT-1-X, to generalize to a previously unseen robot type, a SCARA robot. This is different from previous work that focused on generalizing across tasks and environments but not robot embodiments.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">18:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03291" target="_blank">@arXiv 2409.03291</a>
                    <span class="tweet-title">LLM Detectors:  A Case of Fake News, Real Problems</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne, University of Applied Sciences and Arts Western Switzerland</span>
                </div>
                <div class="primary-text">
                    This research focuses on the real-world effectiveness of LLM detectors in a specific setting: short news-like posts generated by moderately sophisticated attackers. It goes beyond previous benchmarks by testing detectors against a wider range of attacks, including temperature increase and prompting strategies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">19:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03005" target="_blank">@arXiv 2409.03005</a>
                    <span class="tweet-title">Robots Go Off-Road: Physics-Powered AI Navigates Unseen Terrain</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Mitsubishi Electric Research Laboratories, George Mason University...</span>
                </div>
                <div class="primary-text">
                    This research introduces PIETRA, a self-supervised learning framework that integrates physics priors into evidential neural networks. Unlike previous methods that avoid out-of-distribution terrain, PIETRA seamlessly transitions between learned and physics-based predictions for unseen terrain.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">19:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03478" target="_blank">@arXiv 2409.03478</a>
                    <span class="tweet-title">LLMs: The New Event Planners for Your Smart Home!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Catholic University of Louvain, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research explores using Large Language Models (LLMs) to automate the process of creating event logs from raw sensor data collected by Internet of Things (IoT) devices. This approach differs from previous work by leveraging the capabilities of LLMs in natural language understanding and generation to abstract sensor readings into meaningful events and integrate logs from multiple sources.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">20:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03171" target="_blank">@arXiv 2409.03171</a>
                    <span class="tweet-title">Multi-Task RAG:  When One Adapter Isn't Enough!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Darkhive</span>
                </div>
                <div class="primary-text">
                    This research introduces MARAGS, a multi-adapter system for retrieval augmented generation (RAG) question answering. Unlike previous work that focuses on single-task RAG, MARAGS utilizes multiple adapters to handle diverse tasks within a single LLM, improving efficiency and performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">20:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03431" target="_blank">@arXiv 2409.03431</a>
                    <span class="tweet-title">UV-Mamba Strikes:  A New Model for Urban Village Boundary Detection!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Beijing Jiaotong University, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces UV-Mamba, a neural network model that uses a state space model (SSM) with deformable convolutions (DCN) to improve urban village boundary identification in high-resolution remote sensing images. This approach addresses the memory loss problem that arises in SSM when dealing with large images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">21:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03495" target="_blank">@arXiv 2409.03495</a>
                    <span class="tweet-title">MLE for Multiaﬃne Models:  A New Algorithm That's Not Afraid of Complexity!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne, ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel Alternating and Iteratively-Reweighted Least Squares (AIRLS) algorithm for maximum likelihood estimation (MLE) problems where variables are related by multiaﬃne expressions. This approach differs from previous work by considering a wider class of likelihoods and providing a deeper theoretical analysis of the algorithm's convergence.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">21:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02963" target="_blank">@arXiv 2409.02963</a>
                    <span class="tweet-title">Fair Clustering:  Making Sure Everyone Gets a Seat at the Table</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Georgia Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new fairness criterion for clustering called "minimum representation fairness" (MR-fairness), which requires each group to be represented above a certain threshold in a specified number of clusters. This differs from previous work that focuses on proportional representation across all clusters.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">21:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03109" target="_blank">@arXiv 2409.03109</a>
                    <span class="tweet-title">Fake News, Real Problem: Vision-Language Model Detects and Attributes Deepfakes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Univ. Polytechnique Hauts-de-France, Univ. Rennes, CNRS...</span>
                </div>
                <div class="primary-text">
                    This research introduces FIDAVL, a novel approach that combines synthetic image detection and attribution within a single framework. Unlike previous methods that handle these tasks separately, FIDAVL leverages the synergies between vision and language models to achieve more accurate and robust results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">22:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.03061" target="_blank">@arXiv 2409.03061</a>
                    <span class="tweet-title">Depth Perception:  How 3D Models Learned to See the World Like We Do</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, Toyota Research Institute</span>
                </div>
                <div class="primary-text">
                    This research introduces a method to incorporate dense metric depth into the training of neural 3D representations. This is different from previous work that relied on sparse depth or monocular depth priors.
                </div>
            </div>
        </div>

        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Listen and learn ^.^</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202409062206_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>    
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/privacy.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading privacy.html:', error));
    </script>
</body>
</html>