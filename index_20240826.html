<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - AI Paper Picks of the Day</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 80px;"></div>
            <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                Fresh Picks:
                <span class="highlightNumber" style="font-size: 28px;">47</span> out of <span
                    class="highlightNumber">241</span> arXiv AI
                papers
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="line-height: 1.2;">just released on</div>
                <div id="data-default-date" data-date="2024-08-26"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 10px;">
            </div>
        </div>

        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">
                00:52
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13221" target="_blank">
                        @arXiv 2408.13221
                    </a>
                    <span class="tweet-title">
                        Backdoor Bonanza: When Multiple Attacks Sneak into Your AI Model
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the impact of multiple simultaneous data poisoning attacks on machine
                    learning models, a scenario not previously investigated. It demonstrates that multiple backdoors can
                    be installed in a single model without significantly affecting its performance on clean data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">
                01:21
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13150" target="_blank">
                        @arXiv 2408.13150
                    </a>
                    <span class="tweet-title">
                        Backtracking Linesearch: A New Trick to Speed Up Optimization
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT, Northeastern University
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a new way to adjust step sizes in backtracking linesearch, a fundamental
                    technique in numerical optimization. Instead of using a constant factor, the proposed method uses a
                    variable factor that adapts to the degree of violation of the chosen criterion.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">
                01:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13252" target="_blank">
                        @arXiv 2408.13252
                    </a>
                    <span class="tweet-title">
                        Layered Panoramas: A 3D Scene Generator That's Not Just a Pretty Picture!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Shanghai Jiao Tong University, Chinese University of Hong Kong, Zhejiang University...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces LAYERPANO3D, a framework that uses a multi-layered 3D panorama
                    representation to generate full-view, explorable panoramic scenes from text prompts. This approach
                    differs from previous methods that either rely on sequential scene expansion or employ a single
                    panorama representation, which often suffer from semantic drift or limitations in handling complex
                    scene hierarchies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">
                02:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13233" target="_blank">
                        @arXiv 2408.13233
                    </a>
                    <span class="tweet-title">
                        Transformers Get a Speed Boost: Training LLMs in Almost Linear Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Hong Kong, University of Wisconsin-Madison, Tsinghua University...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method for calculating gradients in multi-layer transformer models,
                    achieving almost linear time complexity. Unlike previous work that focused on inference
                    acceleration, this paper tackles the computational bottleneck during training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">
                02:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12664" target="_blank">
                        @arXiv 2408.12664
                    </a>
                    <span class="tweet-title">
                        Brainpower for AI: How Neuroscience Can Help Us Understand Deep Learning
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a multilevel framework for interpreting artificial neural networks, drawing
                    inspiration from neuroscience's decades of experience analyzing the brain. It argues that
                    understanding both biological and artificial neural systems requires analyzing them at multiple
                    levels of analysis, with different analytic tools for each level.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">
                03:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13242" target="_blank">
                        @arXiv 2408.13242
                    </a>
                    <span class="tweet-title">
                        Training Equivariant Networks: Relax, It's Okay to Break the Rules!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Pennsylvania, MIT
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel training framework for equivariant neural networks that relaxes the
                    equivariance constraint during training. Unlike previous work that focuses on addressing model
                    mis-specification, this method aims to improve the optimization process itself by allowing the model
                    to explore a larger hypothesis space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">
                03:42
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12734" target="_blank">
                        @arXiv 2408.12734
                    </a>
                    <span class="tweet-title">
                        Fair-Speech: Unmasking Bias in Voice Assistants, One Utterance at a Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Meta
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset, Fair-Speech, specifically designed to evaluate the fairness
                    of speech recognition models across diverse demographic groups. Unlike previous datasets,
                    Fair-Speech includes self-reported demographic information like age, gender, ethnicity, and
                    geographic location, allowing researchers to assess how well models perform for different groups.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">
                04:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12748" target="_blank">
                        @arXiv 2408.12748
                    </a>
                    <span class="tweet-title">
                        LLMs: Big Brains, Big Problems? Small Models to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel two-stage framework for hallucination detection in LLMs. It combines
                    a small language model (SLM) for initial detection with a larger LLM as a "constrained reasoner" to
                    provide explanations for detected hallucinations. This approach aims to balance the latency
                    challenges of using LLMs for reasoning with the need for interpretability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">
                04:34
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12658" target="_blank">
                        @arXiv 2408.12658
                    </a>
                    <span class="tweet-title">
                        Singing the Blues: AI Learns to Improvise Hindustani Music
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT, Montreal Institute for Learning Algorithms, Google
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a hierarchical generative model for Hindustani music that uses a finely
                    quantized pitch contour as an intermediate representation, unlike previous work that relied on
                    coarse symbolic representations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">
                04:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12812" target="_blank">
                        @arXiv 2408.12812
                    </a>
                    <span class="tweet-title">
                        Fallacies Unmasked: New Dataset Exposes How Misinformation Hijacks Science
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        TU Darmstadt, Hessian Center for AI, IBM Research...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces MISSCIPLUS, a new dataset that grounds logical fallacies in real-world
                    scientific publications, unlike previous datasets that relied on paraphrased information or
                    explicitly stated fallacies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">
                05:20
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12747" target="_blank">
                        @arXiv 2408.12747
                    </a>
                    <span class="tweet-title">
                        Diffusion Makes 3D Object Detection a Breeze: CatFree3D Says Goodbye to Categories!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel 3D object detection pipeline that decouples the task from 2D detection
                    and depth prediction, enabling category-agnostic detection. Unlike previous methods that rely on
                    category information, this approach leverages a diffusion-based model to predict 3D bounding boxes
                    from random noise, conditioned on visual prompts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">
                05:52
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12674" target="_blank">
                        @arXiv 2408.12674
                    </a>
                    <span class="tweet-title">
                        Robots Learn New Tricks by Watching YouTube!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Carnegie Mellon University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for teaching robots by interpreting video demonstrations as
                    Parameterized Symbolic Abstraction Graphs (PSAG). Unlike previous approaches that focus on replaying
                    object relationships or actor trajectories, PSAGs capture both geometric and non-geometric
                    attributes, including forces, which are often invisible in videos.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">
                06:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13256" target="_blank">
                        @arXiv 2408.13256
                    </a>
                    <span class="tweet-title">
                        Diffusion Models: Factorizing Images, One Blob at a Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research investigates how diffusion models learn to represent and generate images by breaking
                    down complex features into simpler, independent components. Unlike previous studies that focused on
                    disentanglement, this paper delves into the specific mechanisms of factorization and
                    compositionality in diffusion models, using a controlled experiment with 2D Gaussian data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">
                06:49
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13211" target="_blank">
                        @arXiv 2408.13211
                    </a>
                    <span class="tweet-title">
                        Quantum Circuit Design: Neural Networks Go Unitary!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Cracow University of Technology, Ferdowsi University of Mashhad, University College London...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to quantum circuit synthesis by training a neural network
                    with unitary weight matrices, directly mapping input-output relationships of quantum computations.
                    This differs from previous work that focused on optimizing parameters within existing quantum
                    circuits or using neural networks for gate selection probabilities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">
                07:23
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13257" target="_blank">
                        @arXiv 2408.13257
                    </a>
                    <span class="tweet-title">
                        New Benchmark Challenges LLMs: Can They See What We See?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CASIA, NJU, HKUST...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces MME-RealWorld, a new benchmark for evaluating multimodal large language
                    models (MLLMs). Unlike previous benchmarks, MME-RealWorld focuses on high-resolution images and
                    real-world scenarios, making it more challenging for models to achieve high accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">
                07:54
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12787" target="_blank">
                        @arXiv 2408.12787
                    </a>
                    <span class="tweet-title">
                        LLMs: They're Not Just Chatty, They're Nosy! New Toolkit Exposes Data Privacy Risks
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Texas at Austin, University of Illinois, UC Berkeley...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces LLM-PBE, a toolkit specifically designed to systematically evaluate data
                    privacy risks in LLMs. Unlike previous studies that focused on limited data types, models, and
                    attack methods, LLM-PBE offers a comprehensive approach, encompassing diverse datasets, attack
                    methodologies, and defense strategies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">
                08:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13073" target="_blank">
                        @arXiv 2408.13073
                    </a>
                    <span class="tweet-title">
                        LLMs for Healthcare: A New Recipe for Better Predictions!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a framework called IntelliCare that uses Large Language Models (LLMs) to
                    provide patient-level knowledge for improving healthcare predictions. Unlike previous work that
                    focused on medical code-level knowledge, IntelliCare leverages LLMs to analyze individual patient
                    health conditions, generating personalized insights.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">
                08:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12834" target="_blank">
                        @arXiv 2408.12834
                    </a>
                    <span class="tweet-title">
                        LLMs Learn to Spot Names with a Little Help from Their Friends
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        iFLYTEK, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes CLLMFS, a framework that enhances large language models (LLMs) for few-shot
                    named entity recognition (NER) by integrating contrastive learning and Low-Rank Adaptation (LoRA).
                    This approach differs from previous work by focusing on improving the model's internal
                    representations, specifically its ability to distinguish between entities and their surrounding
                    context.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">
                09:31
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13237" target="_blank">
                        @arXiv 2408.13237
                    </a>
                    <span class="tweet-title">
                        Learning Functions by Their Derivatives: A Jacobian Journey
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes learning a function by directly learning its Jacobian, the matrix of its partial
                    derivatives. This approach allows for the enforcement of specific derivative conditions, such as
                    invertibility or Lipschitz continuity, by structuring the Jacobian's output activation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">
                09:58
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12832" target="_blank">
                        @arXiv 2408.12832
                    </a>
                    <span class="tweet-title">
                        LLMs Go on a Road Trip: Predicting Your Next Move with Intent!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel framework called LIMP that uses large language models (LLMs) to
                    predict human mobility by incorporating the underlying intentions behind each movement. Unlike
                    previous models that primarily focus on spatiotemporal patterns, LIMP leverages LLMs' commonsense
                    reasoning abilities to infer intentions, leading to more accurate predictions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">
                10:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12606" target="_blank">
                        @arXiv 2408.12606
                    </a>
                    <span class="tweet-title">
                        AI Doctor for Breasts: Multimodal MRI Makes Cancer Diagnosis a Breeze!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        The Hong Kong University of Science and Technology, Harvard University, Shenzhen People’s
                        Hospital...
                    </span>
                </div>
                <div class="primary-text">
                    This research distinguishes itself by utilizing a large mixture-of-modality-experts model (MOME)
                    that integrates multiparametric MRI information, unlike previous studies that primarily relied on
                    single sequences.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">
                10:47
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13195" target="_blank">
                        @arXiv 2408.13195
                    </a>
                    <span class="tweet-title">
                        AI Architect Designs Chip Capacitance: NAS-Cap Makes Extraction Faster, More Accurate
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research uses neural architecture search (NAS) to find a better CNN model for 3-D capacitance
                    extraction, improving accuracy and efficiency compared to previous CNN-Cap models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">
                11:07
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12662" target="_blank">
                        @arXiv 2408.12662
                    </a>
                    <span class="tweet-title">
                        Turbulent Twisters: TDA Helps Spot Whirlwind Chaos!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        French Alternative Energies and Atomic Energy Commission, Sorbonne University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for automatically identifying turbulent vortices within 2D
                    flows using Topological Data Analysis (TDA). Unlike previous studies that relied on traditional
                    turbulence descriptors, this approach leverages the geometry of vortices extracted through TDA to
                    distinguish between turbulent and laminar states.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">
                11:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12801" target="_blank">
                        @arXiv 2408.12801
                    </a>
                    <span class="tweet-title">
                        Time Delays Got You Down? Bootstrap Your Way to Better Predictions!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Meta
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Time Series Model Bootstrap (TSMB), a framework that addresses the
                    challenge of potentially varying or nondeterministic time delays in multivariate time series
                    modeling. Unlike traditional methods that assume a fixed time delay, TSMB acknowledges and
                    incorporates time delay uncertainties.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">
                11:59
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12976" target="_blank">
                        @arXiv 2408.12976
                    </a>
                    <span class="tweet-title">
                        Event Cameras Get Smart: On-the-Fly Feedback Control for Better Vision
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich, Sony
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach for dynamically controlling the activation thresholds of
                    event-based vision sensors (EVS) on a per-column basis, unlike previous work that focused on global
                    control parameters.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">
                12:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12821" target="_blank">
                        @arXiv 2408.12821
                    </a>
                    <span class="tweet-title">
                        AI Goes Street Smart: Can ChatGPT-4V and Gemini Pro Navigate Urban Landscapes?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research evaluates the capabilities of ChatGPT-4V and Gemini Pro in analyzing urban
                    environments, specifically Street View Imagery, Built Environment, and Interior, focusing on tasks
                    like object recognition, counting, and measurement. This differs from previous work by examining the
                    models' performance across multiple domains and tasks within a specific urban context.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">
                12:54
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12629" target="_blank">
                        @arXiv 2408.12629
                    </a>
                    <span class="tweet-title">
                        Skeleton-Based Gesture Recognition: Data-Free Learning Gets a Synthetic Boost!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU, ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on data-free class incremental learning (DFCIL) for skeleton-based gesture
                    recognition, a domain that has received less attention than image-based DFCIL. The paper proposes a
                    novel Synthetic Feature Replay (SFR) algorithm that samples synthetic features from class prototypes
                    to replay old classes and augment new classes, eliminating the need for complex data generation
                    methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">
                13:21
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12781" target="_blank">
                        @arXiv 2408.12781
                    </a>
                    <span class="tweet-title">
                        AI Mastery: From Supervised to Autonomous, How Humans and Machines Will Work Together
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces the "Model Mastery Lifecycle" framework, which outlines how human-AI
                    interaction should evolve as AI models progress towards mastery in a specific task. This framework
                    differs from previous work by explicitly addressing the changing nature of human and AI roles as AI
                    capabilities improve.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">
                13:46
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12602" target="_blank">
                        @arXiv 2408.12602
                    </a>
                    <span class="tweet-title">
                        Fiber Optic Neural Networks: Light Speed Computing, No Wires Required!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nanjing University of Posts and Telecommunications, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel fiber optical neural network architecture that integrates signal
                    transmission and computation within the optical domain, eliminating the need for signal conversion
                    to the electronic domain. This approach differs from previous work by directly processing
                    communication signals within the fiber itself, potentially leading to significant gains in
                    processing efficiency and power consumption.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">
                14:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12970" target="_blank">
                        @arXiv 2408.12970
                    </a>
                    <span class="tweet-title">
                        SUMO: A Search-Based Uncertainty Estimator for Offline RL, It's a KNN-dergarten of Knowledge!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Harbin Institute of Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new uncertainty estimation method called SUMO for model-based offline
                    reinforcement learning. Unlike previous methods that rely on model ensembles, SUMO uses a
                    search-based approach to measure the cross-entropy between simulated and real dynamics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">
                14:46
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12682" target="_blank">
                        @arXiv 2408.12682
                    </a>
                    <span class="tweet-title">
                        MultiMed: A Medical AI Benchmark That's Got All The Modalities!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces MultiMed, a benchmark dataset designed to evaluate and enable large-scale
                    learning across a wide spectrum of medical modalities and tasks. Unlike previous benchmarks that
                    focus on single modalities or tasks, MultiMed integrates data from ten diverse modalities, including
                    imaging, electrophysiology, molecular data, and text, and structures it into eleven challenging
                    tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">
                15:11
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13146" target="_blank">
                        @arXiv 2408.13146
                    </a>
                    <span class="tweet-title">
                        Change-Point Detection: A Kernel-Based Scan B-Statistic Takes the Lead!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research reproduces a recently proposed online change-point detection algorithm based on a
                    kernel-based scan B-statistic. The algorithm is distribution-free, making it robust for real-world
                    applications. The paper also explores the use of subsampling techniques to improve the algorithm's
                    efficiency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">
                15:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13147" target="_blank">
                        @arXiv 2408.13147
                    </a>
                    <span class="tweet-title">
                        ShapeICP: No Pose Data? No Problem!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel mesh-based active shape model (ASM) for category-level object pose
                    and shape estimation. Unlike previous methods, it does not rely on pose-annotated data for training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">
                16:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12980" target="_blank">
                        @arXiv 2408.12980
                    </a>
                    <span class="tweet-title">
                        MedDec: Decoding Doctor's Notes, One Decision at a Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Massachusetts Lowell, MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset called MedDec, specifically designed for extracting and
                    classifying medical decisions from discharge summaries. Unlike previous datasets focused on other
                    aspects of clinical notes, MedDec focuses solely on the decision-making process, providing a
                    valuable resource for understanding how doctors make choices.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">
                16:27
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13231" target="_blank">
                        @arXiv 2408.13231
                    </a>
                    <span class="tweet-title">
                        Fourier Features Go Spherical: A New Way to Approximate Kernels
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new family of quadrature rules for approximating the Gaussian measure in
                    higher dimensions, leveraging its isotropy. This approach differs from previous work by using a
                    tensor product of radial and spherical quadrature rules, leading to improved approximation bounds.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">
                17:00
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12825" target="_blank">
                        @arXiv 2408.12825
                    </a>
                    <span class="tweet-title">
                        WSI Classification: When More is Less, and Less is More!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a semi-weakly supervised learning framework for WSI classification, called
                    SWS-MIL. It differs from previous work by employing adaptive pseudo bag augmentation (AdaPse) to
                    assign labels to unlabeled data based on a confidence threshold. This approach aims to reduce noise
                    introduced by traditional pseudo bag augmentation methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">
                17:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12742" target="_blank">
                        @arXiv 2408.12742
                    </a>
                    <span class="tweet-title">
                        Vision Transformers: Attention, Please! Reusing Attention for Efficiency
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Yale University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes TReX, a framework that reuses the attention output of one encoder in
                    subsequent encoders of a Vision Transformer (ViT) model. This differs from prior work that focused
                    on token pruning or weight sharing, which only partially address the computational overhead of
                    attention blocks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">
                17:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12767" target="_blank">
                        @arXiv 2408.12767
                    </a>
                    <span class="tweet-title">
                        Spiking Neural Networks: Brain-Inspired AI Gets a Memory Boost!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Yale University
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the intersection of Spiking Neural Networks (SNNs) with In-Memory Computing
                    (IMC) architectures, highlighting the potential for low-power edge computing. The paper emphasizes
                    the need for a holistic approach to design, considering the interdependencies between algorithms,
                    devices, circuits, and system parameters.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">
                18:25
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12614" target="_blank">
                        @arXiv 2408.12614
                    </a>
                    <span class="tweet-title">
                        Image-Feature Consistency: A New Trick for Semi-Supervised Learning
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces feature-level perturbation, a new way to augment data in semi-supervised
                    learning. Unlike previous methods that only perturb images, this approach also alters the features
                    extracted by the model, creating a more diverse and challenging learning environment.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">
                18:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12733" target="_blank">
                        @arXiv 2408.12733
                    </a>
                    <span class="tweet-title">
                        SQL-GEN: Bridging the Dialect Gap with Synthetic Data and Model Merging
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces SQL-GEN, a framework for generating synthetic data tailored to specific SQL
                    dialects. Unlike previous approaches that rely on translating queries across dialects, SQL-GEN
                    leverages dialect-specific tutorials to create training datasets that better capture the unique
                    capabilities of each dialect.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">
                19:11
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13012" target="_blank">
                        @arXiv 2408.13012
                    </a>
                    <span class="tweet-title">
                        Drug-Matching Made Easy: AI Predicts Cancer Treatments from Tiny Samples
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge, The National Institute of Agricultural Botany, ValiRx Plc...
                    </span>
                </div>
                <div class="primary-text">
                    This research uses a machine learning approach to predict drug responses in patient-derived cell
                    cultures, focusing on the cell's functional response to a drug panel rather than relying solely on
                    omics data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">
                19:40
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12622" target="_blank">
                        @arXiv 2408.12622
                    </a>
                    <span class="tweet-title">
                        AI Risk: A Taxonomy of All the Ways Things Can Go Wrong
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research differs from previous work by creating a comprehensive database of AI risks extracted
                    from 43 taxonomies, along with two new taxonomies: a Causal Taxonomy and a Domain Taxonomy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">
                20:05
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13114" target="_blank">
                        @arXiv 2408.13114
                    </a>
                    <span class="tweet-title">
                        Neural Networks Get a Spline Makeover: Learning Smooth Activations with a Twist!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        École Polytechnique Fédérale de Lausanne
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel variational framework for training freeform nonlinearities in
                    layered computational architectures, specifically focusing on pointwise nonlinearities. The key
                    innovation lies in the use of a second-order total variation (TV(2)) regularization, which promotes
                    sparsity and leads to solutions that are adaptive nonuniform linear splines. This approach allows
                    for the imposition of slope constraints, enabling the design of stable, monotone, and firmly
                    non-expansive activations, crucial for the proper functioning of signal processing algorithms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">
                20:34
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12636" target="_blank">
                        @arXiv 2408.12636
                    </a>
                    <span class="tweet-title">
                        Wave-LSTM: Unraveling Cancer's Multi-Scale Secrets with a Deep Learning Twist!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford, Alan Turing Institute, Health Data Research UK
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Wave-LSTM, a novel approach that analyzes cancer copy number alterations
                    (CNAs) at multiple scales, unlike previous methods that focused on single-scale analysis.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">
                21:04
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13154" target="_blank">
                        @arXiv 2408.13154
                    </a>
                    <span class="tweet-title">
                        CNNs See Spots: Explaining Breast Cancer Diagnosis with AI's "Seeing Eye"
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge, Tilburg University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the interpretability of CNNs for mammogram classification, specifically
                    exploring how post-hoc techniques like Grad-CAM can reveal the reasoning behind the model's
                    predictions. This differs from previous work that primarily focused on interpretability in numerical
                    breast cancer datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">
                21:30
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12608" target="_blank">
                        @arXiv 2408.12608
                    </a>
                    <span class="tweet-title">
                        Frugal Spiking Neural Network: A Tiny Brain for Big Data!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Univ. Grenoble Alpes, INSERM, Grenoble Institut Neurosciences...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a single-layer Spiking Neural Network (SNN) that uses a unique type of
                    neuron called LTS (Leaky Threshold-Switching) to classify patterns in continuous data streams.
                    Unlike previous SNNs that often rely on multiple layers and complex architectures, this approach is
                    highly frugal, requiring only a handful of neurons.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">
                21:55
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12732" target="_blank">
                        @arXiv 2408.12732
                    </a>
                    <span class="tweet-title">
                        AI's Got Grain: Segment Anything Model for Hard Drive Design
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Seagate Technology, University of Minnesota
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the use of Meta's Segment Anything Model (SAM) for grain segmentation in hard
                    drive design, a task that has traditionally relied on rule-based models or training neural networks
                    on extensive hand-labeled data. SAM's zero-shot generalization capability allows for segmentation
                    across different imaging conditions with minimal labeled data.
                </div>
            </div>
        </div>


        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202408260847_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>

</html>