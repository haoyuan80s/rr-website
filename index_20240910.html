<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                    Fresh Picks:
                    <span class="highlightNumber" style="font-size: 28px;">91</span> out of <span
                        class="highlightNumber">424</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-09-10"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>


        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">01:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05688" target="_blank">@arXiv
                        2409.05688</a>
                    <span class="tweet-title">Seeing Through Walls: A New Benchmark for Optical Flow with
                        Transparent
                        Objects</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Princeton University</span>
                </div>
                <div class="primary-text">
                    This research introduces LayeredFlow, a real-world benchmark for optical flow that includes
                    multi-layer 3D annotations for objects occluded by transparent surfaces. This is different from
                    previous benchmarks, which typically focus on single-layer annotations or lack the diversity of
                    scenes and objects found in LayeredFlow.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">01:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05862" target="_blank">@arXiv
                        2409.05862</a>
                    <span class="tweet-title">Vision Models: Can They See Like Us? A New Test Puts Them to the 3D
                        Shape
                        Challenge!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of California Berkeley, Massachusetts Institute of
                        Technology</span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark for evaluating computer vision models' ability to
                    understand 3D shapes. It uses a task from cognitive science that requires zero-shot inference,
                    meaning the models haven't been specifically trained on the task. This approach is different
                    from
                    previous work that often relies on tasks like depth estimation, which can be solved using 2D
                    features.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">01:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05867" target="_blank">@arXiv
                        2409.05867</a>
                    <span class="tweet-title">Flash Cache: Unbiased Inverse Rendering, No More Bias!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, Google Research</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new method for inverse rendering that uses a radiance cache to reduce
                    bias
                    in the rendering process. Unlike previous methods that rely on approximations, this approach
                    leverages two techniques: occlusion-aware importance sampling and a fast cache architecture that
                    acts as a control variate.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">02:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05864" target="_blank">@arXiv
                        2409.05864</a>
                    <span class="tweet-title">Robot Motion Planning: From Scratch to Super-Smart!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to motion planning that leverages large-scale data
                    generation in simulation to train a generalist neural policy. Unlike previous methods that plan
                    from
                    scratch for each new problem, this approach distills the planning process into a reactive
                    policy,
                    enabling faster and more accurate motion planning in real-world environments.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">02:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05816" target="_blank">@arXiv
                        2409.05816</a>
                    <span class="tweet-title">LLMs: They're Not Just for Pretraining Anymore!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel data selection method that leverages the existing collection of
                    publicly available, high-performance LLMs without requiring any additional model training. It
                    uses
                    correlations between perplexity and benchmark performance to identify high-quality pretraining
                    data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">02:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04574" target="_blank">@arXiv
                        2409.04574</a>
                    <span class="tweet-title">Want Your AI to Write Like Shakespeare? This New Trick Makes It
                        Possible!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research explores using parameter-efficient fine-tuning (PEFT) with Low-Rank Adaptation
                    (LoRA)
                    to customize the writing style of large language models (LLMs). This approach differs from
                    previous
                    methods like full model fine-tuning and prompt engineering, which are less efficient or rely
                    heavily
                    on prompt structure.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">03:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05539" target="_blank">@arXiv
                        2409.05539</a>
                    <span class="tweet-title">Collaborative Learning: When Clients Get Together, Models Get
                        Smarter!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel bilevel optimization framework for collaborative learning. Unlike
                    previous methods that rely on global models or clustering, this approach dynamically selects
                    collaborators based on gradient alignment, allowing for more flexible and fine-grained
                    collaboration.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">03:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05334" target="_blank">@arXiv
                        2409.05334</a>
                    <span class="tweet-title">Neural Fields Go Point-Cloud Crazy: Lagrangian Hashing for Compact
                        Representations</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Simon Fraser University, University of Trento, University of
                        Toronto...</span>
                </div>
                <div class="primary-text">
                    This paper introduces Lagrangian Hashing, a new representation for neural fields that combines
                    the
                    speed of Eulerian grid-based methods with the memory efficiency of point-based representations.
                    It
                    achieves this by incorporating a point-based representation into the high-resolution layers of a
                    hierarchical hash table, similar to InstantNGP.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">04:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05247" target="_blank">@arXiv
                        2409.05247</a>
                    <span class="tweet-title">Data Detox: How to Train AI Models Without Exploiting Languages</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research paper focuses on the ethical implications of training large language models (LLMs)
                    with data from underrepresented languages. It highlights the need for community engagement and
                    data
                    sovereignty to avoid perpetuating colonial power dynamics and linguistic disenfranchisement.
                    Unlike
                    previous work, this paper emphasizes the importance of understanding the cultural and historical
                    context of language data, rather than simply treating it as a resource to be mined.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">04:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05804" target="_blank">@arXiv
                        2409.05804</a>
                    <span class="tweet-title">Celcomen: Unraveling the Hidden Forces of Gene Expression in
                        Space!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Wellcome Sanger Institute, Cambridge Stem Cell Institute,
                        University of
                        Cambridge...</span>
                </div>
                <div class="primary-text">
                    This research introduces Celcomen, a model that leverages a mathematical causality framework to
                    disentangle intra- and inter-cellular gene regulation programs in spatial transcriptomics data.
                    Unlike previous models, Celcomen provides mathematically guaranteed identifiability, ensuring
                    robustness and interpretability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">05:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04597" target="_blank">@arXiv
                        2409.04597</a>
                    <span class="tweet-title">Smart Contracts: Buggy Code? Let's Ask a Chatbot!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Columbia University, The University of Chicago</span>
                </div>
                <div class="primary-text">
                    This research introduces SMARTSYS, a system that uses foundation models to decide when to use
                    fuzzing or concolic execution for smart contract testing. This approach differs from previous
                    work
                    by incorporating a forecast model to predict the best testing technique for specific code
                    segments.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">05:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05112" target="_blank">@arXiv
                        2409.05112</a>
                    <span class="tweet-title">Watermarked Text: Finding the Needle in the Haystack</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research focuses on detecting watermarked segments within large documents, a scenario often
                    overlooked in previous work. Existing methods primarily focused on distinguishing fully
                    watermarked
                    text from non-watermarked text.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">05:58</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05286" target="_blank">@arXiv
                        2409.05286</a>
                    <span class="tweet-title">LLMs Learn to Seek and Solve Table Questions Like Humans Do!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research proposes a "Seek-and-Solve" pipeline for table question answering (TQA) that
                    leverages
                    LLMs' reasoning capabilities by mimicking how humans approach complex TQA tasks. Unlike previous
                    work that focuses on simplifying the task, this approach emphasizes the reasoning process
                    itself,
                    integrating two stages of information seeking and question answering into a coherent reasoning
                    path.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">06:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04556" target="_blank">@arXiv
                        2409.04556</a>
                    <span class="tweet-title">Code-Fueled Language Models: A Recipe for Better (and Worse)
                        AI!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research investigates the impact of code in language model pretraining by systematically
                    varying the proportion of code in the training data, unlike previous studies that focused on
                    binary
                    comparisons (code vs. no code).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">06:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04882" target="_blank">@arXiv
                        2409.04882</a>
                    <span class="tweet-title">Legged Robots Learn to Open Doors Like Humans, No Prior Knowledge
                        Needed!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research presents a single control policy for a legged manipulator that can open both push
                    and
                    pull doors without prior knowledge of the door type. Unlike previous work, which often required
                    separate policies or user input for different door types, this approach uses a teacher-student
                    learning method to infer the opening direction during deployment.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">07:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05294" target="_blank">@arXiv
                        2409.05294</a>
                    <span class="tweet-title">Diffusion Models: Backdoor-Proofed with a Trigger-Reversing
                        Trick!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This paper proposes TERD, a defense framework that tackles backdoor attacks in diffusion models.
                    Unlike previous defenses, TERD focuses on the noise space, where diffusion models operate, and
                    uses
                    a unified loss function to reverse engineer the trigger.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">07:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05558" target="_blank">@arXiv
                        2409.05558</a>
                    <span class="tweet-title">CAPTCHA's New Mask: Fooling AI with Simple Patterns</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research explores the effectiveness of geometric masks in fooling state-of-the-art image
                    recognition models, focusing on visible perturbations that preserve semantic information for
                    humans.
                    This approach differs from traditional adversarial attacks that aim for imperceptibility.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">07:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04927" target="_blank">@arXiv
                        2409.04927</a>
                    <span class="tweet-title">Speech LLMs: Can They Tell Who's Talking? A New Test for Voice
                        Recognition!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, University of Illinois Urbana-Champaign,
                        Columbia University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new way to evaluate speech language models (SpeechLLMs) by focusing
                    on
                    their ability to identify speakers in spoken dialogue. The authors propose two categories of
                    questions: Identity-Critical Questions (ICQs) that require speaker identification and
                    Context-Based
                    Questions (CBQs) that can be answered without it.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">08:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05033" target="_blank">@arXiv
                        2409.05033</a>
                    <span class="tweet-title">Diffusion Models for Recommender Systems: From Noise to
                        Nirvana!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Shanghai Jiao Tong University, Texas A&M University, Yale
                        University</span>
                </div>
                <div class="primary-text">
                    This research provides the first comprehensive survey on the application of diffusion models for
                    recommender systems, categorizing existing works into three primary domains: data engineering &
                    encoding, recommender models, and content presentation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">08:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05284" target="_blank">@arXiv
                        2409.05284</a>
                    <span class="tweet-title">Learning from Dynamics: MRFs Get a Speed Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research explores learning Markov Random Fields (MRFs) from dynamical samples, specifically
                    Glauber dynamics, rather than the traditional i.i.d. samples. The key difference is that the
                    algorithm's runtime scales near quadratically with the dimension, avoiding the exponential
                    dependence on the order of interactions seen in previous work.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">09:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04760" target="_blank">@arXiv
                        2409.04760</a>
                    <span class="tweet-title">Point Clouds: Seeing is Believing, But Understanding Takes
                        Fusion!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for point cloud recognition that combines both geometric
                    and
                    semantic features, unlike previous training-free methods that focused on only one type of
                    feature.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">09:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05735" target="_blank">@arXiv
                        2409.05735</a>
                    <span class="tweet-title">LLMs Go Database Diving: A New Benchmark for Question Answering Across
                        Heterogeneous Data Sources</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM Research</span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark and system called "siwarex" that specifically addresses
                    the
                    challenge of question answering across heterogeneous data sources, unlike previous benchmarks
                    that
                    focus on either databases or APIs exclusively.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">09:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05866" target="_blank">@arXiv
                        2409.05866</a>
                    <span class="tweet-title">Air Pollution Forecasts: Can We Predict When to Wear a Mask?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework for evaluating PM2.5 forecasts, focusing on how they
                    can
                    help individuals make decisions about their exposure to air pollution. It goes beyond
                    traditional
                    metrics and considers the impact of forecasts on real-world choices.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">10:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04919" target="_blank">@arXiv
                        2409.04919</a>
                    <span class="tweet-title">Collaborative Learning: When More Clients Means Less Data!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Duke University, Northeastern University, Tsinghua
                        University</span>
                </div>
                <div class="primary-text">
                    This research identifies the optimal statistical rate for collaborative learning in linear
                    settings,
                    bridging a gap between existing upper and lower bounds. It introduces a new spectral estimator
                    with
                    local averaging that achieves this optimal rate.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">10:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05474" target="_blank">@arXiv
                        2409.05474</a>
                    <span class="tweet-title">Sparse Views, Big Dreams: A New Way to Reconstruct 3D Objects with
                        Fewer
                        Images</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Beijing University of Technology, Beijing
                        Jiaotong
                        University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to 3D surface reconstruction from sparse views by
                    progressively planning the most informative viewpoints for image capture. Unlike previous
                    methods
                    that rely on predefined sets of sparse images, this system dynamically adds new views during the
                    reconstruction process, leading to more accurate and detailed results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">11:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04585" target="_blank">@arXiv
                        2409.04585</a>
                    <span class="tweet-title">AI-Powered Tuning: CubicML Makes Distributed Deep Learning
                        Faster</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta</span>
                </div>
                <div class="primary-text">
                    This research proposes CubicML, an automated machine learning (AutoML) system that uses a
                    machine
                    learning model to predict the performance of distributed deep learning training. This approach
                    differs from previous work by focusing on a generic black-box AutoML solution for optimizing
                    large-scale distributed ML systems, rather than specific co-design situations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">11:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04897" target="_blank">@arXiv
                        2409.04897</a>
                    <span class="tweet-title">Fairness in the Face of Bias: How to Match Students to Schools Without
                        Screwing Up!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University, IIT Delhi</span>
                </div>
                <div class="primary-text">
                    This paper tackles the problem of bias in centralized selection systems, where candidates are
                    evaluated based on factors that may unfairly disadvantage certain groups. Unlike previous work
                    that
                    focused on single institutions, this research explores the multi-institution setting, where
                    candidates have preferences over multiple options.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">11:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05325" target="_blank">@arXiv
                        2409.05325</a>
                    <span class="tweet-title">Bayesian Optimization Gets a Transfer Learning Makeover for
                        Heterogeneous
                        Search Spaces!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Minnesota Twin Cities, Meta</span>
                </div>
                <div class="primary-text">
                    This research tackles the challenge of transferring knowledge between Bayesian optimization
                    experiments with different search spaces. Unlike previous work that assumes identical search
                    spaces,
                    this paper proposes two methods that can handle these heterogeneous scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">12:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05312" target="_blank">@arXiv
                        2409.05312</a>
                    <span class="tweet-title">Open-World Learning: Prompts Get Dynamic, Images Get Smarter!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University, Amazon</span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to continual learning in open-world visual
                    representation
                    learning. Unlike previous methods that rely on static prompt pools, this paper proposes a
                    dynamic
                    prompt generation network that learns to generate prompts on-the-fly, improving generalization
                    to
                    unseen classes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">12:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05358" target="_blank">@arXiv
                        2409.05358</a>
                    <span class="tweet-title">Reward Shaping: Don't Just Explore, Explore Smartly!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This paper extends the concept of potential-based reward shaping from traditional Markov
                    Decision
                    Processes (MDPs) to Bayes-Adaptive Markov Decision Processes (BAMDPs), which model the agent's
                    learning process. This allows for a more nuanced understanding of how pseudo-rewards can guide
                    exploration and learning in reinforcement learning (RL) algorithms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">13:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05847" target="_blank">@arXiv
                        2409.05847</a>
                    <span class="tweet-title">Video Segmentation Goes Big: New Challenge Pushes the Limits of Object
                        Tracking!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Fudan University, Institute of Big Data</span>
                </div>
                <div class="primary-text">
                    This research introduces the 6th Large-scale Video Object Segmentation (LSVOS) challenge, which
                    uses
                    three new datasets (MOSE, LVOS, and MeViS) to evaluate video object segmentation models in more
                    complex and realistic scenarios. These datasets feature longer videos, more crowded scenes, and
                    motion-based language descriptions, pushing the boundaries of current models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">13:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04888" target="_blank">@arXiv
                        2409.04888</a>
                    <span class="tweet-title">Deep Learning for Alzheimer's: Can We See What They See?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, University of Illinois</span>
                </div>
                <div class="primary-text">
                    This research introduces a quantitative disease-focusing strategy to assess how well deep
                    learning
                    models for Alzheimer's Disease classification focus on brain regions known to be affected by the
                    disease. This approach combines saliency maps, which highlight important areas in an image, with
                    brain segmentations to quantify the model's attention to specific regions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">13:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04779" target="_blank">@arXiv
                        2409.04779</a>
                    <span class="tweet-title">Solving Stiff Equations: A Neural Network Gets a Boost from Asymptotic
                        Analysis</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanjing University of Aeronautics and Astronautics, Tsinghua
                        University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new neural network architecture called Component Fourier Neural
                    Operator
                    (ComFNO) specifically designed for solving singularly perturbed differential equations (SPDEs).
                    Unlike previous methods that rely solely on data, ComFNO incorporates prior knowledge from
                    asymptotic analysis, a mathematical technique that helps understand the behavior of solutions
                    near
                    specific points.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">14:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04481" target="_blank">@arXiv
                        2409.04481</a>
                    <span class="tweet-title">LLMs: From Disease Detectives to Drug Designers</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Monash University, Griffith University, Harvard
                        University...</span>
                </div>
                <div class="primary-text">
                    This research focuses on the application of Large Language Models (LLMs) in drug discovery and
                    development, specifically highlighting their potential to automate tasks across all three
                    stages:
                    understanding disease mechanisms, drug discovery, and clinical trials. It differentiates itself
                    from
                    previous work by providing a comprehensive overview of LLM applications in this field, including
                    a
                    maturity assessment of various tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">14:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04992" target="_blank">@arXiv
                        2409.04992</a>
                    <span class="tweet-title">LLMs Get a Storage Upgrade: Flash-Powered Attention for Long-Context
                        Inference!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Xiamen University, Institute of Computing
                        Technology
                        Chinese Academy of Sciences...</span>
                </div>
                <div class="primary-text">
                    This research proposes InstInfer, a system that offloads the computationally intensive attention
                    operations and large KV caches to Computational Storage Drives (CSDs) during LLM inference. This
                    approach differs from previous work by leveraging the high internal bandwidth of flash chips
                    within
                    CSDs, bypassing the bottleneck of PCIe bandwidth.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">15:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05096" target="_blank">@arXiv
                        2409.05096</a>
                    <span class="tweet-title">Deep Learning Gets a Time Machine: Classifying IoT Traffic with a
                        Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Toronto Metropolitan University, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel time-distributed feature learning method for network traffic
                    classification, specifically focusing on the unique characteristics of IoT traffic. Unlike
                    previous
                    work that primarily relies on spatial and temporal features, this approach incorporates
                    pseudo-temporal features, capturing the dynamic and complex nature of IoT data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">15:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04613" target="_blank">@arXiv
                        2409.04613</a>
                    <span class="tweet-title">Decentralized Learning: A Near-Potential Solution for Multi-Agent
                        Games</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research introduces the concept of a Markov Near-Potential Function (MNPF) to analyze the
                    convergence of decentralized learning algorithms in general-sum Markov games. Unlike previous
                    work
                    that focused on zero-sum or potential games, this approach tackles the complexities of
                    real-world
                    interactions where agents can be both cooperative and competitive.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">16:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05076" target="_blank">@arXiv
                        2409.05076</a>
                    <span class="tweet-title">"Is There a Clock?" - Tricking AI with Silly Questions to Spot Fake
                        Images</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, University of Science and Technology
                        Beijing</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method called PIP (Probe Question-based Irrelevant Probe) to
                    detect
                    adversarial examples in large vision-language models (LVLMs). Unlike previous work that focuses
                    on
                    detecting adversarial examples in isolated vision models, PIP leverages the attention patterns
                    of
                    irrelevant probe questions to distinguish between clean and adversarial images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">16:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05028" target="_blank">@arXiv
                        2409.05028</a>
                    <span class="tweet-title">LLMs: The New GUI Test Migrators?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Singapore Management University, Shanghai Jiao
                        Tong
                        University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new paradigm for GUI test migration, the abstraction-concretization
                    paradigm, which first extracts a general test logic from multiple source test cases and then
                    uses
                    this logic to guide the generation of concrete GUI test cases for the target app. This differs
                    from
                    existing approaches that rely on widget mapping, which can lead to incomplete or buggy test
                    cases.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">17:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04834" target="_blank">@arXiv
                        2409.04834</a>
                    <span class="tweet-title">Log-Based Anomaly Detection: Less is More!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Alibaba</span>
                </div>
                <div class="primary-text">
                    This research quantifies the impact of log event reduction on anomaly detection models,
                    exploring
                    the effectiveness of reducing log events without compromising model performance. Previous work
                    has
                    focused on anomaly detection methods, but the quantitative effects of log reduction have
                    remained
                    unexplored.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">17:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05798" target="_blank">@arXiv
                        2409.05798</a>
                    <span class="tweet-title">Stop Staring, Start Clicking: How Response Times Can Supercharge
                        Preference Learning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Harvard University</span>
                </div>
                <div class="primary-text">
                    This research integrates human response times into preference-based linear bandits, a common
                    framework for interactive learning. Unlike previous work that relies solely on choices, this
                    approach leverages the inverse relationship between response time and preference strength to
                    enhance
                    utility estimation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">17:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04979" target="_blank">@arXiv
                        2409.04979</a>
                    <span class="tweet-title">Radar and Camera: A Match Made in BEV Heaven!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, University of Electronic Science and Technology
                        of
                        China</span>
                </div>
                <div class="primary-text">
                    This research introduces RCBEVDet++, a framework that fuses radar and camera data in a
                    bird's-eye
                    view (BEV) space for 3D object detection. Unlike previous methods that simply concatenate or sum
                    features, RCBEVDet++ uses a cross-attention mechanism to dynamically align and fuse the
                    multi-modal
                    features, resulting in more accurate and robust perception.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">18:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05780" target="_blank">@arXiv
                        2409.05780</a>
                    <span class="tweet-title">Modular Networks: Breaking the Curse of Dimensionality with a Little
                        Help
                        from Their Friends</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research investigates how modular neural networks can overcome the exponential increase in
                    training data required for generalization as task dimensionality increases. Unlike previous work
                    that focused on empirical observations, this paper provides a theoretical framework for
                    understanding the benefits of modularity and proposes a novel learning rule to exploit these
                    benefits.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">18:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05782" target="_blank">@arXiv
                        2409.05782</a>
                    <span class="tweet-title">Big Models, Small Time: Training Efficiency Gets a Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel concept called "scale-time equivalence," which suggests that
                    increasing the size of a neural network is functionally equivalent to increasing its training
                    time
                    proportionally. This differs from previous work that primarily focused on scaling laws with
                    respect
                    to model size and data volume, neglecting the impact of training time.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">19:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05666" target="_blank">@arXiv
                        2409.05666</a>
                    <span class="tweet-title">Deep Learning Helps See Through Skin to Guide Cancer Treatment</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Dartmouth College, Nvidia, University of Wisconsin-Madison</span>
                </div>
                <div class="primary-text">
                    This research introduces a deep learning framework for segmenting bio-morphological features in
                    Cherenkov images, which is a novel application of transfer learning in this field. Previous work
                    relied on manual segmentation, which was slow and inconsistent.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">19:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04822" target="_blank">@arXiv
                        2409.04822</a>
                    <span class="tweet-title">LLMs on Trial: Can AI Red Teams Crack the Code of Conversational
                        Safety?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research explores the effectiveness of using off-the-shelf LLMs as "red teamers" in a
                    conversational setting, focusing on multi-turn interactions rather than single-turn attacks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">19:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05401" target="_blank">@arXiv
                        2409.05401</a>
                    <span class="tweet-title">Multilingual Retrieval Model Learns to Speak 200 Languages... Without
                        Ever
                        Leaving English!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Indian Institute of Technology Patna, IBM</span>
                </div>
                <div class="primary-text">
                    This research introduces NLLB-E5, a multilingual retrieval model that leverages a pre-trained
                    multilingual encoder to handle multiple languages without requiring any multilingual training
                    data.
                    This approach differs from previous work that relied on large datasets of labeled multilingual
                    examples.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">20:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05834" target="_blank">@arXiv
                        2409.05834</a>
                    <span class="tweet-title">Vision-Driven BEV: Training 3D Perception with 2D Annotations</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a fine-tuning method for BEV perception networks that utilizes 2D
                    semantic
                    perception from surround-view cameras, reducing the reliance on expensive LiDAR data for ground
                    truth generation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">20:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04840" target="_blank">@arXiv
                        2409.04840</a>
                    <span class="tweet-title">Reinforcement Learning Gets a Computational Makeover: Skipping Over
                        Low-Range States for Efficiency!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces a new reinforcement learning algorithm that efficiently finds
                    near-optimal
                    policies in MDPs with linearly realizable value functions. Unlike previous work, this algorithm
                    relies on a cost-sensitive classification oracle, which can be efficiently implemented when the
                    feature dimension is constant.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">21:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05477" target="_blank">@arXiv
                        2409.05477</a>
                    <span class="tweet-title">TGNNs Get a Transformer Makeover: Faster, More Accurate Graph
                        Modeling!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Wuhan University, ETH Zurich, Peking University</span>
                </div>
                <div class="primary-text">
                    This paper proposes TF-TGN, a temporal graph neural network (TGNN) that leverages the
                    Transformer
                    decoder architecture for efficient training. Unlike previous TGNNs that rely on specialized
                    models
                    and tailored training frameworks, TF-TGN adapts the Transformer's codebase, benefiting from its
                    optimized kernels and distributed training schemes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">21:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05701" target="_blank">@arXiv
                        2409.05701</a>
                    <span class="tweet-title">Federated Learning Gets a Diffusion Makeover: Say Goodbye to Linear
                        Aggregation!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to parameter aggregation in federated learning (FL) by
                    leveraging diffusion models. Unlike traditional methods that rely on linear averaging, this
                    framework uses a diffusion model to learn the distribution of model parameters across clients,
                    enabling the generation of personalized parameters for each client.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">22:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04787" target="_blank">@arXiv
                        2409.04787</a>
                    <span class="tweet-title">LLMs: Learning From Their Own Successes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This paper introduces Selective Self-Rehearsal (SSR), a fine-tuning approach that uses a model's
                    own
                    correct responses to improve generalization. Unlike standard supervised fine-tuning (SFT), which
                    relies solely on gold responses, SSR leverages the model's ability to generate multiple valid
                    answers, reducing overfitting and preserving the model's original capabilities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">22:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04778" target="_blank">@arXiv
                        2409.04778</a>
                    <span class="tweet-title">Teacher's Pet: New Method Calibrates Logits to Avoid Mis-Instruction
                        in
                        Knowledge Distillation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research identifies a problem called "mis-instruction" in knowledge distillation, where the
                    student model learns incorrect information from the teacher model's logits. The paper proposes a
                    novel Logit Calibration (LoCa) method to address this issue by adjusting the teacher's logits to
                    ensure they align with the ground truth labels.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">22:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05427" target="_blank">@arXiv
                        2409.05427</a>
                    <span class="tweet-title">Text Toucher: Feeling the Words, Generating the Textures!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Zhejiang University, Yale University</span>
                </div>
                <div class="primary-text">
                    This research focuses on generating tactile images directly from text descriptions, unlike
                    previous
                    work that relied on visual input. The paper analyzes tactile images at two levels of detail:
                    object-level (texture and shape) and sensor-level (gel status), and proposes a method to model
                    these
                    aspects through text.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">23:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05387" target="_blank">@arXiv
                        2409.05387</a>
                    <span class="tweet-title">Motion Style Transfer: Hip Velocity is the New Black!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Zhejiang University, Tencent Technology Co. Ltd., University
                        College
                        London</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for fine-grained control over contact in motion style
                    transfer
                    by indirectly controlling contact through hip velocity. This approach differs from previous
                    methods
                    that either treat contact as part of the content or struggle to decouple it from style.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">23:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05819" target="_blank">@arXiv
                        2409.05819</a>
                    <span class="tweet-title">Gaussian Splatting Gets Physical: Simulating 3D Scenes with a
                        Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Jagiellonian University, Cambridge, IDEAS</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method called GASP (Gaussian Splatting for Physics-Based
                    Simulations)
                    that integrates physical simulations directly into the Gaussian Splatting framework. Unlike
                    previous
                    approaches that relied on additional meshing techniques, GASP operates directly on Gaussian
                    distributions, simplifying the process and eliminating the need for external modifications to
                    the
                    physical engine.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">24:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05585" target="_blank">@arXiv
                        2409.05585</a>
                    <span class="tweet-title">Brain MRI Counterfactuals: A Peek into What Could Be</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Weill Cornell Medicine, Imperial College
                        London</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for generating counterfactual 3D brain MRIs by
                    incorporating
                    causality into a latent space. Unlike previous approaches that directly model causality in the
                    high-dimensional MRI space, this method leverages a VQ-VAE to encode the MRI into a
                    lower-dimensional representation, making it computationally more efficient.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">24:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04596" target="_blank">@arXiv
                        2409.04596</a>
                    <span class="tweet-title">Two X-rays, One 3D Heart: AI Reconstructs Coronary Arteries with a
                        Single
                        Patient's Data!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research proposes a self-supervised deep learning method called NeCA for 3D coronary artery
                    tree reconstruction from only two projections. Unlike previous methods that require large
                    training
                    datasets and 3D ground truth, NeCA leverages implicit neural representation and a
                    multiresolution
                    hash encoder to learn from a single patient's data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">24:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05622" target="_blank">@arXiv
                        2409.05622</a>
                    <span class="tweet-title">Diffusion Models Get a Preference Makeover: Forward KL Regularization
                        Makes Them More Human-Friendly!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">China Telecom, Tsinghua University, Northwest Polytechnical
                        University...</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel framework called Forward KL regularized Preference optimization
                    for
                    aligning Diffusion policies (FKPD) to align diffusion policies with human preferences directly.
                    Unlike previous methods that rely on pre-defined reward functions, FKPD learns a basic diffusion
                    policy from offline data and then aligns it with preference data through a direct preference
                    optimization process. This approach avoids the potential inaccuracies of manually defined reward
                    functions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">25:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05863" target="_blank">@arXiv
                        2409.05863</a>
                    <span class="tweet-title">Traffic Simulation Gets a Prompt: Researchers Teach Cars to Follow
                        Instructions!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UT Austin, NVIDIA</span>
                </div>
                <div class="primary-text">
                    This research introduces a new type of traffic simulation that allows users to give specific
                    instructions to individual cars, influencing their behavior and creating more realistic and
                    controllable scenarios. Unlike previous work that focused on generating realistic traffic
                    without
                    user control, this approach allows for more targeted and customized simulations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">25:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04723" target="_blank">@arXiv
                        2409.04723</a>
                    <span class="tweet-title">Sleep Talk: How Last Night's Zzz's Can Predict Your Mood Today!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Queen’s University</span>
                </div>
                <div class="primary-text">
                    This research introduces NapTune, a framework that uses sleep data from the previous night to
                    improve mood classification using wearable time-series data. This approach differs from previous
                    work by integrating sleep measures as an additional input modality, rather than relying solely
                    on
                    real-time wearable signals.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">26:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04831" target="_blank">@arXiv
                        2409.04831</a>
                    <span class="tweet-title">Mutation Testing: Giving LLMs a "Prompt-Up"</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research proposes a mutation testing framework specifically designed for in-context
                    learning
                    (ICL) systems, focusing on mutating the ICL prompts rather than the model itself. This approach
                    is
                    distinct from previous mutation testing work that primarily targeted conventional deep learning
                    systems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">26:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04633" target="_blank">@arXiv
                        2409.04633</a>
                    <span class="tweet-title">Mars Helicopter Gets Its Bearings: New Tech Navigates Rugged
                        Terrain</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Zurich, California Institute of Technology, Jet
                        Propulsion Laboratory</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel range-visual-inertial odometry (VIO) system that integrates
                    1D-LRF
                    measurements without relying on terrain planarity assumptions, unlike previous methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">27:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05459" target="_blank">@arXiv
                        2409.05459</a>
                    <span class="tweet-title">Matching Made in Heaven: A Geometric Approach to Causal
                        Inference</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces GeoMatching, a novel matching method for treatment effect estimation
                    that
                    accounts for the geometry and uncertainty of confounders. Unlike previous methods that rely on
                    Euclidean distances, GeoMatching leverages Riemannian geometry to define distances along the
                    data
                    manifold, resulting in more accurate estimates.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon" title="Play from here">27:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05635" target="_blank">@arXiv
                        2409.05635</a>
                    <span class="tweet-title">Naïve Bayes Gets a Makeover: Optimal Projections for Smarter
                        Classification!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Lancaster University, EPFL, Kohort</span>
                </div>
                <div class="primary-text">
                    This research explores finding an optimal basis for factorizing class conditional densities in
                    the
                    Naïve Bayes model, instead of using the standard cardinal basis. This approach aims to improve
                    classification accuracy by finding a better way to represent the data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon" title="Play from here">27:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04980" target="_blank">@arXiv
                        2409.04980</a>
                    <span class="tweet-title">Self-Driving Cars Get a Social Life: New Dataset Simulates Connected
                        Vehicles</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset called Multi-V2X, which simulates various penetration
                    rates
                    of connected and autonomous vehicles (CAVs) in a traffic environment. Unlike previous datasets,
                    Multi-V2X allows researchers to explore how cooperative perception algorithms perform under
                    different levels of CAV adoption.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon" title="Play from here">28:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05486" target="_blank">@arXiv
                        2409.05486</a>
                    <span class="tweet-title">Tiny Brains, Big Results: Can Smaller LLMs Rule the Biomedical
                        World?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Elsevier, OpenAI, Meta</span>
                </div>
                <div class="primary-text">
                    This research compares the performance of a smaller, domain-specific LLM trained on Elsevier's
                    biomedical data against larger, general-purpose models like GPT-3.5-turbo and Llama 2. The study
                    uses human evaluation to assess the models' ability to generate accurate and fluent responses in
                    the
                    biomedical domain.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet67">
            <div class="start-time-icon" title="Play from here">28:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05283" target="_blank">@arXiv
                        2409.05283</a>
                    <span class="tweet-title">Truthful AI, Left-Leaning Bias: When Facts Get Political</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research investigates the relationship between truthfulness and political bias in language
                    models, specifically examining how training models on datasets designed to capture objective
                    truth
                    can still lead to a left-leaning bias. This differs from previous work that focused on the
                    political
                    bias of base models and the influence of alignment datasets on their political stance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet68">
            <div class="start-time-icon" title="Play from here">28:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05799" target="_blank">@arXiv
                        2409.05799</a>
                    <span class="tweet-title">Speaker Verification: Don't Just Listen, Decode the Phoneme!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel Phoneme-Debiasing Attention Framework (PDAF) that explicitly
                    accounts for the phonetic sequence underlying a speech recording, unlike previous methods that
                    treat
                    speech as a generic time series.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet69">
            <div class="start-time-icon" title="Play from here">29:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04732" target="_blank">@arXiv
                        2409.04732</a>
                    <span class="tweet-title">Surgical Videos Get a Language Lesson: AI Learns to Understand the
                        Operating Room</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Intuitive Surgical Inc.</span>
                </div>
                <div class="primary-text">
                    This research introduces VidLPRO, a video-language pre-training framework that goes beyond
                    contrastive learning to capture the intricate temporal dynamics of surgical videos. Unlike
                    previous
                    methods, VidLPRO incorporates video-text matching and masked language modeling objectives,
                    allowing
                    it to learn richer and more nuanced representations of surgical procedures.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet70">
            <div class="start-time-icon" title="Play from here">29:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05692" target="_blank">@arXiv
                        2409.05692</a>
                    <span class="tweet-title">Building a Better Map: Unsupervised Learning Classifies Millions of US
                        Buildings</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">George Mason University, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research differs from previous work by using an unsupervised machine learning method to
                    classify building types in the US, relying solely on OpenStreetMap data and auxiliary
                    information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet71">
            <div class="start-time-icon" title="Play from here">29:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05672" target="_blank">@arXiv
                        2409.05672</a>
                    <span class="tweet-title">Outlier Detection: Model Selection? Who Needs It?!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This paper introduces FoMo-0D, a foundation model for outlier detection that bypasses the need
                    for
                    model selection and training. It is pretrained on synthetic data generated from a novel data
                    prior,
                    enabling zero-shot inference on new datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet72">
            <div class="start-time-icon" title="Play from here">30:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05242" target="_blank">@arXiv
                        2409.05242</a>
                    <span class="tweet-title">FedFT: Shrinking Models, Boosting Brains in Federated Learning!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Robert Gordon University, University College London</span>
                </div>
                <div class="primary-text">
                    This paper introduces FedFT, a new method for improving communication efficiency in federated
                    learning by transforming model parameters into the frequency space using the Discrete Cosine
                    Transform (DCT). Unlike previous work that focuses on compressing training data, FedFT
                    compresses
                    model parameters, enabling efficient communication and aggregation in the frequency domain.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet73">
            <div class="start-time-icon" title="Play from here">30:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05314" target="_blank">@arXiv
                        2409.05314</a>
                    <span class="tweet-title">Telecom Talk: LLMs Get a Domain-Specific Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University, Amazon</span>
                </div>
                <div class="primary-text">
                    This research focuses on adapting large language models (LLMs) specifically for the
                    telecommunications domain, a gap in current research. Unlike previous work that relies on
                    general-purpose LLMs, this study creates a series of specialized models trained on a curated
                    dataset
                    of telecommunications materials.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet74">
            <div class="start-time-icon" title="Play from here">31:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05556" target="_blank">@arXiv
                        2409.05556</a>
                    <span class="tweet-title">SciAgents: AI's New Brain for Scientific Discovery</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces SciAgents, a multi-agent system that uses a large ontological knowledge
                    graph and LLMs to generate and refine research hypotheses. Unlike previous work, SciAgents
                    incorporates a random path approach for knowledge graph exploration, leading to more diverse and
                    potentially novel hypotheses.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet75">
            <div class="start-time-icon" title="Play from here">31:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05169" target="_blank">@arXiv
                        2409.05169</a>
                    <span class="tweet-title">Polycrystal Plasticity Gets a Graph-ic Makeover: GNNs Speed Up
                        Simulations
                        150x!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research uses Graph Neural Networks (GNNs) to learn the relationship between strain and
                    stress
                    in polycrystal plasticity, a departure from traditional methods that rely on fixed-size data.
                    The
                    GNN is trained on subgraphs of finite element meshes, allowing it to handle data with varying
                    dimensions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet76">
            <div class="start-time-icon" title="Play from here">31:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05636" target="_blank">@arXiv
                        2409.05636</a>
                    <span class="tweet-title">3D Forest Scan: Machine Learning Helps Us See Trees (and Carbon)
                        Better!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge, Drexel University</span>
                </div>
                <div class="primary-text">
                    This research uses 3D tomographic SAR data, which captures multiple images from different
                    angles, to
                    create a more detailed representation of forest structure than traditional SAR images. This
                    approach
                    is particularly relevant for the upcoming Biomass Satellite mission.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet77">
            <div class="start-time-icon" title="Play from here">32:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05305" target="_blank">@arXiv
                        2409.05305</a>
                    <span class="tweet-title">Neural Networks: Unlocking the Secrets of Latent Spaces with Symbolic
                        Search!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, Perimeter Institute for Theoretical
                        Physics</span>
                </div>
                <div class="primary-text">
                    This research introduces a framework for interpreting individual neurons within the latent
                    spaces of
                    neural networks by finding closed-form expressions that represent the underlying concepts. This
                    differs from previous work that focused on interpreting output neurons or required prior
                    knowledge
                    of the concept.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet78">
            <div class="start-time-icon" title="Play from here">32:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04707" target="_blank">@arXiv
                        2409.04707</a>
                    <span class="tweet-title">Deep Learning's New Trick: Borrowing from Math Class!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Johns Hopkins University, Columbia University, Washington
                        University in
                        St. Louis...</span>
                </div>
                <div class="primary-text">
                    This research explores the connection between deep learning optimization algorithms and
                    numerical
                    methods, specifically using the Taylor multi-step method to improve the SGD optimizer. This
                    approach
                    differs from previous work that focused on accelerating training speed, as this paper aims to
                    enhance interpretability and accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet79">
            <div class="start-time-icon" title="Play from here">33:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05207" target="_blank">@arXiv
                        2409.05207</a>
                    <span class="tweet-title">Transformers on FPGAs: Making Physics Calculations Super
                        Speedy!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, Massachusetts Institute of
                        Technology</span>
                </div>
                <div class="primary-text">
                    This research extends the hls4ml compiler to automatically convert any TensorFlow-based
                    transformer
                    model into an FPGA-friendly format, enhancing the versatility of this method. Previous work
                    mainly
                    focused on implementing specific transformer models on FPGAs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet80">
            <div class="start-time-icon" title="Play from here">33:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04475" target="_blank">@arXiv
                        2409.04475</a>
                    <span class="tweet-title">Database Q&A Bots Get a Brain Upgrade: LLMs Learn to Talk the Talk and
                        Walk the Walk</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Xiamen University, Tsinghua University, Huawei</span>
                </div>
                <div class="primary-text">
                    This research introduces DQA, a comprehensive benchmark for evaluating Large Language Models
                    (LLMs)
                    in database question-answering tasks. Unlike previous work, DQA focuses specifically on
                    database-related queries, including general database concepts, product-specific instructions,
                    and
                    instance-specific troubleshooting.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet81">
            <div class="start-time-icon" title="Play from here">33:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04759" target="_blank">@arXiv
                        2409.04759</a>
                    <span class="tweet-title">Deep Learning Gets a Contextual Makeover: Say Goodbye to Batch Size
                        Blues!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sorbonne Paris Nord University, Paris-Saclay University,
                        UVSQ...</span>
                </div>
                <div class="primary-text">
                    This research introduces Adaptative Context Normalization (ACN), a new method for normalizing
                    activations in deep neural networks. Unlike previous methods like Batch Normalization (BN) and
                    Mixture Normalization (MN), ACN leverages the concept of "context" to group data with similar
                    characteristics, allowing for more efficient and accurate normalization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet82">
            <div class="start-time-icon" title="Play from here">34:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04838" target="_blank">@arXiv
                        2409.04838</a>
                    <span class="tweet-title">Seizure Prediction on a Chip: TinyML Makes Big Moves!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research presents SPIRIT, a system-on-a-chip (SoC) designed for seizure prediction. Unlike
                    previous work that focused on software-based implementations or hardware-based seizure
                    detection,
                    SPIRIT integrates both a low-power classifier and analog frontends (AFEs) for on-chip seizure
                    prediction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet83">
            <div class="start-time-icon" title="Play from here">34:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05601" target="_blank">@arXiv
                        2409.05601</a>
                    <span class="tweet-title">Punctuate This! Training Speech Models on Longer Sentences for Better
                        Recognition and Translation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nvidia</span>
                </div>
                <div class="primary-text">
                    This research explores training speech recognition and translation models on longer audio
                    segments
                    containing complete sentences with proper punctuation and capitalization. Unlike previous work
                    that
                    primarily focused on lowercase text and partial punctuation, this study investigates the
                    benefits of
                    incorporating full PnC information directly into the training process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet84">
            <div class="start-time-icon" title="Play from here">35:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05699" target="_blank">@arXiv
                        2409.05699</a>
                    <span class="tweet-title">Handwriting Recognition Gets a Brain Boost with Learnable Relaxation
                        Labelling!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Italian Institute of Technology, ETH Zurich, Ca’ Foscari University
                        of
                        Venice</span>
                </div>
                <div class="primary-text">
                    This research integrates Relaxation Labelling (RL) processes, a technique with roots in the
                    1970s,
                    into modern handwriting recognition systems. Unlike attention mechanisms, RL offers a principled
                    approach with theoretical foundations in variational inequality and game theory.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet85">
            <div class="start-time-icon" title="Play from here">35:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04994" target="_blank">@arXiv
                        2409.04994</a>
                    <span class="tweet-title">NMF: Shrinking Big Data Without Losing the Good Stuff!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Princeton University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new framework for Nonnegative Matrix Factorization (NMF) that uses
                    compressed data, allowing for efficient processing of large datasets. Unlike previous work, this
                    paper provides theoretical guarantees on the accuracy of the compressed NMF solutions and
                    introduces
                    new algorithms specifically designed for these compressed problems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet86">
            <div class="start-time-icon" title="Play from here">35:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04913" target="_blank">@arXiv
                        2409.04913</a>
                    <span class="tweet-title">NGD: The Degeneracy Buster for Deep Learning Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Cambridge AI Safety Hub</span>
                </div>
                <div class="primary-text">
                    This research compares the effective dimension of models trained using natural gradient descent
                    (NGD) and stochastic gradient descent (SGD). It finds that NGD consistently leads to models with
                    a
                    higher effective dimension, suggesting that NGD avoids flatter, more degenerate regions in the
                    loss
                    landscape.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet87">
            <div class="start-time-icon" title="Play from here">36:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04909" target="_blank">@arXiv
                        2409.04909</a>
                    <span class="tweet-title">Tiny Data, Big Brains: A Transformer Trick for Predicting Drug Brain
                        Power</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research proposes a modified GPS Transformer architecture that incorporates Self Attention,
                    designed to perform well with limited data. This approach differs from previous work by focusing
                    on
                    improving performance with smaller datasets, which is crucial for computational efficiency in
                    drug
                    discovery.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet88">
            <div class="start-time-icon" title="Play from here">36:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05225" target="_blank">@arXiv
                        2409.05225</a>
                    <span class="tweet-title">Hemarthrosis Detection: Can Fake Images Help Real Patients?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research compares two augmentation methods, data synthesis and traditional augmentation
                    techniques, to improve the accuracy of hemophilia detection models. It's unique because it
                    focuses
                    on the similarity between synthetic and real images, which is often overlooked in augmentation
                    studies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet89">
            <div class="start-time-icon" title="Play from here">36:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.05200" target="_blank">@arXiv
                        2409.05200</a>
                    <span class="tweet-title">Lung-DETR: A Deformable Transformer That Can Spot Tiny Lung
                        Tumors!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research combines Deformable-DETR, Focal Loss, and Maximum Intensity Projection (MIP) into
                    a
                    unified framework for detecting sparse lung nodules. This approach is unique because it
                    addresses
                    the challenge of nodule sparsity in real-world data, which is often overlooked in previous
                    studies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet90">
            <div class="start-time-icon" title="Play from here">37:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.04465" target="_blank">@arXiv
                        2409.04465</a>
                    <span class="tweet-title">AI Agents: The Semantic Web's New BFFs?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research focuses on building a network of semi-autonomous agents that can represent
                    individuals
                    and organizations on the web, leveraging LLMs for natural language interaction and Notation3
                    rules
                    for safety guarantees. This approach differs from previous work by emphasizing user control over
                    data and decisions while utilizing LLMs for enhanced communication and task completion.
                </div>
            </div>
        </div>

        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202409102200_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>

</html>