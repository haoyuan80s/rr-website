
daily_data = {
    "date": "2024-10-29",
    "tweets": [
        
        {
            "startTime": "01:00",
            "arxivId": "2410.19803",
            "arxivLink": "https://arxiv.org/abs/2410.19803",
            "title": "Chatbots Got Names? New Study Uncovers Bias in AI Conversations!",
            "institute": "OpenAI",
            "text": "This research focuses on \"first-person fairness\" in chatbots, specifically examining how user names can influence chatbot responses. Unlike previous work that primarily focused on third-person fairness in decision-making tasks, this study analyzes real-world chatbot interactions, considering the user's perspective.",
            "paper-title": "First-Person Fairness in Chatbots",
            "image-path": "flux_paper_image/2410.19803_1730242113.png"
        },

        {
            "startTime": "01:22",
            "arxivId": "2410.20247",
            "arxivLink": "https://arxiv.org/abs/2410.20247",
            "title": "API Audit: Is Your Llama Model a Fake?",
            "institute": "Stanford University",
            "text": "This research introduces a new method for auditing language model APIs, focusing on comparing the distribution of outputs from the API to a reference model, rather than just evaluating accuracy on specific tasks.",
            "paper-title": "Model Equality Testing: Which Model Is This API Serving?",
            "image-path": "flux_paper_image/2410.20247_1730241431.png"
        },

        {
            "startTime": "01:43",
            "arxivId": "2410.21211",
            "arxivLink": "https://arxiv.org/abs/2410.21211",
            "title": "Mamba Strikes Back: Point Cloud Segmentation Gets a Speed Boost!",
            "institute": "Tsinghua University, Bosch Corporate Research",
            "text": "This research explores the use of State Space Models (SSMs) for point cloud segmentation, specifically focusing on the Mamba model. Unlike previous attempts, this paper identifies and addresses two key limitations of Mamba when applied to point clouds, leading to significant performance improvements.",
            "paper-title": "Exploring contextual modeling with linear complexity for point cloud segmentation",
            "image-path": "flux_paper_image/2410.21211_1730242060.png"
        },

        {
            "startTime": "02:09",
            "arxivId": "2410.20399",
            "arxivLink": "https://arxiv.org/abs/2410.20399",
            "title": "ThunderKittens: Tiny Abstractions, Big AI Performance",
            "institute": "Stanford University",
            "text": "This research proposes a framework called ThunderKittens (TK) that simplifies the development of high-performance AI kernels by using a small set of abstractions that map to different levels of the GPU hierarchy. This approach differs from previous work by focusing on a more opinionated set of abstractions, aiming to achieve high performance with a simpler interface.",
            "paper-title": "ThunderKittens: Simple, Fast, and Adorable AI Kernels",
            "image-path": "flux_paper_image/2410.20399_1730240814.png"
        },

        {
            "startTime": "02:27",
            "arxivId": "2410.20107",
            "arxivLink": "https://arxiv.org/abs/2410.20107",
            "title": "Deep Learning's Hidden Bias: How Activations Make Networks Go Orthogonal (or Not)",
            "institute": "ETH Zurich",
            "text": "This paper introduces a theoretical framework for analyzing the evolution of the kernel sequence in deep neural networks, which measures the similarity between hidden representations for two different inputs. It uses Hermite polynomials to derive an explicit form for the kernel map and fully characterize its fixed points, revealing that for nonlinear activations, the kernel sequence converges globally to a unique fixed point.",
            "paper-title": "Emergence of Globally Attracting Fixed Points in Deep Neural Networks With Nonlinear Activations",
            "image-path": "flux_paper_image/2410.20107_1730241517.png"
        },

        {
            "startTime": "02:50",
            "arxivId": "2410.21035",
            "arxivLink": "https://arxiv.org/abs/2410.21035",
            "title": "LLMs Get a Speed Boost: Self-Distillation Through Time Makes Text Generation Faster!",
            "institute": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne",
            "text": "This research introduces Self-Distillation Through Time (SDTT), a novel distillation method for discrete diffusion language models. Unlike previous distillation methods for continuous diffusion models, SDTT does not rely on deterministic mappings.",
            "paper-title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
            "image-path": "flux_paper_image/2410.21035_1730241061.png"
        },

        {
            "startTime": "03:08",
            "arxivId": "2410.21153",
            "arxivLink": "https://arxiv.org/abs/2410.21153",
            "title": "Synthetic Data: Robots Now See the World in 2.7 Million Images!",
            "institute": "NVIDIA, University of Toronto",
            "text": "This research focuses on generating a massive dataset of synthetic images for training object detectors, surpassing previous efforts in scale and achieving real-time inference speeds.",
            "paper-title": "Synthetica: Large Scale Synthetic Data for Robot Perception",
            "image-path": "flux_paper_image/2410.21153_1730240867.png"
        },

        {
            "startTime": "03:27",
            "arxivId": "2410.20245",
            "arxivLink": "https://arxiv.org/abs/2410.20245",
            "title": "Tired of Saturated Benchmarks? SMART Filtering to the Rescue!",
            "institute": "Pennsylvania State University, Meta",
            "text": "This research proposes a novel approach called SMART filtering to select a high-quality subset of examples from existing benchmark datasets by systematically removing less informative and less challenging examples. This differs from previous work by focusing on algorithmic filtering rather than human intervention.",
            "paper-title": "Improving Model Evaluation using SMART Filtering of Benchmark Datasets",
            "image-path": "flux_paper_image/2410.20245_1730239672.png"
        },

        {
            "startTime": "03:51",
            "arxivId": "2410.20783",
            "arxivLink": "https://arxiv.org/abs/2410.20783",
            "title": "LLMs Get a Reality Check: Graph-Based Uncertainty Metrics for Long-Form Text Generation",
            "institute": "Stanford University, IBM",
            "text": "This research proposes a novel approach to estimating uncertainty in long-form text generated by LLMs. Unlike previous methods that rely on self-consistency or verbalized confidence, this work leverages a bipartite graph to capture the semantic relationships between claims and responses, then uses graph centrality metrics to assess the uncertainty of each claim.",
            "paper-title": "Graph-based Uncertainty Metrics for Long-form Language Model Outputs",
            "image-path": "flux_paper_image/2410.20783_1730241255.png"
        },

        {
            "startTime": "04:16",
            "arxivId": "2410.19750",
            "arxivLink": "https://arxiv.org/abs/2410.19750",
            "title": "Language Models Have Brains? New Study Maps the \"Galaxy\" of Concepts",
            "institute": "MIT",
            "text": "This research goes beyond simply identifying concepts in language models. It delves into the geometric structure of these concepts, analyzing how they cluster and relate to each other at different scales. Unlike previous work that focused on individual concept pairs, this study examines the overall organization of the concept space.",
            "paper-title": "The Geometry of Concepts: Sparse Autoencoder Feature Structure",
            "image-path": "flux_paper_image/2410.19750_1730242122.png"
        },

        {
            "startTime": "04:40",
            "arxivId": "2410.20502",
            "arxivLink": "https://arxiv.org/abs/2410.20502",
            "title": "ARLON: Diffusion Transformers Get a Long-Term Memory Boost!",
            "institute": "Huazhong University of Science and Technology, The Chinese University of Hong Kong, Microsoft Corporation...",
            "text": "This paper introduces ARLON, a framework that combines autoregressive (AR) models with diffusion Transformers (DiT) for long video generation. Unlike previous approaches that rely solely on DiT models, ARLON leverages the AR model's ability to capture long-range dependencies and provide coarse spatial information, guiding the DiT model to generate more dynamic and temporally consistent videos.",
            "paper-title": "ARLON: Boosting Diffusion Transformers with Autoregressive Models for Long Video Generation",
            "image-path": "flux_paper_image/2410.20502_1730238916.png"
        },

        {
            "startTime": "05:10",
            "arxivId": "2410.20763",
            "arxivLink": "https://arxiv.org/abs/2410.20763",
            "title": "LLMs: Can They Explain Stuff Like a Human Tutor?",
            "institute": "University of Michigan, Google",
            "text": "This research focuses on a new task called \"targeted concept simplification,\" which aims to rewrite text to help readers understand specific difficult concepts within a larger text, rather than simplifying the entire text. This differs from previous work that focused on simplifying text for general audiences or simplifying all difficult concepts within a text.",
            "paper-title": "Evaluating LLMs for Targeted Concept Simplification forDomain-Specific Texts",
            "image-path": "flux_paper_image/2410.20763_1730241124.png"
        },

        {
            "startTime": "05:28",
            "arxivId": "2410.21195",
            "arxivLink": "https://arxiv.org/abs/2410.21195",
            "title": "AI's Got a Belief Problem: Can Language Models Tell Fact from Fiction?",
            "institute": "Stanford University",
            "text": "This research focuses on the epistemological reasoning capabilities of large language models (LLMs), specifically their ability to distinguish between belief, knowledge, and fact. Unlike previous work that focused on more complex issues like theory of mind, this study systematically evaluates LLMs' understanding of these fundamental epistemic concepts using a new dataset called KaBLE.",
            "paper-title": "Belief in the Machine: Investigating Epistemological Blind Spots of Language Models",
            "image-path": "flux_paper_image/2410.21195_1730240955.png"
        },

        {
            "startTime": "05:52",
            "arxivId": "2410.20478",
            "arxivLink": "https://arxiv.org/abs/2410.20478",
            "title": "MusicFlow: Text-to-Music Generation Gets a Flow-tastic Makeover!",
            "institute": "University of Oxford, Meta",
            "text": "This paper introduces MusicFlow, a cascaded text-to-music generation model that uses flow matching. Unlike previous models that rely on autoregressive language models or diffusion models, MusicFlow leverages flow matching for improved efficiency in both training and inference.",
            "paper-title": "MusicFlow: Cascaded Flow Matching for Text Guided Music Generation",
            "image-path": "flux_paper_image/2410.20478_1730241043.png"
        },

        {
            "startTime": "06:08",
            "arxivId": "2410.20625",
            "arxivLink": "https://arxiv.org/abs/2410.20625",
            "title": "LoRA Done Rite: A New Optimizer That Makes LLMs Learn Like Pros!",
            "institute": "UC Los Angeles, Google, University of Texas at Austin",
            "text": "This research introduces LoRA-RITE, a novel optimizer for fine-tuning large language models (LLMs) that addresses the lack of transformation invariance in existing methods. Unlike previous optimizers, LoRA-RITE ensures that updates to the model remain consistent regardless of how the low-rank matrices are scaled or rotated.",
            "paper-title": "LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization",
            "image-path": "flux_paper_image/2410.20625_1730241078.png"
        },

        {
            "startTime": "06:37",
            "arxivId": "2410.21265",
            "arxivLink": "https://arxiv.org/abs/2410.21265",
            "title": "Deep Learning's New Duality: A Modular Approach to Faster Training",
            "institute": "MIT",
            "text": "This paper introduces a new method called \"modular dualization\" for constructing duality maps for general neural architectures. This approach differs from previous work by assigning operator norms to individual layers and then recursively inducing a duality map on the full weightspace of the architecture.",
            "paper-title": "Modular Duality in Deep Learning",
            "image-path": "flux_paper_image/2410.21265_1730240080.png"
        },

        {
            "startTime": "07:02",
            "arxivId": "2410.20672",
            "arxivLink": "https://arxiv.org/abs/2410.20672",
            "title": "LLMs on a Diet: Sharing Weights to Slim Down Big Models",
            "institute": "Korea Advanced Institute of Science and Technology, Google",
            "text": "This paper revisits \"layer tying\" in Transformers, a technique where weights are shared across layers. It introduces a novel method called \"Recursive Transformers\" that uses a single block of unique layers repeated multiple times, achieving impressive performance with a smaller model size. The paper further improves performance by introducing \"Relaxed Recursive Transformers\" that add flexibility to the layer tying constraint via LoRA modules.",
            "paper-title": "Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA",
            "image-path": "flux_paper_image/2410.20672_1730242210.png"
        },

        {
            "startTime": "07:33",
            "arxivId": "2410.20057",
            "arxivLink": "https://arxiv.org/abs/2410.20057",
            "title": "Confounded by Confounders? Mechanism Learning to the Rescue!",
            "institute": "University of Birmingham, MIT",
            "text": "This research proposes \"mechanism learning,\" a method that uses front-door causal bootstrapping to deconfound observational data. This allows for the training of machine learning models that learn causal relationships between effects and their causes, even in the presence of multiple unknown confounders. This differs from previous work by focusing on reverse causal inference, where the goal is to predict causes from effects, rather than the traditional approach of predicting effects from causes.",
            "paper-title": "Mechanism learning: Reverse causal inference in the presence of multiple unknown confounding through front-door causal bootstrapping",
            "image-path": "flux_paper_image/2410.20057_1730239478.png"
        },

        {
            "startTime": "08:00",
            "arxivId": "2410.20056",
            "arxivLink": "https://arxiv.org/abs/2410.20056",
            "title": "Stop Treating Documents Like Text! Multi-Field Retrieval is Here!",
            "institute": "Northeastern University, Microsoft",
            "text": "This research introduces a new framework called Multi-Field Adaptive Retrieval (MFAR) that specifically addresses the retrieval of structured documents, which are documents that have distinct fields like titles, authors, and timestamps. Unlike previous work that treats documents as a single block of text, MFAR breaks down documents into their individual fields and scores each field separately using both lexical and dense methods.",
            "paper-title": "Multi-Field Adaptive Retrieval",
            "image-path": "flux_paper_image/2410.20056_1730239176.png"
        },

        {
            "startTime": "08:29",
            "arxivId": "2410.20220",
            "arxivLink": "https://arxiv.org/abs/2410.20220",
            "title": "Robots Get a 3D Brain: Neural Fields Revolutionize Robotics!",
            "institute": "Toyota Research Institute, University of Bristol, Nvidia...",
            "text": "This survey paper provides a comprehensive overview of Neural Fields (NFs) in robotics, highlighting their applications in various domains like pose estimation, manipulation, navigation, physics, and autonomous driving. It distinguishes itself from previous work by covering a broader range of NFs beyond just NeRFs, including Occupancy Networks, Signed Distance Fields, and 3D Gaussian Splatting.",
            "paper-title": "Neural Fields in Robotics: A Survey",
            "image-path": "flux_paper_image/2410.20220_1730239397.png"
        },

        {
            "startTime": "08:57",
            "arxivId": "2410.20294",
            "arxivLink": "https://arxiv.org/abs/2410.20294",
            "title": "Wrestling with Occlusion: New Dataset Helps AI See Through Close Human Interactions",
            "institute": "Carnegie Mellon University",
            "text": "This research introduces Harmony4D, a dataset of multi-view videos featuring dynamic activities with close human contact, such as wrestling and dancing. Unlike previous datasets, Harmony4D is collected in the wild, capturing diverse environments and unchoreographed activities. The paper also proposes a novel method for tracking 3D human poses in these challenging scenarios, using instance segmentation and 3D pose forecasting.",
            "paper-title": "Harmony4D: A Video Dataset for In-The-Wild Close Human Interactions",
            "image-path": "flux_paper_image/2410.20294_1730240631.png"
        },

        {
            "startTime": "09:18",
            "arxivId": "2410.21081",
            "arxivLink": "https://arxiv.org/abs/2410.21081",
            "title": "Safe LQR Learning: No More Crashing, Just Smooth Sailing!",
            "institute": "Harvard University",
            "text": "This research improves upon previous work by achieving a faster regret rate of \u02dcO(\u221aT) compared to the previous \u02dcO(T2/3) rate. It also uses a stronger baseline of truncated linear controllers, which are better suited for constrained problems than linear controllers.",
            "paper-title": "Stronger Regret Bounds for Safe Online Reinforcement Learning in the Linear Quadratic Regulator",
            "image-path": "flux_paper_image/2410.21081_1730240853.png"
        },

        {
            "startTime": "09:39",
            "arxivId": "2410.20596",
            "arxivLink": "https://arxiv.org/abs/2410.20596",
            "title": "Bayesian Algorithm Execution: Sampling Your Way to Smarter Decisions!",
            "institute": "Caltech, Lawrence Livermore National Laboratory",
            "text": "This research introduces PS-BAX, a new Bayesian algorithm execution method that uses posterior sampling instead of expected information gain. This approach is significantly faster and simpler to implement than previous methods.",
            "paper-title": "Practical Bayesian Algorithm Execution via Posterior Sampling",
            "image-path": "flux_paper_image/2410.20596_1730242197.png"
        },

        {
            "startTime": "09:58",
            "arxivId": "2410.19920",
            "arxivLink": "https://arxiv.org/abs/2410.19920",
            "title": "LLMs: Prompt-ing for Trouble? New Research Reveals How AI Overfits to Instructions!",
            "institute": "Sorbonne Universit\u00e9, Inria, Hugging Face...",
            "text": "This research investigates the impact of fine-tuning large language models (LLMs) with reinforcement learning (RL) in interactive environments. Unlike previous work that focuses on improving performance with a single prompt format, this study explores the sensitivity of LLMs to variations in prompt formulations.",
            "paper-title": "Reinforcement Learning for Aligning Large Language Models Agents with Interactive Environments: Quantifying and Mitigating Prompt Overfitting",
            "image-path": "flux_paper_image/2410.19920_1730241387.png"
        },

        {
            "startTime": "10:22",
            "arxivId": "2410.21236",
            "arxivLink": "https://arxiv.org/abs/2410.21236",
            "title": "\ud83d\udd25 Hot Start, Cold Finish: A New Trick for Training Smarter Language Models",
            "institute": "University of Southern California, CMU, ByteDance",
            "text": "This paper introduces a new sampling method called \"Flaming-hot Initiation with Regular Execution\" (FIRE) for training large language models (LLMs). FIRE differs from previous work by focusing on the initial token of a response, sampling it at a high temperature to encourage diversity, and then continuing with regular sampling for the rest of the sequence.",
            "paper-title": "Flaming-hot Initiation with Regular Execution Sampling for Large Language Models",
            "image-path": "flux_paper_image/2410.21236_1730239643.png"
        },

        {
            "startTime": "10:41",
            "arxivId": "2410.20688",
            "arxivLink": "https://arxiv.org/abs/2410.20688",
            "title": "Drug Design Gets a Dual-Target Makeover: Diffusion Models Go Double-Duty!",
            "institute": "Tsinghua University",
            "text": "This research proposes a novel method for dual-target drug design using diffusion models. Unlike previous work that focused on single-target drug design, this study repurposes pretrained diffusion models for dual-target scenarios, enabling the generation of molecules that can bind to two different protein targets simultaneously.",
            "paper-title": "Reprogramming Pretrained Target-Specific Diffusion Models for Dual-Target Drug Design",
            "image-path": "flux_paper_image/2410.20688_1730241003.png"
        },

        {
            "startTime": "11:12",
            "arxivId": "2410.20349",
            "arxivLink": "https://arxiv.org/abs/2410.20349",
            "title": "Skeleton-Based Action Recognition: Idempotent Models Make a Move!",
            "institute": "Peking University",
            "text": "This research proposes an idempotent generative model for skeleton-based action recognition. Unlike previous methods that focus on either generative or contrastive learning, this approach combines both, aiming to extract more compact and relevant motion information.",
            "paper-title": "Idempotent Unsupervised Representation Learning for Skeleton-Based Action Recognition",
            "image-path": "flux_paper_image/2410.20349_1730242103.png"
        },

        {
            "startTime": "11:31",
            "arxivId": "2410.20389",
            "arxivLink": "https://arxiv.org/abs/2410.20389",
            "title": "Dance Like a Pro: AI Choreographs Ultra-Long Dances!",
            "institute": "Tsinghua University, Beijing Normal University",
            "text": "Lodge++ introduces a two-stage approach to dance generation, using a Global Choreography Network to learn complex choreography patterns and a Primitive-based Dance Diffusion Model to generate high-quality, long-sequence dances in parallel. This differs from previous methods that either focused on short-term dependencies or struggled to model complex choreography patterns.",
            "paper-title": "Lodge++: High-quality and Long Dance Generation with Vivid Choreography Patterns",
            "image-path": "flux_paper_image/2410.20389_1730240244.png"
        },

        {
            "startTime": "11:50",
            "arxivId": "2410.21083",
            "arxivLink": "https://arxiv.org/abs/2410.21083",
            "title": "Jailbreaking LLMs: A Stealthy New Attack Using Benign Data Mirroring",
            "institute": "Harbin Institute of Technology, Central South University, East China Normal University...",
            "text": "This research introduces a novel jailbreak attack method called ShadowBreak that utilizes benign data mirroring to create a local mirror model of the target LLM. This approach aims to improve attack success rates while minimizing detectable queries, enhancing attack stealth. Unlike previous methods that rely on repeated malicious queries, ShadowBreak leverages benign data to align the mirror model with the target model, making it more difficult for content moderators to detect malicious intent.",
            "paper-title": "Stealthy Jailbreak Attacks on Large Language Models via Benign Data Mirroring",
            "image-path": "flux_paper_image/2410.21083_1730240502.png"
        },

        {
            "startTime": "12:17",
            "arxivId": "2410.20317",
            "arxivLink": "https://arxiv.org/abs/2410.20317",
            "title": "Protein Dynamics: A Deep Learning Dance to Map Conformations",
            "institute": "Yale University",
            "text": "This research introduces ProtSCAPE, a deep learning architecture that leverages the geometric scattering transform alongside transformer-based attention mechanisms to capture protein dynamics from molecular dynamics (MD) simulations. This approach differs from previous methods by incorporating multi-scale structural information and dual attention structures, enabling more precise and interpretable upsampling of dynamics.",
            "paper-title": "ProtSCAPE: Mapping the landscape of protein conformations in molecular dynamics",
            "image-path": "flux_paper_image/2410.20317_1730240841.png"
        },

        {
            "startTime": "12:37",
            "arxivId": "2410.21109",
            "arxivLink": "https://arxiv.org/abs/2410.21109",
            "title": "Pricing & Replenishment: A Deep Learning Duet for Profit Harmony",
            "institute": "Peking University",
            "text": "This research introduces a dual-agent deep reinforcement learning (DRL) algorithm for dynamic pricing and replenishment, addressing the discrepancies in decision frequencies between pricing and replenishment. This approach differs from previous work by incorporating a two-timescale stochastic approximation scheme, ensuring convergence to a local optimum.",
            "paper-title": "Dual-Agent Deep Reinforcement Learning for Dynamic Pricing and Replenishment",
            "image-path": "flux_paper_image/2410.21109_1730241593.png"
        },

        {
            "startTime": "13:03",
            "arxivId": "2410.20336",
            "arxivLink": "https://arxiv.org/abs/2410.20336",
            "title": "LLMs Get Vocal: A Late-Fusion Trick for Speech Generation",
            "institute": "Massachusetts Institute of Technology, Meta",
            "text": "This research explores a new approach to speech generation by fine-tuning a text-based LLM (Large Language Model) using a parameter-efficient technique called PEFT (Parameter-Efficient Fine-Tuning). Unlike previous methods that require full pretraining or fine-tuning, this approach leverages the existing knowledge of the LLM and focuses on adapting it for speech generation.",
            "paper-title": "Get Large Language Models Ready to Speak: A Late-fusion Approach for Speech Generation",
            "image-path": "flux_paper_image/2410.20336_1730239992.png"
        },

        {
            "startTime": "13:26",
            "arxivId": "2410.20788",
            "arxivLink": "https://arxiv.org/abs/2410.20788",
            "title": "Long Prompts, Short Attention Spans: A New Way to Tune LLMs",
            "institute": "Microsoft",
            "text": "This research introduces SCULPT, a framework that systematically refines long prompts by structuring them hierarchically and applying an iterative actor-critic mechanism. Unlike previous methods that primarily focus on short prompts or few-shot scenarios, SCULPT specifically targets the intricacies of longer prompts with multiple instructions, examples, and layered structures.",
            "paper-title": "SCULPT: Systematic Tuning of Long Prompts",
            "image-path": "flux_paper_image/2410.20788_1730239125.png"
        },

        {
            "startTime": "13:52",
            "arxivId": "2410.20723",
            "arxivLink": "https://arxiv.org/abs/2410.20723",
            "title": "3D Objects, 2D Brains: How a New AI Makes Composing 3D Scenes a Breeze!",
            "institute": "University of Hong Kong, UC Berkeley",
            "text": "This research introduces COMPGS, a generative framework that uses 3D Gaussian Splatting to create compositional 3D scenes. Unlike previous methods that struggle with multiple objects, COMPGS leverages 2D compositionality to initialize 3D Gaussians, enabling more accurate and consistent generation.",
            "paper-title": "CompGS: Unleashing 2D Compositionality for Compositional Text-to-3D via Dynamically Optimizing 3D Gaussians",
            "image-path": "flux_paper_image/2410.20723_1730240119.png"
        },

        {
            "startTime": "14:16",
            "arxivId": "2410.20011",
            "arxivLink": "https://arxiv.org/abs/2410.20011",
            "title": "Small Language Models: Tiny Brains, Big Ideas!",
            "institute": "University of Oregon, Northeastern University, Carnegie Mellon University...",
            "text": "This research provides a comprehensive survey of small language models (SLMs), focusing on their architectures, training techniques, and model compression techniques. It introduces a novel taxonomy for categorizing the methods used to optimize SLMs, including model compression, pruning, and quantization techniques.",
            "paper-title": "A Survey of Small Language Models",
            "image-path": "flux_paper_image/2410.20011_1730241643.png"
        },

        {
            "startTime": "14:41",
            "arxivId": "2410.20176",
            "arxivLink": "https://arxiv.org/abs/2410.20176",
            "title": "Rewarding Robots: Beyond Simple Sums, Learning from Complex Feedback",
            "institute": "The University of Tokyo, RIKEN, Nanjing University...",
            "text": "This research introduces a new approach to reinforcement learning (RL) that goes beyond the traditional assumption that rewards are simply the sum of individual steps. It proposes a model that can handle more complex, non-Markovian reward structures, where the reward for a sequence of actions depends on the entire sequence, not just the current state and action.",
            "paper-title": "Beyond Simple Sum of Delayed Rewards: Non-Markovian Reward Modeling for Reinforcement Learning",
            "image-path": "flux_paper_image/2410.20176_1730240329.png"
        },

        {
            "startTime": "14:58",
            "arxivId": "2410.19986",
            "arxivLink": "https://arxiv.org/abs/2410.19986",
            "title": "Brainwaves to Speech: A New Trick to Decode Your Thoughts!",
            "institute": "University of Oxford",
            "text": "This research applies adversarial domain adaptation, a technique commonly used in computer vision, to MEG neuroimaging data for the first time. This approach aims to improve the ability of speech decoding models to generalize across different datasets.",
            "paper-title": "Resolving Domain Shift For Representations Of Speech In Non-Invasive Brain Recordings",
            "image-path": "flux_paper_image/2410.19986_1730239795.png"
        },

        {
            "startTime": "15:16",
            "arxivId": "2410.20552",
            "arxivLink": "https://arxiv.org/abs/2410.20552",
            "title": "Stress-Busting Tech: Can Your Webcam See Your Sweat?",
            "institute": "ETH Zurich, University of Washington, Microsoft",
            "text": "This research introduces a new 3D convolutional neural network architecture, SympCam, specifically designed to predict sympathetic arousal from facial videos. Unlike previous work that relied on signal processing techniques, SympCam incorporates a temporal attention module (TAM) to enhance the temporal coherence of the data processing.",
            "paper-title": "SympCam: Remote Optical Measurement of Sympathetic Arousal",
            "image-path": "flux_paper_image/2410.20552_1730239227.png"
        },

        {
            "startTime": "15:37",
            "arxivId": "2410.20749",
            "arxivLink": "https://arxiv.org/abs/2410.20749",
            "title": "LLMs Inside LLMs: A Matryoshka Doll of AI Power!",
            "institute": "Georgia Institute of Technology, Google",
            "text": "This research introduces Matryoshka, a framework that uses a smaller, white-box LLM to guide a larger, black-box LLM. This approach differs from previous methods that rely on in-context learning or adapters, which often struggle with complex tasks.",
            "paper-title": "Matryoshka: Learning to Drive Black-Box LLMs with LLMs",
            "image-path": "flux_paper_image/2410.20749_1730239700.png"
        },

        {
            "startTime": "15:56",
            "arxivId": "2410.19838",
            "arxivLink": "https://arxiv.org/abs/2410.19838",
            "title": "Brainwave Decoding: From Sensor Noise to Brain Space Bliss",
            "institute": "University of Oxford",
            "text": "This research explores decoding brain activity from MEG data by reconstructing the neural activity in brain space, a 3D voxel grid. This approach differs from previous work that typically uses sensor measurements as input, which often leads to challenges in combining datasets and building models with inductive biases.",
            "paper-title": "Non-invasive Neural Decoding in Source Reconstructed Brain Space",
            "image-path": "flux_paper_image/2410.19838_1730241798.png"
        },

        {
            "startTime": "16:21",
            "arxivId": "2410.21269",
            "arxivLink": "https://arxiv.org/abs/2410.21269",
            "title": "Sound Separation Gets a Multi-Modal Makeover: Query-Mixup Makes Audio Extraction a Breeze!",
            "institute": "Zhejiang University, Harvard University",
            "text": "This research introduces OmniSep, a sound separation model that can handle queries from multiple modalities (text, image, and audio) simultaneously. Unlike previous methods that rely on single-modal queries, OmniSep uses a Query-Mixup strategy to blend features from different modalities during training, enabling it to optimize for all modalities concurrently.",
            "paper-title": "OmniSep: Unified Omni-Modality Sound Separation with Query-Mixup",
            "image-path": "flux_paper_image/2410.21269_1730239718.png"
        },

        {
            "startTime": "16:48",
            "arxivId": "2410.21194",
            "arxivLink": "https://arxiv.org/abs/2410.21194",
            "title": "Subgaussian Distributions: Certifiably Not That Bad!",
            "institute": "University of Wisconsin-Madison, MIT, UC Berkeley...",
            "text": "This paper proves that all subgaussian distributions are certifiably subgaussian, meaning that their moment bounds can be proven using a low-degree sum-of-squares (SoS) proof. This is a significant departure from previous work, which only established certifiability for certain structured subgaussian distributions.",
            "paper-title": "SoS Certifiability of Subgaussian Distributions and its Algorithmic Applications",
            "image-path": "flux_paper_image/2410.21194_1730241400.png"
        },

        {
            "startTime": "17:17",
            "arxivId": "2410.20197",
            "arxivLink": "https://arxiv.org/abs/2410.20197",
            "title": "AI's Got a New Weakness: Hacking the Foundation Model",
            "institute": "Nanyang Technological University, Chinese Academy of Sciences, Beihang University...",
            "text": "This research explores the feasibility of attacking downstream models fine-tuned from the Segment Anything Model (SAM) without accessing the downstream task or dataset. It proposes a universal meta-initialization (UMI) algorithm to extract the intrinsic vulnerability of the foundation model and a gradient robust loss to mitigate the deviation in adversarial updates.",
            "paper-title": "Transferable Adversarial Attacks on SAM and Its Downstream Models",
            "image-path": "flux_paper_image/2410.20197_1730240339.png"
        },

        {
            "startTime": "17:52",
            "arxivId": "2410.20727",
            "arxivLink": "https://arxiv.org/abs/2410.20727",
            "title": "Faster WIND: LLM Alignment Gets a Turbo Boost!",
            "institute": "CMU, Google",
            "text": "This paper introduces a novel algorithmic framework called WIND, which accelerates iterative best-of-N distillation for LLM alignment. Unlike previous methods, WIND leverages a game-theoretic connection between iterative BOND and self-play alignment, enabling more efficient training.",
            "paper-title": "Faster WIND: Accelerating Iterative Best-of-$N$ Distillation for LLM Alignment",
            "image-path": "flux_paper_image/2410.20727_1730238922.png"
        },

        {
            "startTime": "18:17",
            "arxivId": "2410.20158",
            "arxivLink": "https://arxiv.org/abs/2410.20158",
            "title": "Fake Videos, Real Images: How Tricking AI Makes It Better at Art",
            "institute": "Imperial College London, University of Cambridge",
            "text": "This paper explores the idea of using \"pseudo-videos\" - sequences of images created by applying data augmentation to a single image - to improve the performance of image generative models. This approach differs from traditional methods that rely solely on training on real images.",
            "paper-title": "Your Image is Secretly the Last Frame of a Pseudo Video",
            "image-path": "flux_paper_image/2410.20158_1730240181.png"
        },

        {
            "startTime": "18:42",
            "arxivId": "2410.21228",
            "arxivLink": "https://arxiv.org/abs/2410.21228",
            "title": "LoRA: A Fine-Tuned Illusion?",
            "institute": "MIT",
            "text": "This research investigates the structural differences between full fine-tuning and LoRA, a parameter-efficient fine-tuning method, by analyzing the singular value decomposition of weight matrices. It finds that LoRA introduces \"intruder dimensions\" that are not present in fully fine-tuned models, leading to distinct generalization behaviors.",
            "paper-title": "LoRA vs Full Fine-tuning: An Illusion of Equivalence",
            "image-path": "flux_paper_image/2410.21228_1730241209.png"
        },

        {
            "startTime": "19:02",
            "arxivId": "2410.20008",
            "arxivLink": "https://arxiv.org/abs/2410.20008",
            "title": "LLMs: Not Just a One-Trick Pony, They've Got Layers!",
            "institute": "University of Edinburgh, Nvidia",
            "text": "This research goes beyond just looking at how well instruction-tuned LLMs perform on tasks. It dives deep into the models' internal workings, analyzing how different layers of the model encode task-specific information.",
            "paper-title": "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models",
            "image-path": "flux_paper_image/2410.20008_1730240065.png"
        },

        {
            "startTime": "19:26",
            "arxivId": "2410.20957",
            "arxivLink": "https://arxiv.org/abs/2410.20957",
            "title": "Neuro-Symbolic Learning: A Game of Logic and Perception!",
            "institute": "Nanjing University, University of Toronto, Birkbeck University of London",
            "text": "This research proposes a novel neuro-symbolic learning framework that combines neural network training and logical constraint synthesis simultaneously. Unlike previous work, this approach directly learns explicit logical constraints, enabling exact reasoning with off-the-shelf constraint solvers during inference.",
            "paper-title": "Neuro-symbolic Learning Yielding Logical Constraints",
            "image-path": "flux_paper_image/2410.20957_1730242098.png"
        },

        {
            "startTime": "19:49",
            "arxivId": "2410.20533",
            "arxivLink": "https://arxiv.org/abs/2410.20533",
            "title": "AI Teachers Get a Grade: How Much Does a Wrong Answer Really Matter?",
            "institute": "Tsinghua University, UC Los Angeles",
            "text": "This research focuses on how the quality of supervision from \"weak\" AI teachers impacts the performance of large language models (LLMs) on complex reasoning tasks. Unlike previous work that assumes strong teacher models, this study explores the impact of varying levels of error in the supervision provided by less capable models.",
            "paper-title": "Guiding Through Complexity: What Makes Good Supervision for Hard Reasoning Tasks?",
            "image-path": "flux_paper_image/2410.20533_1730242074.png"
        },

        {
            "startTime": "20:14",
            "arxivId": "2410.20203",
            "arxivLink": "https://arxiv.org/abs/2410.20203",
            "title": "Shadowgraph Gets Smart: AI Reconstructs Density Fields",
            "institute": "Tsinghua University",
            "text": "This research integrates physics-informed neural networks (PINNs) with the traditional shadowgraph method to reconstruct density fields, addressing limitations of previous approaches like noise and limited spatial resolution.",
            "paper-title": "Physics informed Shadowgraph Density Field Reconstruction",
            "image-path": "flux_paper_image/2410.20203_1730241710.png"
        },

        {
            "startTime": "20:40",
            "arxivId": "2410.20470",
            "arxivLink": "https://arxiv.org/abs/2410.20470",
            "title": "Hamiltonian Flows: Turning Physics into Pixel-Perfect Images!",
            "institute": "MIT, Nvidia",
            "text": "This research introduces Hamiltonian Velocity Predictors (HVPs) as a tool for score matching and generative modeling. Unlike previous work that focused on specific force fields, this paper explores the potential of designing force fields for Hamiltonian ODEs.",
            "paper-title": "Hamiltonian Score Matching and Generative Flows",
            "image-path": "flux_paper_image/2410.20470_1730241632.png"
        },

        {
            "startTime": "21:06",
            "arxivId": "2410.20092",
            "arxivLink": "https://arxiv.org/abs/2410.20092",
            "title": "OGBench: Reinforcement Learning's New Playground for Goal-Driven Bots!",
            "institute": "UC Berkeley",
            "text": "This research introduces OGBench, a new benchmark specifically designed for offline goal-conditioned reinforcement learning (GCRL) algorithms. Unlike previous benchmarks, OGBench focuses on evaluating GCRL algorithms across a wider range of challenges, including goal stitching, long-horizon reasoning, and handling stochasticity.",
            "paper-title": "OGBench: Benchmarking Offline Goal-Conditioned RL",
            "image-path": "flux_paper_image/2410.20092_1730241536.png"
        },

        {
            "startTime": "21:32",
            "arxivId": "2410.20898",
            "arxivLink": "https://arxiv.org/abs/2410.20898",
            "title": "One-Step Text-to-Image: A Diffusion Model's Dream Come True!",
            "institute": "Peking University, Xiaohongshu, CMU",
            "text": "This paper introduces Diff-Instruct*, a novel approach for training one-step text-to-image generators that aligns with human preferences. Unlike previous methods that rely on KL divergence for regularization, Diff-Instruct* utilizes score-based divergences, leading to more stable training dynamics and improved performance.",
            "paper-title": "Diff-Instruct*: Towards Human-Preferred One-step Text-to-image Generative Models",
            "image-path": "flux_paper_image/2410.20898_1730240701.png"
        },

        {
            "startTime": "22:05",
            "arxivId": "2410.20649",
            "arxivLink": "https://arxiv.org/abs/2410.20649",
            "title": "Fast Learning for Complex Problems: Variational Inequalities Get a Speed Boost!",
            "institute": "UC Berkeley",
            "text": "This research extends stability-based generalization arguments from convex optimization to variational inequalities, demonstrating how to achieve fast learning rates in strongly monotone settings.",
            "paper-title": "Learning Variational Inequalities from Data: Fast Generalization Rates under Strong Monotonicity",
            "image-path": "flux_paper_image/2410.20649_1730241934.png"
        },

        {
            "startTime": "22:22",
            "arxivId": "2410.19917",
            "arxivLink": "https://arxiv.org/abs/2410.19917",
            "title": "Privacy-Preserving AI: Whispering Secrets Over Wireless Waves",
            "institute": "Princeton University",
            "text": "This research introduces a new privacy framework called \"feature differential privacy\" for collaborative inference over wireless channels. It differs from previous work by focusing on protecting extracted features during transmission, rather than just data during model training.",
            "paper-title": "Collaborative Inference over Wireless Channels with Feature Differential Privacy",
            "image-path": "flux_paper_image/2410.19917_1730239811.png"
        },

        {
            "startTime": "22:46",
            "arxivId": "2410.20254",
            "arxivLink": "https://arxiv.org/abs/2410.20254",
            "title": "Simulators: Not Just for Solving Problems, But for Exploring Them Too!",
            "institute": "UC Berkeley",
            "text": "This research explores a new approach to simulation-to-real transfer in reinforcement learning. Instead of directly transferring a policy that solves the task in the simulator, it proposes transferring a set of exploratory policies that provide rich data coverage in the simulator. This data is then used to learn a near-optimal policy in the real world.",
            "paper-title": "Overcoming the Sim-to-Real Gap: Leveraging Simulation to Learn to Explore for Real-World RL",
            "image-path": "flux_paper_image/2410.20254_1730239187.png"
        },

        {
            "startTime": "23:09",
            "arxivId": "2410.21067",
            "arxivLink": "https://arxiv.org/abs/2410.21067",
            "title": "LLMs Get a Brain: New Framework Makes Translations Smarter",
            "institute": "Peking University, Tencent",
            "text": "This research introduces CRAT, a multi-agent framework for machine translation that uses retrieval-augmented generation (RAG) and causality-enhanced self-reflection to improve translation accuracy, especially for context-dependent terms. Unlike previous methods that rely on manual identification of such terms, CRAT automates this process, making it more practical for real-world applications.",
            "paper-title": "CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models",
            "image-path": "flux_paper_image/2410.21067_1730240532.png"
        },

        {
            "startTime": "23:31",
            "arxivId": "2410.19931",
            "arxivLink": "https://arxiv.org/abs/2410.19931",
            "title": "Transformers Can Sort Your Laundry (and Prove It!)",
            "institute": "MIT, Boston University",
            "text": "This paper demonstrates that transformers can solve the optimal transport problem with provable guarantees, unlike previous work that focused on empirical observations. The authors achieve this by showing that transformers can implement gradient descent on the dual optimal transport problem with a specific prompt engineering technique.",
            "paper-title": "Provable optimal transport with transformers: The essence of depth and prompt engineering",
            "image-path": "flux_paper_image/2410.19931_1730239973.png"
        },

        {
            "startTime": "23:56",
            "arxivId": "2410.20986",
            "arxivLink": "https://arxiv.org/abs/2410.20986",
            "title": "Skinned Motion Retargeting: Say Goodbye to Jittery, Interpenetrating Characters!",
            "institute": "Tsinghua University, National University of Singapore",
            "text": "This research introduces a new framework called MeshRet that directly models dense geometric interactions in motion retargeting, unlike previous methods that either ignored geometry or added a correction stage after skeletal motion retargeting.",
            "paper-title": "Skinned Motion Retargeting with Dense Geometric Interaction Perception",
            "image-path": "flux_paper_image/2410.20986_1730239997.png"
        },

        {
            "startTime": "24:19",
            "arxivId": "2410.20035",
            "arxivLink": "https://arxiv.org/abs/2410.20035",
            "title": "Untrainable Networks? Not Anymore! Guidance Makes Them Learn!",
            "institute": "MIT, Johns Hopkins University",
            "text": "This paper introduces a new technique called \"guidance\" that uses representational alignment to transfer inductive biases from one neural network to another. This differs from previous work like model distillation, which focuses on transferring knowledge from a teacher model to a student model through output predictions.",
            "paper-title": "Training the Untrainable: Introducing Inductive Bias via Representational Alignment",
            "image-path": "flux_paper_image/2410.20035_1730239425.png"
        },

        {
            "startTime": "24:42",
            "arxivId": "2410.19889",
            "arxivLink": "https://arxiv.org/abs/2410.19889",
            "title": "Ensemble Up! Boosting Text Classification with Finetuned Models",
            "institute": "University of Freiburg",
            "text": "This research explores the potential of ensembling fine-tuned language models for text classification, a technique that has been largely overlooked in previous studies.",
            "paper-title": "Ensembling Finetuned Language Models for Text Classification",
            "image-path": "flux_paper_image/2410.19889_1730240997.png"
        },

        {
            "startTime": "25:06",
            "arxivId": "2410.20916",
            "arxivLink": "https://arxiv.org/abs/2410.20916",
            "title": "NeuGPT: Brainwaves to Text, It's Like Mind Reading But With AI!",
            "institute": "Hong Kong University of Science and Technology, Kyung Hee University, University of Oxford...",
            "text": "This research introduces NeuGPT, a multi-modal language generation model that can process and generate neural signals, unlike previous models that could only process them as input.",
            "paper-title": "NeuGPT: Unified multi-modal Neural GPT",
            "image-path": "flux_paper_image/2410.20916_1730239513.png"
        },

        {
            "startTime": "25:35",
            "arxivId": "2410.19744",
            "arxivLink": "https://arxiv.org/abs/2410.19744",
            "title": "LLMs for Recommending: From Ratings to Reasoning!",
            "institute": "Jilin University, Meta, University of Technology Sydney...",
            "text": "This research goes beyond simply classifying LLM-based recommender systems. It examines how LLMs can enhance recommendation tasks from the perspective of the recommender system community, focusing on the integration of LLMs into research and practical applications.",
            "paper-title": "Towards Next-Generation LLM-based Recommender Systems: A Survey and Beyond",
            "image-path": "flux_paper_image/2410.19744_1730241488.png"
        },

        {
            "startTime": "26:04",
            "arxivId": "2410.20488",
            "arxivLink": "https://arxiv.org/abs/2410.20488",
            "title": "LLMs Get a Speed Boost: Predicting the Future, One Token at a Time!",
            "institute": "Peking University, Meituan, Meta",
            "text": "This research proposes a novel method called FIRP, which predicts the hidden states of future tokens in a large language model (LLM) during inference. Unlike previous methods that rely on separate models or specific layers, FIRP uses simple linear projections in intermediate layers to generate draft tokens, leading to faster inference without sacrificing accuracy.",
            "paper-title": "FIRP: Faster LLM inference via future intermediate representation prediction",
            "image-path": "flux_paper_image/2410.20488_1730239902.png"
        },

        {
            "startTime": "26:28",
            "arxivId": "2410.19766",
            "arxivLink": "https://arxiv.org/abs/2410.19766",
            "title": "Radio-Frequency Recognition: Teaching Robots to See with Sound!",
            "institute": "Southern University of Science and Technology, Sichuan University, Nanyang Technological University",
            "text": "This research introduces a novel cross-modal knowledge distillation framework, FM-Fi, which leverages the knowledge of vision-based foundation models to enhance RF-based human activity recognition systems. This approach addresses the scarcity of labeled RF data by transferring knowledge from visual models to RF models, enabling zero-shot and few-shot learning capabilities.",
            "paper-title": "Large Model for Small Data: Foundation Model for Cross-Modal RF Human Activity Recognition",
            "image-path": "flux_paper_image/2410.19766_1730241751.png"
        },

        {
            "startTime": "26:53",
            "arxivId": "2410.19941",
            "arxivLink": "https://arxiv.org/abs/2410.19941",
            "title": "Privacy-Preserving Generative Models: Slicing Through the Noise!",
            "institute": "IBM, University of Illinois",
            "text": "This research introduces a new privacy mechanism called \"slicing privacy\" that injects noise into random low-dimensional projections of the private data, rather than directly into gradient updates. This approach offers several advantages, including easier hyperparameter tuning and stable convergence.",
            "paper-title": "Privacy without Noisy Gradients: Slicing Mechanism for Generative Model Training",
            "image-path": "flux_paper_image/2410.19941_1730240054.png"
        },

        {
            "startTime": "27:18",
            "arxivId": "2410.19811",
            "arxivLink": "https://arxiv.org/abs/2410.19811",
            "title": "AI Engineers: Get Ready to Chill, ControlAgent's Got This!",
            "institute": "University of Illinois at Urbana\u2013Champaign, University of California San Diego, University of Michigan",
            "text": "This research introduces ControlAgent, a framework that automates control system design by integrating LLMs with domain expertise and tool utilization. Unlike previous work that focused on LLMs answering textbook-level questions, ControlAgent mimics the iterative design processes used by human engineers, addressing the performance/robustness trade-offs inherent in control design.",
            "paper-title": "ControlAgent: Automating Control System Design via Novel Integration of LLM Agents and Domain Expertise",
            "image-path": "flux_paper_image/2410.19811_1730239956.png"
        },

        {
            "startTime": "27:40",
            "arxivId": "2410.20132",
            "arxivLink": "https://arxiv.org/abs/2410.20132",
            "title": "Infrared Sniffing: A New Way to Detect COVID-19 in 10 Minutes!",
            "institute": "Nanyang Technological University, Beihang University",
            "text": "This research combines infrared spectroscopy with a channel-wise attention-based PLS-1D-CNN model for rapid and accurate COVID-19 screening, differentiating itself from previous methods by utilizing a unique biomolecular importance (BMI) evaluation method to assess signal quality.",
            "paper-title": "On-Site Precise Screening of SARS-CoV-2 Systems Using a Channel-Wise Attention-Based PLS-1D-CNN Model with Limited Infrared Signatures",
            "image-path": "flux_paper_image/2410.20132_1730242079.png"
        },

        {
            "startTime": "28:04",
            "arxivId": "2410.20445",
            "arxivLink": "https://arxiv.org/abs/2410.20445",
            "title": "TrajAgent: The AI That Makes Trajectory Modeling a Breeze!",
            "institute": "Tsinghua University",
            "text": "This research proposes TrajAgent, an LLM-based agent framework for unified trajectory modeling. Unlike previous methods designed for specific tasks, TrajAgent can handle diverse trajectory data and tasks automatically.",
            "paper-title": "TrajAgent: An Agent Framework for Unified Trajectory Modelling",
            "image-path": "flux_paper_image/2410.20445_1730240789.png"
        },

        {
            "startTime": "28:25",
            "arxivId": "2410.20691",
            "arxivLink": "https://arxiv.org/abs/2410.20691",
            "title": "Window Shopping: How AI is Designing Buildings for Better Wi-Fi and Sunlight",
            "institute": "University of Sheffield, University of Cambridge, Zhejiang Gongshang University...",
            "text": "This research proposes a novel large language model (LLM)-based optimization framework for simultaneously optimizing indoor wireless and daylight performance by adjusting window positions and beam directions of window-deployed reconfigurable intelligent surfaces (RISs) in RIS-aided outdoor-to-indoor (O2I) networks. This approach differs from previous work by focusing on the role of windows in enhancing the overall quality of experience (QoE) of indoor users through a joint optimization problem formulation.",
            "paper-title": "Wireless-Friendly Window Position Optimization for RIS-Aided Outdoor-to-Indoor Networks based on Multi-Modal Large Language Model",
            "image-path": "flux_paper_image/2410.20691_1730239346.png"
        },

        {
            "startTime": "28:47",
            "arxivId": "2410.20046",
            "arxivLink": "https://arxiv.org/abs/2410.20046",
            "title": "Tiny Brains, Big Recommendations: How Quantization Makes AI Models More Efficient",
            "institute": "CMU, University of Texas at Austin",
            "text": "This research focuses on applying quantization-aware training (QAT) to Deep Learning Recommendation Models (DLRMs), a type of AI model commonly used for personalized recommendations. Unlike previous work that primarily focused on post-training quantization (PTQ), this study explores the benefits of QAT for DLRMs, demonstrating its ability to overcome overfitting issues and achieve higher accuracy with significantly smaller model sizes.",
            "paper-title": "DQRM: Deep Quantized Recommendation Models",
            "image-path": "flux_paper_image/2410.20046_1730239170.png"
        },

        {
            "startTime": "29:08",
            "arxivId": "2410.19849",
            "arxivLink": "https://arxiv.org/abs/2410.19849",
            "title": "Python for Math Nerds: Deep Learning Gets a Dose of Algebra",
            "institute": "Rutgers University, Purdue University, University of Wisconsin-Madison...",
            "text": "This research focuses on the application of fundamental mathematical concepts, particularly linear algebra and matrix operations, within the context of deep learning programming. It delves into the importance of these concepts for understanding and optimizing deep learning models, going beyond the typical focus on neural network architectures and algorithms.",
            "paper-title": "Deep Learning and Machine Learning -- Python Data Structures and Mathematics Fundamental: From Theory to Practice",
            "image-path": "flux_paper_image/2410.19849_1730241661.png"
        },

        {
            "startTime": "29:27",
            "arxivId": "2410.20354",
            "arxivLink": "https://arxiv.org/abs/2410.20354",
            "title": "Protein Watermarks: A New Way to Protect Your AI-Designed Drugs",
            "institute": "Princeton University",
            "text": "This research proposes FoldMark, a method for embedding watermarks into protein generative models and their outputs. Unlike previous watermarking methods for text and images, FoldMark is specifically designed to handle the unique challenges of protein structure data, such as sensitivity to minute changes and complex geometrical symmetries.",
            "paper-title": "FoldMark: Protecting Protein Generative Models with Watermarking",
            "image-path": "flux_paper_image/2410.20354_1730238964.png"
        },

        {
            "startTime": "29:53",
            "arxivId": "2410.20624",
            "arxivLink": "https://arxiv.org/abs/2410.20624",
            "title": "Robot Butler: AI-Powered Speech Interface for Feeding the Elderly",
            "institute": "CMU",
            "text": "This research focuses on using a Large Language Model (LLM) to create a speech interface for a commercially available feeding robot. Unlike previous work that focused on prompt engineering and code generation, this study involves human subjects interacting directly with an LLM-integrated robot for physically assistive tasks.",
            "paper-title": "Towards an LLM-Based Speech Interface for Robot-Assisted Feeding",
            "image-path": "flux_paper_image/2410.20624_1730240324.png"
        },

        {
            "startTime": "30:14",
            "arxivId": "2410.20855",
            "arxivLink": "https://arxiv.org/abs/2410.20855",
            "title": "Seeing Bytes: A Visual Approach to Multimedia File Classification",
            "institute": "Nanyang Technological University, Huazhong University of Science and Technology, Hong Kong Polytechnic University",
            "text": "This research introduces a novel visual representation model, Byte2Image, which converts 1D byte sequences into 2D grayscale images, incorporating previously overlooked intrabyte information. This approach differs from existing methods that primarily focus on interbyte relationships.",
            "paper-title": "ByteNet: Rethinking Multimedia File Fragment Classification through Visual Perspectives",
            "image-path": "flux_paper_image/2410.20855_1730240190.png"
        },

        {
            "startTime": "30:39",
            "arxivId": "2410.20362",
            "arxivLink": "https://arxiv.org/abs/2410.20362",
            "title": "Data Synthesis: No Masking, More Magic!",
            "institute": "University of Washington",
            "text": "This research explores a new approach to training models for data synthesis, focusing on the impact of prompt masking and the optimal size of training data. Unlike previous work that primarily relies on prompt engineering with standard instruction-fine-tuned models, this paper proposes a paradigm shift by investigating how to specifically train models for data generation.",
            "paper-title": "Rethinking Data Synthesis: A Teacher Model Training Recipe with Interpretation",
            "image-path": "flux_paper_image/2410.20362_1730239538.png"
        },

        {
            "startTime": "31:00",
            "arxivId": "2410.19896",
            "arxivLink": "https://arxiv.org/abs/2410.19896",
            "title": "Tobacco Talk: AI Learns to Spot Smoking in Videos, But Can It Handle the Smoke?",
            "institute": "University of Arkansas, CMU",
            "text": "This research introduces a new multi-modal deep learning framework called FLAASH, which uses a hierarchical fusion mechanism inspired by flow network theory to analyze tobacco-related video content. This approach differs from previous methods by incorporating a flow-attention mechanism, adaptive weighting, and a gating mechanism to capture nuanced interactions between visual and textual modalities.",
            "paper-title": "FLAASH: Flow-Attention Adaptive Semantic Hierarchical Fusion for Multi-Modal Tobacco Content Analysis",
            "image-path": "flux_paper_image/2410.19896_1730241420.png"
        },

        {
            "startTime": "31:24",
            "arxivId": "2410.20305",
            "arxivLink": "https://arxiv.org/abs/2410.20305",
            "title": "Stop Repeating Yourself! LLMs Learn Faster with Prefix Sharing",
            "institute": "MIT",
            "text": "This paper introduces a new technique called \"prefix sharing\" for training large language models (LLMs) on preference data. Unlike traditional methods that process the same prompt twice for each response, prefix sharing combines the responses into a single sequence, reducing redundant computations.",
            "paper-title": "Accelerating Direct Preference Optimization with Prefix Sharing",
            "image-path": "flux_paper_image/2410.20305_1730239984.png"
        },

        {
            "startTime": "31:47",
            "arxivId": "2410.21060",
            "arxivLink": "https://arxiv.org/abs/2410.21060",
            "title": "Cybersecurity Knowledge Graphs: LLMs Learn from a Few Examples!",
            "institute": "Virginia Tech, UC Berkeley",
            "text": "This research proposes CTINEXUS, a framework that leverages optimized in-context learning (ICL) of large language models (LLMs) for data-efficient cybersecurity knowledge graph (CSKG) construction. Unlike existing methods, CTINEXUS requires neither extensive data nor parameter tuning and can adapt to various ontologies with minimal annotated examples.",
            "paper-title": "CTINEXUS: Leveraging Optimized LLM In-Context Learning for Constructing Cybersecurity Knowledge Graphs Under Data Scarcity",
            "image-path": "flux_paper_image/2410.21060_1730239365.png"
        },

        {
            "startTime": "32:11",
            "arxivId": "2410.20742",
            "arxivLink": "https://arxiv.org/abs/2410.20742",
            "title": "Say What?! New Tech Makes Your Voice Unlearnable for Deepfakes!",
            "institute": "Beijing University of Posts and Telecommunications, Commonwealth Scientific and Industrial Research Organisation, National University of Singapore...",
            "text": "This research proposes a new data protection method called Pivotal Objective Perturbation (POP) that embeds imperceptible noise into speech samples, making them unusable for training text-to-speech (TTS) models. Unlike previous methods that focused on spoofing speaker verification systems, POP aims to prevent the generation of high-quality deepfake speech altogether.",
            "paper-title": "Mitigating Unauthorized Speech Synthesis for Voice Protection",
            "image-path": "flux_paper_image/2410.20742_1730240555.png"
        },

        {
            "startTime": "32:51",
            "arxivId": "2410.21257",
            "arxivLink": "https://arxiv.org/abs/2410.21257",
            "title": "Diffusion Models Get a Speed Boost: One-Step Policy for Speedy Robots!",
            "institute": "NVIDIA, The University of Texas at Austin",
            "text": "This paper introduces a new method called One-Step Diffusion Policy (OneDP) that distills knowledge from pre-trained diffusion policies into a single-step action generator, significantly accelerating inference speed for robotic control tasks. This differs from previous work that relied on multiple iterative steps for action generation.",
            "paper-title": "One-Step Diffusion Policy: Fast Visuomotor Policies via Diffusion Distillation",
            "image-path": "flux_paper_image/2410.21257_1730241727.png"
        },

        {
            "startTime": "33:13",
            "arxivId": "2410.20532",
            "arxivLink": "https://arxiv.org/abs/2410.20532",
            "title": "Fetal Brain Extraction: A Sliding Window Search for Tiny Brains!",
            "institute": "Harvard Medical School, Massachusetts General Hospital",
            "text": "This research proposes a test-time strategy that uses a breadth-fine search (BFS) followed by a deep-focused sliding window (DFS) to improve fetal brain extraction accuracy in deep learning models trained on sparse, synthetic data. This approach differs from previous work by combining predictions from multiple models trained at different window sizes, progressively refining the regions of interest and minimizing false positives.",
            "paper-title": "Search Wide, Focus Deep: Automated Fetal Brain Extraction with Sparse Training Data",
            "image-path": "flux_paper_image/2410.20532_1730240107.png"
        },

        {
            "startTime": "33:39",
            "arxivId": "2410.19831",
            "arxivLink": "https://arxiv.org/abs/2410.19831",
            "title": "NeRF Gets a Speed Boost: Gauss-Laguerre Quadrature Makes Rendering a Breeze!",
            "institute": "CMU",
            "text": "This paper proposes GL-NeRF, a method that uses Gauss-Laguerre quadrature to accelerate volume rendering in neural radiance fields (NeRFs). Unlike previous approaches that introduce new networks or data structures, GL-NeRF is training-free and can be directly incorporated into existing NeRF models.",
            "paper-title": "GL-NeRF: Gauss-Laguerre Quadrature Enables Training-Free NeRF Acceleration",
            "image-path": "flux_paper_image/2410.19831_1730239894.png"
        },

        {
            "startTime": "33:56",
            "arxivId": "2410.20357",
            "arxivLink": "https://arxiv.org/abs/2410.20357",
            "title": "Robots Learn to Play by the Rules: In-Context Learning for Sim-to-Real Dynamics",
            "institute": "CMU",
            "text": "This research proposes a novel approach to sim-to-real transfer by dynamically adjusting simulation environment parameters online using in-context learning. Unlike traditional methods that rely on gradient updates, this method leverages past interaction histories as context to adapt the simulation environment dynamics to real-world dynamics without requiring any network parameter updates.",
            "paper-title": "Dynamics as Prompts: In-Context Learning for Sim-to-Real System Identifications",
            "image-path": "flux_paper_image/2410.20357_1730241264.png"
        },

        {
            "startTime": "34:19",
            "arxivId": "2410.20976",
            "arxivLink": "https://arxiv.org/abs/2410.20976",
            "title": "AI Chemists: Large Language Models Predict Quantum Material Synthesis",
            "institute": "MIT",
            "text": "This research uses large language models (LLMs) to predict synthesis pathways for inorganic materials, including quantum materials. Unlike previous work that focused on generating stable structures or predicting material properties, this study addresses the entire experimental workflow, particularly the sequence of chemical reactions required to synthesize a given material.",
            "paper-title": "Large Language Model-Guided Prediction Toward Quantum Materials Synthesis",
            "image-path": "flux_paper_image/2410.20976_1730239462.png"
        },

        {
            "startTime": "34:41",
            "arxivId": "2410.20753",
            "arxivLink": "https://arxiv.org/abs/2410.20753",
            "title": "RAG Gets a Plan: Reasoning DAGs for Smarter AI",
            "institute": "Aalto University, Microsoft",
            "text": "This paper introduces Plan\u00d7 RAG, a framework that augments the retrieve-then-reason paradigm of existing RAG frameworks to plan-then-retrieve. Plan\u00d7 RAG formulates a reasoning plan as a directed acyclic graph (DAG), decomposing queries into interrelated atomic sub-queries.",
            "paper-title": "Plan$\\times$RAG: Planning-guided Retrieval Augmented Generation",
            "image-path": "flux_paper_image/2410.20753_1730241893.png"
        },

        {
            "startTime": "35:05",
            "arxivId": "2410.19762",
            "arxivLink": "https://arxiv.org/abs/2410.19762",
            "title": "Sidewalk Sleuths: AI Maps Pedestrian Paths for a More Accessible World",
            "institute": "University of Washington",
            "text": "This research presents a novel sociotechnical approach for collecting, managing, and maintaining pedestrian path data at a statewide scale. Unlike previous methods that rely solely on aerial imagery or crowdsourcing, this study combines automated predictions from aerial imagery with interactive expert vetting and community engagement.",
            "paper-title": "Reliable, Routable, and Reproducible: Collection of Pedestrian Pathways at Statewide Scale",
            "image-path": "flux_paper_image/2410.19762_1730240549.png"
        },

        {
            "startTime": "35:25",
            "arxivId": "2410.20716",
            "arxivLink": "https://arxiv.org/abs/2410.20716",
            "title": "Photometric Stereo Goes Spectral: Uncalibrated, Physics-Free, and Ready for Action!",
            "institute": "National Institute of Informatics, Tokyo Institute of Technology",
            "text": "This research introduces a new method for spectrally multiplexed photometric stereo that doesn't require calibrated lighting or sensors. It leverages spectral ambiguity as an advantage, allowing for training data generation without specialized multispectral rendering frameworks.",
            "paper-title": "Physics-Free Spectrally Multiplexed Photometric Stereo under Unknown Spectral Composition",
            "image-path": "flux_paper_image/2410.20716_1730240927.png"
        },

        {
            "startTime": "35:46",
            "arxivId": "2410.19802",
            "arxivLink": "https://arxiv.org/abs/2410.19802",
            "title": "Head Motion: The Secret Weapon for Better Breath Tracking in fMRI",
            "institute": "University of Calgary",
            "text": "This study explores using head motion parameters, both raw and filtered, to improve the accuracy of respiratory variation (RV) estimation from fMRI data. This approach differs from previous methods that primarily relied on filtering techniques to isolate respiratory-induced motion artifacts.",
            "paper-title": "The Useful Side of Motion: Using Head Motion Parameters to Correct for Respiratory Confounds in BOLD fMRI",
            "image-path": "flux_paper_image/2410.19802_1730240421.png"
        },

        {
            "startTime": "36:06",
            "arxivId": "2410.20913",
            "arxivLink": "https://arxiv.org/abs/2410.20913",
            "title": "Fuel-Efficient Cars: When Reality Bites (and Your Battery Dies)",
            "institute": "University of Hong Kong, Tsinghua University",
            "text": "This research explores the impact of inaccurate observations, like battery state of charge (SOC) and speed, on the optimal fuel consumption (COFC) problem for hybrid electric vehicles (HEVs). Unlike previous work, it focuses on solving the COFC problem under these observational perturbations, using robust constrained reinforcement learning (CRL) approaches.",
            "paper-title": "Constrained Optimal Fuel Consumption of HEV:Considering the Observational Perturbation",
            "image-path": "flux_paper_image/2410.20913_1730240137.png"
        },

        {
            "startTime": "36:31",
            "arxivId": "2410.19826",
            "arxivLink": "https://arxiv.org/abs/2410.19826",
            "title": "LLMs: The New Oncolog y Matchmakers?",
            "institute": "McNeil High School, University of Chicago",
            "text": "This research proposes a novel framework for standardizing and interoperating oncology data using LLMs to translate unstructured data into FHIR and mCODE profiles. This approach differs from previous work by leveraging LLMs for data transformation, which is shown to improve accuracy rates compared to existing methods.",
            "paper-title": "Novel Development of LLM Driven mCODE Data Model for Improved Clinical Trial Matching to Enable Standardization and Interoperability in Oncology Research",
            "image-path": "flux_paper_image/2410.19826_1730238911.png"
        },

        {
            "startTime": "36:56",
            "arxivId": "2410.19738",
            "arxivLink": "https://arxiv.org/abs/2410.19738",
            "title": "AI's Trusty Sidekick: Logic Steps Up to the Plate!",
            "institute": "University of Texas at Austin, University of Wisconsin-Madison, University of Chicago...",
            "text": "This research focuses on integrating diverse reasoning systems, including logic programming, constraint programming, and mathematical programming, to enhance the trustworthiness of AI systems. This approach differs from previous work by emphasizing the use of rules and constraints to express and solve knowledge-intensive problems, aiming to address the limitations of AI systems trained solely on massive datasets.",
            "paper-title": "Integrating Reasoning Systems for Trustworthy AI, Proceedings of the 4th Workshop on Logic and Practice of Programming (LPOP)",
            "image-path": "flux_paper_image/2410.19738_1730239784.png"
        },

        {
            "startTime": "37:20",
            "arxivId": "2410.21252",
            "arxivLink": "https://arxiv.org/abs/2410.21252",
            "title": "AI Judges Long-Winded LLMs: A New Reward System for Chatbots",
            "institute": "Tsinghua University, University of Chinese Academy of Sciences, Zhipu AI",
            "text": "This research proposes a novel method called LongReward, which uses an off-the-shelf LLM to evaluate long-context responses from four human-valued dimensions: helpfulness, logicality, faithfulness, and completeness. This differs from previous work that primarily focused on short-context scenarios or relied on human feedback, which is expensive and time-consuming.",
            "paper-title": "LongReward: Improving Long-context Large Language Models with AI Feedback",
            "image-path": "flux_paper_image/2410.21252_1730240099.png"
        },

        {
            "startTime": "37:49",
            "arxivId": "2410.20007",
            "arxivLink": "https://arxiv.org/abs/2410.20007",
            "title": "LLMs Get a Brain Boost: Teamwork Makes the Dream Work!",
            "institute": "CMU",
            "text": "This research introduces a novel multi-agent framework called CoPlanner, which enhances reasoning capabilities in large language models (LLMs) by separating reasoning steps and assigning distinct duties to different agents. Unlike previous multi-agent frameworks that focus on independent problem-solving and communication after obtaining a solution, CoPlanner leverages the strengths of specialized agents for planning and reasoning, enabling more effective cooperation through structured interactions.",
            "paper-title": "Cooperative Strategic Planning Enhances Reasoning Capabilities in Large Language Models",
            "image-path": "flux_paper_image/2410.20007_1730241312.png"
        },

        {
            "startTime": "38:11",
            "arxivId": "2410.20293",
            "arxivLink": "https://arxiv.org/abs/2410.20293",
            "title": "Fake News Detectives: Unmasking the Bias in Social Media Bots!",
            "institute": "University of Pennsylvania, Northeastern University, New York University...",
            "text": "This research goes beyond just identifying fake news and dives into the biases that can creep into the machine learning models used to detect it. It uses the PROBAST tool to analyze these biases across the entire machine learning lifecycle, from data collection to model evaluation.",
            "paper-title": "A Systematic Review of Machine Learning Approaches for Detecting Deceptive Activities on Social Media: Methods, Challenges, and Biases",
            "image-path": "flux_paper_image/2410.20293_1730241929.png"
        },

        {
            "startTime": "38:35",
            "arxivId": "2410.20030",
            "arxivLink": "https://arxiv.org/abs/2410.20030",
            "title": "SCube: Building 3D Scenes from a Few Snapshots, Faster Than You Can Say \"Voxel!\"",
            "institute": "NVIDIA, University of Toronto, Vector Institute...",
            "text": "This paper introduces SCube, a method that reconstructs large-scale 3D scenes from sparse images using a novel representation called VoxSplats. Unlike previous methods that rely on per-scene optimization or low-resolution models, SCube leverages high-resolution sparse networks and produces sharp outputs from few views.",
            "paper-title": "SCube: Instant Large-Scale Scene Reconstruction using VoxSplats",
            "image-path": "flux_paper_image/2410.20030_1730241167.png"
        },

        {
            "startTime": "38:52",
            "arxivId": "2410.20839",
            "arxivLink": "https://arxiv.org/abs/2410.20839",
            "title": "Asteroid Mining: ESA's Team Takes a \"Beam\" to the Top (But Not Quite)",
            "institute": "European Space Agency",
            "text": "This research introduces novel techniques for approximating low-thrust trajectories in asteroid mining missions, using machine learning and analytical methods to bridge the gap between impulsive and low-thrust solutions. This approach differs from previous work by focusing on self-sufficient ships, which simplifies the search process and allows for efficient parallelization.",
            "paper-title": "Asteroid Mining: ACT&Friends' Results for the GTOC 12 Problem",
            "image-path": "flux_paper_image/2410.20839_1730240538.png"
        },

        {
            "startTime": "39:13",
            "arxivId": "2410.21258",
            "arxivLink": "https://arxiv.org/abs/2410.21258",
            "title": "Quantum Computing: Holes in Data, Speed in Code!",
            "institute": "Pasqal, MIT, Caltech...",
            "text": "This paper explores a new problem in topological data analysis (TDA) called Harmonic Persistence, which focuses on determining whether a hole in a simplicial complex persists in a larger complex. This differs from previous work that focused on estimating normalized or persistent Betti numbers.",
            "paper-title": "Quantum computing and persistence in topological data analysis",
            "image-path": "flux_paper_image/2410.21258_1730239209.png"
        },

        {
            "startTime": "39:36",
            "arxivId": "2410.20142",
            "arxivLink": "https://arxiv.org/abs/2410.20142",
            "title": "Masking Secrets: How to Sniff Out Hidden Data in AI Chatbots",
            "institute": "Nanyang Technological University",
            "text": "This research proposes a new method for membership inference attacks (MIAs) in retrieval-augmented generation (RAG) systems. Unlike previous work that relies on the RAG system's judgment or semantic similarity, this approach uses a mask-based technique to infer membership by analyzing the accuracy of mask prediction.",
            "paper-title": "Mask-based Membership Inference Attacks for Retrieval-Augmented Generation",
            "image-path": "flux_paper_image/2410.20142_1730240654.png"
        },

        {
            "startTime": "39:56",
            "arxivId": "2410.20936",
            "arxivLink": "https://arxiv.org/abs/2410.20936",
            "title": "LLMs Go Formal: A New Framework for Autoformalizing Math Statements",
            "institute": "Nanjing University, Peking University, Microsoft",
            "text": "This research introduces a novel framework for improving the autoformalization performance of LLMs by combining two self-consistency methods: symbolic equivalence and semantic consistency. This approach addresses the limitations of traditional self-consistency methods, which struggle to cope with the variance in LLM outputs.",
            "paper-title": "Autoformalize Mathematical Statements by Symbolic Equivalence and Semantic Consistency",
            "image-path": "flux_paper_image/2410.20936_1730240638.png"
        },

        {
            "startTime": "40:18",
            "arxivId": "2410.20424",
            "arxivLink": "https://arxiv.org/abs/2410.20424",
            "title": "AutoKaggle: Data Science's New AI Assistant, Ready to Compete!",
            "institute": "ByteDance Inc., Beihang University, 2077AI...",
            "text": "This research introduces AutoKaggle, a multi-agent framework that automates data science competitions. Unlike previous work that focuses on single-step tasks, AutoKaggle tackles the entire data science pipeline, from data cleaning to model building.",
            "paper-title": "AutoKaggle: A Multi-Agent Framework for Autonomous Data Science Competitions",
            "image-path": "flux_paper_image/2410.20424_1730239529.png"
        },

        {
            "startTime": "40:38",
            "arxivId": "2410.20771",
            "arxivLink": "https://arxiv.org/abs/2410.20771",
            "title": "Byte-Sized Language Models: MrT5 Eats Tokens, Saves Time!",
            "institute": "Stanford University",
            "text": "This paper introduces MrT5, a new approach to byte-level language modeling that dynamically merges tokens in the encoder to reduce sequence length and improve efficiency. Unlike previous methods that rely on fixed-span downsampling or pooling, MrT5 uses a learned delete gate to selectively remove tokens based on their contextual relevance.",
            "paper-title": "MrT5: Dynamic Token Merging for Efficient Byte-level Language Models",
            "image-path": "flux_paper_image/2410.20771_1730241869.png"
        },

        {
            "startTime": "40:58",
            "arxivId": "2410.20750",
            "arxivLink": "https://arxiv.org/abs/2410.20750",
            "title": "Off-Dynamics RL: A Benchmark for When Robots Go Rogue!",
            "institute": "Tsinghua University",
            "text": "This research introduces ODRL, the first benchmark specifically designed for evaluating off-dynamics reinforcement learning (RL) methods. Unlike previous work, ODRL focuses on the challenge of transferring policies across domains with different dynamics, offering a standardized platform for comparing algorithms.",
            "paper-title": "ODRL: A Benchmark for Off-Dynamics Reinforcement Learning",
            "image-path": "flux_paper_image/2410.20750_1730240508.png"
        },

        {
            "startTime": "41:25",
            "arxivId": "2410.19843",
            "arxivLink": "https://arxiv.org/abs/2410.19843",
            "title": "AI for PDEs: Solving Physics Problems with a Neural Network Twist!",
            "institute": "Tsinghua University, Bauhaus University Weimar",
            "text": "This research explores the use of Physics-Informed Neural Operators (PINO), a new approach that combines operator learning with physical equations to solve partial differential equations (PDEs). This differs from previous work by leveraging existing data to provide an initial solution, which is then fine-tuned using physical laws.",
            "paper-title": "Artificial intelligence for partial differential equations in computational mechanics: A review",
            "image-path": "flux_paper_image/2410.19843_1730239963.png"
        },

        {
            "startTime": "41:46",
            "arxivId": "2410.20659",
            "arxivLink": "https://arxiv.org/abs/2410.20659",
            "title": "Deep Learning's Secret Weapon: Data's Hidden Dimension!",
            "institute": "UC Berkeley",
            "text": "This research explores the generalization error of deep federated learning, focusing on the intrinsic dimension of the data using the entropic dimension, a more efficient measure than the Minkowski dimension. It also incorporates a two-stage Bayesian sampling model to account for the heterogeneity and interdependencies among clients' distributions.",
            "paper-title": "A Statistical Analysis of Deep Federated Learning for Intrinsically Low-dimensional Data",
            "image-path": "flux_paper_image/2410.20659_1730241638.png"
        },

        {
            "startTime": "42:02",
            "arxivId": "2410.20483",
            "arxivLink": "https://arxiv.org/abs/2410.20483",
            "title": "Explaining AI Decisions: It's Not About the Whole Model, It's About You!",
            "institute": "Duke University, Yale University",
            "text": "This research expands on the concept of \"Sparse Explanation Value\" (SEV), a measure of decision sparsity, by introducing new methods to improve the credibility and closeness of explanations. Unlike previous work that focused on global sparsity, SEV focuses on the specific information relevant to an individual's decision.",
            "paper-title": "Improving Decision Sparsity",
            "image-path": "flux_paper_image/2410.20483_1730239340.png"
        },

        {
            "startTime": "42:22",
            "arxivId": "2410.21053",
            "arxivLink": "https://arxiv.org/abs/2410.21053",
            "title": "Deep Learning's New Trick: Taming the Wild Lipschitz Constant!",
            "institute": "French Institute for Research in Computer Science and Automation, Sorbonne University",
            "text": "This research proposes two novel bounds for the Lipschitz constant of deep neural networks, specifically for the l1 and l\u221e norms, and extends these bounds to convolutional neural networks. It also introduces an implicit approach for max-pooling layers, simplifying the implementation and improving computational efficiency.",
            "paper-title": "Computable Lipschitz Bounds for Deep Neural Networks",
            "image-path": "flux_paper_image/2410.21053_1730239038.png"
        },

        {
            "startTime": "42:42",
            "arxivId": "2410.20017",
            "arxivLink": "https://arxiv.org/abs/2410.20017",
            "title": "New Study: Personalized Learning for Everyone, Even the \"Hint-Happy\" Students!",
            "institute": "Stanford University, North Carolina State University",
            "text": "This research focuses on off-policy selection (OPS) for human-centric systems (HCSs), like healthcare and education. Unlike previous OPS methods that treat all participants the same, this study introduces a new approach called First-Glance Off-Policy Selection (FPS) that considers individual characteristics and assigns policies accordingly.",
            "paper-title": "Off-Policy Selection for Initiating Human-Centric Experimental Design",
            "image-path": "flux_paper_image/2410.20017_1730241743.png"
        },

        {
            "startTime": "43:07",
            "arxivId": "2410.20034",
            "arxivLink": "https://arxiv.org/abs/2410.20034",
            "title": "Chatting with Your Sensors: A New Model Turns Wearable Data into Conversations",
            "institute": "Chinese Academy of Sciences, Massachusetts Institute of Technology, Vanderbilt University...",
            "text": "This research introduces Sensor2Text, a model that uses wearable sensor data to engage in conversations about daily activities. Unlike previous work that relies on video, Sensor2Text leverages the information density of multiple sensor modalities to understand and describe activities.",
            "paper-title": "Sensor2Text: Enabling Natural Language Interactions for Daily Activity Tracking Using Wearable Sensors",
            "image-path": "flux_paper_image/2410.20034_1730239692.png"
        },

        {
            "startTime": "43:27",
            "arxivId": "2410.20926",
            "arxivLink": "https://arxiv.org/abs/2410.20926",
            "title": "LLMs Get a Tensor Makeover: Long Sequences, Short Attention Spans!",
            "institute": "Yale University",
            "text": "This paper proposes a novel approach to long-sequence modeling by tensorizing input sequences, transforming them into compact tensor representations. This differs from previous methods that rely on attention approximations like sparse or low-rank attention.",
            "paper-title": "Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning",
            "image-path": "flux_paper_image/2410.20926_1730241135.png"
        },

        {
            "startTime": "43:47",
            "arxivId": "2410.20718",
            "arxivLink": "https://arxiv.org/abs/2410.20718",
            "title": "Algorithms: Not Just for Netflix, They're Shaping Our World!",
            "institute": "Australian National University, Stanford University, Oxford University Press",
            "text": "This paper argues that existing theories of freedom of expression are insufficient to guide the design of online platforms. It proposes a new framework called \"communicative justice\" to address the unique challenges of shaping communication and distributing attention in the digital public sphere.",
            "paper-title": "Lecture II: Communicative Justice and the Distribution of Attention",
            "image-path": "flux_paper_image/2410.20718_1730242203.png"
        },

        {
            "startTime": "44:10",
            "arxivId": "2410.20720",
            "arxivLink": "https://arxiv.org/abs/2410.20720",
            "title": "Algorithms Are the New City Hall: How Code Shapes Our Social Lives",
            "institute": "Australian National University, Stanford University, Oxford University Press",
            "text": "This research introduces the concept of the \"Algorithmic City,\" a network of algorithmically mediated social relations, and explores how this new form of power operates differently from traditional governance. It distinguishes between extrinsic and intermediary power, highlighting how algorithmic intermediaries shape social relations from the inside out.",
            "paper-title": "Lecture I: Governing the Algorithmic City",
            "image-path": "flux_paper_image/2410.20720_1730242220.png"
        },

        {
            "startTime": "44:35",
            "arxivId": "2410.19814",
            "arxivLink": "https://arxiv.org/abs/2410.19814",
            "title": "Weather Forecasting Gets a Flow-tastic Upgrade: New Model Tackles Small-Scale Physics",
            "institute": "Imperial College London, Nvidia",
            "text": "This research introduces a new method called Stochastic Flow Matching (SFM) for super-resolving small-scale details in physical sciences, particularly weather forecasting. Unlike previous methods that rely on residual learning, SFM combines deterministic encoding of large-scale dynamics with stochastic flow matching in latent space, effectively capturing both deterministic and stochastic components of the data.",
            "paper-title": "Stochastic Flow Matching for Resolving Small-Scale Physics",
            "image-path": "flux_paper_image/2410.19814_1730239874.png"
        },

        {
            "startTime": "44:55",
            "arxivId": "2410.19990",
            "arxivLink": "https://arxiv.org/abs/2410.19990",
            "title": "Score-Matching Gets a Dimension Reduction Makeover: Learning Low-Dimensional Structure in Data",
            "institute": "Caltech, MIT",
            "text": "This research extends gradient-based dimension reduction techniques to problems where gradients are unavailable. It does this by introducing a framework based on score-ratio matching, which directly learns the score ratio function needed to compute diagnostic matrices.",
            "paper-title": "Dimension reduction via score ratio matching",
            "image-path": "flux_paper_image/2410.19990_1730241198.png"
        },

        {
            "startTime": "45:17",
            "arxivId": "2410.20516",
            "arxivLink": "https://arxiv.org/abs/2410.20516",
            "title": "Galaxy Clustering: A Cosmic-Scale Benchmark for Symmetry-Preserving Data Processing",
            "institute": "MIT",
            "text": "This research benchmarks the performance of graph neural networks (GNNs) on a large-scale cosmological dataset, focusing on the ability of symmetry-preserving GNNs to capture both local and long-range correlations in galaxy distributions. Previous work has explored GNNs for cosmology, but this study is unique in its focus on symmetry-preserving architectures and its use of a larger dataset with O(10^4) points.",
            "paper-title": "A Cosmic-Scale Benchmark for Symmetry-Preserving Data Processing",
            "image-path": "flux_paper_image/2410.20516_1730239567.png"
        },

        {
            "startTime": "45:50",
            "arxivId": "2410.20490",
            "arxivLink": "https://arxiv.org/abs/2410.20490",
            "title": "LLMs: Hate Speech Detectives or Biased Buddies?",
            "institute": "Northeastern University, Georgia Institute of Technology, CMU",
            "text": "This research investigates the robustness of LLMs in hate speech detection by injecting explicit and implicit markers of speaker ethnicity into the input. Unlike previous work, it focuses on the impact of dialectal features, going beyond just mentioning the speaker's ethnicity.",
            "paper-title": "$\\textit{Who Speaks Matters}$: Analysing the Influence of the Speaker's Ethnicity on Hate Classification",
            "image-path": "flux_paper_image/2410.20490_1730239015.png"
        },

        {
            "startTime": "46:13",
            "arxivId": "2410.20161",
            "arxivLink": "https://arxiv.org/abs/2410.20161",
            "title": "Unraveling the Black Box: Causal Abstraction Makes AI Models Speak Plain English",
            "institute": "Peking University",
            "text": "This research extends the concept of causal abstraction to neural networks, specifically focusing on the interchange intervention method. This method allows researchers to calculate potential causal relationships between internal vectors of the model and its output behavior, providing a means to trace causality from within the model to its observable behavior.",
            "paper-title": "Causal Abstraction in Model Interpretability: A Compact Survey",
            "image-path": "flux_paper_image/2410.20161_1730240488.png"
        },

        {
            "startTime": "46:33",
            "arxivId": "2410.20180",
            "arxivLink": "https://arxiv.org/abs/2410.20180",
            "title": "AI Art's New Deal: Paying Artists for Their Pixels",
            "institute": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, Nanyang Technological University, Sony",
            "text": "This research proposes a novel copyright metric for generative art models, which is grounded in legal precedent and court practices. It also introduces a hierarchical reinforcement learning approach to allocate budget for training these models, taking into account both the contribution of data holders and the copyright loss associated with their data. This differs from previous work that either focused on perturbing models to reduce copyright infringement or solely on economic compensation without considering copyright loss.",
            "paper-title": "Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning",
            "image-path": "flux_paper_image/2410.20180_1730240897.png"
        },

        {
            "startTime": "47:02",
            "arxivId": "2410.20990",
            "arxivLink": "https://arxiv.org/abs/2410.20990",
            "title": "Drifting into the Future: AI Learns to Slide Cars Like a Pro",
            "institute": "Toyota Research Institute, Rensselaer Polytechnic Institute",
            "text": "This research presents a reinforcement learning (RL) approach to autonomous drifting that doesn't rely on pre-defined reference trajectories. Instead, it uses a tire energy-based reward function to encourage the vehicle to maintain high sideslip angles while tracking waypoints. This differs from previous work that often relied on pre-computed trajectories or heuristics for drift control.",
            "paper-title": "Reference-Free Formula Drift with Reinforcement Learning: From Driving Data to Tire Energy-Inspired, Real-World Policies",
            "image-path": "flux_paper_image/2410.20990_1730241541.png"
        },

        {
            "startTime": "47:24",
            "arxivId": "2410.20856",
            "arxivLink": "https://arxiv.org/abs/2410.20856",
            "title": "Traffic Prediction Gets a Language Model Makeover: Strada-LLM Navigates the Roads",
            "institute": "Vrije Universiteit Brussel, IMEC, Macq...",
            "text": "This research introduces Strada-LLM, a graph-aware large language model (LLM) for traffic prediction. Unlike previous LLM-based approaches that rely on text prompts, Strada-LLM directly incorporates the spatial dependencies of the road network into its input tokens.",
            "paper-title": "Strada-LLM: Graph LLM for traffic prediction",
            "image-path": "flux_paper_image/2410.20856_1730241350.png"
        },

        {
            "startTime": "47:58",
            "arxivId": "2410.20274",
            "arxivLink": "https://arxiv.org/abs/2410.20274",
            "title": "LLMs Learn Libraries? Not So Fast!",
            "institute": "University of Toronto, Dalhousie University",
            "text": "This research investigates the effectiveness of \"library learning\" in Large Language Models (LLMs) for mathematical reasoning. Unlike previous studies that focused on accuracy, this paper analyzes the actual reuse of learned tools, finding that it's far less common than expected.",
            "paper-title": "Library Learning Doesn't: The Curious Case of the Single-Use\"Library\"",
            "image-path": "flux_paper_image/2410.20274_1730240947.png"
        },

        {
            "startTime": "48:18",
            "arxivId": "2410.21242",
            "arxivLink": "https://arxiv.org/abs/2410.21242",
            "title": "Zero-Shot Retrieval: LLMs Say \"No\" to Hypothetical Documents!",
            "institute": "MIT",
            "text": "This research proposes a new method called ReDE-RF for zero-shot dense retrieval. Unlike previous approaches that rely on LLMs to generate hypothetical documents, ReDE-RF uses LLMs to estimate the relevance of real documents, making it more efficient and less reliant on the LLM's domain-specific knowledge.",
            "paper-title": "Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback",
            "image-path": "flux_paper_image/2410.21242_1730241605.png"
        },

        {
            "startTime": "48:43",
            "arxivId": "2410.21091",
            "arxivLink": "https://arxiv.org/abs/2410.21091",
            "title": "AI-Powered Speech Makes VR Object Selection a Breeze!",
            "institute": "University of Cambridge, Coburg University",
            "text": "This research explores using large language models (LLMs) to assist with multi-object selection in virtual reality (VR) through a multimodal speech and raycast interaction technique. This differs from previous work that primarily focused on single object selection using a single interaction modality.",
            "paper-title": "Large Language Model-assisted Speech and Pointing Benefits Multiple 3D Object Selection in Virtual Reality",
            "image-path": "flux_paper_image/2410.21091_1730239949.png"
        },

        {
            "startTime": "49:06",
            "arxivId": "2410.20171",
            "arxivLink": "https://arxiv.org/abs/2410.20171",
            "title": "Image Captioning Goes Both Ways: A Model That Can Generate Images From Text!",
            "institute": "IBM Research, IIIT Bangalore",
            "text": "This research proposes a novel approach to image generation by training an invertible neural network on the task of image captioning. Unlike previous methods that require separate training for image generation, this model leverages the learned invertible mapping to generate images from text without additional training.",
            "paper-title": "Image Generation from Image Captioning -- Invertible Approach",
            "image-path": "flux_paper_image/2410.20171_1730239607.png"
        },

        {
            "startTime": "49:27",
            "arxivId": "2410.20135",
            "arxivLink": "https://arxiv.org/abs/2410.20135",
            "title": "Clipped SGD: A Near-Optimal Solution for Heavy-Tailed Data?",
            "institute": "Stanford University, Google, Adobe",
            "text": "This paper improves upon the existing analysis of Clipped-SGD for stochastic convex optimization (SCO) problems with heavy-tailed gradients. It achieves nearly sub-Gaussian statistical rates, bridging the gap between previous sub-optimal rates and the optimal sub-Gaussian rates. The key difference lies in the use of a novel iterative refinement strategy for martingale concentration, which leads to tighter concentration inequalities compared to previous approaches.",
            "paper-title": "Near-Optimal Streaming Heavy-Tailed Statistical Estimation with Clipped SGD",
            "image-path": "flux_paper_image/2410.20135_1730240444.png"
        },

        {
            "startTime": "49:53",
            "arxivId": "2410.20790",
            "arxivLink": "https://arxiv.org/abs/2410.20790",
            "title": "CNNs for Video: Skipping the Boring Bits for Faster Results!",
            "institute": "Shanghai Jiao Tong University, UC Berkeley, Fudan University",
            "text": "This research proposes SparseTem, a framework that leverages temporal continuity in videos to accelerate CNN-based video encoders. Unlike previous methods that focus on architectural design or general CNN optimization, SparseTem specifically targets the temporal redundancy inherent in video data.",
            "paper-title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by Exploiting Temporal Continuity",
            "image-path": "flux_paper_image/2410.20790_1730241276.png"
        },

        {
            "startTime": "50:18",
            "arxivId": "2410.20760",
            "arxivLink": "https://arxiv.org/abs/2410.20760",
            "title": "Outlier-Proofing Your Data: A Smoother Way to Estimate Probability Densities",
            "institute": "Science Tokyo, RIKEN",
            "text": "This research extends the application of GAN-like estimators to a broader class of statistical models, specifically the kernel exponential family, which includes both finite and infinite-dimensional models. It proposes the smoothed total variation (STV) distance as a class of integral probability metrics (IPMs) to construct robust estimators and analyzes their robustness properties.",
            "paper-title": "Robust estimation for kernel exponential families with smoothed total variation distances",
            "image-path": "flux_paper_image/2410.20760_1730241290.png"
        },

        {
            "startTime": "50:43",
            "arxivId": "2410.20068",
            "arxivLink": "https://arxiv.org/abs/2410.20068",
            "title": "GCN's Deep Dive: Unmasking the Secrets of Graph Convolutional Networks",
            "institute": "University of Twente, University of Chicago, ESSEC Business School...",
            "text": "This research delves into the statistical properties of graph convolutional networks (GCNs) in regression tasks, specifically analyzing the impact of convolution operators on the learning error as a function of neighborhood topology and the number of convolutional layers. Unlike previous work that focused on classification tasks or asymptotic analysis, this paper provides a non-asymptotic analysis of GCNs in the fixed design setting, offering insights into the bias-variance trade-off and the role of neighborhood size and structure in determining the performance of GCNs.",
            "paper-title": "Understanding the Effect of GCN Convolutions in Regression Tasks",
            "image-path": "flux_paper_image/2410.20068_1730239938.png"
        },

        {
            "startTime": "51:13",
            "arxivId": "2410.20542",
            "arxivLink": "https://arxiv.org/abs/2410.20542",
            "title": "PPG's New BFF: Meet PAPAGEI, the Open Foundation Model for Optical Signals!",
            "institute": "Nokia Bell Labs, Dartmouth College, University of Cambridge...",
            "text": "This research introduces PAPAGEI, the first open foundation model specifically trained on photoplethysmography (PPG) signals. Unlike previous work that often focused on single-device datasets or task-specific models, PAPAGEI is pre-trained on a massive dataset of 20 million PPG segments from diverse public sources, enabling it to generalize across various health-related tasks.",
            "paper-title": "PaPaGei: Open Foundation Models for Optical Physiological Signals",
            "image-path": "flux_paper_image/2410.20542_1730241899.png"
        },

        {
            "startTime": "51:35",
            "arxivId": "2410.21076",
            "arxivLink": "https://arxiv.org/abs/2410.21076",
            "title": "Flowing Fast: New Trick Makes Gravitational Wave Hunting 15 Times Faster!",
            "institute": "University College London, Utrecht University, National Institute for Subatomic Physics Netherlands...",
            "text": "This research combines normalizing flows with a learned harmonic mean estimator to accelerate Bayesian evidence estimation, a crucial step in model selection. This approach is distinct from traditional nested sampling methods, which are often computationally expensive.",
            "paper-title": "Accelerated Bayesian parameter estimation and model selection for gravitational waves with normalizing flows",
            "image-path": "flux_paper_image/2410.21076_1730239818.png"
        },

        {
            "startTime": "51:58",
            "arxivId": "2410.21237",
            "arxivLink": "https://arxiv.org/abs/2410.21237",
            "title": "From Pixels to Products: Building a Knowledge Graph for E-Commerce with Images",
            "institute": "CMU",
            "text": "This research proposes a novel method for constructing knowledge graphs directly from product images, automating the process and enabling timely updates for fast-paced e-commerce environments. Unlike previous work that relies on text descriptions, this approach leverages the power of vision-language models (VLMs) and large language models (LLMs) to extract and infer information from images.",
            "paper-title": "Hierarchical Knowledge Graph Construction from Images for Scalable E-Commerce",
            "image-path": "flux_paper_image/2410.21237_1730241738.png"
        },

        {
            "startTime": "52:21",
            "arxivId": "2410.20140",
            "arxivLink": "https://arxiv.org/abs/2410.20140",
            "title": "AI Detectives Debate to Bust Fake News: MAD-Sherlock Solves Out-of-Context Images!",
            "institute": "University of Oxford, Adobe",
            "text": "This research introduces MAD-Sherlock, a system that uses multiple AI agents to debate the authenticity of image-text pairs, incorporating external information retrieval to enhance contextual reasoning and explainability. This approach differs from previous work by focusing on multi-agent collaboration and leveraging external knowledge to improve accuracy and provide human-understandable explanations.",
            "paper-title": "MAD-Sherlock: Multi-Agent Debates for Out-of-Context Misinformation Detection",
            "image-path": "flux_paper_image/2410.20140_1730239403.png"
        },

        {
            "startTime": "52:45",
            "arxivId": "2410.21159",
            "arxivLink": "https://arxiv.org/abs/2410.21159",
            "title": "AI Assistants: Not So Helpful, Kinda Harmful?",
            "institute": "University of Oxford",
            "text": "This research introduces a new benchmark, CURATe, to evaluate how well AI assistants can remember and use safety-critical information from users in multi-turn conversations. This is different from previous work that focused on single-turn interactions or general alignment.",
            "paper-title": "CURATe: Benchmarking Personalised Alignment of Conversational AI Assistants",
            "image-path": "flux_paper_image/2410.21159_1730241832.png"
        },

        {
            "startTime": "53:03",
            "arxivId": "2410.19780",
            "arxivLink": "https://arxiv.org/abs/2410.19780",
            "title": "Bayesian Neural Networks Get a Speed Boost with Symmetric Minibatch Splitting",
            "institute": "University of Edinburgh, ETH Zurich, City University of Hong Kong",
            "text": "This research introduces a new algorithm called SMS-UBU for sampling parameter spaces in Bayesian neural networks. It combines asymmetric forward/backward sweeps over minibatches with an asymmetric discretization of Langevin dynamics. This approach achieves second-order accuracy in terms of sampling bias, even when using only one minibatch per iteration.",
            "paper-title": "Sampling from Bayesian Neural Network Posteriors with Symmetric Minibatch Splitting Langevin Dynamics",
            "image-path": "flux_paper_image/2410.19780_1730239833.png"
        },

        {
            "startTime": "53:28",
            "arxivId": "2410.20527",
            "arxivLink": "https://arxiv.org/abs/2410.20527",
            "title": "CodeRosetta: Unsupervised Code Translation for Parallel Programming - It's Like Google Translate for Supercomputers!",
            "institute": "Cisco Systems, Google",
            "text": "This research introduces CodeRosetta, an encoder-decoder transformer model specifically designed for unsupervised translation between programming languages and their high-performance computing (HPC) extensions. Unlike previous work that relies on language-specific metrics, CodeRosetta employs new pre-training and training objectives to learn the inherent features and semantics of code in an unsupervised manner.",
            "paper-title": "CodeRosetta: Pushing the Boundaries of Unsupervised Code Translation for Parallel Programming",
            "image-path": "flux_paper_image/2410.20527_1730240666.png"
        },

        {
            "startTime": "53:52",
            "arxivId": "2410.20459",
            "arxivLink": "https://arxiv.org/abs/2410.20459",
            "title": "Unlocking Comics: AI Learns to See the Story, Not Just the Pictures",
            "institute": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne",
            "text": "This research introduces a new dataset, AI4VA, focusing on Franco-Belgian comics from the 1950s. Unlike previous datasets that primarily focus on American or Japanese comics, AI4VA provides annotations for depth estimation, semantic segmentation, and saliency detection, enabling a more comprehensive understanding of visual narratives.",
            "paper-title": "Unlocking Comics: The AI4VA Dataset for Visual Understanding",
            "image-path": "flux_paper_image/2410.20459_1730238981.png"
        },

        {
            "startTime": "54:17",
            "arxivId": "2410.20817",
            "arxivLink": "https://arxiv.org/abs/2410.20817",
            "title": "Zeno's Paradox: Why 'Low-Resource' Languages Are Always Playing Catch-Up",
            "institute": "UC Berkeley",
            "text": "This research goes beyond simply stating that some languages have less data. It identifies four distinct aspects that contribute to a language being labeled \"low-resource,\" including socio-political factors, human and digital resources, artifacts, and community agency.",
            "paper-title": "The Zeno's Paradox of `Low-Resource' Languages",
            "image-path": "flux_paper_image/2410.20817_1730239524.png"
        },

        {
            "startTime": "54:43",
            "arxivId": "2410.20318",
            "arxivLink": "https://arxiv.org/abs/2410.20318",
            "title": "Matrix Completion Gets a Geodesic Makeover: Sampling on Stiefel Manifolds for Bayesian Inference",
            "institute": "University of Sydney, University of Michigan",
            "text": "This research introduces a new Bayesian matrix completion approach that utilizes a singular value decomposition (SVD) parameterization and a geodesic Hamiltonian Monte Carlo (HMC) algorithm. This differs from previous work by directly sampling from the posterior distribution on the Stiefel manifold, rather than relying on traditional Gibbs samplers with Gaussian priors.",
            "paper-title": "Low-rank Bayesian matrix completion via geodesic Hamiltonian Monte Carlo on Stiefel manifolds",
            "image-path": "flux_paper_image/2410.20318_1730239055.png"
        },

        {
            "startTime": "55:13",
            "arxivId": "2410.20204",
            "arxivLink": "https://arxiv.org/abs/2410.20204",
            "title": "AI for Health Economics: From Robots to Real-World Results!",
            "institute": "National Institutes of Health, Tulane University, Intelligent Medical Objects...",
            "text": "This research paper focuses on the application of generative AI, specifically foundation models, in health economics and outcomes research (HEOR). It goes beyond previous work by exploring the use of these models in various HEOR tasks, including systematic literature reviews, economic modeling, real-world evidence generation, and dossier development.",
            "paper-title": "Generative AI in Health Economics and Outcomes Research: A Taxonomy of Key Definitions and Emerging Applications, an ISPOR Working Group Report",
            "image-path": "flux_paper_image/2410.20204_1730240873.png"
        },

        {
            "startTime": "55:34",
            "arxivId": "2410.21107",
            "arxivLink": "https://arxiv.org/abs/2410.21107",
            "title": "Tree-mendous Distance: A New Way to Measure Similarity in High-Dimensional Data",
            "institute": "Technion - Israel Institute of Technology, Yale University, University of California San Diego",
            "text": "This research introduces a new tree-Wasserstein distance (TWD) specifically designed for high-dimensional data with a latent feature hierarchy. Unlike previous TWD methods that focus on embedding samples in hyperbolic space, this approach embeds features into a multi-scale hyperbolic space using diffusion geometry. The key innovation lies in a novel tree decoding method that leverages analogies between hyperbolic embedding and trees to learn the latent feature hierarchy.",
            "paper-title": "Tree-Wasserstein Distance for High Dimensional Data with a Latent Feature Hierarchy",
            "image-path": "flux_paper_image/2410.21107_1730241406.png"
        },

        {
            "startTime": "55:59",
            "arxivId": "2410.19816",
            "arxivLink": "https://arxiv.org/abs/2410.19816",
            "title": "Volunteer Photos: A Biased Look at Biodiversity?",
            "institute": "Stanford University, UC Berkeley",
            "text": "This research introduces the DivShift framework, which analyzes how biases in volunteer-collected biodiversity datasets affect the performance of deep learning models. Unlike previous work, this study focuses on quantifying the impact of specific biases, such as spatial, temporal, observation quality, and sociopolitical biases, on model accuracy.",
            "paper-title": "DivShift: Exploring Domain-Specific Distribution Shift in Volunteer-Collected Biodiversity Datasets",
            "image-path": "flux_paper_image/2410.19816_1730239272.png"
        },

        {
            "startTime": "56:20",
            "arxivId": "2410.20843",
            "arxivLink": "https://arxiv.org/abs/2410.20843",
            "title": "Solar Corona's Future: AI Predicts Sun's Next Moves!",
            "institute": "University of Rome Tor Vergata, University of Coimbra, University of Rome La Sapienza...",
            "text": "This research uses a Denoising Diffusion Probabilistic Model (DDPM) to simulate the evolution of the solar corona, a technique not previously applied to this specific problem.",
            "paper-title": "Generative Simulations of The Solar Corona Evolution With Denoising Diffusion : Proof of Concept",
            "image-path": "flux_paper_image/2410.20843_1730241676.png"
        },

        {
            "startTime": "56:46",
            "arxivId": "2410.20587",
            "arxivLink": "https://arxiv.org/abs/2410.20587",
            "title": "Markov Models: Not Just for Diffusion Anymore!",
            "institute": "MIT, FAIR Meta, Weizmann Institute of Science",
            "text": "This paper introduces a new framework called \"generator matching\" that unifies various generative modeling methods under a single umbrella. It expands the design space of generative models by introducing a new class of models called \"jump models\" and explores the possibility of combining different model classes.",
            "paper-title": "Generator Matching: Generative modeling with arbitrary Markov processes",
            "image-path": "flux_paper_image/2410.20587_1730241010.png"
        },

        {
            "startTime": "57:13",
            "arxivId": "2410.21024",
            "arxivLink": "https://arxiv.org/abs/2410.21024",
            "title": "Moon Rocks: AI Detectives Solve Lunar Mysteries!",
            "institute": "European Space Agency",
            "text": "This research uses contrastive learning, a technique that helps AI models learn more generalizable features from images, to classify lunar rock types. This approach differs from previous studies that relied on traditional machine learning methods.",
            "paper-title": "Breccia and basalt classification of thin sections of Apollo rocks with deep learning",
            "image-path": "flux_paper_image/2410.21024_1730241611.png"
        },

        {
            "startTime": "57:32",
            "arxivId": "2410.20081",
            "arxivLink": "https://arxiv.org/abs/2410.20081",
            "title": "Typing Without a Keyboard? This Dataset Will Make You Say \"Wow!\"",
            "institute": "Meta",
            "text": "This research introduces emg2qwerty, a large-scale dataset of electromyographic signals recorded from users' wrists while they type on a QWERTY keyboard. Unlike previous datasets, emg2qwerty focuses on the natural behavior of typing, rather than static hand poses, and includes recordings from 108 users across 1,135 sessions.",
            "paper-title": "emg2qwerty: A Large Dataset with Baselines for Touch Typing using Surface Electromyography",
            "image-path": "flux_paper_image/2410.20081_1730240914.png"
        },

        {
            "startTime": "57:56",
            "arxivId": "2410.19845",
            "arxivLink": "https://arxiv.org/abs/2410.19845",
            "title": "LLMs: The New Scam Busters?",
            "institute": "Google",
            "text": "This research explores the use of Large Language Models (LLMs) for scam detection in digital payment systems, specifically focusing on UPI transactions in India and Google Pay. Unlike previous work that primarily relied on traditional machine learning models, this study leverages the unique capabilities of LLMs to analyze textual data and identify subtle patterns indicative of fraudulent activities.",
            "paper-title": "Enhancing Trust and Safety in Digital Payments: An LLM-Powered Approach",
            "image-path": "flux_paper_image/2410.19845_1730240801.png"
        },

        {
            "startTime": "58:14",
            "arxivId": "2410.20626",
            "arxivLink": "https://arxiv.org/abs/2410.20626",
            "title": "Tabular Data's New BFF: A Diffusion Model That's Got Your Back (and Your Data)",
            "institute": "Stanford University, University of Southern California, University of Illinois at Chicago",
            "text": "This paper introduces TABDIFF, a multi-modal diffusion model for tabular data generation that directly operates on the data space and handles numerical and categorical features in their native formats. Unlike previous methods that either rely on discrete-time diffusion or transform features into a latent continuous space, TABDIFF utilizes a continuous-time diffusion framework and learns feature-wise learnable noise schedules to address the heterogeneity of feature distributions.",
            "paper-title": "TabDiff: a Multi-Modal Diffusion Model for Tabular Data Generation",
            "image-path": "flux_paper_image/2410.20626_1730239484.png"
        },

        {
            "startTime": "58:36",
            "arxivId": "2410.21052",
            "arxivLink": "https://arxiv.org/abs/2410.21052",
            "title": "RL Agents Need a Mentor: When AI Gets Lost, Asking for Help is the Key",
            "institute": "UC Berkeley, McGill University",
            "text": "This research explores the use of \"ask-for-help\" strategies to mitigate goal misgeneralization in reinforcement learning agents, a phenomenon where agents learn incorrect goals during deployment. Previous work has focused on increasing training environment diversity or real-world training, but this paper investigates a novel approach of allowing agents to request assistance when they encounter unfamiliar situations.",
            "paper-title": "Getting By Goal Misgeneralization With a Little Help From a Mentor",
            "image-path": "flux_paper_image/2410.21052_1730241624.png"
        },

        {
            "startTime": "59:03",
            "arxivId": "2410.20098",
            "arxivLink": "https://arxiv.org/abs/2410.20098",
            "title": "Tired of Neural Networks Forgetting? Self-Normalized Resets to the Rescue!",
            "institute": "MIT",
            "text": "This paper introduces Self-Normalized Resets (SNR), a new algorithm that tackles plasticity loss in continual learning. Unlike previous methods that rely on fixed reset rates or regularization, SNR dynamically resets neurons based on their individual firing rates, using a simple hypothesis test.",
            "paper-title": "Self-Normalized Resets for Plasticity in Continual Learning",
            "image-path": "flux_paper_image/2410.20098_1730240909.png"
        },

        {
            "startTime": "59:22",
            "arxivId": "2410.20187",
            "arxivLink": "https://arxiv.org/abs/2410.20187",
            "title": "LLMs: Learning to Love (and Hate) with a Pinch of Uncertainty",
            "institute": "ETH Zurich",
            "text": "This research introduces uncertainty penalization schemes for Direct Preference Optimization (DPO), a technique for aligning Large Language Models (LLMs) with human preferences. Unlike previous work that focuses on adding a margin to the loss function, this paper proposes a multiplicative penalization scheme that multiplies the implicit rewards by an energy-based function of the uncertainty.",
            "paper-title": "Uncertainty-Penalized Direct Preference Optimization",
            "image-path": "flux_paper_image/2410.20187_1730242009.png"
        },

        {
            "startTime": "59:51",
            "arxivId": "2410.20198",
            "arxivLink": "https://arxiv.org/abs/2410.20198",
            "title": "Inflation Nowcasting: Can AI Smell a Price Hike?",
            "institute": "EPFL",
            "text": "This research explores the use of a BERT-based large language model (LLM) called InflaBERT to analyze news sentiment related to inflation and incorporate it into a traditional inflation nowcasting model. This approach differs from previous work by directly using an LLM to capture real-time sentiment from news articles, rather than relying solely on traditional economic indicators.",
            "paper-title": "Enhancing Inflation Nowcasting with LLM: Sentiment Analysis on News",
            "image-path": "flux_paper_image/2410.20198_1730241961.png"
        }
    ],
    "stats": {
        "num_pick": 152,
        "num_total": 609,
    },
    "audio": "https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202410291631_audio.mp3"
}
