<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                    Fresh Picks:
                    <span class="highlightNumber">59</span> out of <span class="highlightNumber">247</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-08-02"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>

        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">
                00:55
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00298" target="_blank">
                        @arXiv 2408.00298
                    </a>
                    <span class="tweet-title">
                        Manga for the Visually Impaired: A Tail-Waggingly Good Transcript Generator!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research extends previous work by generating chapter-wide manga transcripts with consistent
                    character names, improving speaker diarization, and distinguishing between essential and
                    non-essential text.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">
                01:16
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00714" target="_blank">
                        @arXiv 2408.00714
                    </a>
                    <span class="tweet-title">
                        Segment Anything, Now in Motion: SAM2 Brings Segmentation to Videos!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Meta
                    </span>
                </div>
                <div class="primary-text">
                    This research extends the SegmentAnything (SAM) model to the video domain, introducing a new
                    model
                    called SAM2 that can segment objects across multiple frames. Unlike previous video segmentation
                    models, SAM2 utilizes a memory mechanism to store information about the object and previous
                    interactions, enabling it to track objects more effectively and with fewer user prompts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">
                01:37
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00754" target="_blank">
                        @arXiv 2408.00754
                    </a>
                    <span class="tweet-title">
                        Stop Staring, LLMs! New Trick Makes 'Em See in 3D
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington, Tsinghua University, Tencent...
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces COARSE CORRESPONDENCES, a visual prompting method that helps multimodal
                    language models (MLLMs) understand 3D space and time without requiring any training. Unlike
                    previous
                    methods that focus on either 3D or temporal understanding, COARSE CORRESPONDENCES combines
                    lightweight tracking models to extract object correspondences across multiple frames, enabling
                    the
                    MLLM to reason about both spatial and temporal relationships.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">
                02:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00724" target="_blank">
                        @arXiv 2408.00724
                    </a>
                    <span class="tweet-title">
                        Smaller Brains, Smarter Moves: How to Make LLMs Solve Problems with Less Compute
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on optimizing the inference stage of LLMs, specifically exploring how to
                    design models and inference strategies that trade off additional compute for improved
                    performance.
                    This is distinct from previous work that primarily focused on scaling laws during training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">
                02:42
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00283" target="_blank">
                        @arXiv 2408.00283
                    </a>
                    <span class="tweet-title">
                        Indic Languages Get Their Own Text-to-Image Bias Test!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        IIT Jodhpur, Meta, Weir P.B.C.
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces the IndicTTI benchmark, a new evaluation tool specifically designed to
                    assess the performance and cultural biases of text-to-image models across 30 Indic languages.
                    This
                    benchmark expands upon previous work by focusing on a wider range of languages and using more
                    complex prompts, making it more relevant to real-world applications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">
                03:05
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00713" target="_blank">
                        @arXiv 2408.00713
                    </a>
                    <span class="tweet-title">
                        Insurance Firms: Stop Guessing, Start Learning!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge, Accenture
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel reinforcement learning algorithm for the portfolio pursuit
                    problem
                    in insurance, which is different from previous work that focused on either local decision-making
                    or
                    constrained portfolio optimization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">
                03:37
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00712" target="_blank">
                        @arXiv 2408.00712
                    </a>
                    <span class="tweet-title">
                        MotionFix: Text-to-Motion Editing Gets a Makeover!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Max Planck Institute for Intelligent Systems, LIGM, École des Ponts...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces MotionFix, a new dataset specifically designed for training models to
                    edit
                    3D human motion based on text descriptions. Unlike previous datasets that focus on
                    text-to-motion
                    generation, MotionFix provides triplets of source motion, target motion, and edit text, enabling
                    the
                    development of models that can modify existing motions according to user instructions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">
                04:07
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00427" target="_blank">
                        @arXiv 2408.00427
                    </a>
                    <span class="tweet-title">
                        Spatial Awareness for Cancer Prognosis: A Regularization Revolution
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel regularization scheme called CARMIL, which incorporates spatial
                    context into Multiple Instance Learning (MIL) models for Whole Slide Images (WSIs). Unlike
                    previous
                    methods that rely on complex architectures like Graph Neural Networks (GNNs) or Transformers,
                    CARMIL
                    leverages a spatial encoder and decoder to distill spatial information directly into tile
                    embeddings, making it applicable to any MIL model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">
                04:31
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00211" target="_blank">
                        @arXiv 2408.00211
                    </a>
                    <span class="tweet-title">
                        Models as Data: A Toolkit for Tinkering with Transformers
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Penzai, a JAX library that treats neural network models as data
                    structures,
                    enabling direct manipulation and visualization. Unlike previous approaches that rely on hooks or
                    tracing, Penzai allows users to modify model components directly, making it easier to understand
                    and
                    control model behavior.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">
                04:52
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00765" target="_blank">
                        @arXiv 2408.00765
                    </a>
                    <span class="tweet-title">
                        MM-Vet v2: A New Benchmark for Multimodal Models That Can Handle More Than Just One Image at
                        a
                        Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        National University of Singapore, Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces MM-Vet v2, a new benchmark for evaluating large multimodal models
                    (LLMs).
                    Unlike previous benchmarks, MM-Vet v2 includes a new capability called "image-text sequence
                    understanding," which assesses the ability of LLMs to process sequences of images and text.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">
                05:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00118" target="_blank">
                        @arXiv 2408.00118
                    </a>
                    <span class="tweet-title">
                        Tiny Models, Big Brains: Gemma2 Makes Small LLMs Super Smart
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the impact of knowledge distillation on smaller language models. Unlike
                    previous work that focused on increasing training data size, this study trains smaller models
                    with
                    distilled knowledge from a larger model, achieving performance comparable to models 2-3 times
                    their
                    size.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">
                05:48
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00122" target="_blank">
                        @arXiv 2408.00122
                    </a>
                    <span class="tweet-title">
                        LLMs Go to School: Teaching AI Ethics with a Dose of Clinical Questions
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        IBM Research Europe, Technical University of Darmstadt
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a shared task designed to evaluate the output of Large Language Models
                    (LLMs) when answering clinical questions. The task focuses on identifying harmful content
                    generated
                    by LLMs, a crucial aspect often overlooked in traditional LLM evaluation. This approach differs
                    from
                    previous work by emphasizing the ethical implications of LLM outputs in a real-world context.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">
                06:10
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00001" target="_blank">
                        @arXiv 2408.00001
                    </a>
                    <span class="tweet-title">
                        AI's Got a Memory: How Diffusion Models Remember Their Training Data
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Peking University, Zhejiang University
                    </span>
                </div>
                <div class="primary-text">
                    This research paper provides the first comprehensive survey of replication in visual diffusion
                    models, systematically categorizing existing studies into unveiling, understanding, and
                    mitigating
                    this phenomenon.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">
                06:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00624" target="_blank">
                        @arXiv 2408.00624
                    </a>
                    <span class="tweet-title">
                        SynesLM: Speech Recognition Gets a Visual Makeover!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces SynesLM, a unified model that can perform audio-visual speech
                    recognition,
                    visual speech translation, and visual machine translation. Unlike previous work that focused on
                    lip
                    movements, SynesLM leverages more general visual information within entire frames, such as
                    objects
                    and actions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">
                07:15
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00254" target="_blank">
                        @arXiv 2408.00254
                    </a>
                    <span class="tweet-title">
                        Looping for Views: A New Trick to Make 3D Images From Sparse Photos
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, Pengcheng Laboratory, University of Nottingham
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces LoopSparseGS, a novel 3D Gaussian Splatting framework that leverages a
                    looping
                    mechanism to generate denser Gaussian initialization and precise geometric constraints for
                    sparse-input novel view synthesis. Unlike previous methods that rely on dense input views,
                    LoopSparseGS effectively handles sparse input data by iteratively refining the 3D scene
                    representation using rendered pseudo-images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">
                07:42
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00735" target="_blank">
                        @arXiv 2408.00735
                    </a>
                    <span class="tweet-title">
                        Turbocharged Editing: Text-to-Image Edits in a Flash!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tel-Aviv University, NVIDIA
                    </span>
                </div>
                <div class="primary-text">
                    This paper focuses on adapting a popular text-based image editing technique, DDPM noise
                    inversion,
                    to work with fast-sampling diffusion models. Unlike previous methods that rely on many steps,
                    this
                    approach achieves high-quality edits in just a few steps, significantly speeding up the process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">
                08:03
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00359" target="_blank">
                        @arXiv 2408.00359
                    </a>
                    <span class="tweet-title">
                        Fine-Tuning's Secret Weapon: How Many Neurons Does It Take to Memorize?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Yonsei University, University of Seoul, Korea Institute for Advanced Study...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new metric called Fine-Tuning Capacity (FTC) to analyze the ability
                    of
                    neural networks to adapt to new data during fine-tuning. Unlike previous work focusing on
                    memorization capacity, FTC specifically considers the scenario where a pre-trained network is
                    fine-tuned by adding a new network.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">
                08:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00278" target="_blank">
                        @arXiv 2408.00278
                    </a>
                    <span class="tweet-title">
                        Convolution Chaos: New Tensor Layouts Speed Up Deep Learning!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nanchang Hangkong University, Microsoft, University of Washington...
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the impact of different tensor data layouts on the performance of
                    convolution
                    operations, a core component of deep neural networks. It introduces three novel layouts for the
                    im2win convolution method, specifically NHWC, CHWN, and CHWN8, and compares their performance
                    with
                    the traditional NCHW layout.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">
                08:58
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00700" target="_blank">
                        @arXiv 2408.00700
                    </a>
                    <span class="tweet-title">
                        Graph Denoising: When Structure and Features Get Messy, It's Time to Clean Up!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, Huawei
                    </span>
                </div>
                <div class="primary-text">
                    This research tackles the problem of graph denoising by considering both structure and feature
                    noise
                    simultaneously, unlike previous methods that focused on only one type of noise.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">
                09:23
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00057" target="_blank">
                        @arXiv 2408.00057
                    </a>
                    <span class="tweet-title">
                        Protein Knowledge Graphs: Giving AI a Brain Boost for Drug Discovery!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Technion – Israel Institute of Technology, Meta
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces GOProteinGNN, a novel architecture that integrates protein knowledge
                    graph
                    information into protein language models (PLMs) during the creation of amino acid-level
                    representations. Unlike previous methods that focus on either amino acid level learning or
                    protein
                    level learning, GOProteinGNN uniquely learns the entire protein knowledge graph during training,
                    allowing it to capture broader relational nuances and dependencies beyond mere triplets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">
                09:52
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00550" target="_blank">
                        @arXiv 2408.00550
                    </a>
                    <span class="tweet-title">
                        Multilingual AI Models: Stop Hallucinating, Start Speaking!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Huazhong University of Science and Technology, Fudan University, Peking University...
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on mitigating hallucinations in large vision-language models (LVLMs) when
                    they
                    are used with non-English languages. Unlike previous work that primarily addressed
                    English-language
                    hallucinations, this study proposes a two-stage framework to improve the model's ability to
                    understand instructions and distinguish between accurate and inaccurate responses in multiple
                    languages.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">
                10:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00112" target="_blank">
                        @arXiv 2408.00112
                    </a>
                    <span class="tweet-title">
                        Sperm Segmentation: A Tailored Approach to Infertility Diagnosis
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Dalian University of Technology, Chinese University of Hong Kong, University of Toronto
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel attention-based instance-aware part segmentation network for
                    sperm
                    morphology analysis. Unlike previous "detect-then-segment" methods, this approach addresses
                    context
                    loss and feature distortion by refining preliminary segmented masks using features extracted by
                    a
                    feature pyramid network.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">
                10:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00315" target="_blank">
                        @arXiv 2408.00315
                    </a>
                    <span class="tweet-title">
                        Diffusion Models Get a Bridge: A New Way to Purify Adversarial Noise!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Peking University, Beijing Institute of Technology...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel Adversarial Diffusion Bridge Model (ADBM) for adversarial
                    purification. Unlike previous methods that rely on the similarity between diffused clean and
                    adversarial data distributions, ADBM directly constructs a reverse bridge from the diffused
                    adversarial data distribution to the clean data distribution.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">
                11:23
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00129" target="_blank">
                        @arXiv 2408.00129
                    </a>
                    <span class="tweet-title">
                        "AI Hijacking: Now Your Model Can Be a Language Translator... Without You Knowing!"
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CISPA Helmholtz Center for Information Security, Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research extends the model hijacking attack to a multimodal setting, allowing an adversary
                    to
                    implement an NLP task within a CV model. This differs from previous work which focused solely on
                    homogeneous-modality tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">
                11:52
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00303" target="_blank">
                        @arXiv 2408.00303
                    </a>
                    <span class="tweet-title">
                        Smoothing Out the Noise: A Neural Octahedral Field for Sharp Edge Reconstruction
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel neural field, the octahedral field, which is jointly trained
                    with a
                    neural implicit representation to simultaneously smooth and emphasize sharp edges in unoriented
                    point cloud surface reconstruction. Unlike previous methods that rely on gradient regularization
                    or
                    local neighborhood information, this approach leverages the spherical harmonics representation
                    of
                    octahedral frames to guide the reconstruction process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">
                12:15
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00521" target="_blank">
                        @arXiv 2408.00521
                    </a>
                    <span class="tweet-title">
                        Code's New Look: Ditching the Word-by-Word for a Global Image
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new paradigm for encoding code, moving away from the autoregressive
                    next-word prediction approach used in GPTs. Instead, it encodes code as a heterogeneous image,
                    capturing global information about the code structure.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">
                12:37
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00157" target="_blank">
                        @arXiv 2408.00157
                    </a>
                    <span class="tweet-title">
                        Diffusion Models: Solving PDEs with a Little Help from Their Friends
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a generative learning framework for parametric PDEs that uses gradient
                    guidance and virtual observations to improve accuracy and efficiency. Unlike previous work, this
                    approach leverages multi-level information to guide the diffusion model, ensuring that the
                    generated
                    solutions maintain high fidelity to the underlying physical phenomena across different parameter
                    settings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">
                13:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00429" target="_blank">
                        @arXiv 2408.00429
                    </a>
                    <span class="tweet-title">
                        Indoor Positioning Gets a Confidence Boost: New AI Model Cuts Training Costs by 40%
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        China Mobile Research Institute, Beijing University of Posts and Telecommunications, The
                        Hong
                        Kong University of Science and Technology...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel semi-supervised learning approach for indoor positioning that
                    utilizes a biased teacher model to assign confidence scores to unlabeled data, thereby reducing
                    the
                    need for extensive labeled data and lowering training costs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">
                13:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00310" target="_blank">
                        @arXiv 2408.00310
                    </a>
                    <span class="tweet-title">
                        Online Scheduling: When Delaying Decisions is Actually Smart
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research explores Online Linear Programming (OLP) with batching, where decisions on
                    customers
                    can be delayed to the end of a batch. Unlike previous work that focused on finite support
                    distributions, this paper analyzes the case where the conditional distribution of the reward
                    given
                    resource consumption is continuous.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">
                13:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00677" target="_blank">
                        @arXiv 2408.00677
                    </a>
                    <span class="tweet-title">
                        One Fractal, One Million Images? Scaling Backwards in Pre-training!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        National Institute of Advanced Industrial Science and Technology (AIST), Tohoku University,
                        University of Amsterdam...
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the minimal requirements for successful pre-training in image
                    recognition.
                    Unlike previous work that used large datasets, this study demonstrates that a single fractal
                    image
                    with perturbations can achieve comparable performance to pre-training on millions of images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">
                14:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00251" target="_blank">
                        @arXiv 2408.00251
                    </a>
                    <span class="tweet-title">
                        Deep Learning Cracks the Code of Car-Following Dynamics!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research uses deep symbolic regression (DSR) to discover car-following dynamics directly
                    from
                    trajectory data, incorporating a variable intersection selection (VIS) method to guide the
                    search
                    for interpretable and parsimonious mathematical expressions. This approach differs from previous
                    work by minimizing human involvement in the data analysis and hypothesis steps.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">
                15:05
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00766" target="_blank">
                        @arXiv 2408.00766
                    </a>
                    <span class="tweet-title">
                        Diffusion Models Get a Speed Boost for Autonomous Driving!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces two novel techniques, Optimal Gaussian Diffusion (OGD) and Estimated
                    Clean
                    Manifold (ECM) Guidance, to improve the computational efficiency of diffusion models for joint
                    trajectory prediction and controllable generation in autonomous driving. Unlike previous work
                    that
                    focused on optimizing the reverse diffusion process or guided sampling separately, this paper
                    tackles both aspects simultaneously.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">
                15:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00113" target="_blank">
                        @arXiv 2408.00113
                    </a>
                    <span class="tweet-title">
                        Chess Bots, Sparse Brains: New Metrics for Language Model Interpretability
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT, University of Massachusetts Amherst, University of Mannheim...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces two novel metrics for evaluating the quality of sparse autoencoders
                    (SAEs)
                    trained on language models. Unlike previous work that relied on proxy metrics or subjective
                    evaluations, these metrics leverage the natural interpretable features present in board game
                    transcripts to assess how well SAEs capture the model's internal representation of the game
                    state.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">
                16:04
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00706" target="_blank">
                        @arXiv 2408.00706
                    </a>
                    <span class="tweet-title">
                        Point-Supervised Brain Tumor Segmentation: A Box-Prompted MedSAM Adventure!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Yale University, Massachusetts General Hospital, Harvard Medical School
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces an iterative framework that leverages the MedSAM model for
                    point-supervised
                    brain tumor segmentation. Unlike previous methods, it uses a semantic box-prompt generator to
                    convert point annotations into potential bounding boxes, which are then refined through a
                    prompt-guided spatial refinement module.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">
                16:31
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00024" target="_blank">
                        @arXiv 2408.00024
                    </a>
                    <span class="tweet-title">
                        AI Explains Itself Into Your Heart (and Maybe Your Head)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research examines the impact of AI-generated explanations on people's beliefs, specifically
                    focusing on how deceptive explanations can amplify belief in misinformation. Previous work has
                    explored the influence of AI-generated misinformation, but this study delves deeper into the
                    persuasive power of explanations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">
                16:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00620" target="_blank">
                        @arXiv 2408.00620
                    </a>
                    <span class="tweet-title">
                        Bigger Vision Encoders Don't Always Mean Better Vision Models: A Scaling Law Surprise!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research investigates scaling laws for vision language models (VLMs) using a connected
                    vision
                    paradigm, a common architecture where a vision encoder (like ViT) is connected to a large
                    language
                    model (LLM) backbone. Unlike previous scaling law studies that focused on training LLMs and
                    modality
                    encoders from scratch, this paper explores the scaling behavior of VLMs when using pre-trained
                    components.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">
                17:23
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00117" target="_blank">
                        @arXiv 2408.00117
                    </a>
                    <span class="tweet-title">
                        AI Pose Estimation: Not Just Accurate, But Certified Robust!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on certifying the robustness of two-stage keypoint-based pose estimation
                    methods, a crucial aspect often overlooked in previous work. The authors introduce a novel
                    approach
                    that transforms the certification problem into a standard neural network verification problem
                    for
                    classification tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">
                17:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00764" target="_blank">
                        @arXiv 2408.00764
                    </a>
                    <span class="tweet-title">
                        AI Agents Get a Planning Power-Up: From Peanut Butter to Perfect Plans!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on automatically generating diverse environments and planning tasks for
                    training AI agents, unlike previous work that relied heavily on manual design.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">
                18:40
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00636" target="_blank">
                        @arXiv 2408.00636
                    </a>
                    <span class="tweet-title">
                        Brain Tumor Detection: MobileNet-BT Takes the Lead!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Arizona State University, UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new model, MobileNet-BT, based on MobileNetV2, which achieves higher
                    accuracy and F1-score in brain tumor classification compared to other pre-trained models. The
                    key
                    difference lies in unfreezing all layers of MobileNetV2 and modifying the classifier to better
                    capture specific characteristics of the brain tumor dataset.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">
                19:04
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00203" target="_blank">
                        @arXiv 2408.00203
                    </a>
                    <span class="tweet-title">
                        GPT-4V Gets a Vision Makeover: OmniParser Helps AI Agents See the World (and Click the Right
                        Buttons)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces OmniParser, a method for parsing user interface screenshots into
                    structured
                    elements. Unlike previous work that relies on HTML information or view hierarchies, OmniParser
                    uses
                    a pure vision-based approach, making it applicable to a wider range of platforms and
                    applications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">
                19:27
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00573" target="_blank">
                        @arXiv 2408.00573
                    </a>
                    <span class="tweet-title">
                        PINNs Get a Speed Boost: Natural Gradient Descent to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Nanjing University of Aeronautics and Astronautics
                    </span>
                </div>
                <div class="primary-text">
                    This paper analyzes the convergence of natural gradient descent (NGD) for training
                    over-parameterized two-layer Physics-Informed Neural Networks (PINNs). Unlike previous work that
                    focused on gradient descent, this study demonstrates that NGD achieves a faster convergence rate
                    and
                    is independent of the smallest eigenvalue of the Gram matrix.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">
                19:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00004" target="_blank">
                        @arXiv 2408.00004
                    </a>
                    <span class="tweet-title">
                        AI's Got Numbers: Teaching Machines to Speak Like Humans (and Format Them Right!)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Karlsruhe Institute of Technology, CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on improving the formatting of numeric expressions in automatic speech
                    recognition (ASR) transcripts. Unlike previous work that relied on post-processing techniques,
                    this
                    study explores both cascaded and end-to-end approaches, utilizing large language models (LLMs)
                    and
                    synthetic data generation to adapt ASR models for accurate numeric expression formatting.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">
                20:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00753" target="_blank">
                        @arXiv 2408.00753
                    </a>
                    <span class="tweet-title">
                        Sleep Tight, Smart Shirt: AI-Powered Garment Tracks Your Snoozing Habits
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a smart garment with a printed strain sensor array that monitors sleep
                    patterns by detecting subtle vibrations from the extrinsic laryngeal muscles. This approach
                    differs
                    from previous work by using a single modality of strain response signals to comprehensively
                    analyze
                    and recognize sleep patterns, eliminating the need for precise positioning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">
                20:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00741" target="_blank">
                        @arXiv 2408.00741
                    </a>
                    <span class="tweet-title">
                        LLMs: Big Brains, Big Energy Bills? DynamoLLM to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Illinois at Urbana-Champaign, Microsoft Azure Research
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the energy efficiency of LLM inference clusters, a topic often
                    overlooked
                    in previous work. It proposes DynamoLLM, a framework that dynamically adjusts the cluster
                    configuration to optimize energy consumption while meeting performance requirements.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">
                21:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00690" target="_blank">
                        @arXiv 2408.00690
                    </a>
                    <span class="tweet-title">
                        MiniCPM Gets a Boost: Tiny Language Models Learn to Speak Big!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on improving text embeddings for smaller language models (LLMs) by using
                    contrastive fine-tuning. Unlike previous work that primarily focused on large LLMs, this study
                    explores the potential of smaller models for text embedding tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">
                21:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00376" target="_blank">
                        @arXiv 2408.00376
                    </a>
                    <span class="tweet-title">
                        Making AI Forget: Can We Unlearn What We've Taught?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This paper focuses on the limitations of machine unlearning in generative AI models,
                    specifically
                    LLMs and image generative models. It goes beyond simply highlighting the potential of unlearning
                    and
                    delves into the challenges and risks associated with it.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">
                21:55
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00458" target="_blank">
                        @arXiv 2408.00458
                    </a>
                    <span class="tweet-title">
                        Reenact Anything: Turning Videos into Motion Magic with Textual Tricks!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for transferring the semantic motion of a reference video
                    to a
                    target image using a pre-trained image-to-video diffusion model. Unlike previous methods that
                    rely
                    on fine-tuning or spatial alignment, this approach optimizes a set of text/image embedding
                    tokens,
                    referred to as a motion-text embedding, directly on the motion reference video.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">
                22:25
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00751" target="_blank">
                        @arXiv 2408.00751
                    </a>
                    <span class="tweet-title">
                        Policy Gradient Gets a Game Face: New Algorithm Solves Imperfect-Information Games!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel policy gradient approach for solving imperfect-information
                    games,
                    using a new regularizer called the "bidilated regularizer" and trajectory Q-values. This differs
                    from previous work that relied on counterfactual values and average-iterate convergence.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">
                22:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00672" target="_blank">
                        @arXiv 2408.00672
                    </a>
                    <span class="tweet-title">
                        AI Coach: Giving You the Play-by-Play for Better Skills!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UT Austin, FAIR Meta, Carnegie Mellon University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on generating actionable feedback from videos of people performing
                    physical
                    activities, going beyond simply scoring demonstrations. It uses a multimodal approach, combining
                    video, 3D body pose, and expert commentary to provide both verbal and visual feedback.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">
                23:10
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00343" target="_blank">
                        @arXiv 2408.00343
                    </a>
                    <span class="tweet-title">
                        Robots That Push: Navigating the World by Moving Obstacles
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich, Technical University of Munich, Intel
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to visual navigation that allows robots to interact
                    with
                    their environment by pushing movable obstacles out of the way. Unlike previous methods that
                    treat
                    the environment as static, this system learns to identify and manipulate objects, enabling more
                    efficient and adaptable navigation strategies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">
                23:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00738" target="_blank">
                        @arXiv 2408.00738
                    </a>
                    <span class="tweet-title">
                        Virchow 2: Pathology's New Supermodel, Trained on 3.1 Million Slides!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft, Paige
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces two new models, Virchow 2 and Virchow 2G, which are trained on a
                    significantly larger dataset of 3.1 million whole slide images (WSIs) compared to previous work.
                    The
                    study also incorporates domain-specific adaptations to the DINOv2 training algorithm,
                    specifically
                    focusing on augmentations and regularization techniques tailored for pathology images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">
                24:01
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00657" target="_blank">
                        @arXiv 2408.00657
                    </a>
                    <span class="tweet-title">
                        Unraveling the Secrets of Text: How Sparse Autoencoders Decode Language Models
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Australian National University, Stanford University, Space Telescope Science Institute
                    </span>
                </div>
                <div class="primary-text">
                    This research applies sparse autoencoders (SAEs) to dense text embeddings from large language
                    models, a novel approach that hasn't been explored before.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">
                24:34
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00196" target="_blank">
                        @arXiv 2408.00196
                    </a>
                    <span class="tweet-title">
                        Music's New Maestro: AI Learns to Mimic Styles, Not Just Notes!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Sorbonne Université CNRS Ircam
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for disentangling timbre and structure representations in
                    audio
                    generation, allowing for more precise control over the generated audio's style and content.
                    Unlike
                    previous approaches that rely on text prompts or limited sets of predefined instruments, this
                    method
                    enables one-shot timbre transfer between arbitrary audio examples.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">
                24:59
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00181" target="_blank">
                        @arXiv 2408.00181
                    </a>
                    <span class="tweet-title">
                        SAM's Got a New Trick: Ultrasound Segmentation with a Chatty Sidekick!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for adapting the Segment Anything Model (SAM) for medical
                    image segmentation, specifically ultrasound images. It differs from previous work by
                    incorporating a
                    frozen convolutional neural network (CNN) branch alongside SAM's original vision transformer
                    (ViT)
                    encoder, using a variational attention fusion module to combine their features. Additionally,
                    the
                    study utilizes text prompts generated by ChatGPT to provide contextual information to SAM,
                    enhancing
                    its understanding of the nuances of ultrasound images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">
                25:31
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00131" target="_blank">
                        @arXiv 2408.00131
                    </a>
                    <span class="tweet-title">
                        Taming Wild Tails: A New Way to Handle Extreme Events with Robust Optimization
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Duke University, Stanford University, George Mason University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework for robustifying extreme value distributions (EVDs)
                    using
                    distributionally robust optimization (DRO). Unlike previous work that focused on univariate
                    EVDs,
                    this paper tackles the more complex multivariate case, where the dependence structure between
                    variables is crucial. The key innovation lies in incorporating max-stability constraints within
                    the
                    DRO framework, ensuring that the robustified distribution retains the essential properties of
                    EVT.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">
                25:59
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00256" target="_blank">
                        @arXiv 2408.00256
                    </a>
                    <span class="tweet-title">
                        Blurry Vision, Clear Results: Federated Learning for Self-Driving Cars
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Jiangnan University, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new federated learning algorithm called FLSimCo, which addresses the
                    issue
                    of image blurring caused by vehicle movement during training. Unlike previous methods, FLSimCo
                    incorporates blur level as a weight during model aggregation, leading to more accurate and
                    stable
                    results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">
                26:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00084" target="_blank">
                        @arXiv 2408.00084
                    </a>
                    <span class="tweet-title">
                        Neural Networks: The New Light in Exoplanet Atmospheres?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Max Planck Society
                    </span>
                </div>
                <div class="primary-text">
                    This research uses physics-informed neural networks (PINNs) to model radiative transfer in
                    exoplanetary atmospheres, specifically focusing on scattering phenomena. Unlike traditional
                    models
                    that simplify scattering as absorption, PINNs directly incorporate the governing differential
                    equations into their loss function, offering a more precise and potentially faster modeling
                    technique.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">
                26:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00611" target="_blank">
                        @arXiv 2408.00611
                    </a>
                    <span class="tweet-title">
                        ASL-DVS: Spiking Neural Networks Learn Sign Language, One Spike at a Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Tennessee
                    </span>
                </div>
                <div class="primary-text">
                    This research utilizes a convolutional spiking neural network (CSNN) architecture to process and
                    classify event-based data from the ASL-DVS dataset, a neuromorphic dataset containing hand
                    gestures
                    for American Sign Language (ASL). This approach differs from previous work by leveraging the
                    temporal and spatial information inherent in event-based data, which is captured by dynamic
                    vision
                    sensors (DVS).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">
                27:20
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.00527" target="_blank">
                        @arXiv 2408.00527
                    </a>
                    <span class="tweet-title">
                        Brain Stiffness: The New Age Game Changer?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge, Cardiff University, University of Delaware
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel contrastive loss function that dynamically focuses on localized
                    neighborhoods of samples during training, improving the accuracy of brain age prediction. This
                    approach differs from previous methods by adapting to the non-uniform distribution of data often
                    found in medical imaging.
                </div>
            </div>
        </div>


        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202408020659_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>

</html>