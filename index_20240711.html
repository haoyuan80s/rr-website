<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - AI Paper Picks of the Day</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 80px;"></div>
            <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                Freshest
                Top Picks:
                <span class="highlightNumber" style="font-size: 28px;">58</span> out of <span
                    class="highlightNumber">250</span> arXiv AI
                papers
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="line-height: 1.2;">just released on</div>
                <div id="data-default-date" data-date="2024-07-11"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 10px;">
            </div>
        </div>

        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">
                00:52
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07726" target="_blank">
                        @arXiv 2407.07726
                    </a>
                    <span class="tweet-title">
                        PaliGemma: A Tiny Vision-Language Model With Big Dreams
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    PaliGemma is a Vision-Language Model (VLM) that uses a smaller image encoder and language model
                    compared to previous VLMs. It achieves comparable performance to larger models, demonstrating the
                    potential for smaller, more efficient VLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">
                01:10
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07140" target="_blank">
                        @arXiv 2407.07140
                    </a>
                    <span class="tweet-title">
                        Top-k Classifiers Get a Cardinality Makeover: Learning to Predict Sets with Finesse!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google, NYU, Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to top-k classification, where the model dynamically
                    adjusts the size of its prediction sets based on the difficulty of the input instance. This differs
                    from previous work that typically uses a fixed size for the prediction set.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">
                01:34
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07263" target="_blank">
                        @arXiv 2407.07263
                    </a>
                    <span class="tweet-title">
                        Tired of Retraining? This Recipe Makes Your Language Model a Master Chef!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nvidia
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on improving the general capabilities of already trained language models
                    through continued pretraining, rather than retraining from scratch. It differs from previous work by
                    focusing on improving general abilities, not just adapting to new domains or data shifts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">
                01:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07860" target="_blank">
                        @arXiv 2407.07860
                    </a>
                    <span class="tweet-title">
                        4DiM: Diffusion Models That Control Space and Time Like a Boss
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google DeepMind
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces 4DiM, a diffusion model for novel view synthesis that can be conditioned on
                    both camera pose and time. Unlike previous work, 4DiM is trained on a mixture of 3D, 4D, and video
                    data, allowing it to generate more realistic and diverse views.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">
                02:16
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07737" target="_blank">
                        @arXiv 2407.07737
                    </a>
                    <span class="tweet-title">
                        Privacy for Your Tweets: How to Train AI Models Without Spilling Your Secrets
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for training large language models (LLMs) with user-level
                    differential privacy, which protects all the data contributed by each user, not just individual
                    examples. This approach is different from previous work that focused on example-level privacy, which
                    can be insufficient to protect user privacy in settings where users contribute multiple correlated
                    examples.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">
                02:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07616" target="_blank">
                        @arXiv 2407.07616
                    </a>
                    <span class="tweet-title">
                        Satellite Images: Seeing the Changes, But Not the Shifts!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        LIGM, Ecole des Ponts, Univ Gustave Eiffel...
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the impact of temporal and spatial domain shifts on satellite image time
                    series semantic change detection (SITS-SCD), an area that has been largely overlooked in previous
                    studies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">
                03:27
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07577" target="_blank">
                        @arXiv 2407.07577
                    </a>
                    <span class="tweet-title">
                        Movie Magic: AI Learns to Remember Who's Who!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        The University of Hong Kong, Tsinghua University, ByteDance
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to visual instruction tuning for Large Vision-Language
                    Models (LVLMs) by incorporating character identity references. This allows the model to associate
                    instances across different scenes, a capability not explored in previous work.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">
                03:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07561" target="_blank">
                        @arXiv 2407.07561
                    </a>
                    <span class="tweet-title">
                        Robot Chef: Feeding You What You Want, Not What's Easy!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Cornell University, Stanford University, University of British Columbia
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces FLAIR, a system that uses foundation models to plan long-horizon bite
                    sequences for robot-assisted feeding. Unlike previous work that focuses on individual food
                    manipulation skills, FLAIR considers the entire meal and incorporates user preferences.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">
                04:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07580" target="_blank">
                        @arXiv 2407.07580
                    </a>
                    <span class="tweet-title">
                        Layout Synthesis Gets a Semantic Makeover: Graph Diffusion Models Take the Stage!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, ByteDance
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces INSTRUCTLAYOUT, a generative framework for 2D and 3D layout synthesis that
                    incorporates a semantic graph prior. Unlike previous methods that implicitly model object relations,
                    INSTRUCTLAYOUT explicitly represents these relationships using a graph, enhancing controllability
                    and fidelity in layout generation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">
                04:45
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07810" target="_blank">
                        @arXiv 2407.07810
                    </a>
                    <span class="tweet-title">
                        LLMs: Not Just Big Brains, But Aligned Ones Too!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto
                    </span>
                </div>
                <div class="primary-text">
                    This research examines the internal workings of LLMs by analyzing the trajectories of individual
                    tokens as they pass through transformer blocks. It introduces the concept of "Transformer
                    Alignment," which describes the alignment of singular vectors in the linearizations of transformer
                    blocks. This is distinct from previous work on Residual Networks, which focused on "Residual
                    Alignment."
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">
                05:10
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07775" target="_blank">
                        @arXiv 2407.07775
                    </a>
                    <span class="tweet-title">
                        Robot Navigation Gets a Multimodal Makeover: Tour Videos and Long-Context VLMs Take the Wheel!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new navigation task called Multimodal Instruction Navigation with Tours
                    (MINT), where a robot navigates based on multimodal instructions (text and images) and a
                    pre-recorded demonstration tour of the environment. This differs from previous work by leveraging
                    long-context Vision-Language Models (VLMs) to understand the tour and instructions, and then using a
                    topological graph to generate navigation actions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">
                05:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07575" target="_blank">
                        @arXiv 2407.07575
                    </a>
                    <span class="tweet-title">
                        Digital Twins Get a Ride: How AI is Optimizing Vehicular Edge Computing
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Jiangnan University, Tsinghua University, Shanghai Jiao Tong University...
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on resource allocation for both digital twin maintenance and computing task
                    processing in vehicular edge computing networks. It differs from previous work by considering the
                    simultaneous demands of these two tasks, which are often treated separately.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">
                06:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07427" target="_blank">
                        @arXiv 2407.07427
                    </a>
                    <span class="tweet-title">
                        Open-Vocabulary Video Segmentation: Aligning Embeddings for Better Object Tracking
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Shandong University, ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This research tackles the problem of open-vocabulary video instance segmentation by introducing a
                    novel unified embedding alignment approach. Unlike previous methods that rely on image-level
                    training, this paper proposes a video-level training paradigm to improve temporal consistency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">
                06:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07755" target="_blank">
                        @arXiv 2407.07755
                    </a>
                    <span class="tweet-title">
                        Neural Surfaces Get a Geometry Makeover: No More Meshing!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University College London
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a new way to represent surfaces using neural networks, called Spherical Neural
                    Surfaces (SNS). Unlike previous methods that rely on discretizing surfaces into meshes, SNS allows
                    for direct computation of geometric operators like normals, curvatures, and the Laplace-Beltrami
                    operator, without any meshing.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">
                06:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07433" target="_blank">
                        @arXiv 2407.07433
                    </a>
                    <span class="tweet-title">
                        AI Gets Its Directions: New Model Generates Customizable Navigation Instructions
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Beihang University, Zhejiang University, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for generating navigation instructions that allows for control
                    over both the style and content of the instructions. Unlike previous models that only generate
                    instructions in a single style, this model can generate instructions in different styles, such as
                    step-by-step or high-level, based on the user's needs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">
                07:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07885" target="_blank">
                        @arXiv 2407.07885
                    </a>
                    <span class="tweet-title">
                        Robots Get a Feel for the World: New Tactile Skin Makes In-Hand Manipulation a Breeze!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Meta, University of Pennsylvania, UC Berkeley...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new tactile skin model that enables zero-shot sim-to-real transfer of
                    shear and normal forces, unlike previous work that often relies on simplified tactile signals.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">
                07:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07848" target="_blank">
                        @arXiv 2407.07848
                    </a>
                    <span class="tweet-title">
                        Transformers: Not All Neurons Are Created Equal!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This research goes beyond previous work by examining how sparsity patterns evolve over the course of
                    training, not just at the end. It also analyzes sparsity at different levels: per-token,
                    per-sequence, and per-batch.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">
                08:11
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07671" target="_blank">
                        @arXiv 2407.07671
                    </a>
                    <span class="tweet-title">
                        Should We Let Robots Decide What's Right?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Carnegie Mellon University, University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This paper explores the reasons why we might want to automate moral decision-making, even without a
                    perfect mathematical framework for ethics. It goes beyond simply arguing for or against AI in moral
                    contexts, instead focusing on the practical considerations and potential benefits of using AI for
                    moral reasoning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">
                08:33
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07279" target="_blank">
                        @arXiv 2407.07279
                    </a>
                    <span class="tweet-title">
                        Deep Learning's New Groove: Unlocking the Secrets of State Space Models in the Frequency Domain
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, Columbia University
                    </span>
                </div>
                <div class="primary-text">
                    This research delves into the learning dynamics of linear state space models (SSMs) by analyzing
                    them in the frequency domain. This approach offers analytical solutions, unlike previous work that
                    primarily focused on time-domain analysis.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">
                08:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07825" target="_blank">
                        @arXiv 2407.07825
                    </a>
                    <span class="tweet-title">
                        Speech Enhancement on Steroids: Real-Time Audio-Visual Magic with 40ms Latency!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Meta, Imperial College London
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on real-time audio-visual speech enhancement (AVSE) with minimal latency.
                    Unlike previous work, it specifically addresses low-SNR scenarios and aims to achieve frame-by-frame
                    enhancement with minimal delay. The paper proposes RT-LA-VocE, a causal AVSE model that re-designs
                    components of the non-causal LA-VocE model to enable real-time inference.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">
                09:20
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07333" target="_blank">
                        @arXiv 2407.07333
                    </a>
                    <span class="tweet-title">
                        Forget Memory, Just Learn to Forget: A New Trick for AI in Partially Observable Worlds
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley, Brown University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces the λ-discrepancy, a metric that measures how well an agent's observations
                    support Markovian value prediction. Unlike previous methods, it doesn't require knowledge of the
                    underlying state space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">
                09:49
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07596" target="_blank">
                        @arXiv 2407.07596
                    </a>
                    <span class="tweet-title">
                        Targeting the Needy While Learning: A Balancing Act for Social Programs
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a framework for designing randomized allocation rules that balance the goal
                    of targeting high-need individuals with learning treatment effects. This differs from previous work
                    that focuses solely on either targeting or learning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">
                10:19
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07700" target="_blank">
                        @arXiv 2407.07700
                    </a>
                    <span class="tweet-title">
                        Conformal Prediction: Outlier-Proofing Your Predictions
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford, University of Tübingen
                    </span>
                </div>
                <div class="primary-text">
                    This research investigates the robustness of split conformal prediction when the calibration data is
                    contaminated with outliers. It quantifies the impact of corrupted data on the coverage and
                    efficiency of prediction sets, and proposes a new method called Contamination Robust Conformal
                    Prediction (CRCP) to address over-coverage in classification settings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">
                10:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07506" target="_blank">
                        @arXiv 2407.07506
                    </a>
                    <span class="tweet-title">
                        AI to the Rescue: How Generative AI is Fixing Broken RF Sensors in the IoT
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        École des Ponts ParisTech
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the use of Generative AI (GenAI) techniques to address challenges in Radio
                    Frequency (RF) sensing within Internet of Things (IoT) systems. Unlike previous work that primarily
                    focused on traditional deep learning methods, this paper investigates the potential of GenAI models
                    to overcome limitations such as noise, interference, and incomplete data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">
                11:23
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07403" target="_blank">
                        @arXiv 2407.07403
                    </a>
                    <span class="tweet-title">
                        AI Vision-Language Models: A Guide to Their (Not-So-Secret) Weaknesses
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, Huazhong University of Science and Technology, Chinese University of Hong
                        Kong
                    </span>
                </div>
                <div class="primary-text">
                    This paper provides a comprehensive survey of attacks targeting Large Vision-Language Models
                    (LVLMs), focusing on the unique challenges and vulnerabilities presented by their multimodal nature.
                    It goes beyond previous surveys by offering a detailed taxonomy of attack methods, including
                    adversarial attacks, jailbreak attacks, prompt injection attacks, and data poisoning/backdoor
                    attacks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">
                11:52
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07402" target="_blank">
                        @arXiv 2407.07402
                    </a>
                    <span class="tweet-title">
                        Action-Aware Video Segmentation: It's Not Just What You See, It's What You Do!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Tokyo
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces ActionVOS, a new approach to video object segmentation that uses action
                    prompts, like "putting a carrot in a bowl," to identify active objects in egocentric videos. This
                    differs from previous methods that rely solely on static attributes like object names.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">
                12:20
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07786" target="_blank">
                        @arXiv 2407.07786
                    </a>
                    <span class="tweet-title">
                        AI Red Teaming: The Human Factor in the AI Arms Race
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU, Microsoft, University of Chicago...
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the human element of AI red teaming, a practice that involves testing AI
                    systems for potential harm. Unlike previous work that primarily focuses on the technical aspects of
                    red teaming, this study examines the social and collaborative aspects of the practice, including the
                    labor involved, the potential for harm to red teamers, and the need for safeguards to protect their
                    well-being.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">
                12:49
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07277" target="_blank">
                        @arXiv 2407.07277
                    </a>
                    <span class="tweet-title">
                        Blood Biomarkers: Your Lifestyle's Secret Code?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel deep metric learning framework to predict future blood biomarker
                    values, incorporating lifestyle factors like physical activity and sleep, which is a departure from
                    previous work that primarily relied on population-level statistics and demographics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">
                13:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07235" target="_blank">
                        @arXiv 2407.07235
                    </a>
                    <span class="tweet-title">
                        Trans Voices, Transformed: A Dataset That Makes Speech Models Squirm!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces the Versatile Voice Dataset (VVD), a collection of audio recordings from
                    three trans-feminine voice teachers who deliberately modify their voices along various axes. This
                    dataset is unique because it focuses on intra-speaker variability, specifically how a single speaker
                    can produce a wide range of vocal textures, challenging the assumptions of current speaker identity
                    models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">
                13:34
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07884" target="_blank">
                        @arXiv 2407.07884
                    </a>
                    <span class="tweet-title">
                        Robots Learn to Peel Like Pros: A Dexterous Manipulation Masterclass
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on constrained dexterous manipulation for food peeling, specifically
                    addressing the challenge of stopping object reorientation and firmly holding it in place for
                    downstream tasks. Unlike previous works that primarily focus on continuous object rotation, this
                    study introduces a framework for training a reorientation controller that can stop and hold the
                    object, making it suitable for tasks like peeling.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">
                13:58
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07586" target="_blank">
                        @arXiv 2407.07586
                    </a>
                    <span class="tweet-title">
                        Object Detection: A Simpler, Smarter Approach to Adapting to New Worlds
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        École Polytechnique Fédérale de Lausanne
                    </span>
                </div>
                <div class="primary-text">
                    This research explores simpler approaches to source-free domain adaptation for object detection,
                    focusing on the importance of batch normalization layers and proposing a novel strategy that
                    combines AdaBN with training on a fixed set of pseudo-labels.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">
                14:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07395" target="_blank">
                        @arXiv 2407.07395
                    </a>
                    <span class="tweet-title">
                        Neural Network Wraps Video Coding: A Slimmer, Faster Way to Stream!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        New York University, Google LLC
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new framework for video coding that uses neural networks to improve
                    compression efficiency while keeping decoding complexity low. Unlike previous approaches that
                    focused on replacing entire coding loops with neural networks, this method uses neural networks as
                    "wrappers" around a standard video codec, allowing for a more practical implementation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">
                14:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07461" target="_blank">
                        @arXiv 2407.07461
                    </a>
                    <span class="tweet-title">
                        NeRF's Got Blurs? Diffusion to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Science and Technology of China, Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This paper tackles the aliasing issue in NeRF renderings by treating it as a degradation problem and
                    leveraging a pretrained diffusion model to restore high-quality images from aliased inputs. This
                    approach differs from previous methods that focused on scene parameterization or regularization
                    techniques.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">
                15:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07666" target="_blank">
                        @arXiv 2407.07666
                    </a>
                    <span class="tweet-title">
                        LLMs in Healthcare: Beyond Accuracy, It's All About S.C.O.R.E.
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Singapore National Eye Centre, Singapore Eye Research Institute, Singapore Health Services...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new evaluation framework called S.C.O.R.E. for assessing large language
                    models (LLMs) in healthcare, focusing on qualitative aspects like safety, consensus, objectivity,
                    reproducibility, and explainability, rather than solely relying on traditional quantitative metrics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">
                15:25
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07338" target="_blank">
                        @arXiv 2407.07338
                    </a>
                    <span class="tweet-title">
                        Expert Knowledge: Unlocking the Secrets of Causal Relationships
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research expands on previous work by considering a more general class of background knowledge,
                    allowing for any kind of (consistent) edge mark orientations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">
                15:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07356" target="_blank">
                        @arXiv 2407.07356
                    </a>
                    <span class="tweet-title">
                        Video In-Context Learning: Teaching AI to Mimic Your Moves!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Science and Technology of China, Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research extends in-context learning to video data, allowing models to generate video sequences
                    based on demonstrations, unlike previous work that focused on image-based tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">
                16:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07735" target="_blank">
                        @arXiv 2407.07735
                    </a>
                    <span class="tweet-title">
                        NeRFs Get Watermarked: A Plug-and-Play Copyright Protector
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Hong Kong Baptist University, Nvidia
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for embedding copyright messages directly into NeRF models
                    during their creation, unlike previous methods that embed watermarks after the model is built.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">
                16:37
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07239" target="_blank">
                        @arXiv 2407.07239
                    </a>
                    <span class="tweet-title">
                        RotRNN: Spinning Long Sequences with a Twist!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University College London
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes RotRNN, a linear recurrent model that uses rotation matrices to address the
                    challenges of state-of-the-art models like SSMs and LRUs. Unlike previous methods, RotRNN simplifies
                    initialization and normalization, leading to a more robust and efficient model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">
                17:01
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07510" target="_blank">
                        @arXiv 2407.07510
                    </a>
                    <span class="tweet-title">
                        Invisible Stripes Trick Self-Driving Cars: A New Kind of Optical Illusion
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nanyang Technological University, University of Pittsburgh, Singapore Institute of Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research differs from previous work by focusing on achieving stable adversarial attacks over a
                    sequence of frames, rather than just single frames. It introduces GhostStripe, a system that adapts
                    to the camera's rolling shutter effect and the victim vehicle's movement to create consistent,
                    invisible stripes that mislead traffic sign recognition.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">
                17:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07612" target="_blank">
                        @arXiv 2407.07612
                    </a>
                    <span class="tweet-title">
                        Transformers Learn Causal Reasoning: From Simple Chains to Complex Graphs!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT, Indian Institute of Technology Hyderabad, Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research explores a novel approach called "axiomatic training" where a transformer model learns
                    causal reasoning directly from symbolic demonstrations of axioms, rather than relying on data
                    generated from those axioms. This differs from previous work that typically incorporates axioms as
                    inductive biases or infers them from data values.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">
                17:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07342" target="_blank">
                        @arXiv 2407.07342
                    </a>
                    <span class="tweet-title">
                        LLMs: Multilingual Blending Makes Safety Go "Poof!"
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Alberta, University of Tokyo
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the safety of LLMs in multilingual contexts, specifically focusing on the
                    impact of mixed-language queries and responses, a concept called "Multilingual Blending." This
                    differs from previous work that primarily focused on single-language safety alignment.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">
                18:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07225" target="_blank">
                        @arXiv 2407.07225
                    </a>
                    <span class="tweet-title">
                        AI-Generated Text? We've Got Eyes for That!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        NYU, IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to AI-generated text detection by using image
                    representations of word embeddings instead of traditional text-based methods. This approach
                    leverages the strengths of vision models, which are known for their efficiency and ability to
                    capture spatial patterns, to analyze text.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">
                18:47
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07364" target="_blank">
                        @arXiv 2407.07364
                    </a>
                    <span class="tweet-title">
                        Traffic Jams? Let's Get Physical! Reinforcement Learning Meets Physics Models for Smarter
                        Routing.
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces TransRL, an algorithm that combines reinforcement learning with physics
                    models for real-time traffic routing. Unlike previous methods that rely solely on either physics
                    models or reinforcement learning, TransRL leverages the strengths of both approaches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">
                19:07
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07827" target="_blank">
                        @arXiv 2407.07827
                    </a>
                    <span class="tweet-title">
                        Can a Neural Network See the "Stability" of a Graph?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        RelationalAI, Rice University
                    </span>
                </div>
                <div class="primary-text">
                    This research explores using convolutional neural networks (CNNs) to predict the stability number of
                    a graph by treating its adjacency matrix as an image. This approach differs from previous work that
                    uses graph neural networks (GNNs) to learn representations directly from the graph structure.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">
                19:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07296" target="_blank">
                        @arXiv 2407.07296
                    </a>
                    <span class="tweet-title">
                        AI-Powered Radiation Therapy: ChatGPT Helps Target Tumors!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University, Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Radformer, a novel network that integrates large language models (LLMs)
                    with a hierarchical vision transformer for 3D medical image segmentation. Unlike previous approaches
                    that rely solely on visual features, Radformer leverages text-rich clinical information alongside
                    visual features to improve the accuracy of target delineation in radiation therapy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">
                19:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07295" target="_blank">
                        @arXiv 2407.07295
                    </a>
                    <span class="tweet-title">
                        Deforming Images with a Diffusion Model: It's Like Stretching a Rubber Band, But for Pictures!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford, Chinese Academy of Medical Sciences
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel diffusion model, DRDM, that focuses on generating deformation fields
                    rather than directly synthesizing images. Unlike previous methods that rely on intensity or latent
                    features, DRDM emphasizes morphological changes through deformation fields, enabling the generation
                    of diverse and anatomically plausible deformations for individual images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">
                20:21
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07218" target="_blank">
                        @arXiv 2407.07218
                    </a>
                    <span class="tweet-title">
                        Machine Learning for Fluids: A Reproducibility Crisis?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Princeton University, Princeton Plasma Physics Laboratory
                    </span>
                </div>
                <div class="primary-text">
                    This research systematically reviews the literature on machine learning (ML) for solving
                    fluid-related partial differential equations (PDEs). It identifies two common issues: weak baselines
                    and reporting biases, which lead to overoptimistic conclusions about the effectiveness of ML-based
                    PDE solvers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">
                20:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07591" target="_blank">
                        @arXiv 2407.07591
                    </a>
                    <span class="tweet-title">
                        Tired of the Same Old Soft Robots? This New Method Finds Designs That Actually Work!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge, CSIRO
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new method called OIDD (Optimizing Initial Design Domain) that combines
                    topology optimization with quality diversity algorithms. This allows for a more comprehensive
                    exploration of the design space, leading to the discovery of diverse and high-performing soft robot
                    designs. Unlike traditional topology optimization methods, which focus on finding a single optimal
                    solution, OIDD dynamically adjusts the design domain during the optimization process, enabling the
                    exploration of a wider range of possibilities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">
                21:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07796" target="_blank">
                        @arXiv 2407.07796
                    </a>
                    <span class="tweet-title">
                        LLMs Play Games: Who's the Tic-Tac-Toe Champ?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Florida Polytechnic University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark for evaluating large language models (LLMs) using
                    grid-based games like Tic-Tac-Toe, Connect Four, and Gomoku. Unlike previous benchmarks that focus
                    on language understanding tasks, this one assesses LLMs' strategic thinking and decision-making
                    abilities in a game context.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">
                21:37
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07873" target="_blank">
                        @arXiv 2407.07873
                    </a>
                    <span class="tweet-title">
                        Sampling Made Easy: A PDE-Powered Approach to Generative Modeling
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        California Institute of Technology, Zuse Institute Berlin, dida Datenschmiede GmbH...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a unified framework for sampling from probability densities using partial
                    differential equations (PDEs). Unlike previous methods that rely on time-reversals or trajectory
                    simulations, this approach leverages physics-informed neural networks (PINNs) for simulation-free
                    optimization, leading to improved mode coverage and faster convergence.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">
                22:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07765" target="_blank">
                        @arXiv 2407.07765
                    </a>
                    <span class="tweet-title">
                        Trees, Privacy, and the Littlestone Dimension: A Tale of Impossible Learning
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Gran Sasso Science Institute (GSSI), Purdue University, Technion...
                    </span>
                </div>
                <div class="primary-text">
                    This research extends the link between differential privacy and online learning to more general
                    settings, including partial concept classes and multiclass classification with infinite label
                    spaces. Unlike previous work that relied on thresholds, this paper directly reasons about
                    Littlestone trees, establishing new Ramsey-type theorems for trees.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">
                22:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07439" target="_blank">
                        @arXiv 2407.07439
                    </a>
                    <span class="tweet-title">
                        SHAPing Up: A New Way to Encode Features for Algorithm Selection in Mixed-Variable Optimization
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Dresden University of Technology, University of Münster, Sorbonne University...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new encoding scheme based on SHAP values to represent categorical
                    variables in mixed-variable optimization problems. This approach differs from previous work that
                    used target-encoding, as it considers the individual contribution of each feature to the prediction,
                    rather than aggregating the average effect across all categories.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">
                23:05
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07627" target="_blank">
                        @arXiv 2407.07627
                    </a>
                    <span class="tweet-title">
                        Fake It 'Til You Make It: Boosting Face Recognition with Realistic 3D Faces
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        École Polytechnique Fédérale de Lausanne, Idiap Research Institute
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the use of image-to-image translation techniques to enhance the realism of
                    3D-rendered facial images, aiming to improve the performance of face recognition systems trained on
                    these synthetic datasets. Unlike previous work that relies on pre-trained face recognition models or
                    identity labels, this study demonstrates performance gains without these prerequisites.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">
                23:30
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07662" target="_blank">
                        @arXiv 2407.07662
                    </a>
                    <span class="tweet-title">
                        Backdoor Buster: Unlearning Bad Behavior in AI with a Pinch of Activation
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        National Institute of Informatics, Technical University of Munich, Imperial College London...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel backdoor mitigation approach using machine unlearning. Unlike
                    previous unlearning methods, this approach is computationally inexpensive and achieves
                    state-of-the-art performance while requiring only a handful of unseen samples for unlearning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">
                23:54
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07541" target="_blank">
                        @arXiv 2407.07541
                    </a>
                    <span class="tweet-title">
                        Swiss DINO: A Vision Transformer That's More Than Just a Pretty Face!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Samsung, University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for personal object search, which involves identifying and
                    localizing personal items in images captured by robotic devices. Unlike previous methods that rely
                    on large foundation models or adaptation training, Swiss DINO uses a pre-trained DINOv2 transformer
                    model and does not require any adaptation training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">
                24:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07858" target="_blank">
                        @arXiv 2407.07858
                    </a>
                    <span class="tweet-title">
                        Building Chatbots That Don't Just Talk, They Know!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nvidia
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on building enterprise-grade chatbots using Retrieval Augmented Generation
                    (RAG) and highlights the challenges and solutions for ensuring content freshness, security, and
                    cost-effectiveness. It differs from previous work by presenting a comprehensive framework called
                    FACTS, which addresses these critical aspects.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">
                24:52
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07325" target="_blank">
                        @arXiv 2407.07325
                    </a>
                    <span class="tweet-title">
                        Billiards Bot: AI Learns to Talk the Talk and Shoot the Pucks!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Shenzhen Motern Technology Co. Ltd.
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new video encoder called CLIP-ViP+ that incorporates local loss
                    calculations to improve spatial correspondence in video understanding. This approach differs from
                    previous work by explicitly modeling spatial relationships between objects and their corresponding
                    positions in the video.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">
                25:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07294" target="_blank">
                        @arXiv 2407.07294
                    </a>
                    <span class="tweet-title">
                        Quantum Computing: HPC's New BFF?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Texas A&amp;M University, Oak Ridge National Laboratory
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the performance of a hybrid quantum machine learning (QML) workflow that
                    combines classical and quantum computations in a high-performance computing (HPC) environment. The
                    study focuses on the practical aspects of running such programs on real HPC systems, analyzing the
                    feasibility and performance of this approach.
                </div>
            </div>
        </div>


        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Listen and learn ^.^</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202407111341_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>

</html>