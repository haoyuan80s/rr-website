<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                    Fresh Picks:
                    <span class="highlightNumber" style="font-size: 28px;">103</span> out of <span
                        class="highlightNumber">487</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-07-23"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>

        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">
                00:46
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15850" target="_blank">
                        @arXiv 2407.15850
                    </a>
                    <span class="tweet-title">
                        Zero-Shot Audio Description: Training-Free, But Still Awesome!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford, Ã‰cole des Ponts ParisTech, Shanghai Jiao Tong University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a training-free method for generating audio descriptions (ADs) for movies
                    and
                    TV series. Unlike previous approaches that rely on fine-tuning large models on AD datasets, this
                    method leverages pre-trained visual-language models (VLMs) and large language models (LLMs) with
                    novel visual and text prompting strategies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">
                01:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15838" target="_blank">
                        @arXiv 2407.15838
                    </a>
                    <span class="tweet-title">
                        Tired of Vision-Language Models Hallucinating? MMInstruct to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces MMInstruct, a visual instruction tuning dataset that addresses
                    limitations
                    in existing datasets by focusing on instruction annotation quality, instruction and image
                    diversity,
                    and cost-effectiveness. Unlike previous datasets that rely heavily on automatic generation,
                    MMInstruct leverages a semi-automatic data engine that combines GPT-4V, GPT-3.5, and manual
                    correction to ensure high-quality and diverse instructions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">
                01:45
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15208" target="_blank">
                        @arXiv 2407.15208
                    </a>
                    <span class="tweet-title">
                        Robots Learn New Tricks with Flow-Based Manipulation!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, Columbia University, J.P. Morgan AI Research...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Im2Flow2Act, a framework that uses object flow as a unifying interface
                    to
                    bridge the gap between human demonstrations and simulated robot data. Unlike previous work that
                    relies on either real-world robot data or task-specific simulations, Im2Flow2Act leverages both
                    cross-embodiment videos and simulated robot play data, enabling robots to learn new manipulation
                    skills more efficiently.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">
                02:16
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15680" target="_blank">
                        @arXiv 2407.15680
                    </a>
                    <span class="tweet-title">
                        Visual Hallucinations: When AI Sees Things That Aren't There!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces HaloQuest, a new dataset for evaluating and mitigating visual
                    hallucinations in vision-language models (VLMs). Unlike previous datasets, HaloQuest leverages
                    both
                    real and synthetic images, allowing for a more comprehensive and scalable evaluation of VLM
                    performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">
                02:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15211" target="_blank">
                        @arXiv 2407.15211
                    </a>
                    <span class="tweet-title">
                        Image Jailbreaks: Can You Trick a Vision-Language Model?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, Anthropic
                    </span>
                </div>
                <div class="primary-text">
                    This research investigates the transferability of image-based "jailbreaks" against
                    vision-language
                    models (VLMs), a type of AI that processes both text and images. Unlike previous work that
                    focused
                    on text-based attacks, this study explores the effectiveness of adversarial images in
                    manipulating
                    VLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">
                03:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15229" target="_blank">
                        @arXiv 2407.15229
                    </a>
                    <span class="tweet-title">
                        Stop the Presses! New AI Alignment Method Makes LLMs More Concise and Less Prone to
                        Overfitting!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Southern California, Microsoft, Information Sciences Institute
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the robustness of preference optimization (*PO) methods for aligning
                    large
                    language models (LLMs) with human preferences. Unlike previous work that primarily focused on
                    finding the best-performing *PO* method through extensive hyperparameter searches, this paper
                    investigates the stability of different *PO* methods across a range of hyperparameters.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">
                03:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14717" target="_blank">
                        @arXiv 2407.14717
                    </a>
                    <span class="tweet-title">
                        Cross-Attention Gets a Privacy Makeover: New Algorithm Keeps Your Secrets Safe!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Adobe, University of Hong Kong, University of Wisconsin-Madison...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel data structure, DPTreeSoftmaxAdaptive, that provides a provable
                    guarantee for differential privacy in cross-attention computations. Unlike previous work that
                    focused on empirical methods, this paper offers a theoretical framework for privacy protection
                    in
                    large generative models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">
                04:03
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15754" target="_blank">
                        @arXiv 2407.15754
                    </a>
                    <span class="tweet-title">
                        Hour-Long Videos? No Problem! New Benchmark Tests AI's Long-Term Memory
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google, Stanford University, University of California Berkeley...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces LONGVIDEOBENCH, a new benchmark for evaluating large multimodal models
                    (LMMs) on their ability to understand long-duration videos. Unlike previous benchmarks that
                    focus on
                    short videos or summary-level tasks, LONGVIDEOBENCH specifically targets the challenge of
                    understanding detailed multimodal information from videos that can be up to an hour long.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">
                04:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14679" target="_blank">
                        @arXiv 2407.14679
                    </a>
                    <span class="tweet-title">
                        Shrinking Super Brains: How to Make Tiny Language Models That Still Know Everything!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nvidia
                    </span>
                </div>
                <div class="primary-text">
                    This research explores a new approach to creating smaller language models by pruning an existing
                    large model and retraining it with a fraction of the original data. Unlike previous work that
                    focuses on either depth or width pruning, this study investigates the effectiveness of combining
                    both techniques.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">
                04:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14916" target="_blank">
                        @arXiv 2407.14916
                    </a>
                    <span class="tweet-title">
                        Stop Saying "Pineapple on Pizza is Bad!" New Research Shows Context is King!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto, Microsoft Research, Johns Hopkins University...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a two-step approach to preference modeling for language models, first
                    identifying the context and then evaluating preference within that context. This differs from
                    previous work that directly models preference without explicitly considering context.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">
                05:27
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15078" target="_blank">
                        @arXiv 2407.15078
                    </a>
                    <span class="tweet-title">
                        Neural Network Compilers: Turning Code into AI!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new technique called "neural surrogate compilation" which directly
                    compiles program text into neural networks, bypassing the traditional approach of training on
                    input-output pairs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">
                05:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14681" target="_blank">
                        @arXiv 2407.14681
                    </a>
                    <span class="tweet-title">
                        Learning to Want What They Want: How AI Internalizes Social Values
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel model of value internalization in reinforcement learning agents,
                    where
                    an agent learns to generate internal rewards based on social feedback from a caregiver, enabling
                    it
                    to persist in goal-directed behavior even when the caregiver is absent. This differs from
                    previous
                    work by explicitly modeling the process of internalizing social rewards and its impact on
                    generalization and continual learning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">
                06:19
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15806" target="_blank">
                        @arXiv 2407.15806
                    </a>
                    <span class="tweet-title">
                        Fingerspelling for Smartphones: A 3 Million Character Dataset for Sign Language Recognition
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This research presents FSboard, a dataset of over 3 million characters of American Sign Language
                    (ASL) fingerspelling collected from 147 Deaf signers using smartphone cameras. This dataset is
                    significantly larger than previous fingerspelling datasets and is specifically designed for a
                    mobile
                    text entry use case.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">
                06:49
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14622" target="_blank">
                        @arXiv 2407.14622
                    </a>
                    <span class="tweet-title">
                        BOND: Making LLMs Smarter, One Sample at a Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes BOND, a new RLHF algorithm that aims to achieve the quality of Best-of-N
                    sampling without the computational overhead. It does this by distilling the Best-of-N strategy
                    into
                    the policy through online distribution matching.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">
                07:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15018" target="_blank">
                        @arXiv 2407.15018
                    </a>
                    <span class="tweet-title">
                        Transformers: They're Not Just for Language Models Anymore!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Allen Institute for AI, University of Washington, Technion
                    </span>
                </div>
                <div class="primary-text">
                    This research investigates how transformer language models answer formatted multiple-choice
                    questions, focusing on the specific mechanisms responsible for selecting the correct answer
                    symbol.
                    Unlike previous work, this study uses vocabulary projection and activation patching to pinpoint
                    the
                    causal role of individual layers and attention heads in the prediction process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">
                07:40
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14957" target="_blank">
                        @arXiv 2407.14957
                    </a>
                    <span class="tweet-title">
                        Neural Networks Learn to Warp Space: A New Way to Match Data in Different Dimensions
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel neural framework for learning optimal transport maps between
                    distributions supported on spaces of different dimensionality. Unlike previous methods that rely
                    on
                    comparable spaces, this approach leverages the concept of strong isomorphism to decompose the
                    transport map into two components: an isomorphism and a Gromov-Monge optimal map.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">
                08:10
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14779" target="_blank">
                        @arXiv 2407.14779
                    </a>
                    <span class="tweet-title">
                        AI's Indian Fashion Faux Pas: When Sarees Become Stereotypes
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington, Pennsylvania State University
                    </span>
                </div>
                <div class="primary-text">
                    This research investigates the impact of text-to-image generators on the representation of
                    non-Western cultures, specifically focusing on Indian contexts. Unlike previous work, it uses a
                    community-centered approach to identify novel forms of representational harm, such as exoticism
                    and
                    cultural misappropriation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">
                08:37
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15703" target="_blank">
                        @arXiv 2407.15703
                    </a>
                    <span class="tweet-title">
                        Astronomers Teach AI to See Stars, Not Just Their Shadows!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto
                    </span>
                </div>
                <div class="primary-text">
                    This research combines a Transformer model, commonly used for language processing, with a
                    Denoising
                    Diffusion Probabilistic Model (DDPM) to estimate probability density distributions for
                    astronomical
                    data. This approach differs from previous work that primarily focused on predicting scalar
                    values
                    with Gaussian uncertainty.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">
                09:04
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15645" target="_blank">
                        @arXiv 2407.15645
                    </a>
                    <span class="tweet-title">
                        Can AI Really Think Like Us? New Test Measures How Well Language Models Mimic Human
                        Knowledge
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new metric called "psychometric alignment" to assess how closely the
                    knowledge distribution of language models (LLMs) aligns with that of humans. Unlike previous
                    work
                    that focused on overall accuracy, this metric analyzes the differences in item functioning
                    between
                    LLMs and humans using Item Response Theory (IRT).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">
                09:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15337" target="_blank">
                        @arXiv 2407.15337
                    </a>
                    <span class="tweet-title">
                        ThermalNeRF: Seeing the Heat, Literally!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research extends radiance field models to include thermal (LWIR) data, allowing for 3D
                    reconstruction of scenes using both visible and infrared light. Unlike previous work, it models
                    material interactions with different wavelengths separately, improving reconstruction quality.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">
                09:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15007" target="_blank">
                        @arXiv 2407.15007
                    </a>
                    <span class="tweet-title">
                        Behavior Cloning: Not All You Need, But Maybe All You Want?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft Research, MIT
                    </span>
                </div>
                <div class="primary-text">
                    This paper revisits the sample complexity of imitation learning, focusing on general policy
                    classes,
                    including deep neural networks. It shows that behavior cloning with the logarithmic loss can
                    achieve
                    horizon-independent sample complexity under certain conditions, challenging the conventional
                    wisdom
                    that online methods are more efficient.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">
                10:19
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15771" target="_blank">
                        @arXiv 2407.15771
                    </a>
                    <span class="tweet-title">
                        Grasping the Unseen: How AI Completes Objects for Robot Hands
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on improving robotic grasping by predicting the occupancy of objects in a
                    scene, even when parts are hidden from view. Unlike previous methods that rely on complete
                    object
                    information, this approach uses a multi-group tri-plane scheme to infer the shape of objects
                    locally
                    around the grasp point.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">
                10:46
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15041" target="_blank">
                        @arXiv 2407.15041
                    </a>
                    <span class="tweet-title">
                        Room Layout Estimation: Ray-Casting Away the Occlusion Blues!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        National Tsing Hua University, Industrial Technology Research Institute, Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel geometry-aware self-training framework for room layout
                    estimation
                    that utilizes a ray-casting formulation to aggregate multiple estimates from different viewing
                    positions. This approach differs from previous work by explicitly incorporating geometry
                    reasoning
                    into the pseudo-labeling process, enabling the model to handle complex room geometries and
                    occluded
                    walls without relying on assumptions like Manhattan World or planar room walls.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">
                11:16
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15268" target="_blank">
                        @arXiv 2407.15268
                    </a>
                    <span class="tweet-title">
                        Radiology Reports: Fact-Checking AI for Accurate Diagnoses
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a fact-aware multimodal retriever that leverages RadGraph to mine
                    factual
                    report pairs, enhancing the accuracy of radiology report generation by multimodal foundation
                    models.
                    This approach differs from previous work by explicitly incorporating factual knowledge into the
                    retrieval process, addressing the issue of factual inaccuracies in generated reports.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">
                11:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15431" target="_blank">
                        @arXiv 2407.15431
                    </a>
                    <span class="tweet-title">
                        Pre-Training and Prompting: A New Recipe for Few-Shot Node Classification on Text-Attributed
                        Graphs
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, University of Edinburgh, Anhui University...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel framework called P2TAG that integrates language models (LMs) and
                    graph neural networks (GNNs) for few-shot node classification on text-attributed graphs (TAGs).
                    Unlike previous methods that train LMs and GNNs separately, P2TAG jointly trains them using a
                    masked
                    language modeling objective, enabling a more efficient and effective approach.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">
                12:10
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15320" target="_blank">
                        @arXiv 2407.15320
                    </a>
                    <span class="tweet-title">
                        Edge AI Gets Graph-tastic: A Love Story of Networks and Intelligence
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Sun Yat-sen University, The Hong Kong University of Science and Technology, Tsinghua
                        University...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces the concept of "Edge Graph Intelligence" (EGI), which explores the
                    reciprocal interplay between graph intelligence (GI) and edge networks. Unlike previous work
                    that
                    either focused on GI models or edge networks in isolation, EGI emphasizes their mutual
                    empowerment
                    and proposes a rating system to measure the degree of their integration.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">
                12:40
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15595" target="_blank">
                        @arXiv 2407.15595
                    </a>
                    <span class="tweet-title">
                        Discrete Flow Matching: A New Way to Generate Text and Code, Without the Autoregressive
                        Blues!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Meta AI, Weizmann Institute
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces Discrete Flow Matching (Discrete FM), a new framework for discrete flow
                    models
                    that generates discrete data like text and code in a non-autoregressive fashion. Unlike previous
                    approaches, Discrete FM leverages a general family of probability paths and provides a unified
                    formulation for the generating probability velocity, directly expressed in terms of learned
                    posteriors and schedulers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">
                13:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15160" target="_blank">
                        @arXiv 2407.15160
                    </a>
                    <span class="tweet-title">
                        Can Transformers Count? A New Study Reveals the Limits of LLMs!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        New York University, Google Research, Tel Aviv University...
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the limitations of transformer architectures in performing simple
                    counting
                    tasks. Unlike previous work that focused on complex tasks, this study investigates the ability
                    of
                    transformers to count the occurrences of specific tokens within a sequence.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">
                13:52
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15070" target="_blank">
                        @arXiv 2407.15070
                    </a>
                    <span class="tweet-title">
                        Gaussian Gurus: New Head Model Makes Avatars Look Real!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, NNKosmos
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel 3D Gaussian Parametric Head Model, which uses 3D Gaussians to
                    represent the complexities of the human head. This approach differs from previous methods that
                    relied on mesh-based or NeRF-based representations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">
                14:11
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14774" target="_blank">
                        @arXiv 2407.14774
                    </a>
                    <span class="tweet-title">
                        Turning Text into Art: A Deep Dive into Intelligent Typography
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive overview of artistic text generation, focusing on two
                    main
                    categories: artistic text stylization and semantic typography. It also explores the
                    incorporation of
                    motion for dynamic artistic text generation. The paper highlights the unique challenges of
                    artistic
                    text generation, such as maintaining legibility while incorporating visual effects and semantic
                    meaning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">
                14:37
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15354" target="_blank">
                        @arXiv 2407.15354
                    </a>
                    <span class="tweet-title">
                        BEV-erly Hills Cop: New 3D Object Detection with Vector Queries
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Hong Kong University of Science and Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to 3D object detection using a high-resolution vector
                    representation. Unlike traditional BEV (Bird's-Eye-View) methods that rely on dense grids,
                    VectorFormer factorizes the BEV representation into two low-rank vector queries, enabling
                    efficient
                    modeling of crucial regions at a finer granularity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">
                15:01
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15021" target="_blank">
                        @arXiv 2407.15021
                    </a>
                    <span class="tweet-title">
                        Stop the Information Overload! LLMs Get a Memory Makeover with JSON
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of British Columbia, Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to incremental summarization using structured
                    knowledge
                    representations, specifically JSON, to address the information overload issue faced by LLMs when
                    processing extensive input contexts. Unlike previous methods that rely on unstructured memory,
                    this
                    approach leverages JSON's ability to organize information into distinct, easily accessible
                    segments,
                    facilitating efficient updates and retrievals.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">
                15:23
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15837" target="_blank">
                        @arXiv 2407.15837
                    </a>
                    <span class="tweet-title">
                        Masked Image Modeling Goes Latent: Unlocking High-Level Semantics Without Fine-Tuning!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Wisconsin-Madison, Carnegie Mellon University
                    </span>
                </div>
                <div class="primary-text">
                    This research explores Latent Masked Image Modeling (Latent MIM), a new approach to
                    self-supervised
                    visual representation learning. Unlike traditional MIM methods that reconstruct pixels, Latent
                    MIM
                    reconstructs latent representations, aiming to capture higher-level semantics without relying on
                    supervised fine-tuning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">
                15:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14532" target="_blank">
                        @arXiv 2407.14532
                    </a>
                    <span class="tweet-title">
                        AIOps Algorithms Get a Real-Time Workout: New Benchmark Puts Them Through Their Paces!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nankai University, CNIC CAS, Microsoft...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark called MicroServo that evaluates AIOps algorithms using
                    real-time datasets generated from a live microservice system. Unlike previous benchmarks that
                    rely
                    on offline datasets, MicroServo simulates various operation scenarios, making the evaluation
                    more
                    realistic and relevant to real-world applications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">
                16:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15516" target="_blank">
                        @arXiv 2407.15516
                    </a>
                    <span class="tweet-title">
                        LLMs: Attention is All You Need... But Not All of It!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University College London, University of Edinburgh
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the impact of dropping attention and MLP layers during inference of
                    Llama-v2
                    models, focusing on the performance trade-off when removing deeper layers. This approach differs
                    from previous work that primarily focused on dropping layers from the end of the model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">
                16:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15815" target="_blank">
                        @arXiv 2407.15815
                    </a>
                    <span class="tweet-title">
                        Robots Learn to See the World, No Matter What!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Maniwhere, a framework that enables robots to learn visual tasks and
                    generalize to different camera viewpoints, visual appearances, and even different robot
                    embodiments.
                    Unlike previous methods that focus on a single type of visual change, Maniwhere tackles multiple
                    visual generalization types simultaneously.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">
                17:16
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15187" target="_blank">
                        @arXiv 2407.15187
                    </a>
                    <span class="tweet-title">
                        HoloDreamer: Turning Text into 3D Panoramas, One Splat at a Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, National University of Singapore
                    </span>
                </div>
                <div class="primary-text">
                    HoloDreamer differs from previous methods by directly generating a high-resolution panorama from
                    text prompts, then using 3D Gaussian Splatting for fast and robust 3D reconstruction. This
                    approach
                    avoids the limitations of progressive outpainting, which can lead to visual inconsistencies and
                    chaotic objects.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">
                17:47
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15839" target="_blank">
                        @arXiv 2407.15839
                    </a>
                    <span class="tweet-title">
                        Driving Like a Boss: New AI Learns to Navigate Intersections Without Crashing
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, UC Riverside, Honda Research Institute
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel training framework that integrates guided meta reinforcement
                    learning with importance sampling (IS) to optimize training distributions for navigating highly
                    interactive driving scenarios. Unlike traditional methods that may underrepresent critical
                    interactions or overemphasize extreme cases during training, this approach strategically adjusts
                    the
                    training distribution towards more challenging driving behaviors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">
                18:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14563" target="_blank">
                        @arXiv 2407.14563
                    </a>
                    <span class="tweet-title">
                        Generative VLMs: Turning Detection Datasets into Grounding Goldmines!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Brown University, Google DeepMind, Google Cloud
                    </span>
                </div>
                <div class="primary-text">
                    This research explores using generative vision-language models (VLMs) to automatically generate
                    annotations for visual grounding datasets, bypassing the need for expensive human labeling.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">
                18:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15317" target="_blank">
                        @arXiv 2407.15317
                    </a>
                    <span class="tweet-title">
                        Open-CD: Change Detection's New Playground!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Sun Yat-sen University, Ateneo Pontificio Regina Apostolorum, Wuhan University...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Open-CD, a comprehensive toolbox for change detection, which differs
                    from
                    previous work by offering a unified platform for training, inference, and data analysis,
                    encompassing a wide range of change detection methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">
                19:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15797" target="_blank">
                        @arXiv 2407.15797
                    </a>
                    <span class="tweet-title">
                        Lidar Labeling: One Click, One Thousandth the Work!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Valeo.ai, LIGM, Ecole des Ponts...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for annotating lidar point clouds, dubbed MILAN, which
                    leverages self-supervised representations to significantly reduce the annotation effort. Unlike
                    previous methods that rely on iterative selection of data points or regions, MILAN directly
                    selects
                    a small subset of scans and clusters points within those scans based on their features.
                    Annotators
                    only need to label the cluster centers, and the labels are then propagated to all points within
                    the
                    cluster. This approach achieves a three-order-of-magnitude reduction in the number of manually
                    labeled points while maintaining performance close to fully annotated training sets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">
                19:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15620" target="_blank">
                        @arXiv 2407.15620
                    </a>
                    <span class="tweet-title">
                        Out-of-Distribution Recommendations: When Your Algorithm Needs a Tune-Up!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        IEEE
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel Dual Test-Time Training framework (DT3OR) for
                    out-of-distribution
                    (OOD) recommendation systems. Unlike previous methods that rely on retraining or interventions
                    during the training phase, DT3OR adapts the model during the test phase using self-supervised
                    learning tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">
                20:04
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15047" target="_blank">
                        @arXiv 2407.15047
                    </a>
                    <span class="tweet-title">
                        Video QA: Don't Just Watch, Ask the Right Questions!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, Huawei
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel frame selection strategy for VideoQA, using three scoring
                    mechanisms to evaluate the importance of each frame for a given question. This differs from
                    previous
                    work by incorporating a differentiable adaptive frame sampling mechanism, enabling end-to-end
                    training for the frame selector and answer generator.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">
                20:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15167" target="_blank">
                        @arXiv 2407.15167
                    </a>
                    <span class="tweet-title">
                        Brain-Hacking Images: AI Makes Your Brain See Better!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Fudan University, Microsoft Research
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a closed-loop AI system called the VEP Booster that generates visual
                    stimuli
                    tailored to individual brain responses, improving the reliability and stability of EEG
                    biomarkers
                    compared to traditional methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">
                20:45
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14933" target="_blank">
                        @arXiv 2407.14933
                    </a>
                    <span class="tweet-title">
                        AI's Data Diet: The Web is Going on a Consent-Only Plan
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google, Meta, OpenAI...
                    </span>
                </div>
                <div class="primary-text">
                    This research goes beyond analyzing AI training datasets and instead examines the web domains
                    those
                    datasets are derived from, tracking how consent preferences for data use are changing over time.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">
                21:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15441" target="_blank">
                        @arXiv 2407.15441
                    </a>
                    <span class="tweet-title">
                        LLMs Gone Wild? Microsoft's New Service Tames Hallucinating AI
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a federated hallucination detection and mitigation framework that
                    combines
                    multiple AI techniques, including named entity recognition (NER), natural language inference
                    (NLI),
                    and span-based detection (SBD), to identify and correct errors in LLM outputs. This approach
                    differs
                    from previous work by focusing on a multi-source ensemble for more comprehensive detection and
                    leveraging GPT4 for both data labeling and rewriting.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">
                21:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14982" target="_blank">
                        @arXiv 2407.14982
                    </a>
                    <span class="tweet-title">
                        GreenStableYolo: Making AI Art Faster and Better (Without Breaking the Bank)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Loughborough University, Beijing University of Posts and Telecommunications, University of
                        Lâ€™Aquila...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces GreenStableYolo, a system that optimizes Stable Diffusion for both
                    image
                    quality and inference time, unlike previous work that focused solely on image quality.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">
                21:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15186" target="_blank">
                        @arXiv 2407.15186
                    </a>
                    <span class="tweet-title">
                        LLMs for Text-to-SQL: From Zero to Hero (and Back Again)?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the use of Large Language Models (LLMs) for Text-to-SQL tasks,
                    specifically
                    exploring the effectiveness of prompt engineering and fine-tuning methods. It distinguishes
                    itself
                    from previous work by examining the application of LLMs in this domain, highlighting the
                    challenges
                    and opportunities presented by these powerful models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">
                22:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15792" target="_blank">
                        @arXiv 2407.15792
                    </a>
                    <span class="tweet-title">
                        Outlier Outmaneuvered: New Algorithm Finds Tiny Groups in Noisy Data
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich, TU Munich, Lucerne School of Computer Science and Information Technology...
                    </span>
                </div>
                <div class="primary-text">
                    This paper tackles the problem of estimating the means of well-separated mixtures when outliers
                    overwhelm small groups. Unlike previous work that assumes outliers are less than the smallest
                    group,
                    this research focuses on the scenario where outliers can outnumber smaller groups. The paper
                    proposes a new algorithm that leverages the mixture structure to partially cluster samples
                    before
                    iterating a base learner for list-decodable mean estimation at different scales.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">
                22:44
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15202" target="_blank">
                        @arXiv 2407.15202
                    </a>
                    <span class="tweet-title">
                        Pre-trained Models Get a Boost: Drug Discovery with Nearest Neighbors!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Renmin University of China, Peking University, Microsoft...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach called kNN-DTA, which uses nearest neighbor retrieval to
                    enhance the performance of pre-trained drug-target affinity (DTA) prediction models. Unlike
                    traditional methods that rely on chemical similarity, kNN-DTA leverages embeddings extracted
                    from
                    the pre-trained model to find similar drug-target pairs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">
                23:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15527" target="_blank">
                        @arXiv 2407.15527
                    </a>
                    <span class="tweet-title">
                        Deep Learning Gets a Logic Lesson: New Model Makes AI Decisions Transparent
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        KU Leuven, UniversitÃ  della Svizzera Italiana, University of Cambridge...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new type of Concept Bottleneck Model (CBM) called Concept-based
                    Memory
                    Reasoner (CMR). Unlike previous CBMs, CMR's task predictor uses a memory of learnable logic
                    rules,
                    allowing for inspection, intervention, and verification of the decision-making process before
                    deployment.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">
                23:41
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15233" target="_blank">
                        @arXiv 2407.15233
                    </a>
                    <span class="tweet-title">
                        Layout Generation: Diffusion Models Get a Grip on Graphic Design!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a transformer-based diffusion model for content-aware layout
                    generation,
                    addressing the imbalance between content and graphic features in previous methods. It introduces
                    a
                    content and graphic balance weight to adjust the interaction process between layout
                    representations
                    and image embeddings, optimizing the layout generation space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">
                24:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14911" target="_blank">
                        @arXiv 2407.14911
                    </a>
                    <span class="tweet-title">
                        Plant Pests? No Problem! New AI Model Uses "Masked" Images to Spot Trouble
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        China Agricultural University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new pre-training method for plant pest and disease classification
                    models.
                    It combines Masked Image Modeling (MIM) with contrastive learning, which is different from
                    previous
                    approaches that primarily relied on convolutional neural networks (CNNs).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">
                24:21
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15739" target="_blank">
                        @arXiv 2407.15739
                    </a>
                    <span class="tweet-title">
                        Beyond Road Scenes: Detecting Anomalies with Diffusion Models
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Freiburg
                    </span>
                </div>
                <div class="primary-text">
                    This research extends out-of-distribution (OoD) detection for semantic segmentation beyond the
                    typical road scene domain. It introduces a new benchmark, ADE-OoD, which features a wider range
                    of
                    semantic categories and diverse indoor and outdoor scenes. The paper also proposes a novel
                    approach,
                    DOoD, that utilizes diffusion models for OoD detection, specifically designed to handle the
                    increased semantic diversity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">
                24:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15762" target="_blank">
                        @arXiv 2407.15762
                    </a>
                    <span class="tweet-title">
                        Multi-Objective Language Models: Steering the Chatbot with a Twist of the Knob
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces Conditional Language Policies (CLP), a framework for fine-tuning language
                    models on multiple objectives. Unlike previous approaches that rely on prompt engineering or
                    training separate models for each objective, CLP uses parameter-efficient multi-task training to
                    learn a single model that can be steered to generate outputs that trade off different objectives
                    at
                    inference time.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">
                25:23
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15464" target="_blank">
                        @arXiv 2407.15464
                    </a>
                    <span class="tweet-title">
                        Federated Learning: Don't Be a Copycat, Embrace the Diversity Bonus!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Beihang University, University of Chicago, Hong Kong Polytechnic University
                    </span>
                </div>
                <div class="primary-text">
                    This research challenges the common assumption in personalized federated learning (PFL) that
                    clients
                    benefit most from collaborating with those having similar data distributions. It proposes a new
                    method, DiversiFed, that encourages clients to learn from dissimilar clients by pushing their
                    models
                    apart in the parameter space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">
                25:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15403" target="_blank">
                        @arXiv 2407.15403
                    </a>
                    <span class="tweet-title">
                        Suboptimal Robot? No Problem! Graph Search and Retrieval to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel algorithm called GSR (Graph Search and Retrieval) that learns
                    from
                    suboptimal demonstrations by directly identifying and stitching good behaviors through graph
                    representation and retrieval, without relying on deep reinforcement learning (RL) updates. This
                    approach sidesteps the instability and fragility issues often encountered in offline deep RL
                    methods, particularly in complex environments with high-dimensional visual inputs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">
                26:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15366" target="_blank">
                        @arXiv 2407.15366
                    </a>
                    <span class="tweet-title">
                        LLMs Learn Empathy: Perspective-Taking Prompts Make AI Less Toxic!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Nanyang Technological University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel prompting strategy called "perspective-taking prompting" (PET)
                    that
                    encourages LLMs to consider diverse human perspectives and self-regulate their responses. Unlike
                    previous methods that rely on white-box access or extensive training, PET operates in a
                    black-box
                    scenario and requires no additional training data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">
                26:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15504" target="_blank">
                        @arXiv 2407.15504
                    </a>
                    <span class="tweet-title">
                        Prompt Compression: How Much Can We Squeeze Before It Pops?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        EPFL, UT Austin
                    </span>
                </div>
                <div class="primary-text">
                    This research formalizes the problem of prompt compression for large language models (LLMs) and
                    presents a rate-distortion framework to unify existing methods. It derives the distortion-rate
                    function as a linear program and provides an efficient algorithm to compute this fundamental
                    limit.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">
                27:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15831" target="_blank">
                        @arXiv 2407.15831
                    </a>
                    <span class="tweet-title">
                        Stop the Negative Nancy! New Text Embedding Model Mines Hard Negatives Like a Pro
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nvidia
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on improving text embedding models by exploring different methods for
                    mining
                    hard-negative passages during training. Unlike previous work that often overlooks or
                    inadequately
                    describes this process, the authors introduce a novel family of positive-aware mining methods
                    that
                    leverage the positive relevance score for more effective false negative removal.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">
                27:47
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15567" target="_blank">
                        @arXiv 2407.15567
                    </a>
                    <span class="tweet-title">
                        FedAvg's Secret Weapon: Data Heterogeneity Doesn't Have to Be a Downer!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Utah, IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new metric called the "heterogeneity-driven pseudo-Lipschitz
                    constant"
                    (Lh) to better understand the impact of data heterogeneity in federated learning. Unlike
                    previous
                    work that relied on the local Lipschitz constant, this new metric focuses on the difference
                    between
                    the averaged model and the centralized model, providing a more accurate picture of the error
                    caused
                    by local updates.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">
                28:20
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14958" target="_blank">
                        @arXiv 2407.14958
                    </a>
                    <span class="tweet-title">
                        Motion Transfer Without the Rigmarole: New Method Makes Animation a Breeze!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University College London, Adobe Research, University Of Montreal
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Temporal Residual Jacobians, a novel representation for data-driven
                    motion
                    transfer. Unlike previous methods that rely on rigging or intermediate shape keyframes, this
                    approach directly predicts local geometric and temporal changes, integrating them spatially and
                    temporally to produce the final animated meshes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">
                28:42
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15124" target="_blank">
                        @arXiv 2407.15124
                    </a>
                    <span class="tweet-title">
                        Patent Reactions: Unmasking the Chemistry Behind Innovation
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on extracting reaction spans from chemical patents, a task that has
                    received
                    limited attention in previous work. The authors propose several novel approaches, including the
                    use
                    of BERT embeddings and a special [CHEM] token to improve generalization across different domains
                    of
                    chemical patents.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">
                29:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15786" target="_blank">
                        @arXiv 2407.15786
                    </a>
                    <span class="tweet-title">
                        Reinforcement Learning Gets a Brain: Learning Concepts with Fewer Labels!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new training scheme for reinforcement learning (RL) algorithms that
                    can
                    learn concept-based policies with limited or no human labels. Unlike previous work that relies
                    on
                    continuous human annotation, this approach interleaves concept learning and RL training,
                    actively
                    selects informative data points for labeling, and decorrelates the concept data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon" title="Play from here">
                29:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14653" target="_blank">
                        @arXiv 2407.14653
                    </a>
                    <span class="tweet-title">
                        Offline Safe RL Gets a Data Makeover: OASIS Shapes Up for Success!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces OASIS, a new approach to offline safe reinforcement learning that uses a
                    conditional diffusion model to reshape the data distribution. Unlike previous methods that rely
                    on
                    regularization, OASIS generates new data points that are more aligned with the desired safe and
                    rewarding behavior.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon" title="Play from here">
                30:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15488" target="_blank">
                        @arXiv 2407.15488
                    </a>
                    <span class="tweet-title">
                        DiffX: Laying Down the Law for Cross-Modal Image Generation
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Zhejiang University, Monash University, University of Nottingham...
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces DiffX, a novel diffusion model for generating cross-modal "RGB+X" images,
                    where X represents additional data like thermal or depth information. Unlike previous work that
                    focuses on generating RGB images alone, DiffX simultaneously generates both RGB and X images,
                    guided
                    by layout conditions and text descriptions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon" title="Play from here">
                30:37
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15549" target="_blank">
                        @arXiv 2407.15549
                    </a>
                    <span class="tweet-title">
                        LLMs Gone Rogue? Targeted Latent Training to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Georgia Institute of Technology, University of Maryland, MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces targeted latent adversarial training (LAT), a technique that perturbs
                    the
                    hidden representations of LLMs to elicit specific undesirable behaviors. Unlike previous work,
                    which
                    focused on untargeted attacks, this approach aims to remove specific harmful capabilities by
                    directly targeting them.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet67">
            <div class="start-time-icon" title="Play from here">
                31:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14516" target="_blank">
                        @arXiv 2407.14516
                    </a>
                    <span class="tweet-title">
                        RobocupGym: Kicking Goals in RL with a Simulated Soccer League!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces RobocupGym, a new reinforcement learning (RL) environment based on the
                    3D
                    simulation league of Robocup, a robotic football competition. Unlike previous RL environments
                    that
                    focus on video games or lack real-world applicability, RobocupGym provides a challenging
                    continuous
                    control domain for training RL agents in a robotics context.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet68">
            <div class="start-time-icon" title="Play from here">
                31:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15352" target="_blank">
                        @arXiv 2407.15352
                    </a>
                    <span class="tweet-title">
                        Fact-Checking the News: A Giant Dataset for Event Factuality Detection
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces MAVEN-FACT, a large-scale dataset for event factuality detection, which
                    is
                    significantly larger than previous datasets and includes annotations for supporting evidence.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet69">
            <div class="start-time-icon" title="Play from here">
                32:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15131" target="_blank">
                        @arXiv 2407.15131
                    </a>
                    <span class="tweet-title">
                        Token-Picker: Pruning Unimportant Words in Text Generation for a Speedier Chatbot!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Korea Advanced Institute of Science and Technology, Samsung
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for pruning unimportant tokens in text generation by
                    estimating
                    their probability before all calculations are complete. This differs from previous methods that
                    relied on fixed pruning ratios or required retraining.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet70">
            <div class="start-time-icon" title="Play from here">
                32:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15462" target="_blank">
                        @arXiv 2407.15462
                    </a>
                    <span class="tweet-title">
                        Retrieval Revolution: MoL Makes Learned Similarities Speedy!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft, Meta
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes using Mixture-of-Logits (MoL) to approximate learned similarity
                    functions,
                    enabling efficient retrieval in recommendation systems. Unlike previous work that focused on dot
                    products or specific neural network architectures, MoL offers a universal approach for handling
                    diverse learned similarity functions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet71">
            <div class="start-time-icon" title="Play from here">
                32:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14859" target="_blank">
                        @arXiv 2407.14859
                    </a>
                    <span class="tweet-title">
                        Particle Physics Gets a Data Diet: Graph Neural Networks Go on a Slim-Down
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Sapienza University of Rome, European Organization for Nuclear Research
                    </span>
                </div>
                <div class="primary-text">
                    This research integrates data attribution techniques, specifically TracIn, into the graph
                    classification pipeline for high-energy particle physics. This approach aims to improve the
                    accuracy
                    and efficiency of collision event prediction by identifying and removing non-contributory
                    training
                    samples.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet72">
            <div class="start-time-icon" title="Play from here">
                33:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15073" target="_blank">
                        @arXiv 2407.15073
                    </a>
                    <span class="tweet-title">
                        LLMs Go Multi-Agent: Debating Their Way to Causal Discovery!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the use of multiple LLM agents, each with its own reasoning and
                    knowledge, to
                    tackle causal discovery problems. Unlike previous work that focuses on single LLMs or
                    traditional
                    statistical methods, this approach leverages the collaborative and debating capabilities of
                    multiple
                    agents.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet73">
            <div class="start-time-icon" title="Play from here">
                33:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14880" target="_blank">
                        @arXiv 2407.14880
                    </a>
                    <span class="tweet-title">
                        Blurred Vision? No Problem! New AI Sharpens Images with a Twist
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Kuaishou Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset specifically for blurred images, called ReBlurSR, and a
                    framework called PBaSR that can handle both blurred and general images without extra processing.
                    This is different from previous work that often struggled with blurred images or required
                    separate
                    models for each type of image.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet74">
            <div class="start-time-icon" title="Play from here">
                34:16
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15731" target="_blank">
                        @arXiv 2407.15731
                    </a>
                    <span class="tweet-title">
                        Fine-Tuning Vision-Language Models: A New Measure to Predict Learning and Forgetting
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces the Inter-Intra Modal Measure (IIMM), a new metric that predicts how
                    much a
                    vision-language model will improve or worsen after fine-tuning. Unlike previous methods, IIMM
                    considers both the image and text embedding spaces, providing a more comprehensive assessment of
                    model performance changes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet75">
            <div class="start-time-icon" title="Play from here">
                34:41
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14616" target="_blank">
                        @arXiv 2407.14616
                    </a>
                    <span class="tweet-title">
                        Deep Learning Makes 3D Heart Maps From 2D X-Rays: No More Guesswork!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research uses deep learning to reconstruct 3D coronary trees from just two 2D X-ray
                    projections, unlike previous methods that required more projections or manual intervention.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet76">
            <div class="start-time-icon" title="Play from here">
                35:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15282" target="_blank">
                        @arXiv 2407.15282
                    </a>
                    <span class="tweet-title">
                        Point Transformer Goes Extreme: Multi-Frame Training Wins Waymo Challenge!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        HKU, SH AI Lab, NUS...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to semantic segmentation in 3D point clouds by
                    incorporating
                    multi-frame training and a non-clipping point policy. This differs from previous work by
                    leveraging
                    information from past LiDAR frames to improve the perception of distant objects, which are often
                    poorly sampled in single-frame data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet77">
            <div class="start-time-icon" title="Play from here">
                35:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15171" target="_blank">
                        @arXiv 2407.15171
                    </a>
                    <span class="tweet-title">
                        Don't Judge a Generated Image by Its Cover: New Method Assesses Quality from the Inside Out
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stony Brook University, EPFL
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for assessing the quality of generated samples by directly
                    examining the latent space of the generative model itself, rather than relying on pre-trained
                    feature extractors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet78">
            <div class="start-time-icon" title="Play from here">
                36:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15339" target="_blank">
                        @arXiv 2407.15339
                    </a>
                    <span class="tweet-title">
                        Deep Learning: Economists Get a Leg Up on Big Data
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on using deep learning to extract structured information from unstructured
                    text and image datasets, particularly in contexts where the ground truth is uncontroversial but
                    extraction needs to be automated due to the massive scale of the problem. This differs from
                    previous
                    work by emphasizing the use of deep learning for data exploration and imputation of
                    low-dimensional
                    structured data, rather than solely focusing on prediction tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet79">
            <div class="start-time-icon" title="Play from here">
                36:30
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15498" target="_blank">
                        @arXiv 2407.15498
                    </a>
                    <span class="tweet-title">
                        Chinese Spelling Correction: Cleaning Up the Mess with Confidence!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on refining Chinese spelling correction (CSC) corpora by leveraging model
                    calibration. Unlike previous work that primarily focused on model design, this paper proposes a
                    data-centric approach to improve CSC performance by filtering noisy samples from OCR/ASR-based
                    datasets using a well-calibrated CSC model trained on random replacement data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet80">
            <div class="start-time-icon" title="Play from here">
                36:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15569" target="_blank">
                        @arXiv 2407.15569
                    </a>
                    <span class="tweet-title">
                        ChatGPT's New Trick: Thinking Out Loud with Retrieved Knowledge!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces RAFT, a method that combines retrieval augmented generation (RAG) with
                    chain-of-thought (CoT) for fine-tuning smaller language models. This approach aims to improve
                    the
                    reasoning abilities of these models by integrating external knowledge and prompting them to
                    think
                    through their reasoning steps.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet81">
            <div class="start-time-icon" title="Play from here">
                37:15
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15083" target="_blank">
                        @arXiv 2407.15083
                    </a>
                    <span class="tweet-title">
                        Rocket Landing: From 8% to 97% Success with a Jump Start!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, LandSpace Technology Corporation
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach called Random Annealing Jump Start (RAJS) for
                    reinforcement learning (RL) in goal-oriented tasks. RAJS leverages prior feedback controllers as
                    guide policies to facilitate exploration and learning, unlike previous methods that rely on
                    extensive exploration or manually designed rewards.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet82">
            <div class="start-time-icon" title="Play from here">
                37:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15350" target="_blank">
                        @arXiv 2407.15350
                    </a>
                    <span class="tweet-title">
                        Pedestrians Take the Wheel: New Dataset Captures Fine-Grained Traffic Behavior
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Woven by Toyota, The University of Tokyo, Santa Clara University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset, WTS, that focuses on pedestrian behavior in traffic
                    scenarios, unlike previous datasets that primarily focused on vehicle and driver behavior.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet83">
            <div class="start-time-icon" title="Play from here">
                38:05
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14541" target="_blank">
                        @arXiv 2407.14541
                    </a>
                    <span class="tweet-title">
                        Big Data, Big Problems? How to Fix Bias in Mobility Data
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Southwest Jiaotong University, University of Washington, University of South Florida...
                    </span>
                </div>
                <div class="primary-text">
                    This research goes beyond simply acknowledging bias in big mobility data (BMD) and proposes a
                    new
                    mitigation method using data standardization. Previous studies often relied on linear scaling,
                    which
                    this paper shows is ineffective due to varying relationships across regions and over time.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet84">
            <div class="start-time-icon" title="Play from here">
                38:31
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14662" target="_blank">
                        @arXiv 2407.14662
                    </a>
                    <span class="tweet-title">
                        Neural Networks: More Than Just a Bag of Features?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This paper explores the concept of relational composition in neural networks, focusing on how
                    feature vectors are combined to represent complex relationships. It goes beyond the traditional
                    "bag
                    of features" model, which assumes that information is simply a sum of individual features.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet85">
            <div class="start-time-icon" title="Play from here">
                38:54
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14876" target="_blank">
                        @arXiv 2407.14876
                    </a>
                    <span class="tweet-title">
                        Seizure Prediction: Not Just When, But How Long?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        GROW Research Institute for Oncology and Reproduction, Maastricht University Medical
                        Centre+,
                        MuirMaxwell Epilepsy Centre...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new metric, the Continuous Input-Output Performance Ratio (CIOPR), to
                    evaluate seizure prediction models. Unlike traditional metrics that focus on accuracy at
                    specific
                    time points, CIOPR considers the model's behavior over continuous EEG data, taking into account
                    prediction time, output stability, and transition time between states.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet86">
            <div class="start-time-icon" title="Play from here">
                39:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15199" target="_blank">
                        @arXiv 2407.15199
                    </a>
                    <span class="tweet-title">
                        360Â° Vision for Safer Cycling: How AI Tracks Overtakes in Panoramic Videos
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University College London
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on adapting existing object detection and tracking models to panoramic
                    videos,
                    specifically addressing the challenges of distorted objects and boundary continuity in
                    equirectangular projections.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet87">
            <div class="start-time-icon" title="Play from here">
                40:03
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15730" target="_blank">
                        @arXiv 2407.15730
                    </a>
                    <span class="tweet-title">
                        Sun's Got a New Compression Suit: Transformers Take on Solar Data!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        West Virginia University, NASA Goddard Space Flight Center
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a Transformer-based video compression approach for the Solar Dynamics
                    Observatory (SDO) mission, leveraging both spatial and temporal redundancies in the data. Unlike
                    previous work, this approach utilizes a Hybrid CNN-MLP Window (HCMWin) Transformer block to
                    capture
                    both local and global information, leading to a more efficient compression.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet88">
            <div class="start-time-icon" title="Play from here">
                40:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15828" target="_blank">
                        @arXiv 2407.15828
                    </a>
                    <span class="tweet-title">
                        J-CHAT: A Dialogue Corpus So Big, It's Got More Words Than Your Last Argument!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Tokyo
                    </span>
                </div>
                <div class="primary-text">
                    This research constructs a large-scale, open-source spoken dialogue corpus in Japanese, J-CHAT,
                    addressing the lack of such resources for non-English languages. It also introduces a fully
                    automated method for corpus construction, making it scalable and language-independent.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet89">
            <div class="start-time-icon" title="Play from here">
                40:45
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15593" target="_blank">
                        @arXiv 2407.15593
                    </a>
                    <span class="tweet-title">
                        Robots with Eyesight: Learning Where to Look for Better Localization!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Sapienza University of Rome, ETH Zurich, Microsoft...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a self-supervised learning approach for viewpoint selection in active
                    localization. Unlike previous methods that rely on hand-crafted techniques or require manual
                    labeling, this approach learns from the distribution of 3D landmarks and their contribution to
                    visual localization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet90">
            <div class="start-time-icon" title="Play from here">
                41:11
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15816" target="_blank">
                        @arXiv 2407.15816
                    </a>
                    <span class="tweet-title">
                        H&amp;E Images: A New Way to Predict Cancer Mutations?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tempus AI Inc.
                    </span>
                </div>
                <div class="primary-text">
                    This research explores a multi-task learning approach to predict multiple DNA alterations from
                    H&amp;E whole slide images, unlike previous studies that focused on single biomarker prediction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet91">
            <div class="start-time-icon" title="Play from here">
                41:42
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15138" target="_blank">
                        @arXiv 2407.15138
                    </a>
                    <span class="tweet-title">
                        Dataset Distillation: Diffusion Models Ditch the Matching Game!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Chinese Academy of Sciences, University of Chinese Academy of Sciences, North Carolina State
                        University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new dataset distillation framework called D4M that uses a disentangled
                    diffusion model to generate synthetic datasets. Unlike previous methods that rely on matching
                    architectures, D4M is architecture-free, meaning it can generate datasets that are compatible
                    with
                    various network architectures.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet92">
            <div class="start-time-icon" title="Play from here">
                42:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14558" target="_blank">
                        @arXiv 2407.14558
                    </a>
                    <span class="tweet-title">
                        Soccer's Next Move: A Foundation Model Predicts the Play
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a foundation model for soccer, trained on action data from historical
                    matches, to predict subsequent actions in a match. Unlike previous work, this model considers
                    longer
                    input sequences and includes turnovers, where possession changes hands.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet93">
            <div class="start-time-icon" title="Play from here">
                42:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14684" target="_blank">
                        @arXiv 2407.14684
                    </a>
                    <span class="tweet-title">
                        Data Poisoning: The Silent Hacker Attack on Your Power Grid
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Texas at Austin, United States Air Force
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on data poisoning attacks, a type of cyberattack that manipulates training
                    data used by power grid optimization models, highlighting a gap in previous research that
                    primarily
                    focused on evasion attacks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet94">
            <div class="start-time-icon" title="Play from here">
                42:59
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15264" target="_blank">
                        @arXiv 2407.15264
                    </a>
                    <span class="tweet-title">
                        GNN Training Gets a Speed Boost: How a Shared Cache Makes Graphs Fly!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Illinois, Nvidia
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel communication layer that allows GPUs to share their software
                    caches,
                    effectively creating a system-wide shared cache. This approach avoids the use of high-overhead
                    system-scope operations, maximizing collective cache capacity and minimizing redundant storage
                    accesses.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet95">
            <div class="start-time-icon" title="Play from here">
                43:21
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14766" target="_blank">
                        @arXiv 2407.14766
                    </a>
                    <span class="tweet-title">
                        AI Fairness: When Equal Odds Beat Equal Chances
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        France Travail, Institut Jean-Nicod, Linedata...
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the implementation of fairness in AI classification models by comparing
                    two
                    fairness metrics: Demographic Parity and Equalized Odds. The study uses a novel fairness package
                    called FairDream to demonstrate that even when aiming for Demographic Parity, the model
                    converges
                    towards Equalized Odds.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet96">
            <div class="start-time-icon" title="Play from here">
                43:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14668" target="_blank">
                        @arXiv 2407.14668
                    </a>
                    <span class="tweet-title">
                        Brain Translator: Unlocking the Language of Neurons with a Universal Decoder
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Columbia University, Stanford University, MIT...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel multi-task-masking (MtM) approach for self-supervised learning
                    of
                    neural activity. Unlike previous methods that focus on temporal masking, MtM incorporates
                    masking
                    across different time steps, neurons, and brain regions, allowing the model to learn a more
                    comprehensive representation of neural dynamics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet97">
            <div class="start-time-icon" title="Play from here">
                44:15
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14725" target="_blank">
                        @arXiv 2407.14725
                    </a>
                    <span class="tweet-title">
                        Crowd Forecasting: When Missing People Make a Big Difference!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Keio University, Nvidia
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces CrowdMAC, a new framework for crowd density forecasting that tackles
                    the
                    issue of missing pedestrian data. Unlike previous methods that rely on complete trajectories,
                    CrowdMAC simultaneously reconstructs missing data in past observations while predicting future
                    crowd
                    density maps.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet98">
            <div class="start-time-icon" title="Play from here">
                44:41
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15724" target="_blank">
                        @arXiv 2407.15724
                    </a>
                    <span class="tweet-title">
                        Deep Learning's New BFF: Alpha Diversity for Better Image Classification!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC San Francisco, Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new metric called "alpha diversity" to measure dataset quality in
                    deep
                    learning. Unlike traditional metrics like dataset size and class balance, alpha diversity
                    considers
                    the similarities and differences among images within a dataset.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet99">
            <div class="start-time-icon" title="Play from here">
                45:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14575" target="_blank">
                        @arXiv 2407.14575
                    </a>
                    <span class="tweet-title">
                        Lizard-Powered Predictions: A New Algorithm for Cloud Energy Efficiency
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Illinois Urbana-Champaign, The University of Tokyo, Georgia Institute of
                        Technology...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to predicting cloud energy consumption by combining a
                    bio-inspired optimization algorithm, the Horned Lizard Optimization Algorithm (HLOA), with a
                    deep
                    learning model, Convolutional Neural Networks-Bidirectional Gated Recurrent Units (CNN-BiGRU).
                    This
                    differs from previous work by utilizing HLOA to optimize the parameters of the CNN-BiGRU model,
                    leading to improved prediction accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet100">
            <div class="start-time-icon" title="Play from here">
                45:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15734" target="_blank">
                        @arXiv 2407.15734
                    </a>
                    <span class="tweet-title">
                        TaskGen: Taming the LLMs with StrictJSON and Shared Memory
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich, National University of Singapore
                    </span>
                </div>
                <div class="primary-text">
                    TaskGen introduces a novel approach to agentic frameworks by utilizing StrictJSON for concise
                    LLM
                    output and a shared memory system for efficient information exchange between agents and equipped
                    functions. This contrasts with existing frameworks that rely on free-form text output, often
                    leading
                    to verbosity and increased processing costs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet101">
            <div class="start-time-icon" title="Play from here">
                46:07
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15631" target="_blank">
                        @arXiv 2407.15631
                    </a>
                    <span class="tweet-title">
                        Building Better Hearts: AI Designs Customized Coronary Arteries for Virtual Surgery
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT, Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for generating realistic and controllable 3D models of
                    coronary arteries using Latent Diffusion Models (LDMs). Unlike previous methods that rely on
                    simplified geometries or real patient data, this approach allows for the creation of customized
                    arteries with specific morphological and skeletal features, enabling researchers to study the
                    impact
                    of anatomical variations on medical interventions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet102">
            <div class="start-time-icon" title="Play from here">
                46:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.15817" target="_blank">
                        @arXiv 2407.15817
                    </a>
                    <span class="tweet-title">
                        AI Fills the Gaps: A Deep Learning Fix for Fuzzy Cell Boundaries in SEM Images
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Sorbonne University, Ã‰cole des Ponts ParisTech, Ã‰cole Normale SupÃ©rieure
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel AI-driven approach for refining cell boundary delineation in
                    SEM
                    images. Unlike previous methods that rely on end-to-end segmentation, this study proposes a
                    dedicated CNN-based closing operator (COp-Net) to address gaps in cell contours.
                </div>
            </div>
        </div>


        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202407231649_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>

</html>