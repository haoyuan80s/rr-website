<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - AI Paper Picks of the Day</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 80px;"></div>
            <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                Freshest
                Top Picks:
                <span class="highlightNumber" style="font-size: 28px;">32</span> out of <span
                    class="highlightNumber">181</span> arXiv AI
                papers
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="line-height: 1.2;">just released on</div>
                <div id="data-default-date" data-date="2024-07-22"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 10px;">
            </div>
        </div>

        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">
                00:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14435" target="_blank">
                        @arXiv 2407.14435
                    </a>
                    <span class="tweet-title">
                        Jumpin' ReLU: Sparser, More Faithful Language Model Decompositions
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google DeepMind
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces JumpReLUSAE, a new type of sparse autoencoder (SAE) that uses a JumpReLU
                    activation function. This modification allows for more faithful reconstructions of language model
                    activations at a given sparsity level compared to previous SAE architectures like Gated and TopK
                    SAEs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">
                01:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14482" target="_blank">
                        @arXiv 2407.14482
                    </a>
                    <span class="tweet-title">
                        ChatQA2: Llama3's Long-Context Leap to GPT-4 Turbo Level!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nvidia
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on bridging the gap between open-access LLMs and proprietary models like GPT-4
                    Turbo in terms of long-context understanding and retrieval-augmented generation (RAG) capabilities.
                    The authors achieve this by extending the context window of Llama3-70B-base from 8K to 128K tokens
                    and implementing a three-stage instruction tuning process. This approach differs from previous work
                    by focusing on real-world long-context understanding tasks and combining long-context LLMs with a
                    state-of-the-art long-context retriever for RAG.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">
                01:47
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14309" target="_blank">
                        @arXiv 2407.14309
                    </a>
                    <span class="tweet-title">
                        Asking the Right Questions: How AI Can Make Reading More Engaging
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the use of "guiding questions" within written text, specifically those
                    explicitly posed by the author to engage readers. Unlike previous work on Question Under Discussion
                    (QUD) frameworks, which analyze implicit questions to understand discourse relationships, this study
                    examines the impact of explicit questions on human reading comprehension.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">
                02:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14044" target="_blank">
                        @arXiv 2407.14044
                    </a>
                    <span class="tweet-title">
                        Code Optimization: Can We Have Our Cake and Eat It Too?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces ECCO, a benchmark for evaluating code efficiency that focuses on
                    maintaining functional correctness while optimizing for runtime and memory usage. Unlike previous
                    benchmarks, ECCO uses a cloud-hosted code execution engine to ensure reliable and reproducible
                    results across different hardware specifications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">
                02:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14474" target="_blank">
                        @arXiv 2407.14474
                    </a>
                    <span class="tweet-title">
                        Radiology Reports: "What If" We Swap Patches?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, University of Technology Sydney, Mohamed bin Zayed University of Artificial
                        Intelligence
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to radiology report generation by incorporating
                    counterfactual explanations. Unlike previous methods that rely on direct captioning or incorporating
                    medical knowledge, this framework learns non-spurious visual representations by contrasting
                    representations between factual and counterfactual images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">
                03:19
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13998" target="_blank">
                        @arXiv 2407.13998
                    </a>
                    <span class="tweet-title">
                        RAG-QA Arena: Where LLMs Go to Battle for the Best Answer
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google, UC Santa Barbara, Amazon
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces LFRQA, a new dataset for evaluating long-form question answering systems.
                    Unlike previous datasets that focus on short, extractive answers, LFRQA provides human-written,
                    coherent long-form answers that integrate information from multiple documents.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">
                03:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14095" target="_blank">
                        @arXiv 2407.14095
                    </a>
                    <span class="tweet-title">
                        Playing Games Without Playing: How Our Brains Simulate Fun
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT, University of Cambridge
                    </span>
                </div>
                <div class="primary-text">
                    This research explores how people quickly evaluate novel games without extensive experience,
                    focusing on the mental simulations they use rather than optimal gameplay. It contrasts with previous
                    work that emphasizes expert-level play and extensive search.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">
                04:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14192" target="_blank">
                        @arXiv 2407.14192
                    </a>
                    <span class="tweet-title">
                        Legal AI Gets a Knowledge Update: LeKUBE Benchmarks the Latest Legal LLMs
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces LeKUBE, a benchmark specifically designed to evaluate knowledge update
                    methods for legal LLMs. Unlike existing benchmarks that focus on general domains, LeKUBE addresses
                    the unique challenges of updating legal knowledge, such as the nuanced application of new legal
                    knowledge and the complexity of legal regulations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">
                04:33
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13930" target="_blank">
                        @arXiv 2407.13930
                    </a>
                    <span class="tweet-title">
                        Radar-Vision Fusion: A 4D Tensor Takes on Human Pose Estimation
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        National Cheng Kung University, University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset, RT-Pose, which includes calibrated 4D radar tensors, LiDAR
                    point clouds, and RGB images. This multi-modal approach is unique because it leverages the raw 4D
                    radar tensor, preserving more information than traditional point cloud methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">
                05:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13937" target="_blank">
                        @arXiv 2407.13937
                    </a>
                    <span class="tweet-title">
                        Seeing Double: How Cameras and Radars Team Up to Track Objects in 3D
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington, Cisco Systems
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on enhancing 3D object tracking by fusing data from cameras and radars at the
                    tracking stage, rather than earlier in the detection process. This approach allows for more robust
                    and accurate tracking, especially in challenging conditions like bad weather.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">
                05:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14094" target="_blank">
                        @arXiv 2407.14094
                    </a>
                    <span class="tweet-title">
                        Recommender Systems: When Algorithms Become Echo Chambers
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University, ByteDance
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new model called "user-creator feature dynamics" to capture the dual
                    influence of recommender systems on both users and creators. Unlike previous work that focused on
                    either user or creator influence, this study considers both effects simultaneously.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">
                06:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13943" target="_blank">
                        @arXiv 2407.13943
                    </a>
                    <span class="tweet-title">
                        LLMs Play Werewolf: A New Arena for AI Social Skills
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Werewolf Arena, a novel framework for evaluating LLMs through the lens of
                    the classic social deduction game, Werewolf. Unlike previous work that focused on improving agent
                    performance, this study uses Werewolf as a proving ground to evaluate the relative skills of LLMs by
                    having them play against each other.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">
                06:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14346" target="_blank">
                        @arXiv 2407.14346
                    </a>
                    <span class="tweet-title">
                        Search Engines Got a New Trick Up Their Sleeve: Context is King!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach called "Augmented Unity" that leverages query context
                    signals from web search results and large language models to improve keyword retrieval in sponsored
                    search. This differs from previous work by incorporating a dynamic cache to store and retrieve
                    contextual information, enhancing query understanding, particularly for short and ambiguous queries.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">
                06:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14430" target="_blank">
                        @arXiv 2407.14430
                    </a>
                    <span class="tweet-title">
                        Deep Learning Models That Can Actually Think Outside the Box!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley, VinUniversity
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the extrapolation capabilities of implicit deep learning models, which are
                    distinguished by their adaptability in layer depth and incorporation of feedback within their
                    computational graph. Unlike traditional feed-forward models, implicit models allow information to
                    propagate both forwardly and backwardly through closed-form feedback loops.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">
                07:10
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14402" target="_blank">
                        @arXiv 2407.14402
                    </a>
                    <span class="tweet-title">
                        LLMs: The New Autopilots for Your Cloud?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nanjing University, Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the feasibility of using LLMs to create self-managing microservice systems, a
                    concept known as Autonomic Computing. Unlike previous approaches that relied on rule-based systems,
                    this study proposes a hierarchical multi-agent framework where LLMs handle tasks like monitoring,
                    analysis, and issue mitigation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">
                07:33
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14111" target="_blank">
                        @arXiv 2407.14111
                    </a>
                    <span class="tweet-title">
                        Byzantine Workers, Be Gone! New Algorithm Makes Distributed Learning Corruption-Proof
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        National University of Singapore
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the impact of adversarial corruptions on distributed gradient descent
                    algorithms, a problem that has received limited attention in previous work. The paper proposes a
                    novel algorithm, RDGD, that utilizes a modified lazy mirror descent approach to mitigate the effects
                    of these corruptions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">
                07:55
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13976" target="_blank">
                        @arXiv 2407.13976
                    </a>
                    <span class="tweet-title">
                        PlacidDreamer: Text-to-3D Harmony, No More Color Wars!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Kuaishou Technology
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a new framework called PlacidDreamer that addresses two limitations in
                    text-to-3D generation: conflicting optimization directions and over-saturation in score
                    distillation. It achieves this by harmonizing initialization, multi-view generation, and
                    text-conditioned generation with a single multi-view diffusion model and a novel score distillation
                    algorithm.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">
                08:21
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14121" target="_blank">
                        @arXiv 2407.14121
                    </a>
                    <span class="tweet-title">
                        Seismic Fault Detection: SAM's Got Your Back (and Your Subsurface)!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research adapts the Segment Anything Model (SAM), a pre-trained computer vision model, for
                    seismic fault detection. Unlike previous methods that rely heavily on labeled seismic data, this
                    approach leverages SAM's knowledge from natural images, requiring less training data and achieving
                    faster convergence.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">
                08:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14106" target="_blank">
                        @arXiv 2407.14106
                    </a>
                    <span class="tweet-title">
                        Graph Transformers Go Big: TORCHGT Tames the Beast of Large-Scale Graphs!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nanyang Technological University, Zhejiang University, Shanghai AI Laboratory...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces TORCHGT, a system specifically designed for training graph transformers on
                    large-scale graphs. Unlike previous work that struggled with the computational demands of long
                    sequences, TORCHGT leverages graph sparsity and clustering to optimize training efficiency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">
                09:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14022" target="_blank">
                        @arXiv 2407.14022
                    </a>
                    <span class="tweet-title">
                        Causal Inference: Beyond the Binary, It's Getting Complex!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Zhejiang University, Peking University, Emory University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on causal inference with complex treatments, going beyond the traditional
                    binary treatment setting (treatment or no treatment) to explore multi-valued, continuous, and
                    bundled treatment options.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">
                09:42
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14081" target="_blank">
                        @arXiv 2407.14081
                    </a>
                    <span class="tweet-title">
                        Graph Classification Gets a Disentangled Makeover: Semi-Supervised Learning Goes Factor-Wise!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of International Business and Economics, UC Los Angeles, Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel framework called DisenSemi for semi-supervised graph classification.
                    Unlike previous approaches that transfer the entire knowledge from the unsupervised model to the
                    supervised one, DisenSemi disentangles the graph representation into distinct latent factors and
                    transfers only the relevant semantics that align well with the supervised task.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">
                10:15
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13796" target="_blank">
                        @arXiv 2407.13796
                    </a>
                    <span class="tweet-title">
                        Jailbreaking LLMs: A Direct Attack on the Input, No Suffix Needed!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nanyang Technological University, University of New South Wales, Huazhong University of Science
                        and Technology...
                    </span>
                </div>
                <div class="primary-text">
                    This research explores a new method for jailbreaking LLMs by directly manipulating the input
                    embedding, eliminating the need for suffixes, which have been the primary focus of previous work.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">
                10:46
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13982" target="_blank">
                        @arXiv 2407.13982
                    </a>
                    <span class="tweet-title">
                        ASR's Got a Case of the Mumbles: How Recording Quality Confounds Racial Bias in Speech
                        Recognition
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington, University of Minnesota
                    </span>
                </div>
                <div class="primary-text">
                    This study goes beyond simply noting that automatic speech recognition (ASR) systems perform worse
                    on African American English (AAE) speech. It digs deeper, revealing that the quality of the audio
                    recordings used to train these systems can significantly impact their accuracy, potentially masking
                    or even exaggerating the true extent of racial bias.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">
                11:25
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13775" target="_blank">
                        @arXiv 2407.13775
                    </a>
                    <span class="tweet-title">
                        Drivers vs. Bots: The "Sweet Spot" of Cooperative Driving Advice
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research delves into driver sentiments towards cooperative real-time advisory (CoRTA) systems,
                    focusing on how these systems influence driver trust and behavior. Unlike previous studies that
                    primarily evaluated system effectiveness or UI design, this paper uses a driving simulator study and
                    qualitative analysis to understand driver preferences for various aspects of the interaction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">
                11:54
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14245" target="_blank">
                        @arXiv 2407.14245
                    </a>
                    <span class="tweet-title">
                        Dataset Distillation: Stop Stretching Those Trajectories!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Technical University of Munich, University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research addresses the Accumulated Mismatching Problem (AMP) in long-range matching dataset
                    distillation (LDD) methods. Unlike previous LDD methods that use a fixed trajectory length, this
                    paper proposes Automatic Training Trajectories (ATT), which dynamically adjusts the trajectory
                    length to minimize errors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">
                12:20
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14387" target="_blank">
                        @arXiv 2407.14387
                    </a>
                    <span class="tweet-title">
                        Graph Learning Gets a Sonic Makeover: GLAudio Turns Node Features into Waves!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces GLAudio, a novel graph learning architecture that propagates node features
                    through the graph network according to the discrete wave equation. Unlike previous methods that rely
                    on the heat equation, GLAudio separates information propagation and processing into distinct steps,
                    potentially mitigating over-smoothing and over-squashing.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">
                12:41
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13863" target="_blank">
                        @arXiv 2407.13863
                    </a>
                    <span class="tweet-title">
                        GANs Gone Wild: Cracking Model Secrets with Intermediate Features
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harbin Institute of Technology, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a new method for model inversion attacks, called IF-GMI, which leverages
                    intermediate features within the GAN structure. Unlike previous methods that solely focused on the
                    latent space, IF-GMI explores the rich semantic information encoded in the intermediate layers of
                    the GAN, leading to improved attack accuracy, especially in out-of-distribution scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">
                13:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13928" target="_blank">
                        @arXiv 2407.13928
                    </a>
                    <span class="tweet-title">
                        Bias Busters: New Method Makes LLMs More Respectful
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        American University in Cairo
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a new framework for mitigating bias in LLMs using Direct Preference
                    Optimization (DPO). Unlike previous approaches like Reinforcement Learning from Human Feedback
                    (RLHF), DPO directly optimizes the model to favor less biased completions without requiring a
                    separate reward model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">
                13:42
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14191" target="_blank">
                        @arXiv 2407.14191
                    </a>
                    <span class="tweet-title">
                        AI Predicts ALS Survival: Brain Scans Get a "Normative" Makeover!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University College London
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to predicting survival in ALS by combining normative
                    modeling with diffusion autoencoders. This differs from previous work that used either generative or
                    non-generative normative models alone.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">
                14:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14506" target="_blank">
                        @arXiv 2407.14506
                    </a>
                    <span class="tweet-title">
                        Chart-ing a New Course: How to Teach LLMs to Read Charts Like Humans
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of British Columbia, Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the pre-training process for multimodal language models (MLLMs)
                    specifically tailored for chart understanding. Unlike previous work that primarily relies on
                    fine-tuning, this paper explores the impact of incorporating raw data values during pre-training to
                    improve the model's ability to extract underlying numeric values from charts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">
                14:45
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14352" target="_blank">
                        @arXiv 2407.14352
                    </a>
                    <span class="tweet-title">
                        Flying High, Seeing Low: AI Helps Pilots Avoid Power Line Peril
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        École Polytechnique Fédérale de Lausanne
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new deep learning approach to jointly detect power line cables and pylons
                    from images captured by aircraft-mounted cameras. Unlike previous work that focused on either cables
                    or pylons separately, this method uses a single network to detect both, leveraging a modern
                    convolutional architecture and a loss function designed for curvilinear structure delineation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">
                15:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13833" target="_blank">
                        @arXiv 2407.13833
                    </a>
                    <span class="tweet-title">
                        Phi-3: A Language Model That's Safe, Sound, and Ready for Your Smartphone!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on safety aligning a series of small language models (SLMs) called Phi-3. The
                    study utilizes a "break-fix" cycle, which involves multiple rounds of safety post-training, red
                    teaming, and vulnerability identification. This iterative approach is distinct from traditional
                    single-round fine-tuning methods.
                </div>
            </div>
        </div>


        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Listen and learn ^.^</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202407221523_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>

</html>