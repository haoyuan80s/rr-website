<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Fresh AI Paper Top Picks</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 80px;"></div>
            <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                Freshest
                Top Picks:
                <span class="highlightNumber" style="font-size: 28px;">73</span> out of <span
                    class="highlightNumber">313</span> arXiv AI
                papers
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="line-height: 1.2;">just released on</div>
                <div id="data-default-date" data-date="2024-07-12"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 10px;">
            </div>
        </div>

        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">
                00:49
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07950" target="_blank">
                        @arXiv 2407.07950
                    </a>
                    <span class="tweet-title">
                        AI's Confidence Game: How Chatbots' Warmth Makes Us Trust Their Lies
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on how human reliance on language models (LLMs) is influenced by the
                    interaction context, going beyond just the verbalized confidence of the model. It introduces a new
                    methodology called REL-A.I. to measure this reliance in situ.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">
                01:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08103" target="_blank">
                        @arXiv 2407.08103
                    </a>
                    <span class="tweet-title">
                        Decoding Decoded: How Automata Make Language Models Speak the Right Language
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel approach to constraining language models by leveraging automata
                    theory. Unlike previous methods that rely on bespoke algorithms or dynamic vocabulary matching, this
                    research proposes a closed-form solution using finite-state automata (FSAs) and push-down automata
                    (PDAs) to pre-compute valid token sequences, making it more efficient and scalable.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">
                01:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08351" target="_blank">
                        @arXiv 2407.08351
                    </a>
                    <span class="tweet-title">
                        AutoBencher: Building Better Benchmarks for Language Models, One Question at a Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to creating benchmarks for language models by using a
                    language model itself to automatically search for datasets that are salient, difficult, and novel.
                    This differs from previous work that relied on human-constructed benchmarks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">
                02:03
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07972" target="_blank">
                        @arXiv 2407.07972
                    </a>
                    <span class="tweet-title">
                        Adam's Reign Is Over: New Optimizers Rule the Language Model Kingdom!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This research compares the performance and stability of several optimization algorithms, including
                    Adam, Adafactor, Lion, and Signum, for training autoregressive language models across different
                    model sizes, hyperparameters, and architectures. Unlike previous work, this study focuses on the
                    stability of these optimizers with respect to hyperparameter choices, demonstrating that many of
                    them are comparable to Adam in terms of both performance and stability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">
                02:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08674" target="_blank">
                        @arXiv 2407.08674
                    </a>
                    <span class="tweet-title">
                        Still-Moving: Making Videos Dance Without Video Data!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google DeepMind, Tel Aviv University, Weizmann Institute of Science...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel framework called Still-Moving that allows for customizing
                    text-to-video models without requiring any customized video data. This is achieved by training
                    lightweight adapters on still images, effectively transferring the spatial prior of a customized
                    text-to-image model to the text-to-video model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">
                02:52
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08737" target="_blank">
                        @arXiv 2407.08737
                    </a>
                    <span class="tweet-title">
                        Video Diffusion Gets a Gradient Makeover: Rewarding AI with Pixel-Perfect Feedback!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Carnegie Mellon University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces VADER, a method for adapting video diffusion models using reward gradients.
                    Unlike previous approaches that rely on policy gradients or direct preference optimization, VADER
                    leverages the dense gradient information from reward models to efficiently align video generation
                    with specific tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">
                03:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08447" target="_blank">
                        @arXiv 2407.08447
                    </a>
                    <span class="tweet-title">
                        WildGaussians: 3D Scene Reconstruction Goes Wild!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Czech Technical University in Prague, ETH Zurich, Google DeepMind
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces WildGaussians, a novel approach that extends 3D Gaussian Splatting (3DGS) to
                    handle occlusions and appearance changes in real-world scenes. Unlike previous methods,
                    WildGaussians leverages robust DINO features and integrates an appearance modeling module within
                    3DGS, achieving state-of-the-art results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">
                03:44
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08551" target="_blank">
                        @arXiv 2407.08551
                    </a>
                    <span class="tweet-title">
                        Say What? Speech Synthesis Without the Code-Cracking!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        The Chinese University of Hong Kong, Microsoft Corporation
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new approach to speech synthesis that directly predicts continuous
                    mel-spectrograms, bypassing the need for vector quantization, which is commonly used in other
                    methods. This eliminates the need for a two-stage decoding process and improves robustness and
                    speaker similarity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">
                04:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08694" target="_blank">
                        @arXiv 2407.08694
                    </a>
                    <span class="tweet-title">
                        Cloud Atlas: Fault Finding with AI and a Dash of Causality
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, CMU, Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Atlas, a novel approach to automatically synthesizing causal graphs for
                    cloud systems. Unlike previous data-driven approaches, Atlas leverages large language models (LLMs)
                    to generate causal graphs using system documentation, telemetry, and deployment feedback.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">
                04:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08585" target="_blank">
                        @arXiv 2407.08585
                    </a>
                    <span class="tweet-title">
                        Robots Get Spatial Awareness: Learning to Grasp, Poke, and Place Like a Pro!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU, Meta
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces spatially-grounded motion primitives for robot manipulation. Unlike
                    previous work that uses primitives without spatial grounding, this method grounds each primitive on
                    a specific point in the observed point cloud, improving the robot's ability to reason about spatial
                    relationships and generalize to unseen objects.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">
                05:04
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08185" target="_blank">
                        @arXiv 2407.08185
                    </a>
                    <span class="tweet-title">
                        Censorship Sleuths: AI Uncovers Hidden Web Blocks
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU, École Polytechnique Fédérale de Lausanne, University of British Columbia
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces an automated method for generating probe lists, which are used to identify
                    websites that are blocked due to censorship. Unlike previous manual or crowdsourced approaches, this
                    method leverages natural language processing and search engine queries to discover new potentially
                    censored URLs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">
                05:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08681" target="_blank">
                        @arXiv 2407.08681
                    </a>
                    <span class="tweet-title">
                        FPGA-Powered Brains: Teaching Robots to Race with Neural Networks!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Seville, TU Delft, ETH Zurich...
                    </span>
                </div>
                <div class="primary-text">
                    This research uses hardware FPGA neural networks trained to imitate NMPC controllers, achieving
                    faster control rates and better performance than traditional NMPC methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">
                05:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08707" target="_blank">
                        @arXiv 2407.08707
                    </a>
                    <span class="tweet-title">
                        Vision Models: Spilling Secrets From Training Data!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google, University of Oxford, ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This research investigates the ability of document-based visual question answering (DocVQA) models
                    to memorize and regurgitate training data, even when the relevant visual information is removed.
                    This differs from previous work on training data extraction, which primarily focused on extracting
                    entire training samples from generative models for text and images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">
                06:23
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08159" target="_blank">
                        @arXiv 2407.08159
                    </a>
                    <span class="tweet-title">
                        Backdoor Buster: How to Outsmart Sneaky AI Attacks in Cybersecurity
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Northeastern University, MIT Lincoln Laboratory, Peraton Labs
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel defense mechanism against clean-label backdoor attacks in
                    cybersecurity environments. Unlike previous work, it doesn't require access to clean trusted data or
                    knowledge of the victim's model architecture.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">
                06:47
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08188" target="_blank">
                        @arXiv 2407.08188
                    </a>
                    <span class="tweet-title">
                        Stop Saying "Diverse" - Measure It!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes using measurement theory, a framework commonly used in social sciences, to
                    define and evaluate diversity in machine learning datasets. This approach differs from previous work
                    by focusing on the data collection process itself as a measurement tool, rather than solely relying
                    on downstream model performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">
                07:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08223" target="_blank">
                        @arXiv 2407.08223
                    </a>
                    <span class="tweet-title">
                        RAG's New Trick: Drafting Answers with a Smaller Brain!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC San Diego, Google
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces SPECULATIVE RAG, a new approach to retrieval augmented generation (RAG) that
                    uses a smaller, specialized language model to generate multiple answer drafts in parallel. These
                    drafts are then verified by a larger, generalist language model, which selects the most accurate
                    answer. This differs from previous work that focuses on improving retrieval outcomes or
                    instruction-tuning entire LLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">
                07:44
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07580" target="_blank">
                        @arXiv 2407.07580
                    </a>
                    <span class="tweet-title">
                        Layout Synthesis Gets a Semantic Makeover: Graph Diffusion Models Take the Stage!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, ByteDance
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces INSTRUCTLAYOUT, a generative framework for 2D and 3D layout synthesis that
                    incorporates a semantic graph prior. Unlike previous methods that implicitly model object relations,
                    INSTRUCTLAYOUT explicitly represents these relationships using a graph, enhancing controllability
                    and fidelity in layout generation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">
                08:21
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08699" target="_blank">
                        @arXiv 2407.08699
                    </a>
                    <span class="tweet-title">
                        Language Models: Forget the Past, Learn the Future!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        INSAIT, Sofia University, LogicStar.ai...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Branch-and-Merge (BAM), a novel method for adapting large language models
                    to new languages while minimizing the loss of previously learned capabilities. Unlike previous
                    methods that rely on experience replay or reduced learning rates, BAM iteratively merges multiple
                    models fine-tuned on subsets of the training data, leading to lower magnitude but higher quality
                    weight changes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">
                09:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08462" target="_blank">
                        @arXiv 2407.08462
                    </a>
                    <span class="tweet-title">
                        Self-Driving Cars Learn to Talk Less, Drive Smarter: New AI Technique Makes Gradient
                        Quantization a Breeze!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Jiangnan University, Tsinghua University, University of Kent...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a distributed deep reinforcement learning (DRL) based quantization level
                    allocation scheme for federated learning (FL) in vehicle edge computing (VEC). Unlike previous work,
                    this approach dynamically adjusts the quantization level for each vehicle based on its individual
                    state, including channel conditions and mobility, to optimize both training time and quantization
                    error.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">
                09:45
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08414" target="_blank">
                        @arXiv 2407.08414
                    </a>
                    <span class="tweet-title">
                        MeshAvatar: Turning Videos into 3D Humans, One Triangle at a Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, NNKosmos Technology
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel approach to learning human avatars using triangular meshes, a departure
                    from the popular NeRF-based methods. This representation allows for more efficient editing and
                    manipulation of the avatar within traditional graphics pipelines.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">
                10:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08730" target="_blank">
                        @arXiv 2407.08730
                    </a>
                    <span class="tweet-title">
                        Deep Learning's Trust Issues: A Replicability Study of DNN Reliability Tools
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU, National Aeronautics and Space Administration, IEEE Computer Society
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the replicability and comparative analysis of recent techniques for
                    evaluating the reliability of deep neural networks (DNNs) in deployment. It highlights the
                    challenges in reproducing results and comparing the effectiveness of these approaches due to
                    inconsistencies in evaluation metrics and data preparation methods. The study proposes a unified
                    framework, TrustDNN, to address these issues and facilitate future research in this area.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">
                10:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08642" target="_blank">
                        @arXiv 2407.08642
                    </a>
                    <span class="tweet-title">
                        AI's New Trick: Being a Super-Specialized Generalist!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a new approach to Artificial General Intelligence (AGI) called Specialized
                    Generalist AI (SGI). Unlike previous work that focused on scaling general abilities, SGI emphasizes
                    achieving expert-level performance in at least one specific task while maintaining general
                    capabilities across a wider range of tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">
                11:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07912" target="_blank">
                        @arXiv 2407.07912
                    </a>
                    <span class="tweet-title">
                        GNNs Get a Ranking Makeover: Directly Optimizing Recommendation Metrics!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Conservatoire National des Arts et Métiers, Sorbonne University
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the use of ranking loss functions to directly optimize evaluation metrics for
                    message-passing-based GNNs in top-k recommendation tasks, an area not extensively investigated in
                    the GNN community for collaborative filtering.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">
                11:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08083" target="_blank">
                        @arXiv 2407.08083
                    </a>
                    <span class="tweet-title">
                        MambaVision: When Transformers Met Mamba, Vision Got a Boost!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nvidia
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a hybrid architecture combining Mamba and Transformer blocks, specifically
                    tailored for vision tasks. Unlike previous Mamba-based models, it integrates self-attention blocks
                    at the final layers to capture long-range spatial dependencies, improving performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">
                11:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08711" target="_blank">
                        @arXiv 2407.08711
                    </a>
                    <span class="tweet-title">
                        OmniNOCS: A Dataset So Big, It's Like a 3D Object Party!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google Research, Georgia Institute of Technology, Google DeepMind
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces OmniNOCS, a large-scale dataset for 3D object lifting, which surpasses
                    existing datasets in terms of the number of object classes and instances. It also proposes a novel
                    transformer-based model, NOCSformer, trained on OmniNOCS, that can predict accurate 3D object poses
                    and shapes from 2D object detections.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">
                12:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08022" target="_blank">
                        @arXiv 2407.08022
                    </a>
                    <span class="tweet-title">
                        Deep Learning Auctions: Bidding Wars Get a Neural Network Upgrade!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new deep reinforcement learning framework tailored for sequential
                    combinatorial auctions. Unlike traditional RL methods, it leverages first-order gradients to improve
                    computational efficiency and convergence, especially in large and continuous action spaces.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">
                12:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08250" target="_blank">
                        @arXiv 2407.08250
                    </a>
                    <span class="tweet-title">
                        Boosting Brains: How Trees Conquered Reinforcement Learning
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nvidia
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces Gradient-Boosting Reinforcement Learning (GBRL), a framework that extends the
                    advantages of Gradient Boosting Trees (GBT) to the reinforcement learning domain. Unlike previous
                    work that focused on using GBTs in off-policy RL methods, GBRL demonstrates the scalability and
                    effectiveness of GBTs in complex, high-dimensional RL environments requiring extensive interactions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">
                13:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08039" target="_blank">
                        @arXiv 2407.08039
                    </a>
                    <span class="tweet-title">
                        LLMs: When Facts Get Overshadowed, Hallucinations Bloom!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Illinois Urbana-Champaign, The Hong Kong Polytechnic University, Stanford
                        University
                    </span>
                </div>
                <div class="primary-text">
                    This research identifies a specific type of hallucination in LLMs called "knowledge overshadowing,"
                    where the model prioritizes one condition over others in a query, leading to incorrect outputs. This
                    phenomenon is shown to be caused by data imbalance during training, a factor not fully explored in
                    previous research.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">
                13:55
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08176" target="_blank">
                        @arXiv 2407.08176
                    </a>
                    <span class="tweet-title">
                        Foundation Models: From Code to Crisis, Engineering a Solution
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, University of Texas at Dallas
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a new framework called "Foundation Model Engineering" (FME) to address the
                    growing complexity of developing and managing foundation models. Unlike previous work that focuses
                    on individual model improvements, FME aims to create a comprehensive ecosystem for managing data,
                    models, and their evolution.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">
                14:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08231" target="_blank">
                        @arXiv 2407.08231
                    </a>
                    <span class="tweet-title">
                        Silicon Retina Sees Color: Diffusion Models Bring Event Cameras to Life!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces diffusion models to the field of event-to-video reconstruction. Unlike
                    previous regression-based methods, which often produce deterministic and unrealistic results,
                    diffusion models allow for the generation of diverse and perceptually superior video reconstructions
                    from achromatic event data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">
                14:47
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08150" target="_blank">
                        @arXiv 2407.08150
                    </a>
                    <span class="tweet-title">
                        Brainwaves &amp; Eye-Tracking: Unlocking the Secrets of Video Ads!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset, SRI-ADV, which includes real-time EEG and eye-tracking data
                    from diverse demographics watching advertisements. This dataset is unique because it captures both
                    objective and subjective responses to video content, allowing for a more comprehensive understanding
                    of how people engage with videos.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">
                15:15
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08296" target="_blank">
                        @arXiv 2407.08296
                    </a>
                    <span class="tweet-title">
                        Training Giant Language Models on a Tiny Laptop? Q-GaLore Makes It Possible!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Texas at Austin, University of Surrey, University of Oxford...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Q-GaLore, a method that combines quantization and low-rank projection to
                    reduce memory usage during training of large language models. Unlike previous methods, Q-GaLore
                    adaptively updates the gradient subspace based on its convergence statistics, reducing the number of
                    SVD operations and training time.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">
                15:44
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08716" target="_blank">
                        @arXiv 2407.08716
                    </a>
                    <span class="tweet-title">
                        LLMs: Cheating on Tests? A Taxonomy of Data Contamination
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a formal taxonomy for categorizing different types of data contamination in
                    large language models (LLMs), going beyond previous work that focused on full-dataset contamination.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">
                16:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08303" target="_blank">
                        @arXiv 2407.08303
                    </a>
                    <span class="tweet-title">
                        Million-Word Images: A New Dataset for Super-Smart AI Vision
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Chinese Academy of Sciences, Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for generating detailed image descriptions using a "perceptual
                    fusion" pipeline. This pipeline combines information from various vision experts, such as object
                    detectors and text recognition models, to create more comprehensive captions than traditional
                    methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">
                16:33
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08571" target="_blank">
                        @arXiv 2407.08571
                    </a>
                    <span class="tweet-title">
                        Stop the Stereotype Machine: New Metric Makes Image Search More Fair
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new metric called Multi-group Proportional Representation (MPR) to
                    measure how well image search results represent diverse groups of people. Unlike previous methods
                    that focus on individual attributes like gender or race, MPR considers combinations of attributes,
                    capturing the representation of intersectional groups.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">
                16:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08516" target="_blank">
                        @arXiv 2407.08516
                    </a>
                    <span class="tweet-title">
                        AI's New BFF: LLMs and Symbolic AI Join Forces for Smarter Agents
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft, University of Virginia
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the convergence of connectionist and symbolic AI paradigms in the context of
                    LLM-empowered autonomous agents (LAAs). It contrasts LAAs with Knowledge Graphs (KGs) within the
                    neuro-symbolic AI theme, highlighting the unique strengths of LAAs in mimicking human-like reasoning
                    processes, scaling effectively with large datasets, and leveraging in-context samples without
                    explicit re-training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">
                17:20
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08044" target="_blank">
                        @arXiv 2407.08044
                    </a>
                    <span class="tweet-title">
                        RoLoRA: Spinning LLMs for Better Quantization, No Outliers Allowed!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Hong Kong University of Science and Technology, Meta
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes RoLoRA, a new method for fine-tuning large language models (LLMs) that
                    integrates rotation to eliminate outliers in activation distributions. Unlike previous work that
                    focuses on post-training quantization, RoLoRA tackles the outlier problem during the fine-tuning
                    process, leading to more robust weight-activation quantization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">
                17:48
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08086" target="_blank">
                        @arXiv 2407.08086
                    </a>
                    <span class="tweet-title">
                        Kernels on Manifolds: A Software Package for Smooth Data
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge, University of Oxford, Karlsruhe Institute of Technology...
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces the GeometricKernels package, a software library that implements heat and
                    Matérn kernels for a wide range of geometric spaces, including manifolds, meshes, and graphs. This
                    is different from previous work because it provides a unified framework for working with these
                    kernels in various geometric settings, making it easier to apply them in machine learning tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">
                18:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08056" target="_blank">
                        @arXiv 2407.08056
                    </a>
                    <span class="tweet-title">
                        Low-Rank Adapters: Pareto Front Learning Gets a Memory Makeover!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        École Polytechnique Fédérale de Lausanne, University of Geneva
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces PaLoRA, a new method for Pareto Front Learning that uses low-rank adapters to
                    efficiently parameterize the Pareto Front. Unlike previous methods, PaLoRA dedicates the original
                    model to learning general features and the adapters to learning task-specific features, resulting in
                    a significant reduction in memory overhead.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">
                18:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07921" target="_blank">
                        @arXiv 2407.07921
                    </a>
                    <span class="tweet-title">
                        Indoor Localization Gets a Blockchain Boost: No More Single-Point Failures!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nanyang Technological University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a decentralized federated learning framework called DFLoc for indoor
                    localization. Unlike previous work, DFLoc addresses both privacy and security concerns by using
                    blockchain technology to decentralize the system and mitigate the impact of malicious attacks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">
                19:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08672" target="_blank">
                        @arXiv 2407.08672
                    </a>
                    <span class="tweet-title">
                        Vision-Language Reasoning Gets a Continuous Upgrade: Neural ODEs to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Southern University of Science and Technology, University of Cambridge
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes NODE-Adapter, a method that uses Neural Ordinary Differential Equations (Neural
                    ODEs) to refine prototypes for vision-language reasoning tasks. Unlike previous methods that rely on
                    single-modality fine-tuning or excessive learnable parameters, NODE-Adapter leverages both visual
                    and textual modalities for more accurate prototype estimation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">
                19:30
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07959" target="_blank">
                        @arXiv 2407.07959
                    </a>
                    <span class="tweet-title">
                        LLMs for Code: Are We Summarizing Too Much?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nanyang Technological University, University of New South Wales, Chongqing University...
                    </span>
                </div>
                <div class="primary-text">
                    This research goes beyond simply using LLMs for code summarization. It delves into the effectiveness
                    of different prompting techniques, model settings, and even explores the suitability of LLMs as
                    evaluators for code summaries.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">
                20:01
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08546" target="_blank">
                        @arXiv 2407.08546
                    </a>
                    <span class="tweet-title">
                        Brain Shrinkage? Let's Map It! New Tool Evaluates AI's Understanding of Alzheimer's
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU, University of Illinois
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new metric, Brain Volume Change Score (VCS), to evaluate the saliency
                    maps of Alzheimer's disease classifiers. Unlike previous methods that focus on specific brain
                    regions, VCS considers the entire brain's volume changes, reflecting the disease's global impact.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">
                20:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07995" target="_blank">
                        @arXiv 2407.07995
                    </a>
                    <span class="tweet-title">
                        Flow4D: LiDAR Scene Flow Gets a 4D Makeover!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Daegu Gyeongbuk Institute of Science and Technology, CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Flow4D, a LiDAR scene flow framework that uses a 4D voxel network to
                    explicitly extract spatio-temporal features from multiple point clouds. This differs from previous
                    methods that primarily relied on 2D representations and channel-wise fusion for temporal
                    information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">
                20:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08239" target="_blank">
                        @arXiv 2407.08239
                    </a>
                    <span class="tweet-title">
                        Fake Audio Forensics: When Experts Disagree, Entropy Wins!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Chinese Academy of Sciences, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes an unsupervised domain adaptation method called SDE for locating manipulated
                    regions in partially fake audio. Unlike previous methods that rely on data augmentation, SDE
                    leverages a collection of diverse experts trained on the source domain to identify the most
                    informative samples from the target domain.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">
                21:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08572" target="_blank">
                        @arXiv 2407.08572
                    </a>
                    <span class="tweet-title">
                        Skeleton-Based Action Recognition: When AI Gets Tricked by a Little Jiggle!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Hefei University of Technology, University of Science and Technology of China, University
                        College London
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on adversarial transferability in skeleton-based human activity recognition
                    (S-HAR). Unlike previous work that primarily focused on white-box attacks, this paper proposes a new
                    post-train Dual Bayesian Attack (PDBA) strategy to improve adversarial transferability against
                    unknown S-HAR models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">
                21:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08459" target="_blank">
                        @arXiv 2407.08459
                    </a>
                    <span class="tweet-title">
                        Deep Learning's New Trick: Graph Expansions for Universal Scaling Limits
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford, University of California Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to studying scaling limits of neural networks using the
                    genus expansion technique from random matrix theory. Unlike previous work, which often relied on
                    specific techniques for different architectures and weight distributions, this method provides a
                    unified framework for analyzing a wide range of neural network settings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">
                22:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08625" target="_blank">
                        @arXiv 2407.08625
                    </a>
                    <span class="tweet-title">
                        Cell Morphology: The Secret Weapon for Cancer Detection?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel DeepCMorph model that pre-trains on cell morphology, specifically
                    nuclei segmentation and cell type annotation, before being applied to histopathological image
                    classification. This approach differs from previous work that typically trains models directly on
                    histopathology images or uses unsupervised pre-training methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">
                22:49
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07916" target="_blank">
                        @arXiv 2407.07916
                    </a>
                    <span class="tweet-title">
                        Lightning Network: GNNs Crack the Code for Faster Bitcoin Payments
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This research distinguishes itself by benchmarking Graph Neural Networks (GNNs) on real-world
                    Lightning Network data, demonstrating their effectiveness in predicting various network properties.
                    Unlike previous studies that focused on analyzing the network's structure, this paper explores the
                    potential of GNNs for practical applications within the Lightning Network.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">
                23:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08675" target="_blank">
                        @arXiv 2407.08675
                    </a>
                    <span class="tweet-title">
                        AI Gets a Design Makeover: CAD-Prompted Images for Feasible Engineering Concepts
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method for improving the feasibility of designs generated by
                    text-to-image models. Unlike previous approaches that focus on prompting with specific words or
                    integrating with CAD software, this method utilizes CAD images as prompts alongside text prompts to
                    guide the image generation process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">
                23:45
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08607" target="_blank">
                        @arXiv 2407.08607
                    </a>
                    <span class="tweet-title">
                        Empathy Detection: It's Not Just About Saying "I Feel You"
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Birla Institute of Technology and Science, National University of Singapore
                    </span>
                </div>
                <div class="primary-text">
                    This research goes beyond traditional empathy detection methods that rely on surface-level features
                    like sentiment or keywords. It focuses on six psychological indicators, like emotional language and
                    perspective-taking, to provide a more nuanced understanding of empathy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">
                24:11
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08221" target="_blank">
                        @arXiv 2407.08221
                    </a>
                    <span class="tweet-title">
                        Neural Rendering: From Blurry to Beautiful, No Matter the Mess!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Indian Institute of Technology Madras, University of California San Diego, Massachusetts
                        Institute of Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel neural rendering method called GAURA that can reconstruct 3D scenes
                    from imperfect images, such as those captured in low light or with motion blur. Unlike previous
                    methods that focus on specific degradation types, GAURA generalizes to various degradations by
                    incorporating learnable latent codes that encode information about the imperfection type.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">
                24:44
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07958" target="_blank">
                        @arXiv 2407.07958
                    </a>
                    <span class="tweet-title">
                        Crowdsourcing Object Detection: When Annotators Disagree, Bayes Steps In!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Surrey, University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a Bayesian Detector Combination (BDC) framework for object detection using
                    noisy crowdsourced annotations. Unlike previous methods that rely on pre-defined annotator skill
                    levels or specific object detection models, BDC infers annotator reliability automatically and is
                    model-agnostic.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">
                25:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08488" target="_blank">
                        @arXiv 2407.08488
                    </a>
                    <span class="tweet-title">
                        Lynx: The LLM That Sniffs Out Lies in AI Answers!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Patronus AI, Contextual AI, Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces LYNX, an open-source LLM specifically designed to detect hallucinations in
                    Retrieval Augmented Generation (RAG) systems. Unlike previous work that relies on closed-source LLMs
                    or heuristic-based metrics, LYNX offers a transparent and accessible solution for evaluating the
                    faithfulness of AI-generated answers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">
                25:33
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08029" target="_blank">
                        @arXiv 2407.08029
                    </a>
                    <span class="tweet-title">
                        LLMs: Causal Reasoning or Just Clever Parrots?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research critically reviews existing benchmarks for evaluating causal reasoning in large
                    language models (LLMs). It highlights how many benchmarks rely on knowledge retrieval rather than
                    true causal reasoning, and proposes criteria for designing more robust benchmarks that assess LLMs'
                    ability to handle interventions and counterfactuals.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">
                25:59
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08130" target="_blank">
                        @arXiv 2407.08130
                    </a>
                    <span class="tweet-title">
                        Spiking Tucker Fusion Transformer: Zero-Shot Learning Gets a Brain Boost!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harbin Institute of Technology, Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel Spiking Tucker Fusion Transformer (STFT) for audio-visual zero-shot
                    learning. Unlike previous methods, STFT dynamically measures the significance of each time step in
                    influencing the SNN's output, leading to improved performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">
                26:27
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07934" target="_blank">
                        @arXiv 2407.07934
                    </a>
                    <span class="tweet-title">
                        Unmasking Macro Mysteries: D-Separation and Do-Calculus in Summary Causal Graphs
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Sorbonne Universit´e, INSERM, ENS de Lyon
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on identifying macro conditional independencies and macro total effects in
                    summary causal graphs (SCGs), which are partially specified causal graphs that omit temporal
                    information. This is different from previous work that focused on fully specified causal graphs or
                    micro queries in SCGs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">
                26:55
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08678" target="_blank">
                        @arXiv 2407.08678
                    </a>
                    <span class="tweet-title">
                        Bayesian Adversaries: How to Train Your AI to Outsmart the Sneaky Stats
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Princeton University, University of Manchester, Delft University of Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research explores a new type of adversarial attack in machine learning, where the adversary
                    uses a Bayesian statistical approach instead of traditional optimization methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">
                27:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08608" target="_blank">
                        @arXiv 2407.08608
                    </a>
                    <span class="tweet-title">
                        FlashAttention-3: Attention, But Make It Asynchronous!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Meta, Nvidia
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces FlashAttention-3, a new approach to speeding up attention on GPUs by
                    leveraging asynchronous execution and low-precision computation. Unlike previous FlashAttention
                    versions, FlashAttention-3 takes advantage of the Hopper GPU architecture's specialized hardware
                    units for matrix multiplication and memory loading.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">
                27:55
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08726" target="_blank">
                        @arXiv 2407.08726
                    </a>
                    <span class="tweet-title">
                        Mapping the World: From Street View to Bird's Eye, One Million Images at a Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Carnegie Mellon University, University at Buffalo
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new data engine called MIA (Map It Anywhere) that leverages crowd-sourced
                    platforms like Mapillary and OpenStreetMap to automatically curate large-scale datasets for training
                    and benchmarking BEV (Bird's Eye View) map prediction models. This approach differs from previous
                    work that relied on smaller, manually collected datasets from autonomous vehicles.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">
                28:21
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08689" target="_blank">
                        @arXiv 2407.08689
                    </a>
                    <span class="tweet-title">
                        AI Bill of Rights: From Blueprint to Reality - A Guide for the Confused
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This research paper differs from previous work by providing a practical guide for operationalizing
                    the principles outlined in the Blueprint for an AI Bill of Rights. It summarizes existing research
                    on topics like fairness, privacy, and explainability, making it accessible to practitioners who may
                    not have a deep technical background.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">
                28:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08735" target="_blank">
                        @arXiv 2407.08735
                    </a>
                    <span class="tweet-title">
                        Robots Get a Brain: LLMs Detect Anomalies in Real-Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, Nvidia
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a two-stage anomaly detection system using large language models (LLMs) for
                    real-time control of robots. Unlike previous work that focuses on offline or quasi-static settings,
                    this approach integrates LLMs into a closed-loop control framework, accounting for their inference
                    latency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">
                29:03
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08105" target="_blank">
                        @arXiv 2407.08105
                    </a>
                    <span class="tweet-title">
                        AI Act's Got a New BFF: Federated Learning! Who's the Boss?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Technical University of Munich, University of Bayreuth, University of Toronto
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the legal and technical challenges of applying Federated Learning (FL) within
                    the framework of the European Union's Artificial Intelligence Act (AI Act). It specifically focuses
                    on clarifying the responsibilities of both the FL server operator and the clients participating in
                    the training process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">
                29:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08384" target="_blank">
                        @arXiv 2407.08384
                    </a>
                    <span class="tweet-title">
                        LiDAR-Powered Roadside Units: Giving Self-Driving Cars a Helping Hand!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Tokyo
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a cooperative localization system that uses roadside LiDAR units (RSUs) to
                    enhance the accuracy of autonomous vehicle localization, particularly in areas with limited map
                    features. This approach differs from previous work by leveraging V2I communication to transmit
                    vehicle dimensions to the RSUs, enabling more precise position estimation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon" title="Play from here">
                30:00
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08388" target="_blank">
                        @arXiv 2407.08388
                    </a>
                    <span class="tweet-title">
                        AI's Got Feelings? The Trouble with Measuring Machine Confidence
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google Research
                    </span>
                </div>
                <div class="primary-text">
                    This paper examines the practice of attributing "credence" - a measure of confidence - to large
                    language models (LLMs). It argues that while attributing credence to LLMs is common in the field,
                    the theoretical basis for doing so is unclear. The paper explores the semantic interpretation of LLM
                    credence attributions, the metaphysical question of whether LLMs actually have credences, and the
                    epistemic challenges of measuring LLM credences using current techniques.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon" title="Play from here">
                30:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08458" target="_blank">
                        @arXiv 2407.08458
                    </a>
                    <span class="tweet-title">
                        NR-V2X Gets a NOMA Makeover: Deep Learning for Smarter Car Communication
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Jiangnan University, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the use of NOMA (Non-Orthogonal Multiple Access) in NR-V2X (New Radio
                    Vehicle-to-Everything) communication to mitigate resource collisions and improve the age of
                    information (AoI). Unlike previous work, it focuses on jointly optimizing AoI and energy consumption
                    using a deep reinforcement learning (DRL) approach.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon" title="Play from here">
                30:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08722" target="_blank">
                        @arXiv 2407.08722
                    </a>
                    <span class="tweet-title">
                        Robot Control From Vision: No More Modeling Needed!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Neural Jacobian Fields, a deep learning approach that learns to control
                    robots from vision alone. Unlike previous methods that rely on expert-designed models and sensors,
                    this approach directly learns the robot's 3D geometry and kinematics from a single camera stream.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet67">
            <div class="start-time-icon" title="Play from here">
                31:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08442" target="_blank">
                        @arXiv 2407.08442
                    </a>
                    <span class="tweet-title">
                        Deep Learning for Medical Time-Series Imputation: A Taxonomy of Inductive Biases
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        King’s College London, University of Warwick, University College London...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel taxonomy of deep learning imputation strategies for medical
                    time-series data, classifying existing methods based on their inductive biases and suitability for
                    specific imputation scenarios. It also highlights the importance of bridging the gap between
                    computational methodologies and medical insights to achieve clinically sound imputation models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet68">
            <div class="start-time-icon" title="Play from here">
                31:42
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08659" target="_blank">
                        @arXiv 2407.08659
                    </a>
                    <span class="tweet-title">
                        Deepfakes Get a Density Makeover: Controlling Generative Model Fidelity and Diversity
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        École Polytechnique Fédérale de Lausanne, City University of Hong Kong
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel metric called "pseudo density" to estimate the density of image
                    data, which is then used to control the fidelity and diversity of generated images. Unlike previous
                    approaches that focus on manipulating the latent space, this method directly manipulates the
                    probability density of the generated data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet69">
            <div class="start-time-icon" title="Play from here">
                32:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08019" target="_blank">
                        @arXiv 2407.08019
                    </a>
                    <span class="tweet-title">
                        Inpainting with a Twist: Optimizing Latent Spaces for Realistic Image Edits
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Xi’an Jiaotong University, EPFL
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces PILOT, a novel inpainting framework that optimizes latent vectors during
                    the reverse diffusion process. Unlike previous methods that rely on model fine-tuning or simple
                    latent blending, PILOT directly manipulates the latent space to generate coherent and high-quality
                    inpainted regions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet70">
            <div class="start-time-icon" title="Play from here">
                32:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08169" target="_blank">
                        @arXiv 2407.08169
                    </a>
                    <span class="tweet-title">
                        Forget Me Not: A Speedy New Way to Delete Data from Machine Learning Models
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new algorithm for machine unlearning that leverages Natural Gradient
                    Descent (NGD). Unlike previous methods that rely on the computationally expensive Newton step, NGD
                    offers a faster and more efficient way to remove data from trained models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet71">
            <div class="start-time-icon" title="Play from here">
                33:05
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08074" target="_blank">
                        @arXiv 2407.08074
                    </a>
                    <span class="tweet-title">
                        Latent Space: Where Geometry Meets Stiffness in 3D Printing
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Carnegie Mellon University, The Pennsylvania State University
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the use of a hybrid Variational Autoencoder (VAE) that incorporates both
                    geometric and mechanical property data to design multi-lattice structures in additive manufacturing.
                    This approach differs from previous work that primarily relied on geometric data alone.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet72">
            <div class="start-time-icon" title="Play from here">
                33:31
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.08693" target="_blank">
                        @arXiv 2407.08693
                    </a>
                    <span class="tweet-title">
                        Robots Get Brainy: New AI Makes Them Think Before They Act!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Warsaw, UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Embodied Chain-of-Thought Reasoning (ECoT) for vision-language-action
                    models (VLAs). Unlike previous VLAs that directly map observations to actions, ECoT trains VLAs to
                    reason through a task step-by-step, grounding their reasoning in visual observations and the robot's
                    state.
                </div>
            </div>
        </div>


        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Listen and learn ^.^</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202407120905_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/privacy.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading privacy.html:', error));
    </script>
</body>

</html>