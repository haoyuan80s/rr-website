
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - AI Paper Picks of the Day</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 80px;"></div>
            <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                Freshest
                Top Picks:
                <span class="highlightNumber" style="font-size: 28px;">62</span> out of <span
                    class="highlightNumber">255</span> arXiv AI
                papers
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="line-height: 1.2;">just released on</div>
                <div id="data-default-date" data-date="2024-09-13"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 10px;">
            </div>
        </div>


        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">00:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08273" target="_blank">@arXiv 2409.08273</a>
                    <span class="tweet-title">Robots Learn to Manipulate Like Humans, Thanks to YouTube!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research differs from previous work by using a generative modeling approach to learn a manipulation prior from 3D hand-object interaction trajectories extracted from in-the-wild videos. This approach does not require strict alignment between the human's intent in the video and the downstream robot tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">01:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07606" target="_blank">@arXiv 2409.07606</a>
                    <span class="tweet-title">Deep Learning's Secret Weapon: Regularizing Actors in Offline RL</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research investigates the impact of applying deep learning regularization techniques to actor networks in offline reinforcement learning, a domain where these techniques have been less explored.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">01:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08189" target="_blank">@arXiv 2409.08189</a>
                    <span class="tweet-title">Dress Up Your Digital World:  New Tech Makes Clothes Come Alive!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, Max Planck Institute for Intelligent Systems</span>
                </div>
                <div class="primary-text">
                    This research introduces Gaussian Garments, a novel approach for reconstructing realistic simulation-ready garment assets from multi-view videos. Unlike previous methods that focus on reconstructing holistic avatars, this approach separates garments from the bodies, allowing for greater flexibility in applications like virtual try-on and garment resizing.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">02:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08202" target="_blank">@arXiv 2409.08202</a>
                    <span class="tweet-title">Mazes Made Easy: How AI Learns to See the Big Picture</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces Deep Schema Grounding (DSG), a framework that uses structured representations of abstract concepts to improve visual reasoning in AI models. Unlike previous work that focuses on literal interpretations of images, DSG leverages schemas to understand the underlying patterns and rules that define abstract concepts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">02:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07819" target="_blank">@arXiv 2409.07819</a>
                    <span class="tweet-title">Selling Ads Together: A Learning Algorithm for Joint Profits</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google, Uber, Sapienza University of Rome</span>
                </div>
                <div class="primary-text">
                    This paper explores the problem of selling a single, non-excludable good to two buyers, like an ad slot to a brand and a merchant, from an online learning perspective. It differs from previous work by considering the case where the buyers' valuations are drawn from unknown distributions, requiring the mechanism designer to learn the optimal strategy on the fly.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">02:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07932" target="_blank">@arXiv 2409.07932</a>
                    <span class="tweet-title">Reinforcement Learning: The New Social Network Sherpa</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London, University of Oxford, University of Bologna</span>
                </div>
                <div class="primary-text">
                    This research explores decentralized graph path search using reinforcement learning, where agents have only local visibility of the network, unlike previous centralized approaches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">03:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08278" target="_blank">@arXiv 2409.08278</a>
                    <span class="tweet-title">DreamHOI:  Making 3D Humans Do Cool Stuff with Text!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, University of Oxford</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new method for generating 3D human-object interactions using text prompts. Unlike previous methods that rely on extensive training data, DreamHOI leverages pre-trained text-to-image diffusion models to guide the pose optimization of a skinned human mesh.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">03:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07577" target="_blank">@arXiv 2409.07577</a>
                    <span class="tweet-title">Forget Fine-Tuning, Let's Mask It!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Amsterdam, Google Research</span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel method for adapting pretrained models to downstream tasks using binary masks. Unlike previous approaches that fine-tune the entire network, this method learns a mask that selectively activates specific weights, reducing memory requirements and improving performance in label-sparse settings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">04:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07809" target="_blank">@arXiv 2409.07809</a>
                    <span class="tweet-title">Cloning Clinical Notes:  A Privacy-Preserving Recipe for Training AI in Healthcare</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft Corporation, Gretel.ai</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method for generating synthetic clinical data that preserves the essential characteristics of real data while safeguarding patient privacy. Unlike previous approaches that rely on anonymization, this method leverages differential privacy techniques and instruction tuning to create datasets that are both clinically accurate and free from identifiable information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">04:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08160" target="_blank">@arXiv 2409.08160</a>
                    <span class="tweet-title">Surprisal Theory:  Is Context Just a Fancy Word for Frequency?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, University of Zurich</span>
                </div>
                <div class="primary-text">
                    This research challenges the common assumption that surprisal, a measure of how unexpected a word is in context, is the primary driver of reading time. The authors propose a new method to disentangle the effect of context from frequency, showing that context's role in reading time prediction might be smaller than previously thought.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">04:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07486" target="_blank">@arXiv 2409.07486</a>
                    <span class="tweet-title">Financial Market Simulator:  Trading Orders Meet Generative AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research proposes a generative foundation model, Large Market Model (LMM), specifically designed for financial market simulation. Unlike previous work that focused on modeling the Limit Order Book (LOB), LMM leverages order-level data to generate realistic market trajectories.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">05:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07645" target="_blank">@arXiv 2409.07645</a>
                    <span class="tweet-title">Pedestrian Prediction:  A Context-Aware Look at AI's "Black Box"</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Leeds, University College London</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method called Context-aware Permutation Feature Importance (CAPFI) to assess the importance of input features in pedestrian intention prediction models. Unlike traditional permutation feature importance, CAPFI considers the context of the scenario, such as the traffic light state or the pedestrian's proximity to the vehicle, to mitigate bias and variance in the results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">05:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08264" target="_blank">@arXiv 2409.08264</a>
                    <span class="tweet-title">Windows Agent Arena:  AI Gets a Real-World Job!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft, Carnegie Mellon University, Columbia University</span>
                </div>
                <div class="primary-text">
                    This research introduces WINDOWSAGENTARENA, a benchmark for evaluating multi-modal AI agents within a real Windows operating system. Unlike previous benchmarks that focus on specific modalities or domains, WINDOWSAGENTARENA allows agents to interact with a wide range of applications and tools available to human users.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">06:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08167" target="_blank">@arXiv 2409.08167</a>
                    <span class="tweet-title">DreamBooth's Nightmare: New Defense Makes Fake Images Unremovable</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research proposes a new adversarial attack method that adds strong perturbation to the high-frequency areas of images, making it more robust against adversarial purification techniques. Unlike previous methods, which added noise that could be easily removed, this approach focuses on adding noise to the edges of images, making it difficult to remove even with advanced purification methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">06:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08056" target="_blank">@arXiv 2409.08056</a>
                    <span class="tweet-title">NeRF Training:  Less Rendering, More Learning!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Chinese University of Hong Kong</span>
                </div>
                <div class="primary-text">
                    This research introduces an expansive supervision mechanism for training Neural Radiance Fields (NeRFs). Unlike traditional methods that render all pixels, this approach selectively renders a subset of pixels and expands their error values to estimate the loss for the entire image.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">06:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07564" target="_blank">@arXiv 2409.07564</a>
                    <span class="tweet-title">Mixing It Up: New AI Model Blends Images and Data to Predict Heart Pressure</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sano Centre for Computational Medicine, University of Sheffield, Warsaw University of Technology...</span>
                </div>
                <div class="primary-text">
                    This research introduces TabMixer, a novel module that integrates tabular data (like demographics and clinical measurements) with imaging data (from cardiac MRI videos) for predicting mean pulmonary artery pressure (mPAP). Unlike previous methods that simply concatenate or multiply these data types, TabMixer uses multi-layer perceptrons (MLPs) to mix information across spatial, temporal, and channel dimensions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">07:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07584" target="_blank">@arXiv 2409.07584</a>
                    <span class="tweet-title">Brain Segmentation:  A New Recipe for Early Alzheimer's Diagnosis</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Illinois, CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel dual-stream vision transformer (DS-ViT) pipeline that integrates segmentation and classification tasks for Alzheimer's disease diagnosis. Unlike traditional knowledge distillation methods, DS-ViT utilizes a dual-stream embedding module to bridge the gap between these distinct tasks and model architectures.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">07:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07953" target="_blank">@arXiv 2409.07953</a>
                    <span class="tweet-title">Tensor Factorizations: The Lego Blocks of Circuits!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Edinburgh, École Polytechnique Fédérale de Lausanne, Eindhoven University of Technology...</span>
                </div>
                <div class="primary-text">
                    This paper establishes a formal connection between circuit representations and tensor factorizations, showing how the latter can be used to unify various circuit learning algorithms under a single framework. This connection is novel and goes beyond previous work that only hinted at similarities between the two.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">08:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07571" target="_blank">@arXiv 2409.07571</a>
                    <span class="tweet-title">Voxel Vision:  Rendering Features for Camera Relocalization</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, University of Zurich</span>
                </div>
                <div class="primary-text">
                    This paper proposes FaVoR, a method that uses a sparse voxel representation to render feature descriptors from novel viewpoints. Unlike previous methods that rely on dense scene representations, FaVoR leverages a globally sparse yet locally dense 3D representation of 2D features, making it more efficient and scalable.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">08:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08271" target="_blank">@arXiv 2409.08271</a>
                    <span class="tweet-title">DreamBeast:  Turning Text Prompts into Fantastical 3D Animals, Part by Part!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford, Australian National University, University of Copenhagen</span>
                </div>
                <div class="primary-text">
                    This research introduces DreamBeast, a method that leverages a pre-trained 2D diffusion model to guide the generation of 3D assets with part-level control. Unlike previous methods that struggle with part-aware generation, DreamBeast efficiently extracts part-level knowledge from the 2D model and injects it into the 3D generation process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">08:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07494" target="_blank">@arXiv 2409.07494</a>
                    <span class="tweet-title">Ethereum Fraud Fighters:  New Language Model Detects Shady Transactions!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, University of California  Berkeley, University of Illinois at Urbana-Champaign...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to Ethereum fraud detection by combining a transaction language model with graph-based methods. Unlike previous studies that solely relied on numerical data or separate models, this approach leverages the semantic information and similarity patterns within transactions, resulting in a more comprehensive understanding of fraudulent behavior.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">09:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08065" target="_blank">@arXiv 2409.08065</a>
                    <span class="tweet-title">AI Supercharges the Hunt for High-Temperature Superconductors:  A New Era of Discovery!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Renmin University of China, Hefei National Laboratory, Gaoling School of Artificial Intelligence</span>
                </div>
                <div class="primary-text">
                    This research introduces an AI search engine that integrates deep learning techniques, diffusion models, and physics-based calculations to discover high-temperature superconductors. Unlike previous work, this approach directly generates crystal structures, incorporating detailed structural information and exploring materials beyond existing databases.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">09:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08103" target="_blank">@arXiv 2409.08103</a>
                    <span class="tweet-title">Faetar's Speech Recognition Challenge: Can AI Crack This Under-Resourced Language?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark corpus for speech recognition, focusing on Faetar, a severely under-resourced language with limited data and noisy recordings. This differs from previous benchmarks that often aggregate data from multiple languages, masking performance variations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">10:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07907" target="_blank">@arXiv 2409.07907</a>
                    <span class="tweet-title">COCO's Got a New Problem:  Detectors Can't Tell the Difference Between a Cat and a...  Cacti?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Intellindust, Guilin University Of Electronic Technology, ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research introduces COCO-FP, a new dataset derived from ImageNet-1K. It's designed to specifically evaluate how well object detectors handle false positives caused by background clutter, which is a common problem in real-world applications. Unlike previous datasets, COCO-FP focuses on images that don't contain any objects from the COCO categories, forcing detectors to rely on their ability to distinguish between target and non-target objects.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">10:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07691" target="_blank">@arXiv 2409.07691</a>
                    <span class="tweet-title">Ranking Models:  The Secret Sauce for Smarter Search?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nvidia</span>
                </div>
                <div class="primary-text">
                    This research benchmarks various publicly available ranking models for text retrieval, focusing on question-answering tasks. It introduces a new state-of-the-art ranking model, NV-RerankQA-Mistral-4B-v3, which achieves a significant accuracy increase of 14% compared to pipelines with other rerankers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">10:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07482" target="_blank">@arXiv 2409.07482</a>
                    <span class="tweet-title">AI Gets a Vibration Check: New Model Analyzes Industrial Signals with Expert Help</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces VSLLaVA, a pipeline that integrates expert knowledge into a large multimodal foundation model for industrial vibration signal analysis. Unlike previous work that focuses on general image recognition, VSLLaVA specifically addresses the challenges of analyzing industrial signals by incorporating domain-specific expertise.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">11:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07794" target="_blank">@arXiv 2409.07794</a>
                    <span class="tweet-title">Learning Balanced Graphs: It's Like Finding the Right Friends!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Osaka University, York University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for learning balanced signed graphs directly from data, unlike previous methods that relied on two-step approaches. The key innovation lies in incorporating linear constraints into a linear programming framework to ensure consistent edge weights based on node polarities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">11:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08212" target="_blank">@arXiv 2409.08212</a>
                    <span class="tweet-title">Robots Learn From Language:  AI Gets a Chatty Upgrade!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Princeton, The AI Institute</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method called ALGAE (Adaptive Language-Guided Abstraction from [Contrastive] Explanations) that uses language models to identify missing features in reward functions for robots. This differs from previous work by leveraging language models to iteratively refine the reward function, rather than relying solely on human demonstrations or hand-crafted features.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">11:58</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07961" target="_blank">@arXiv 2409.07961</a>
                    <span class="tweet-title">Typhoon Forecasting Gets a Diffusion Boost: AI Predicts Wind, Pressure, and More!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Imperial College London, University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research uses a Conditional Denoising Diffusion Probability Model (CDDPM) to predict multiple meteorological variables from satellite images, a novel approach compared to previous work that relied on Convolutional Neural Networks (CNN) or Squeeze-and-Excitation Networks (SENet).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">12:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08169" target="_blank">@arXiv 2409.08169</a>
                    <span class="tweet-title">MR to Ultrasound:  Matching Keypoints with a Synthetic Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard Medical School, Brigham and Women’s Hospital, Massachusetts Institute of Technology...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to matching keypoints between preoperative MR and intraoperative ultrasound images by synthesizing ultrasound images from MR data. This differs from previous methods that rely on matching features directly between the two modalities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">12:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07790" target="_blank">@arXiv 2409.07790</a>
                    <span class="tweet-title">LLMs:  Not Just for Chatbots, They're Speech Correction Superheroes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tencent, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research focuses on using LLMs for full-text error correction in Chinese speech recognition, unlike previous work that primarily focused on correcting single sentences.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">13:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08250" target="_blank">@arXiv 2409.08250</a>
                    <span class="tweet-title">Your Photos, Your Memories, Your AI-Powered Storyteller</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Los Angeles, University of Washington, Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces OmniQuery, a system that augments personal memories with contextual information extracted from multiple related memories. Unlike previous work that relies on explicit connections between queries and external data, OmniQuery leverages a taxonomy of contextual information to enhance retrieval on unstructured personal memories.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">13:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07887" target="_blank">@arXiv 2409.07887</a>
                    <span class="tweet-title">Lidar's Got Talent: Unsupervised Object Tracking Without a Single Label!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">LIGM  Ecole des Ponts  Univ Gustave Eiffel  CNRS  Valeo.ai</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel unsupervised online instance segmentation and tracking method called UNIT. Unlike previous methods that rely on pre-defined time windows or require manual annotations, UNIT operates on single scans in an online fashion, enabling it to track objects over an extended period without any human intervention.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">14:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08012" target="_blank">@arXiv 2409.08012</a>
                    <span class="tweet-title">IRL's Got a New Trick: Learning Rewards That Don't Overfit!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel regularization approach for inverse reinforcement learning (IRL) methods based on the causal invariance principle. Unlike previous work that often leads to overfitting, this method aims to learn reward functions that are robust to spurious correlations in expert demonstrations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">14:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07776" target="_blank">@arXiv 2409.07776</a>
                    <span class="tweet-title">Spiking Neural Networks Get a Gradient-Free Makeover:  Randomness Rules!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The University of Tokyo, NTT Device Technology Labs.</span>
                </div>
                <div class="primary-text">
                    This research proposes a new training method for spiking neural networks (SNNs) called augmented direct feedback alignment (aDFA). Unlike traditional methods that rely on backpropagation (BP), aDFA uses random projections to update weights, making it more efficient and biologically plausible.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">15:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07753" target="_blank">@arXiv 2409.07753</a>
                    <span class="tweet-title">Robots Get Smart: New Research Gives Them Human-Like 'Relevance' Sense!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel concept called "relevance" for scene understanding in human-robot collaboration (HRC). Unlike previous work focusing on visual saliency and attention mechanisms, this approach considers the broader context, long-term objectives, and human factors to identify relevant elements in a scene.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">15:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08102" target="_blank">@arXiv 2409.08102</a>
                    <span class="tweet-title">Bayesian Self-Training:  Giving 3D Segmentation a Confidence Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research introduces a Bayesian self-training framework for semi-supervised 3D segmentation, which leverages uncertainty estimation techniques from active learning to generate more reliable pseudo-labels. Unlike previous work that relies on simple confidence thresholding, this approach utilizes both the estimated Shannon entropy and the predicted label from each stochastic forward pass to filter out uncertain or inconsistent points.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">15:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07965" target="_blank">@arXiv 2409.07965</a>
                    <span class="tweet-title">AVs Learn to Drive by Feeling the Road: Differentiable Simulation Makes Autonomous Vehicles Smarter</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">INSAIT, Sofia University, ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research uses a differentiable simulator to train autonomous vehicle controllers, allowing gradients of the environment dynamics to be incorporated into the training loop. This approach differs from previous methods that treat the environment as a black box, leading to slower and less efficient learning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">16:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08239" target="_blank">@arXiv 2409.08239</a>
                    <span class="tweet-title">LLMs Learn New Tricks: Synthetic Data Makes Them Smarter!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta, Oxford University, University College London</span>
                </div>
                <div class="primary-text">
                    This paper introduces Source2Synth, a method for generating synthetic data grounded in real-world sources. Unlike previous work that relies heavily on human annotations, Source2Synth leverages the LLM itself to curate the synthetic data, improving its quality and leading to better performance on complex tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">16:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08098" target="_blank">@arXiv 2409.08098</a>
                    <span class="tweet-title">AI Judges: Can a Computer Predict Your Employment Tribunal Fate?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research distinguishes itself by creating a large-scale dataset of UK Employment Tribunal cases, annotated with legal information extracted using a large language model (LLM). This dataset, called CLC-UKET, is then used to benchmark the performance of various models in predicting case outcomes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">17:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07827" target="_blank">@arXiv 2409.07827</a>
                    <span class="tweet-title">Painting the Sounds of Emotion: AI Turns Art into Music</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Queen Mary University of London</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to music generation by leveraging the emotional content of paintings. Unlike previous work that primarily focused on text-to-music generation, this study explores the unique relationship between visual art and music, creating a system that translates emotions depicted in paintings into musical compositions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">17:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08027" target="_blank">@arXiv 2409.08027</a>
                    <span class="tweet-title">AI Explains Itself:  Social Science Theories Make AI Feedback More Human-Friendly</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">EPFL, IPPR, UCL</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel framework called iLLuMinaTE that uses large language models (LLMs) to translate complex AI explanations into human-understandable feedback for students. Unlike previous work that focused on post-hoc explainers, iLLuMinaTE leverages social science theories to guide the LLM in generating explanations that are more relevant, actionable, and trustworthy for students.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">17:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07822" target="_blank">@arXiv 2409.07822</a>
                    <span class="tweet-title">Federated Learning Gets a Wireless Weight Loss Plan!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">KTH Royal Institute of Technology, Yale University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new federated learning scheme that uses adaptive weights during aggregation, unlike previous over-the-air schemes that used predefined weights. This approach helps mitigate the impact of wireless channel conditions on learning performance without requiring channel state information at the transmitter side.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">18:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08097" target="_blank">@arXiv 2409.08097</a>
                    <span class="tweet-title">Falsifying Controllers: A Multi-Fidelity Bayesian Approach to Finding Bugs</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Rochester Institute of Technology, Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces a multi-fidelity Bayesian optimization framework for falsification in learning-based control systems. Unlike previous work that relies solely on high-fidelity simulators, this approach leverages simulators with varying levels of accuracy to improve efficiency and reduce computational costs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">18:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07653" target="_blank">@arXiv 2409.07653</a>
                    <span class="tweet-title">STAND: Learning Like a Forest, But With Less Data!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University</span>
                </div>
                <div class="primary-text">
                    This research introduces STAND, a method for learning preconditions in interactive task learning settings. Unlike traditional approaches that rely on statistical learning, STAND embraces ambiguity and builds a complete space of possible classifiers, allowing it to learn from small, noisy datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">19:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08145" target="_blank">@arXiv 2409.08145</a>
                    <span class="tweet-title">Learning Speed: The Key to Coordination Success or Failure?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This paper introduces a dynamic coordination game where the state evolves based on past play, allowing for a more realistic representation of economic phenomena like currency crises or networked product adoption. The key innovation is the tight characterization of how the speed of learning shapes equilibrium dynamics, showing that risk-dominant action is selected only when learning is slow enough.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">19:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08058" target="_blank">@arXiv 2409.08058</a>
                    <span class="tweet-title">Electrode Shift? No Problem! New Layer Makes Biosignal Sensors Smarter</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Imperial College London, Meta Reality Labs</span>
                </div>
                <div class="primary-text">
                    This research introduces a Spatial Adaptation Layer (SAL) that can be added to any biosignal array model. Unlike previous methods, SAL learns a parametrized affine transformation at the input between two recording sessions, making it more interpretable and efficient.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">19:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07715" target="_blank">@arXiv 2409.07715</a>
                    <span class="tweet-title">Seeing Through Smoke: New Dataset Helps Robots Navigate Wildfires</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset, FIReStereo, specifically designed for training depth perception algorithms in visually degraded environments, particularly those involving smoke. Unlike previous thermal datasets, FIReStereo includes stereo thermal image pairs, LiDAR, IMU data, and ground truth depth maps, making it suitable for training stereo depth estimation models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">20:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07746" target="_blank">@arXiv 2409.07746</a>
                    <span class="tweet-title">Brain Tumor Detectives:  State-Space Models Crack the Code on High-Resolution Images!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stony Brook University, Harvard University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel state-space model (SSM) based masked autoencoder for 3D multi-contrast MR images. Unlike traditional Vision Transformers (ViTs), this approach scales linearly to high resolution, enabling efficient processing of large volumetric data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">20:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08211" target="_blank">@arXiv 2409.08211</a>
                    <span class="tweet-title">Graph Laplacians: The Secret Sauce for Multi-Fidelity Modeling</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Southern California, University of Birmingham, Caltech</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel multi-fidelity modeling approach that leverages the spectral properties of the graph Laplacian constructed from low-fidelity data to define a prior distribution for the multi-fidelity estimates. This approach differs from previous work by explicitly incorporating a probabilistic framework and utilizing Bayesian inference to derive an optimal multi-fidelity estimate.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">21:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07985" target="_blank">@arXiv 2409.07985</a>
                    <span class="tweet-title">AI Safety Games:  Red Teaming Gets a Formal Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This paper introduces a formal game-theoretic model called "AI-Control Games" to evaluate the safety of AI deployment protocols. Unlike previous work that relies on empirical studies, this model allows for a more rigorous and systematic analysis of protocol safety and usefulness.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">21:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08229" target="_blank">@arXiv 2409.08229</a>
                    <span class="tweet-title">Photonic Quantum Computers:  Shining a Light on the Future of Computing!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Ain Shams University, Zewail City of Science  Technology and Innovation, USTC...</span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive overview of advancements in photonic quantum computing, examining current performance, architectural designs, and strategies for developing large-scale, fault-tolerant photonic quantum computers. It highlights recent groundbreaking experiments that leverage the unique advantages of photonic technologies, underscoring their transformative potential.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">21:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07679" target="_blank">@arXiv 2409.07679</a>
                    <span class="tweet-title">Boltzmann Machines Get a Ratio-nal Makeover: New Learning Method Outperforms the Old Guard!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Fujitsu, University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research introduces a new learning method called "ratio divergence" (RD) for discrete energy-based models. Unlike previous methods that rely solely on forward or reverse Kullback-Leibler divergence (KLD), RD combines the strengths of both approaches, addressing underfitting and mode collapse issues.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">22:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07918" target="_blank">@arXiv 2409.07918</a>
                    <span class="tweet-title">AI Makes Music Feel: New System Generates Emotionally Charged Tunes</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Queen Mary University of London, Vrije Universiteit Brussel</span>
                </div>
                <div class="primary-text">
                    This research combines affective modeling with computational code generation, using reinforcement learning to create music that aligns with specific emotional states. Previous work focused on either affective modeling or code generation, but not both.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">22:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08036" target="_blank">@arXiv 2409.08036</a>
                    <span class="tweet-title">Sheaves to the Rescue:  New Neural Networks Tame Heterogeneous Data</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research proposes using cellular sheaves to model heterogeneous graphs, a departure from previous approaches that focused on modifying model architectures to handle data heterogeneity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">23:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07615" target="_blank">@arXiv 2409.07615</a>
                    <span class="tweet-title">AI Text Detection:  A Team Effort, Not a Lone Wolf!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sorbonne University, Montreal Institute for Learning Algorithms</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for detecting machine-generated text that uses an ensemble of large language models (LLMs) instead of relying on a single detector. This approach aims to improve the robustness of detection by combining the strengths of multiple models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">23:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08232" target="_blank">@arXiv 2409.08232</a>
                    <span class="tweet-title">Brain Tumor Segmentation:  Ensemble of Models Wins the Day!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Children's National Hospital, Universidad Politécnica de Madrid, Princeton University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel ensemble approach for brain tumor segmentation, combining the strengths of two state-of-the-art deep learning models, nnU-Net and SwinUNETR, to improve accuracy and robustness. This differs from previous work by focusing on a label-wise ensemble strategy, tailoring the approach to specific tumor sub-regions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">23:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07656" target="_blank">@arXiv 2409.07656</a>
                    <span class="tweet-title">Turing Test Passed: AI's Got Brains (and They're Learning!)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of S˜ao Paulo, King’s College  University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research argues that generative transformers, a type of AI, can pass the Turing test by learning from experience, unlike earlier AI systems that relied on preprogramming.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">24:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07638" target="_blank">@arXiv 2409.07638</a>
                    <span class="tweet-title">LLMs Can't Count?  The Fixed-Effect Fallacy Strikes Again!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MicrosoftResearch</span>
                </div>
                <div class="primary-text">
                    This paper investigates the performance of GPT-4 on deterministic tasks, such as counting and multiplication, and finds that seemingly trivial changes in the prompt or input data can significantly impact accuracy. This differs from previous work by focusing on the sensitivity of LLMs to these variations, rather than just reporting overall performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">24:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07671" target="_blank">@arXiv 2409.07671</a>
                    <span class="tweet-title">Neural Networks:  A New Trick for Solving Tricky Equations</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Maryland</span>
                </div>
                <div class="primary-text">
                    This research explores using Physics-Informed Neural Networks (PINNs) to correct existing solutions to convection-diffusion equations, rather than directly approximating the solution. The paper also investigates the impact of input transformations on PINN performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">25:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.07960" target="_blank">@arXiv 2409.07960</a>
                    <span class="tweet-title">Vision Models:  A Doctor's New Best Friend for Image Segmentation?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research investigates the effectiveness of foundational models (FMs) for medical image segmentation, specifically focusing on their ability to generalize to data from different sources, a crucial challenge in medical imaging. The study compares various FMs, fine-tuning techniques, and decoder architectures, highlighting the potential of FMs to enhance domain generalization performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">25:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.08171" target="_blank">@arXiv 2409.08171</a>
                    <span class="tweet-title">Drones + Deep Learning = Dieback Detective!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research uses deep learning to automatically segment individual tree crowns from drone imagery, then uses vegetation indices to estimate dieback severity. Unlike previous work, this study validates its results with expert field-based dieback measurements.
                </div>
            </div>
        </div>

        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202409131647_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>    
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>
</html>