
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Roboto:wght@300;400;500;700&display=swap">
</head>

<body>
    <div class="container">
        <div class="header">
            <div style="height: 100px;"></div>
            <img src="assets/AppLaunchFrog.gif" alt="Descriptive text about the GIF" style="width: 20%; height: auto;">
            <div style="height: 30px;"></div>
            <div class=" notification-text">Hop right over soon... We're busy getting everything just right and will be
                here
                before you can say 'Ribbit'!
            </div>
            <div style="height: 100px;"></div>
            <div class="header-text">ArXiv AI Papers - Daily Highlights</div>
            <div class="header-text">Wed. Jul 10, 2024</div>
            <div class="subheader-text">Opening music from <a href="https://ikson.com" target="_blank"
                    style="color: black; text-decoration: none;">TELL YOUR STORY by ikson</a></div>
            <div style="height: 30px;"></div>
        </div>
        <div class="tweet" id="tweet0">
            <div class="start-time-icon">00:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07018" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.07018</a>
                    <span class="tweet-title">Can AI Read Your Diary to Predict Treatment Effects?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, Vector Institute, Meta</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method called NATURAL, which uses large language models (LLMs) to estimate causal effects from unstructured text data. Unlike previous approaches that rely on structured data, NATURAL leverages the ability of LLMs to extract conditional distributions from text, enabling the estimation of treatment effects without manual data structuring.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon">01:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06380" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06380</a>
                    <span class="tweet-title">Data Diet:  How to Build the Perfect Language Model Menu</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nvidia</span>
                </div>
                <div class="primary-text">
                    This research systematically analyzes the entire pipeline of constructing pretraining datasets for language models, going beyond just data curation and exploring a wider range of data sources and attributes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon">01:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06460" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06460</a>
                    <span class="tweet-title">Making AI Forget: A Six-Way Test for Machine Unlearning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, Princeton University, University of Southern California...</span>
                </div>
                <div class="primary-text">
                    This research proposes a new benchmark called MUSE to evaluate machine unlearning algorithms for language models. MUSE considers six key properties, including verbatim memorization, knowledge memorization, privacy leakage, utility preservation, scalability, and sustainability. This comprehensive approach goes beyond previous benchmarks that focused on specific tasks like question answering.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon">02:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06842" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06842</a>
                    <span class="tweet-title">Chatting Your Way to a 3D Makeover: New AI Edits Scenes with Text Prompts</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Beihang University, Google, Megvii...</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to 3D scene editing by decoupling the 2D editing process from the 3D reconstruction process. This allows for flexible integration of various visual models, unlike previous methods that relied on intricate pipelines combining specific 2D and 3D models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon">02:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06946" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06946</a>
                    <span class="tweet-title">Can AI Recognize Itself? New Study Tests Language Models' Self-Awareness</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research explores self-recognition in language models (LLMs) by introducing a novel approach using model-generated "security questions." Unlike previous work that relies on internal model parameters or output probabilities, this method only requires model outputs, making it accessible for external evaluation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon">02:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07082" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.07082</a>
                    <span class="tweet-title">Reinforcement Learning:  Can We Learn to Learn Better?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method called OPEN, which meta-learns an update rule for reinforcement learning (RL) optimizers. Unlike previous learned optimizers, OPEN specifically targets and addresses three key difficulties in RL: non-stationarity, plasticity loss, and exploration.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon">03:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07038" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.07038</a>
                    <span class="tweet-title">Climate Change Chat:  A New Model Decodes Disagreements on Reddit</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research uses Graph Attention Networks (GATs) to analyze comment-reply pairs on Reddit, focusing on climate change discussions. Unlike previous work that primarily relies on textual features, this model incorporates both textual embeddings and sentiment scores, capturing the intricate interactions and sentiment dynamics within these conversations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon">03:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06723" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06723</a>
                    <span class="tweet-title">Captions Get a Graph Makeover:  Images Described with Links and Relationships!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Apple Inc., University of Washington, National Tsing Hua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new way to annotate images using a graph-based captioning (GBC) format. Unlike traditional scene graphs, GBC uses plain text descriptions for each node, allowing for more flexibility and intuitiveness.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon">04:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07074" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.07074</a>
                    <span class="tweet-title">SLAM-ing the Competition: A New Continuous-Time GBP Framework for Distributed SLAM</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, Imperial College London, University of Cyprus</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel continuous-time Gaussian Belief Propagation (GBP) framework, dubbed Hyperion, for distributed SLAM. Unlike traditional centralized approaches, Hyperion leverages message-passing between individual nodes and factors in a factor graph, enabling decentralized probabilistic inference across multiple agents.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon">04:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06334" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06334</a>
                    <span class="tweet-title">Double-Ended Synthesis Planning:  A Chemical Search Party!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Georgia Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to computer-aided synthesis planning (CASP) that incorporates starting material constraints. Unlike previous methods that assume any building block can be used, this method specifically targets desired starting materials, making it more relevant to real-world scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon">05:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07061" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.07061</a>
                    <span class="tweet-title">Internet of Agents:  LLMs Team Up to Solve Problems Like a Boss!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Northern Arizona University, Peking University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new framework called the Internet of Agents (IoA) that allows different AI agents, each with unique skills and knowledge, to collaborate on complex tasks. Unlike previous multi-agent systems, IoA enables agents to work together even if they were developed independently and can be distributed across multiple devices.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon">05:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06325" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06325</a>
                    <span class="tweet-title">Sparsity Saves the Day: How Compressive Sensing Makes Microservices Management a Breeze</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Texas A&M University, University of Texas at Austin</span>
                </div>
                <div class="primary-text">
                    This research introduces CONGO, an algorithm for online convex optimization that leverages sparsity in the gradient of the objective function. Unlike previous methods, CONGO uses compressive sensing to estimate gradients with fewer samples, leading to faster convergence and reduced computational cost.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon">06:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07090" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.07090</a>
                    <span class="tweet-title">Ray Tracing for Fuzzy Particles:  Making 3D Scenes Super Speedy!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">NVIDIA, University of Toronto, Vector Institute</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for ray tracing particle-based scene representations, specifically focusing on 3D Gaussian Splatting. Unlike previous work that relied on rasterization, this approach leverages specialized GPU ray tracing hardware for faster and more flexible rendering.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon">06:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06863" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06863</a>
                    <span class="tweet-title">AI's Got Culture: New Benchmark Tests If Text-to-Image Models Can Tell a Samosa from a Sushi</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark called CUBE to evaluate the cultural competence of text-to-image models. Unlike previous benchmarks that focus on realism and faithfulness, CUBE assesses cultural awareness and diversity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon">06:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07087" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.07087</a>
                    <span class="tweet-title">AI's Got a Copycat Problem: New Benchmark Catches Literal and Non-Literal Copying</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, Cornell University, Allen Institute for AI...</span>
                </div>
                <div class="primary-text">
                    This research introduces COPYBENCH, a benchmark that evaluates both literal and non-literal copying of copyrighted text by language models. Previous work primarily focused on literal copying, neglecting the more nuanced forms of plagiarism that can occur.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon">07:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06464" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06464</a>
                    <span class="tweet-title">Sidewalk Sleuths: New Dataset Helps Us See the City Through Pedestrian Eyes</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of São Paulo, University of Illinois Chicago, Massachusetts Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This research introduces a new multimodal dataset called SideSeeing, which combines video footage, sensor data, and GPS information to provide a comprehensive understanding of sidewalk accessibility. Unlike previous datasets that focus on specific aspects of sidewalks, SideSeeing offers a more holistic view by integrating multiple data sources.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon">08:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06518" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06518</a>
                    <span class="tweet-title">V2X Communication:  Graph Neural Networks to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Jiangnan University, Tsinghua University, Shanghai Jiao Tong University...</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for resource allocation in V2X communication by integrating Graph Neural Networks (GNN) with Deep Reinforcement Learning (DRL). Unlike previous work that primarily focuses on centralized resource allocation, this approach enables distributed decision-making, allowing vehicles to make independent resource allocation decisions based on local observations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon">08:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07088" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.07088</a>
                    <span class="tweet-title">Spacecraft Dating:  How to Train a Deep Learning Controller to Dock Safely and Reliably</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Hebrew University of Jerusalem, IT University of Copenhagen...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel design-for-verification approach that modifies the training loop of deep reinforcement learning (DRL) controllers to make them more verification-friendly. It also utilizes formal verification techniques, specifically k-induction, to ensure the correctness of the training process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon">09:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07059" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.07059</a>
                    <span class="tweet-title">Brain-Like Models:  Are We Measuring the Right Things?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, NYU, University of Tübingen</span>
                </div>
                <div class="primary-text">
                    This research directly optimizes synthetic datasets to become more similar to real neural recordings, allowing for a deeper understanding of how different similarity measures prioritize various aspects of the data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon">09:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06683" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06683</a>
                    <span class="tweet-title">Mapping the Future:  How BEV Features Speed Up Self-Driving Cars</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, Nvidia, Stanford University</span>
                </div>
                <div class="primary-text">
                    This research proposes directly using internal features from online map estimation models for behavior prediction, rather than relying solely on decoded map elements. This approach allows for tighter integration between mapping and prediction, potentially leading to faster inference times and more accurate predictions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon">10:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06249" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06249</a>
                    <span class="tweet-title">LLMs Learn New Tricks: A Benchmark for Teaching Code Models API Updates</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Texas at Austin</span>
                </div>
                <div class="primary-text">
                    This research introduces CodeUpdateArena, a benchmark specifically designed to evaluate how well large language models (LLMs) can incorporate new API function updates into their knowledge. Unlike previous benchmarks that focus on general code generation, CodeUpdateArena focuses on the ability of LLMs to adapt to evolving APIs, a crucial aspect for real-world code generation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon">10:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07093" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.07093</a>
                    <span class="tweet-title">Binary Brains: LLMs Go Full 1-Bit, No Pretraining Needed!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Mohamed bin Zayed University of Artificial Intelligence, CMU</span>
                </div>
                <div class="primary-text">
                    This research presents a novel method for training fully binarized large language models (LLMs) from scratch, without relying on pre-trained models. It utilizes an autoregressive distillation loss function to guide the training process, achieving comparable performance to full-precision LLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon">11:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06800" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06800</a>
                    <span class="tweet-title">Whisper's New Language Trick: Teaching an Old Dog New Sounds!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research focuses on adding new languages to pre-trained Automatic Speech Recognition (ASR) models without sacrificing performance on existing languages. It compares different fine-tuning methods, including a novel approach called Soft Language Code Tuning (SLCT), and explores the use of Elastic Weight Consolidation (EWC) to mitigate catastrophic forgetting.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon">11:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06756" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06756</a>
                    <span class="tweet-title">Fourier Features:  High Frequency, Low Generalization?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London, DeepMind</span>
                </div>
                <div class="primary-text">
                    This research investigates the frequency of representations learned by periodic activation functions in deep reinforcement learning (RL) algorithms. Unlike previous work that focused on either low or high frequency representations, this study empirically shows that periodic activations consistently converge to high frequencies regardless of their initialisation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon">11:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07086" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.07086</a>
                    <span class="tweet-title">LLMs Get a Mind of Their Own:  How AI Learned to Think Like Us (and Win at Games)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to multi-agent reinforcement learning (MARL) by incorporating a Theory of Mind (ToM) module into an LLM-based agent. This module allows the agent to generate, evaluate, and refine hypotheses about the strategies of other agents, enabling it to adapt to diverse social dynamics in multi-agent environments. This differs from previous work by explicitly modeling the mental states of other agents, rather than relying solely on observed actions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon">12:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06323" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06323</a>
                    <span class="tweet-title">LLMs Gone Wild?  New Guardrails Tame Toxic Text with Synthetic Data!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research tackles the "use-mention" distinction problem in social bias detection for LLMs. Unlike previous work, it focuses on generating synthetic data to train guardrail models specifically to differentiate between harmful use and harmless mention of biased language.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon">12:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07055" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.07055</a>
                    <span class="tweet-title">Folding Embryos: AI Predicts Cell Rearrangements Before They Happen!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Massachusetts Institute of Technology, Northeastern University</span>
                </div>
                <div class="primary-text">
                    This research introduces a dual-graph data structure to represent multicellular systems, enabling a geometric deep learning model to predict cell rearrangements in developing embryos. This approach differs from previous work by considering both granular and foam-like physical pictures of the system.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon">13:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06628" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06628</a>
                    <span class="tweet-title">IMU-Powered Action Recognition:  When Your Phone Knows You're Doing the Macarena</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for action recognition that integrates motion data from body-worn IMUs with egocentric video. The key innovation lies in the use of a multimodal masked autoencoder (MAE) for self-supervised pretraining, which leverages the natural correlation between visual and motion signals. This approach addresses the scarcity of labeled multimodal data and allows the model to learn strong representations from unlabeled data. Additionally, the paper introduces a graph-based IMU modeling technique to capture the collaborative dynamics of multiple IMU devices placed across the body.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon">13:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06939" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06939</a>
                    <span class="tweet-title">Robots Learn to Do Chores, But They Still Need Help!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">FAIR  Georgia Tech  Hello Robot Inc  Carnegie Mellon  Bielefeld University  Ukrainian Catholic University  Bosch Center for AI  IIITA  IIT ISM Dhanbad  IIITM  CNAEIT</span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark task for mobile manipulation in homes called Open Vocabulary Mobile Manipulation (OVMM). Unlike previous work that focused on closed-world scenarios with known objects, OVMM challenges robots to find and manipulate objects in novel environments with previously unseen objects.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon">13:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06866" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06866</a>
                    <span class="tweet-title">ChatGPT's Got a Bias: How Your Fandom Affects Its Answers</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This paper examines the biases in the guardrails of large language models (LLMs), specifically focusing on how contextual information about the user influences the model's likelihood to refuse a request. This is distinct from previous work that primarily focused on biases in the model's output rather than its refusal mechanisms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon">14:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06813" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06813</a>
                    <span class="tweet-title">AI Diplomat Richelieu:  Self-Evolving Agent Masters Diplomacy Without Human Help!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Beijing Information Science and Technology University</span>
                </div>
                <div class="primary-text">
                    This research introduces Richelieu, an AI agent that learns to play Diplomacy through self-play, unlike previous methods that relied on human data for training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon">14:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07071" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.07071</a>
                    <span class="tweet-title">LLMs Hallucinating?  LookbackLens to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research focuses on detecting and mitigating contextual hallucinations in LLMs, a type of error where the model generates content inconsistent with the provided context. Unlike previous work that primarily focused on hallucinations arising from the model's internal knowledge, this paper leverages attention maps to identify and address these contextual errors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon">15:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06233" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06233</a>
                    <span class="tweet-title">AI: The New Social Theorist?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This paper proposes a framework for AI-driven social theory, where AI models are used to test and generate new social theories based on their predictive power. This approach differs from previous work by emphasizing the need for AI systems to possess specific capabilities like semanticization, transferability, and generativity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon">15:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06304" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06304</a>
                    <span class="tweet-title">Video Generation Gets a Visual Makeover:  New Model Learns from Images!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Snap Inc., UC Merced, Carnegie Mellon University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to video generation that incorporates visual information during the training process. Unlike previous models that rely solely on text-based prompts, this model leverages a retrieval method to pair multimodal in-context examples with text prompts, enabling it to generate videos grounded in both text and images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon">16:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06529" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06529</a>
                    <span class="tweet-title">Financial Fraud Fighters:  GNN-CL  Outwits  Tricksters  with  Smart  Weights!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Columbia University, Rutgers University, College of William & Mary...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel GNN-CL model for financial fraud detection. It distinguishes itself by incorporating a reinforcement learning mechanism to dynamically adjust the weights assigned to central nodes in the graph, effectively mitigating the issue of feature dilution that often plagues graph-based models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon">16:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06886" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06886</a>
                    <span class="tweet-title">Embodied AI:  From Couch Potato to Kitchen King!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Hong Kong, Peking University</span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive survey of Embodied AI in the era of Multi-modal Large Models (MLMs), focusing on the alignment of cyber space with the physical world. It distinguishes itself from previous surveys by including recent advancements in embodied robots, simulators, and four main research tasks: embodied perception, embodied interaction, embodied agents, and sim-to-real robotic control.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon">16:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06235" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06235</a>
                    <span class="tweet-title">AI Auditing: From Balance Sheets to Bots, It's Time to Get Real!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research goes beyond just looking at the technical aspects of AI systems and also examines the governance structures of the organizations that design and deploy them.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon">17:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06677" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06677</a>
                    <span class="tweet-title">Transformers Get a Makeover:  Dynamic Modules for Better Language Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Renmin University of China, Tsinghua University...</span>
                </div>
                <div class="primary-text">
                    This research proposes a new architecture called Mixture-of-Modules (MoM) that breaks the traditional depth-ordered structure of Transformers. Instead of processing tokens sequentially from shallow to deep layers, MoM allows for dynamic selection and assembly of modules, creating a more flexible and efficient computation graph.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon">17:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06833" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06833</a>
                    <span class="tweet-title">CryoET Segmentation Gets a Prompt-Based Makeover: No Training Required!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, University of Alabama at Birmingham</span>
                </div>
                <div class="primary-text">
                    This research introduces a training-free approach for segmenting CryoET tomograms using prompt-based methods. Unlike previous work that relied on supervised learning or template matching, this method leverages existing 2D foundation models and a novel hierarchical feature matching strategy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon">18:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06549" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06549</a>
                    <span class="tweet-title">Ads Relevance:  One Model to Rule Them All!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel multi-faceted attention model that uses task ID encoding to improve the generalization capability of a single model for multi-task ads relevance. This approach differs from previous work by treating the multi-task problem as a language task, enabling the model to learn from all tasks while only needing to see one task at a time during inference.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon">18:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06938" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06938</a>
                    <span class="tweet-title">RodinHD:  Avatar Generation Gets a Hair-Raising Upgrade!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Science and Technology of China, Tsinghua University, Microsoft Research Asia</span>
                </div>
                <div class="primary-text">
                    This paper tackles the issue of "catastrophic forgetting" in 3D avatar generation, which occurs when the model forgets details from previously trained avatars. To address this, the authors introduce a novel data scheduling strategy called "task replay" and a weight consolidation regularization term.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon">19:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06984" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06984</a>
                    <span class="tweet-title">Stereo Vision:  Seeing Objects in 3D, Even Through Glass!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Tencent</span>
                </div>
                <div class="primary-text">
                    This research introduces CODERS, a one-stage approach for object detection, pose estimation, and reconstruction from stereo images. Unlike previous methods that rely on two-stage frameworks or point cloud data, CODERS leverages stereo information to directly estimate object properties, including shape, in an end-to-end manner.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon">19:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06312" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06312</a>
                    <span class="tweet-title">Koopman Learning: When Data Can't Save You</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This paper establishes fundamental limits on Koopman learning, proving that certain spectral properties of Koopman operators cannot be learned from trajectory data, even with unlimited data and perfect measurements. This is achieved by embedding abrupt spectral changes into the system dynamics, effectively creating adversarial systems that prevent algorithmic convergence.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon">20:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06703" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06703</a>
                    <span class="tweet-title">HERMES:  A Neural Network That Predicts Protein Mutations Like a Boss!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington</span>
                </div>
                <div class="primary-text">
                    This research introduces HERMES, a 3D rotationally equivariant neural network model for mutational effect and stability prediction. Unlike previous work, HERMES is pre-trained to predict amino acid propensity from its surrounding 3D structure, making it more versatile and adaptable for various predictive objectives.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon">20:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06823" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06823</a>
                    <span class="tweet-title">DJs, Meet Your New Sidekick: AI Learns to Drop the Beat (and Cue Points)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for cue point estimation in electronic dance music (EDM) tracks using a pre-trained object detection transformer, fine-tuned on a new dataset of manually annotated cue points. Unlike previous methods, this approach does not rely on low-level musical information analysis.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon">20:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06234" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06234</a>
                    <span class="tweet-title">US vs. EU AI Laws: A Tale of Two Acts (and What They Can Learn From Each Other)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford, University College London, University of Bologna</span>
                </div>
                <div class="primary-text">
                    This research compares the US Algorithmic Accountability Act of 2022 (US AAA) with the EU Artificial Intelligence Act (EU AIA), highlighting their similarities and differences in scope, approach, and enforcement mechanisms. It goes beyond simply describing the acts, offering insights into how each can learn from the other to improve their effectiveness.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon">21:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06483" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06483</a>
                    <span class="tweet-title">Language Models:  Can They Handle a Multi-Task Makeover?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Virginia, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces the concept of "composable interventions" for language models, focusing on how different types of interventions interact when applied sequentially to the same model. This is a departure from previous work that largely studied interventions in isolation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon">21:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06250" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06250</a>
                    <span class="tweet-title">Fairness in Eye Scans:  A Pointy-Headed Approach to Better Diagnosis</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Chinese Academy of Sciences, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel Point-Image Diffusion method for medical image synthesis, specifically for generating SLO fundus images. Unlike previous methods that directly generate images, this approach first generates segmentation masks using 3D point clouds, which allows for more precise control over the boundaries of the synthesized images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon">22:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06541" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06541</a>
                    <span class="tweet-title">Malicious Bots? No Problem! New Algorithm Outwits Cyber-Attackers in Distributed Optimization.</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, Bar-Ilan University, Arizona State University</span>
                </div>
                <div class="primary-text">
                    This research introduces the Resilient Projected Push-Pull (RP3) algorithm, which uses stochastic trust values and gradient tracking to achieve geometric convergence rates in distributed optimization even in the presence of malicious agents. Unlike previous work, RP3 operates on directed graphs and handles both constrained and unconstrained optimization problems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon">22:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06567" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06567</a>
                    <span class="tweet-title">AI Investment Team:  LLMs Learn to Trade Like Humans, But Smarter!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This research introduces FINCON, a multi-agent system that uses LLMs to make financial decisions. Unlike previous work, FINCON incorporates a hierarchical communication structure inspired by real-world investment firms, with manager and analyst agents working together. It also includes a risk control component that updates the system's investment beliefs based on performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon">23:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06783" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06783</a>
                    <span class="tweet-title">Poisson Learning:  From Spikes to Smoothness with Measure Data</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Würzburg, University of Minnesota, University of Bonn</span>
                </div>
                <div class="primary-text">
                    This research proves convergence rates for Poisson Learning, a semi-supervised learning algorithm, to its continuum limit, which is a Poisson equation with measure data. This is a novel contribution as previous work focused on proving convergence for smoother data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon">23:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06516" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06516</a>
                    <span class="tweet-title">VQA-Diff:  Turning Car Pics into 3D Assets with a Chatty AI!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, Huawei, York University</span>
                </div>
                <div class="primary-text">
                    This research proposes VQA-Diff, a framework that leverages the zero-shot prediction ability of Visual Question Answering (VQA) models and the structure and appearance generation capabilities of Diffusion Models to generate 3D vehicle assets from single images. Unlike previous methods that rely solely on image RGB information, VQA-Diff incorporates real-world knowledge from a Large Language Model (LLM) to understand the characteristics of vehicles, enabling robust zero-shot prediction for unseen observations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon">24:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06798" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06798</a>
                    <span class="tweet-title">Lawyers Can't Tell AI From Human Writing, But They're Ready for the Robot Revolution</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Masaryk University, CMU</span>
                </div>
                <div class="primary-text">
                    This study uniquely investigates how lawyers perceive legal documents based on whether they believe the document was written by a human or an AI, specifically ChatGPT. Previous research has focused on the capabilities of LLMs in legal tasks, but not on how legal professionals react to their output.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon">24:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06910" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06910</a>
                    <span class="tweet-title">Microsoft Sellers Get Smart: AI Recommends the Perfect Content for Every Deal!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research focuses on recommending content at the opportunity level, the lowest granularity in a CRM system, which is more relevant for sellers than previous work that focused on broader recommendations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon">24:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06888" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06888</a>
                    <span class="tweet-title">ReLU's Quadratic Constraints: A Complete Set, Finally!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Michigan, University of Illinois</span>
                </div>
                <div class="primary-text">
                    This paper presents a complete set of quadratic constraints (QCs) for the repeated ReLU function, which is commonly used in neural networks. This set is described by a collection of 2nv matrix copositivity conditions, where nv is the dimension of the repeated ReLU. This is different from previous work, which only provided partial sets of QCs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon">25:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06576" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06576</a>
                    <span class="tweet-title">Virtual Personas Get Backstories: LLMs Now Have Lives!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research introduces "Anthology," a method for conditioning LLMs to specific virtual personas by using open-ended life narratives, or "backstories," as a prompt prefix. This differs from previous work that relied on explicit demographic information or question-answer pairs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon">25:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06496" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06496</a>
                    <span class="tweet-title">Privacy Amplification:  It's Our Loss, Not Yours!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research investigates whether privacy guarantees in a popular machine learning algorithm, DP-SGD, can be improved when only the final trained model is released, unlike the current analysis that assumes all intermediate steps are revealed. Unlike previous work that focused on constrained loss functions, this study explores the possibility of privacy amplification for general loss functions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon">26:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06964" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06964</a>
                    <span class="tweet-title">Vision Transformers Get a Memory Makeover:  A Disentangled Approach to Efficient Tuning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, ShanghaiTech University, National University of Singapore</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method called Synthesized Query Tuning (SynQT) for adapting pre-trained Vision Transformers to downstream tasks. Unlike previous methods that insert new structures into the pre-trained model, SynQT separates task-specific learning from pre-trained knowledge utilization, resulting in a more memory-efficient training process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon">26:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06608" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06608</a>
                    <span class="tweet-title">Deep Learning Gets a Makeover:  Image Reconstruction with Learned Attentive Regularizers</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne, Chemnitz University of Technology</span>
                </div>
                <div class="primary-text">
                    This research proposes a new regularization scheme for image reconstruction that leverages deep learning while maintaining interpretability. Unlike previous deep learning-based models, this scheme is based on a series of convex problems, making it easier to analyze theoretically. The key innovation lies in the use of learned masks to refine the regularization strength spatially, making the model progressively attentive to image structure.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon">27:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06226" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06226</a>
                    <span class="tweet-title">Quantum Computing:  Brain-Boosting  PSP  Diagnosis</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Derozio Memorial College</span>
                </div>
                <div class="primary-text">
                    This research explores the use of quantum machine learning (QML) for classifying brain networks affected by Progressive Supranuclear Palsy (PSP), a neurological disorder. Unlike previous studies that primarily relied on classical machine learning techniques, this paper investigates the potential of QML to improve classification accuracy and efficiency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon">27:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06765" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06765</a>
                    <span class="tweet-title">Linear Networks: The Secret Sauce for Deep Learning?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel generalization bound for neural networks by considering them as perturbations of linear networks. This approach allows for a-priori bounds, meaning they can be evaluated without training the model, unlike previous non-vacuous bounds.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon">27:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06295" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06295</a>
                    <span class="tweet-title">Teaching Cells to Build: A New Way to Engineer Morphogenesis</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, University of Lausanne</span>
                </div>
                <div class="primary-text">
                    This research uses automatic differentiation to learn the rules governing cell interactions and genetic networks that lead to complex developmental outcomes. This approach differs from previous work by directly optimizing over the parameters of the physical model, rather than relying on manually crafted rules.
                </div>
            </div>
        </div></div>

    <footer class="player-footer">
        <div id="controls">
            <button id="controllerButton" onclick="scrollToCurrentTweet()">
                <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
            </button>
            <button id="controllerButton" onclick="togglePlayPause()">
                <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                <div id="progressTime">0:00</div>
            </button>
        </div>
        <div id="progressContainer" onclick="seek(event)">
            <div id="progressBar"></div>
            <div id="progressCircle" draggable="true"></div>
        </div>
        <audio id="audioPlayer" src="assets/audio.mp3"></audio>
    </footer>

    <script>
        var audio = document.getElementById('audioPlayer');
        var controllerButton = document.getElementById('controllerButton');
        var playPauseImage = document.getElementById('playPauseImage');
        var progressBar = document.getElementById('progressBar');
        var progressCircle = document.getElementById('progressCircle');
        var progressTime = document.getElementById('progressTime');
        var isDragging = false;

        function scrollToCurrentTweet() {
            // This assumes `start-time-icon` divs have text in format "MM:SS"
            const tweets = document.querySelectorAll('.tweet');
            const currentTime = audio.currentTime;

            console.log("NOODLE", currentTime);

            let targetTweet = null;
            let maxStartTime = -1;

            tweets.forEach((tweet, index) => {
                const timeString = tweet.querySelector('.start-time-icon').textContent;
                const parts = timeString.split(':');

                tweetTime = (parseInt(parts[0]) * 60 + parseInt(parts[1]))
                if (parts.length > 2) {
                    tweetTime = tweetTime * 60 + parseInt(parts[2]) // convert MM:SS to seconds
                }
                if (tweetTime <= currentTime && tweetTime > maxStartTime) {
                    maxStartTime = tweetTime;
                    targetTweet = tweet;
                }
            });

            // If no tweet found that meets the condition, scroll to the top tweet
            if (!targetTweet && tweets.length > 0) {
                targetTweet = tweets[0];
            }

            if (targetTweet) {
                targetTweet.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }

        function togglePlayPause() {
            if (audio.paused) {
                audio.play();
                playPauseImage.src = 'assets/buttonPause.svg';
            } else {
                audio.pause();
                playPauseImage.src = 'assets/buttonPlay.svg';
            }
        }

        audio.addEventListener('timeupdate', function () {
            if (!isDragging) {
                var progress = (audio.currentTime / audio.duration) * 100;
                progressBar.style.width = progress + '%';
                progressCircle.style.left = progress + '%'; // Corrected reference
                updateProgressTime(audio.currentTime);
            }
        });

        audio.addEventListener('ended', function () {
            playPauseImage.src = 'assets/buttonPlay.svg';
            progressBar.style.width = '0%';
            progressCircle.style.left = '0%';
            updateProgressTime(0);
        });

        // Mouse events
        progressCircle.addEventListener('mousedown', function (event) {
            isDragging = true;
            document.addEventListener('mousemove', onMouseMove);
            document.addEventListener('mouseup', onMouseUp);
        });

        // Touch events
        progressCircle.addEventListener('touchstart', function (event) {
            isDragging = true;
            document.addEventListener('touchmove', onTouchMove);
            document.addEventListener('touchend', onTouchEnd);
        });

        function onMouseMove(event) {
            seek(event.clientX);
        }

        function onTouchMove(event) {
            var touch = event.touches[0];
            seek(touch.clientX);
        }

        function onMouseUp() {
            isDragging = false;
            document.removeEventListener('mousemove', onMouseMove);
            document.removeEventListener('mouseup', onMouseUp);
        }

        function onTouchEnd() {
            isDragging = false;
            document.removeEventListener('touchmove', onTouchMove);
            document.removeEventListener('touchend', onTouchEnd);
        }

        function onMouseUp() {
            isDragging = false;
            document.removeEventListener('mousemove', onMouseMove);
            document.removeEventListener('mouseup', onMouseUp);
        }

        progressCircle.addEventListener('dragstart', function (event) {
            event.preventDefault();
        });

        function updateProgressTime(currentTime) {
            var minutes = Math.floor(currentTime / 60);
            var seconds = Math.floor(currentTime % 60);
            if (seconds < 10) {
                seconds = '0' + seconds;
            }
            progressTime.textContent = minutes + ':' + seconds;
        }

        function seek(event) {
            var containerRect = progressContainer.getBoundingClientRect();
            var newTime = ((event.clientX - containerRect.left) / containerRect.width) * audio.duration;
            audio.currentTime = newTime;
            var progress = (audio.currentTime / audio.duration) * 100;
            progressBar.style.width = progress + '%';
            progressCircle.style.left = progress + '%'; // Ensure this reference is consistent
            updateProgressTime(newTime);
        }
    </script>

</body>

</html>