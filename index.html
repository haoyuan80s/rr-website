
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Roboto:wght@300;400;500;700&display=swap">
</head>

<body>
    <div class="container">
        <div class="header">
            <div style="height: 100px;"></div>
            <img src="assets/AppLaunchFrog.gif" alt="Descriptive text about the GIF" style="width: 20%; height: auto;">
            <div style="height: 30px;"></div>
            <div class=" notification-text">Hop right over soon... We're busy getting everything just right and will be
                here
                before you can say 'Ribbit'!
            </div>
            <div style="height: 100px;"></div>
            <div class="header-text">ArXiv AI Papers - Daily Highlights</div>
            <div class="header-text">Tue. Aug 13, 2024</div>
            <div class="subheader-text">Opening music from <a href="https://ikson.com" target="_blank"
                    style="color: black; text-decoration: none;">TELL YOUR STORY by ikson</a></div>
            <div style="height: 30px;"></div>
        </div>
        <div class="tweet" id="tweet0">
            <div class="start-time-icon">00:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04655" target="_blank">@arXiv 2408.04655</a>
                    <span class="tweet-title">AI's Got Feelings?  New Study Uncovers the "Strong" Side of Value Alignment</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sorbonne University</span>
                </div>
                <div class="primary-text">
                    This research distinguishes between "strong" and "weak" value alignment in AI systems.  Previous work focused on technical methods for alignment, while this paper delves into the cognitive abilities required for AI to truly understand and respond to human values.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon">01:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04811" target="_blank">@arXiv 2408.04811</a>
                    <span class="tweet-title">LLMs on Trial:  Jailbreaking AI with Code, Not Just Words!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces a dynamic benchmark for evaluating LLM safety, focusing on composable jailbreak attacks. Unlike previous work that relies on static datasets, this approach uses a domain-specific language to represent attacks as compositions of parameterized string transformations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon">01:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.05147" target="_blank">@arXiv 2408.05147</a>
                    <span class="tweet-title">GemmaScope:  Unlocking Language Model Secrets, One Sparse Autoencoder at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces GemmaScope, an open-source suite of sparse autoencoders (SAEs) trained on various layers of the Gemma2 language model. Unlike previous work that focused on smaller models or single layers, GemmaScope provides a comprehensive set of SAEs across multiple layers and sub-layers, making it easier for researchers to explore the internal workings of large language models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon">02:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04720" target="_blank">@arXiv 2408.04720</a>
                    <span class="tweet-title">AI Simplifies Physics: Machine Learning Tames the Wild World of Scattering Amplitudes</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Caltech, Harvard University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to simplifying scattering amplitudes by combining contrastive learning with a transformer encoder-decoder architecture. This differs from previous work by focusing on identifying and grouping similar terms within complex expressions, allowing for more efficient simplification of lengthy amplitudes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon">02:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04851" target="_blank">@arXiv 2408.04851</a>
                    <span class="tweet-title">Classifiers Can Be Secretly Likelihood-Based OOD Detectors: A Hyperspherical Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, University of Wisconsin-Madison</span>
                </div>
                <div class="primary-text">
                    This research proposes a new framework called Intrinsic Likelihood (INK) for out-of-distribution (OOD) detection. Unlike previous methods that rely on heuristics or strong assumptions, INK offers a rigorous likelihood interpretation by modeling latent embeddings as a mixture of hyperspherical distributions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon">02:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.05200" target="_blank">@arXiv 2408.05200</a>
                    <span class="tweet-title">Language Models Learn New Tricks Without Forgetting the Old Ones!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Hong Kong Polytechnic University, Peking University, University of Illinois at Chicago</span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework called TaSL (Task Skill Localization and Consolidation) for continual learning in language models. Unlike previous methods that rely on separate parameter blocks for each task, TaSL identifies and consolidates task-specific and shared knowledge within a single block, improving knowledge transfer and mitigating catastrophic forgetting.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon">03:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04734" target="_blank">@arXiv 2408.04734</a>
                    <span class="tweet-title">X-ray Laser's Brain: A Model for Smarter Science</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Pennsylvania State University</span>
                </div>
                <div class="primary-text">
                    This research introduces a multi-scale cognitive interaction model that simulates human decision-making during experiments at the Linac Coherent Light Source (LCLS). This model is unique in its ability to integrate simulations at multiple scales, capturing aspects of individual cognition, teamwork, planning, and decision-making both in the moment and across extended data-taking runs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon">04:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.05097" target="_blank">@arXiv 2408.05097</a>
                    <span class="tweet-title">Hyperbolic Language Models:  When Uncertainty Gets a Curveball!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sapienza University of Rome, ItalAIs.r.l., Panasonic Corp. of North America...</span>
                </div>
                <div class="primary-text">
                    This research explores the use of hyperbolic embeddings in large-scale vision-language models (VLMs), specifically BLIP-2, which has 2.7 billion parameters. Unlike previous work that focused on smaller models, this study tackles the challenges of scaling hyperbolic embeddings to such a large model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon">04:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04870" target="_blank">@arXiv 2408.04870</a>
                    <span class="tweet-title">Copilot's Got a Case of the "Confused Deputy" Blues: How Malicious Documents Can Hijack Your Enterprise AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Texas at Austin</span>
                </div>
                <div class="primary-text">
                    This research focuses on a new class of security vulnerabilities in retrieval-augmented generation (RAG) systems, specifically targeting how malicious documents can manipulate the responses generated by AI tools like Copilot for Microsoft 365. Unlike previous work that primarily focused on external threats, this study investigates the potential for insider threats to exploit RAG systems for data corruption and information leakage.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon">04:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04916" target="_blank">@arXiv 2408.04916</a>
                    <span class="tweet-title">Trajectory Learning Gets a Semantic Makeover: PTrajM Makes Movement Meaningful!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Aalborg University, CMU, Beijing Jiaotong University</span>
                </div>
                <div class="primary-text">
                    This research introduces PTrajM, a novel method for learning vehicle trajectories that incorporates both movement behavior and travel purposes. Unlike previous methods that focus solely on movement patterns, PTrajM utilizes a pre-training procedure that aligns trajectory embeddings with road and POI information, enabling it to extract travel purposes without additional computational resources during the embedding process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon">05:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04831" target="_blank">@arXiv 2408.04831</a>
                    <span class="tweet-title">Sparse Views, Big Dreams: 3D Reconstruction Gets a Gaussian Makeover</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel coarse-to-fine Gaussian splatting paradigm for sparse-view 3D reconstruction, incorporating structure-aware masks to enhance the model's robustness to sparse inputs and noise. This approach differs from previous work by utilizing both 3D geometry augmentation and perceptual view augmentation, which are derived from the coarse Gaussian model, to improve the consistency and detail of the reconstructed surfaces.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon">05:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.05141" target="_blank">@arXiv 2408.05141</a>
                    <span class="tweet-title">RAG-ing Against the Machine: A Hybrid System for Smarter AI Reasoning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, University of Wisconsin-Madison</span>
                </div>
                <div class="primary-text">
                    This research introduces a hybrid RAG system that incorporates a suite of optimizations to enhance retrieval quality, reasoning capabilities, and numerical computation ability. It differs from previous work by integrating multiple strategies, including attribute prediction, a numerical calculator, and an LLM knowledge extractor, to address the challenges of hallucination, domain expertise, and time-sensitive information in LLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon">06:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04808" target="_blank">@arXiv 2408.04808</a>
                    <span class="tweet-title">AI Chips Get a Memory Makeover:  T10 Compiler Makes Data Dance for Faster Deep Learning!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Illinois, Peking University, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces T10, a deep learning compiler that leverages the inter-core communication bandwidth and distributed on-chip memory of AI chips. Unlike previous compilers that rely on a virtual global memory, T10 utilizes a "compute-shift" paradigm, where data is shifted between cores in a predictable pattern, reducing memory footprint and communication overhead.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon">06:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04660" target="_blank">@arXiv 2408.04660</a>
                    <span class="tweet-title">COBOL's Got Talent: AI Makes Mainframe Modernization a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">FPT Software AI Center</span>
                </div>
                <div class="primary-text">
                    This research introduces XMainframe, a large language model (LLM) specifically trained on mainframe systems and COBOL codebases. Unlike previous CodeLLMs, XMainframe is designed to understand and interact with legacy code, addressing the challenges of mainframe modernization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon">07:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04718" target="_blank">@arXiv 2408.04718</a>
                    <span class="tweet-title">Diffusion Models: Ensemble Up for Zero-Shot Uncertainty!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research explores the use of diffusion probabilistic models for ensemble prediction in regression tasks, focusing on zero-shot uncertainty quantification without requiring explicit uncertainty estimation during model training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon">07:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04810" target="_blank">@arXiv 2408.04810</a>
                    <span class="tweet-title">UniBench:  50+ Vision-Language Benchmarks in 5 Minutes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta</span>
                </div>
                <div class="primary-text">
                    This research introduces UniBench, a unified framework for evaluating vision-language models (VLMs) across a comprehensive set of 50+ benchmarks. UniBench differs from previous work by providing a standardized and efficient way to evaluate VLMs, enabling researchers to quickly compare model performance across various capabilities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon">08:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04683" target="_blank">@arXiv 2408.04683</a>
                    <span class="tweet-title">Code Backdoors:  Trigger Inversion to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanjing University, Nanyang Technological University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel backdoor defense technique called EliBadCode, which utilizes trigger inversion to eliminate backdoors in neural code models (NCMs). Unlike previous work that focuses on detecting poisoned inputs, EliBadCode aims to identify and remove the backdoor itself without affecting the model's performance on clean inputs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon">08:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04760" target="_blank">@arXiv 2408.04760</a>
                    <span class="tweet-title">Robot's Got a Segmentation Problem: How Uncertainty-Aware Models Help!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to object segmentation by generating a distribution of possible segmentations, rather than just a single prediction. This allows the system to explicitly represent uncertainty and use it to guide robot actions that reduce ambiguity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon">09:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04815" target="_blank">@arXiv 2408.04815</a>
                    <span class="tweet-title">Brainwave Detectives:  Unmasking Alzheimer's with MEG and MRI!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Universiti Teknologi PETRONAS, Hospital Universiti Sains Malaysia, Ulster University...</span>
                </div>
                <div class="primary-text">
                    This research combines MEG and MRI data to identify biomarkers for Alzheimer's disease, going beyond previous studies that focused on a single modality.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon">09:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.05087" target="_blank">@arXiv 2408.05087</a>
                    <span class="tweet-title">Graph Self-Supervised Learning:  Neighbors Are the New Black!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanjing University, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research builds upon Bootstrapped Graph Latents (BGRL), a method for learning graph representations without negative samples. It introduces a novel approach to leverage the inherent homophily in graphs by incorporating node-neighbor pairs as potential positive samples. This differs from previous work by focusing on mining positive pairs from the graph structure itself, rather than relying solely on augmented views.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon">10:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.05058" target="_blank">@arXiv 2408.05058</a>
                    <span class="tweet-title">Phylogeny's New Trick: Semi-Implicit Branch Lengths for Tree-mendous Inference!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Fred Hutchinson Cancer Research Center, UC Los Angeles</span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to variational Bayesian phylogenetic inference (VBPI) by using semi-implicit hierarchical distributions for branch lengths. Unlike previous methods that rely on explicit distributions, this approach allows for more flexible and accurate modeling of branch length variations across different tree topologies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon">10:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04777" target="_blank">@arXiv 2408.04777</a>
                    <span class="tweet-title">Prostate Cancer Detection:  AI Gets a Style Makeover for Multi-Site MRI!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Siemens Healthineers, Vanderbilt University, Radboud University Medical Center...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel unsupervised domain adaptation (UDA) method using a unified generative model for multi-site prostate cancer (PCa) detection. Unlike previous UDA methods that require multiple generators for each domain pair, this approach utilizes a single generator with a dynamic filter to translate image styles across multiple domains.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon">10:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.05178" target="_blank">@arXiv 2408.05178</a>
                    <span class="tweet-title">ECG-FM: The Open-Source Heartbeat Whisperer</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University Health Network, Vector Institute for Artificial Intelligence, University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research introduces ECG-FM, an open-source foundation model for ECG analysis, trained on a massive dataset of 2.5 million ECG segments. Unlike previous task-specific models, ECG-FM can be adapted to various downstream tasks with minimal labeled data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon">11:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04913" target="_blank">@arXiv 2408.04913</a>
                    <span class="tweet-title">Knowledge Graph Embeddings:  From Facts to Fancy Geometry!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">DI ENS, ENS, CNRS...</span>
                </div>
                <div class="primary-text">
                    This research examines recent methods for embedding knowledge bases in description logic into vector spaces, focusing on their geometric-based semantics. It identifies several theoretical properties and investigates how concrete embedding methods fit into this framework.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon">11:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.05075" target="_blank">@arXiv 2408.05075</a>
                    <span class="tweet-title">DeepInteraction++:  Autonomous Driving's New BFF -  Multi-Modal Interaction Makes the Difference!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Fudan University, Nanyang Technological University, University of Surrey...</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new approach to multi-modal interaction for autonomous driving, focusing on maintaining separate representations for LiDAR and camera data throughout the entire perception pipeline. This differs from previous methods that typically fuse these modalities into a single representation, potentially losing valuable information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon">12:17</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04746" target="_blank">@arXiv 2408.04746</a>
                    <span class="tweet-title">XAI for Cybersecurity:  Explanations That Don't Explain?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research investigates the effectiveness of Explainable AI (XAI) techniques in a cybersecurity context, specifically for source code classification. It highlights the challenges of using popular XAI methods like SHAP and LIME with non-technical users, finding that their explanations can be confusing and contradictory.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon">12:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.05006" target="_blank">@arXiv 2408.05006</a>
                    <span class="tweet-title">LLMs Need a Tutor: New Framework Teaches AI to Debug Code</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Northeastern University, Peking University, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces DEBUGEVAL, a benchmark for evaluating LLMs' debugging capabilities, and proposes MASTER, a framework that refines code debugging data for supervised fine-tuning. Unlike previous work, DEBUGEVAL includes tasks beyond code repair, and MASTER uses a communicative agent-based approach to generate high-quality data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon">13:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04872" target="_blank">@arXiv 2408.04872</a>
                    <span class="tweet-title">Stop Saying "Hello World!" - LLMs Learn to Translate with Syntax</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces syntactic knowledge into the selection of in-context examples for machine translation (MT). Unlike previous work that focused on word-level matching or embedding-based scoring, this study leverages the deep syntactic structure of sentences to improve translation quality.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon">13:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04745" target="_blank">@arXiv 2408.04745</a>
                    <span class="tweet-title">AI Sniffs Out Methane Leaks From Space: New System Helps Stop Global Warming</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">United Nations Environment Programme, University of Cambridge, Vector Institute...</span>
                </div>
                <div class="primary-text">
                    This research presents MARS-S2L, an automated AI-driven methane emitter monitoring system that uses Sentinel-2 and Landsat satellite imagery. Unlike previous methods that relied on limited areas or synthetic data, MARS-S2L utilizes a large, expert-verified global dataset of real methane plumes for training and evaluation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon">14:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04713" target="_blank">@arXiv 2408.04713</a>
                    <span class="tweet-title">Mamba Strikes Again:  Modeling Long-Term Graph Dynamics with State-Space Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This paper proposes DyGMamba, a new model for continuous-time dynamic graphs (CTDGs) that uses state-space models (SSMs) to efficiently capture long-term temporal dependencies. Unlike previous methods that rely on Transformers or graph convolutions, DyGMamba leverages the Mamba SSM, which is known for its efficiency in handling long sequences.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon">14:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04678" target="_blank">@arXiv 2408.04678</a>
                    <span class="tweet-title">LLM Speed Boost:  CREST Compresses Data for Faster Decoding</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces CREST, a method for compacting the datastore used in REST, a retrieval-based speculative decoding technique. Unlike REST, which stores the entire pre-training dataset, CREST selectively stores only the smallest and most common n-grams, achieving comparable performance with significantly less storage space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon">15:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.05136" target="_blank">@arXiv 2408.05136</a>
                    <span class="tweet-title">Molecule Design:  A New Way to Spot the Difference Between Ortho, Meta, and Para!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research introduces a new set of descriptors called "cycle-configurations" (CC) to the "two-layered (2L) model" of molecular inference.  CCs capture the arrangement of exterior parts, called "fringe-trees," attached to cycles in a chemical graph, which allows for distinguishing between meta and para patterns in aromatic rings, something the 2L model couldn't do before.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon">15:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04948" target="_blank">@arXiv 2408.04948</a>
                    <span class="tweet-title">Financial Facts:  HybridRAG Makes Knowledge Graphs and Vector Retrieval Sing!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nvidia</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach called HybridRAG, which combines Knowledge Graphs (KGs) and Vector Retrieval Augmented Generation (RAG) techniques to improve information extraction from financial documents. This approach differs from previous work by integrating both structured and unstructured information retrieval methods, aiming to enhance the accuracy and comprehensiveness of the extracted information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon">15:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.05082" target="_blank">@arXiv 2408.05082</a>
                    <span class="tweet-title">Label Smoothing Gets a Boost:  DRO-LS Makes Few Data Go Far!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Beijing Institute of Technology, Tsinghua University, Beijing University of Posts and Telecommunications</span>
                </div>
                <div class="primary-text">
                    This research integrates label smoothing (LS) into the distributionally robust optimization (DRO) framework, creating a novel two-stage problem called DRO-LS. This approach allows for flexible data shifting to unseen domains, effectively generating new samples without extra annotations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon">16:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.05092" target="_blank">@arXiv 2408.05092</a>
                    <span class="tweet-title">Deep Learning Goes Undercover: Training Models Without Sharing Sensitive Data!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Swiss Center for Electronics and Microtechnology, École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method called PriPHiT for training deep neural networks on edge-cloud systems while preserving user privacy. Unlike previous methods that focus on privacy during inference, PriPHiT tackles privacy concerns during the training phase itself.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon">16:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04680" target="_blank">@arXiv 2408.04680</a>
                    <span class="tweet-title">Foggy Thinking:  LLMs Get a Decentralized Makeover for Medical Data</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research proposes a dynamic fog computing architecture for executing LLMs in medical applications, aiming to address privacy, trust, and financial concerns associated with cloud-based solutions. This approach differs from previous work by decentralizing LLM execution and dynamically allocating tasks across edge, fog, and cloud layers based on their capabilities and trust levels.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon">17:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04816" target="_blank">@arXiv 2408.04816</a>
                    <span class="tweet-title">FUSE-ing Language Models:  Zero-Shot Adapter Discovery for Prompt Optimization Across Tokenizers</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel approach for approximating gradients across models and tokenizers during prompt optimization. It introduces an adapter that precisely maps across token and embedding spaces in the forward pass. By leveraging a precomputed linear transformation, it efficiently approximates the behavior of a true differentiable mapping between embedding spaces during the backward pass.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon">17:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.05126" target="_blank">@arXiv 2408.05126</a>
                    <span class="tweet-title">AI's Got the Blues: Can ChatGPT Decode Hate Speech on YouTube?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge, Technical University of Munich</span>
                </div>
                <div class="primary-text">
                    This research explores the use of GPT-4 for thematic analysis of YouTube comments, focusing on hate speech related to Roma migrants in Sweden. Unlike previous studies, it examines the model's ability to handle sensitive content and its potential for human-AI synergy in qualitative research.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon">18:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04661" target="_blank">@arXiv 2408.04661</a>
                    <span class="tweet-title">Ontology-Powered Text Mining:  A Recipe for Extracting Materials Secrets</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research introduces MaterioMiner, a dataset that links a materials mechanics ontology with textual entities from scientific literature. Unlike previous datasets focused on specific subdomains, MaterioMiner aims to be more domain-agnostic, allowing for broader applications in materials science.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon">18:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04658" target="_blank">@arXiv 2408.04658</a>
                    <span class="tweet-title">Shopping Assistant Wins KDD Cup:  AI Makes Online Shopping a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nvidia</span>
                </div>
                <div class="primary-text">
                    This research stands out by focusing on the development of a training dataset for LLMs specifically tailored to the online shopping domain. The authors created this dataset by processing multiple public datasets and augmenting it with synthetic data generated by LLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon">19:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04681" target="_blank">@arXiv 2408.04681</a>
                    <span class="tweet-title">AI Chatbots:  Planting False Memories in Your Head?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This study investigates the impact of conversational AI powered by large language models (LLMs) on false memory formation, a topic not extensively explored in previous research. It simulates witness interviews using a generative chatbot and compares its effects to those of a survey-based method and a pre-scripted chatbot.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon">19:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.04803" target="_blank">@arXiv 2408.04803</a>
                    <span class="tweet-title">NeRFs on a Diet: Meta-Learning Makes 3D Scenes Slim and Speedy!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research uses meta-learning to create a "smart" starting point for NeRF models, allowing them to learn 3D scenes from fewer images than traditional methods. This is different from previous work that either relied on large datasets or used external priors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon">19:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.05148" target="_blank">@arXiv 2408.05148</a>
                    <span class="tweet-title">Floating-Point Fiasco: How Tiny Errors Wreck Big Computations</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, Oak Ridge National Laboratory</span>
                </div>
                <div class="primary-text">
                    This research investigates the impact of floating-point non-associativity (FPNA) on reproducibility in high-performance computing (HPC) and deep learning (DL) applications. It goes beyond previous work by analyzing the statistical properties of FPNA within parallel programming models and examining the effects on specific PyTorch operations.
                </div>
            </div>
        </div></div>

    <footer class="player-footer">
        <div id="controls">
            <button id="controllerButton" onclick="scrollToCurrentTweet()">
                <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
            </button>
            <button id="controllerButton" onclick="togglePlayPause()">
                <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                <div id="progressTime">0:00</div>
            </button>
        </div>
        <div id="progressContainer" onclick="seek(event)">
            <div id="progressBar"></div>
            <div id="progressCircle" draggable="true"></div>
        </div>
        <audio id="audioPlayer" src="assets/audio.mp3"></audio>
    </footer>

    <script>
        var audio = document.getElementById('audioPlayer');
        var controllerButton = document.getElementById('controllerButton');
        var playPauseImage = document.getElementById('playPauseImage');
        var progressBar = document.getElementById('progressBar');
        var progressCircle = document.getElementById('progressCircle');
        var progressTime = document.getElementById('progressTime');
        var isDragging = false;

        function scrollToCurrentTweet() {
            // This assumes `start-time-icon` divs have text in format "MM:SS"
            const tweets = document.querySelectorAll('.tweet');
            const currentTime = audio.currentTime;

            console.log("NOODLE", currentTime);

            let targetTweet = null;
            let maxStartTime = -1;

            tweets.forEach((tweet, index) => {
                const timeString = tweet.querySelector('.start-time-icon').textContent;
                const parts = timeString.split(':');

                tweetTime = (parseInt(parts[0]) * 60 + parseInt(parts[1]))
                if (parts.length > 2) {
                    tweetTime = tweetTime * 60 + parseInt(parts[2]) // convert MM:SS to seconds
                }
                if (tweetTime <= currentTime && tweetTime > maxStartTime) {
                    maxStartTime = tweetTime;
                    targetTweet = tweet;
                }
            });

            // If no tweet found that meets the condition, scroll to the top tweet
            if (!targetTweet && tweets.length > 0) {
                targetTweet = tweets[0];
            }

            if (targetTweet) {
                targetTweet.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }

        function togglePlayPause() {
            if (audio.paused) {
                audio.play();
                playPauseImage.src = 'assets/buttonPause.svg';
            } else {
                audio.pause();
                playPauseImage.src = 'assets/buttonPlay.svg';
            }
        }

        audio.addEventListener('timeupdate', function () {
            if (!isDragging) {
                var progress = (audio.currentTime / audio.duration) * 100;
                progressBar.style.width = progress + '%';
                progressCircle.style.left = progress + '%'; // Corrected reference
                updateProgressTime(audio.currentTime);
            }
        });

        audio.addEventListener('ended', function () {
            playPauseImage.src = 'assets/buttonPlay.svg';
            progressBar.style.width = '0%';
            progressCircle.style.left = '0%';
            updateProgressTime(0);
        });

        // Mouse events
        progressCircle.addEventListener('mousedown', function (event) {
            isDragging = true;
            document.addEventListener('mousemove', onMouseMove);
            document.addEventListener('mouseup', onMouseUp);
        });

        // Touch events
        progressCircle.addEventListener('touchstart', function (event) {
            isDragging = true;
            document.addEventListener('touchmove', onTouchMove);
            document.addEventListener('touchend', onTouchEnd);
        });

        function onMouseMove(event) {
            seek(event.clientX);
        }

        function onTouchMove(event) {
            var touch = event.touches[0];
            seek(touch.clientX);
        }

        function onMouseUp() {
            isDragging = false;
            document.removeEventListener('mousemove', onMouseMove);
            document.removeEventListener('mouseup', onMouseUp);
        }

        function onTouchEnd() {
            isDragging = false;
            document.removeEventListener('touchmove', onTouchMove);
            document.removeEventListener('touchend', onTouchEnd);
        }

        function onMouseUp() {
            isDragging = false;
            document.removeEventListener('mousemove', onMouseMove);
            document.removeEventListener('mouseup', onMouseUp);
        }

        progressCircle.addEventListener('dragstart', function (event) {
            event.preventDefault();
        });

        function updateProgressTime(currentTime) {
            var minutes = Math.floor(currentTime / 60);
            var seconds = Math.floor(currentTime % 60);
            if (seconds < 10) {
                seconds = '0' + seconds;
            }
            progressTime.textContent = minutes + ':' + seconds;
        }

        function seek(event) {
            var containerRect = progressContainer.getBoundingClientRect();
            var newTime = ((event.clientX - containerRect.left) / containerRect.width) * audio.duration;
            audio.currentTime = newTime;
            var progress = (audio.currentTime / audio.duration) * 100;
            progressBar.style.width = progress + '%';
            progressCircle.style.left = progress + '%'; // Ensure this reference is consistent
            updateProgressTime(newTime);
        }
    </script>

</body>

</html>