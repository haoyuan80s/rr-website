
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Roboto:wght@300;400;500;700&display=swap">
</head>

<body>
    <div class="container">
        <div class="header">
            <div style="height: 100px;"></div>
            <img src="assets/AppLaunchFrog.gif" alt="Descriptive text about the GIF" style="width: 20%; height: auto;">
            <div style="height: 30px;"></div>
            <div class=" notification-text">Hop right over soon... We're busy getting everything just right and will be
                here
                before you can say 'Ribbit'!
            </div>
            <div style="height: 100px;"></div>
            <div class="header-text">ArXiv AI Papers - Daily Highlights</div>
            <div class="header-text">Mon. Aug 26, 2024</div>
            <div class="subheader-text">Opening music from <a href="https://ikson.com" target="_blank"
                    style="color: black; text-decoration: none;">TELL YOUR STORY by ikson</a></div>
            <div style="height: 30px;"></div>
        </div>
        <div class="tweet" id="tweet0">
            <div class="start-time-icon">00:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13221" target="_blank">@arXiv 2408.13221</a>
                    <span class="tweet-title">Backdoor Bonanza:  When Multiple Attacks Sneak into Your AI Model</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research explores the impact of multiple simultaneous data poisoning attacks on machine learning models, a scenario not previously investigated. It demonstrates that multiple backdoors can be installed in a single model without significantly affecting its performance on clean data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon">01:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13150" target="_blank">@arXiv 2408.13150</a>
                    <span class="tweet-title">Backtracking Linesearch:  A New Trick to Speed Up Optimization</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Northeastern University</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new way to adjust step sizes in backtracking linesearch, a fundamental technique in numerical optimization. Instead of using a constant factor, the proposed method uses a variable factor that adapts to the degree of violation of the chosen criterion.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon">01:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13252" target="_blank">@arXiv 2408.13252</a>
                    <span class="tweet-title">Layered Panoramas:  A 3D Scene Generator That's Not Just a Pretty Picture!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Shanghai Jiao Tong University, Chinese University of Hong Kong, Zhejiang University...</span>
                </div>
                <div class="primary-text">
                    This research introduces LAYERPANO3D, a framework that uses a multi-layered 3D panorama representation to generate full-view, explorable panoramic scenes from text prompts. This approach differs from previous methods that either rely on sequential scene expansion or employ a single panorama representation, which often suffer from semantic drift or limitations in handling complex scene hierarchies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon">02:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13233" target="_blank">@arXiv 2408.13233</a>
                    <span class="tweet-title">Transformers Get a Speed Boost: Training LLMs in Almost Linear Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Hong Kong, University of Wisconsin-Madison, Tsinghua University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method for calculating gradients in multi-layer transformer models, achieving almost linear time complexity. Unlike previous work that focused on inference acceleration, this paper tackles the computational bottleneck during training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon">02:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12664" target="_blank">@arXiv 2408.12664</a>
                    <span class="tweet-title">Brainpower for AI: How Neuroscience Can Help Us Understand Deep Learning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research proposes a multilevel framework for interpreting artificial neural networks, drawing inspiration from neuroscience's decades of experience analyzing the brain. It argues that understanding both biological and artificial neural systems requires analyzing them at multiple levels of analysis, with different analytic tools for each level.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon">03:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13242" target="_blank">@arXiv 2408.13242</a>
                    <span class="tweet-title">Training Equivariant Networks:  Relax, It's Okay to Break the Rules!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Pennsylvania, MIT</span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel training framework for equivariant neural networks that relaxes the equivariance constraint during training. Unlike previous work that focuses on addressing model mis-specification, this method aims to improve the optimization process itself by allowing the model to explore a larger hypothesis space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon">03:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12734" target="_blank">@arXiv 2408.12734</a>
                    <span class="tweet-title">Fair-Speech:  Unmasking Bias in Voice Assistants, One Utterance at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset, Fair-Speech, specifically designed to evaluate the fairness of speech recognition models across diverse demographic groups. Unlike previous datasets, Fair-Speech includes self-reported demographic information like age, gender, ethnicity, and geographic location, allowing researchers to assess how well models perform for different groups.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon">04:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12748" target="_blank">@arXiv 2408.12748</a>
                    <span class="tweet-title">LLMs:  Big Brains, Big Problems?  Small Models to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel two-stage framework for hallucination detection in LLMs. It combines a small language model (SLM) for initial detection with a larger LLM as a "constrained reasoner" to provide explanations for detected hallucinations. This approach aims to balance the latency challenges of using LLMs for reasoning with the need for interpretability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon">04:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12658" target="_blank">@arXiv 2408.12658</a>
                    <span class="tweet-title">Singing the Blues: AI Learns to Improvise Hindustani Music</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Montreal Institute for Learning Algorithms, Google</span>
                </div>
                <div class="primary-text">
                    This research proposes a hierarchical generative model for Hindustani music that uses a finely quantized pitch contour as an intermediate representation, unlike previous work that relied on coarse symbolic representations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon">04:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12812" target="_blank">@arXiv 2408.12812</a>
                    <span class="tweet-title">Fallacies Unmasked: New Dataset Exposes How Misinformation Hijacks Science</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">TU Darmstadt, Hessian Center for AI, IBM Research...</span>
                </div>
                <div class="primary-text">
                    This research introduces MISSCIPLUS, a new dataset that grounds logical fallacies in real-world scientific publications, unlike previous datasets that relied on paraphrased information or explicitly stated fallacies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon">05:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12747" target="_blank">@arXiv 2408.12747</a>
                    <span class="tweet-title">Diffusion Makes 3D Object Detection a Breeze: CatFree3D Says Goodbye to Categories!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel 3D object detection pipeline that decouples the task from 2D detection and depth prediction, enabling category-agnostic detection. Unlike previous methods that rely on category information, this approach leverages a diffusion-based model to predict 3D bounding boxes from random noise, conditioned on visual prompts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon">05:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12674" target="_blank">@arXiv 2408.12674</a>
                    <span class="tweet-title">Robots Learn New Tricks by Watching YouTube!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for teaching robots by interpreting video demonstrations as Parameterized Symbolic Abstraction Graphs (PSAG). Unlike previous approaches that focus on replaying object relationships or actor trajectories, PSAGs capture both geometric and non-geometric attributes, including forces, which are often invisible in videos.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon">06:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13256" target="_blank">@arXiv 2408.13256</a>
                    <span class="tweet-title">Diffusion Models: Factorizing Images, One Blob at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research investigates how diffusion models learn to represent and generate images by breaking down complex features into simpler, independent components. Unlike previous studies that focused on disentanglement, this paper delves into the specific mechanisms of factorization and compositionality in diffusion models, using a controlled experiment with 2D Gaussian data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon">06:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13211" target="_blank">@arXiv 2408.13211</a>
                    <span class="tweet-title">Quantum Circuit Design:  Neural Networks Go Unitary!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Cracow University of Technology, Ferdowsi University of Mashhad, University College London...</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to quantum circuit synthesis by training a neural network with unitary weight matrices, directly mapping input-output relationships of quantum computations. This differs from previous work that focused on optimizing parameters within existing quantum circuits or using neural networks for gate selection probabilities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon">07:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13257" target="_blank">@arXiv 2408.13257</a>
                    <span class="tweet-title">New Benchmark Challenges LLMs: Can They See What We See?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CASIA, NJU, HKUST...</span>
                </div>
                <div class="primary-text">
                    This research introduces MME-RealWorld, a new benchmark for evaluating multimodal large language models (MLLMs). Unlike previous benchmarks, MME-RealWorld focuses on high-resolution images and real-world scenarios, making it more challenging for models to achieve high accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon">07:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12787" target="_blank">@arXiv 2408.12787</a>
                    <span class="tweet-title">LLMs:  They're Not Just Chatty, They're Nosy! New Toolkit Exposes Data Privacy Risks</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Texas at Austin, University of Illinois, UC Berkeley...</span>
                </div>
                <div class="primary-text">
                    This research introduces LLM-PBE, a toolkit specifically designed to systematically evaluate data privacy risks in LLMs. Unlike previous studies that focused on limited data types, models, and attack methods, LLM-PBE offers a comprehensive approach, encompassing diverse datasets, attack methodologies, and defense strategies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon">08:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13073" target="_blank">@arXiv 2408.13073</a>
                    <span class="tweet-title">LLMs for Healthcare:  A New Recipe for Better Predictions!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research proposes a framework called IntelliCare that uses Large Language Models (LLMs) to provide patient-level knowledge for improving healthcare predictions. Unlike previous work that focused on medical code-level knowledge, IntelliCare leverages LLMs to analyze individual patient health conditions, generating personalized insights.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon">08:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12834" target="_blank">@arXiv 2408.12834</a>
                    <span class="tweet-title">LLMs Learn to Spot Names with a Little Help from Their Friends</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">iFLYTEK, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes CLLMFS, a framework that enhances large language models (LLMs) for few-shot named entity recognition (NER) by integrating contrastive learning and Low-Rank Adaptation (LoRA). This approach differs from previous work by focusing on improving the model's internal representations, specifically its ability to distinguish between entities and their surrounding context.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon">09:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13237" target="_blank">@arXiv 2408.13237</a>
                    <span class="tweet-title">Learning Functions by Their Derivatives: A Jacobian Journey</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This paper proposes learning a function by directly learning its Jacobian, the matrix of its partial derivatives. This approach allows for the enforcement of specific derivative conditions, such as invertibility or Lipschitz continuity, by structuring the Jacobian's output activation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon">09:58</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12832" target="_blank">@arXiv 2408.12832</a>
                    <span class="tweet-title">LLMs Go on a Road Trip: Predicting Your Next Move with Intent!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel framework called LIMP that uses large language models (LLMs) to predict human mobility by incorporating the underlying intentions behind each movement. Unlike previous models that primarily focus on spatiotemporal patterns, LIMP leverages LLMs' commonsense reasoning abilities to infer intentions, leading to more accurate predictions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon">10:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12606" target="_blank">@arXiv 2408.12606</a>
                    <span class="tweet-title">AI Doctor for Breasts:  Multimodal MRI Makes Cancer Diagnosis a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The Hong Kong University of Science and Technology, Harvard University, Shenzhen People’s Hospital...</span>
                </div>
                <div class="primary-text">
                    This research distinguishes itself by utilizing a large mixture-of-modality-experts model (MOME) that integrates multiparametric MRI information, unlike previous studies that primarily relied on single sequences.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon">10:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13195" target="_blank">@arXiv 2408.13195</a>
                    <span class="tweet-title">AI Architect Designs Chip Capacitance: NAS-Cap Makes Extraction Faster, More Accurate</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research uses neural architecture search (NAS) to find a better CNN model for 3-D capacitance extraction, improving accuracy and efficiency compared to previous CNN-Cap models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon">11:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12662" target="_blank">@arXiv 2408.12662</a>
                    <span class="tweet-title">Turbulent Twisters:  TDA Helps Spot Whirlwind Chaos!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">French Alternative Energies and Atomic Energy Commission, Sorbonne University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for automatically identifying turbulent vortices within 2D flows using Topological Data Analysis (TDA). Unlike previous studies that relied on traditional turbulence descriptors, this approach leverages the geometry of vortices extracted through TDA to distinguish between turbulent and laminar states.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon">11:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12801" target="_blank">@arXiv 2408.12801</a>
                    <span class="tweet-title">Time Delays Got You Down? Bootstrap Your Way to Better Predictions!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta</span>
                </div>
                <div class="primary-text">
                    This research introduces Time Series Model Bootstrap (TSMB), a framework that addresses the challenge of potentially varying or nondeterministic time delays in multivariate time series modeling. Unlike traditional methods that assume a fixed time delay, TSMB acknowledges and incorporates time delay uncertainties.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon">11:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12976" target="_blank">@arXiv 2408.12976</a>
                    <span class="tweet-title">Event Cameras Get Smart: On-the-Fly Feedback Control for Better Vision</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, Sony</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach for dynamically controlling the activation thresholds of event-based vision sensors (EVS) on a per-column basis, unlike previous work that focused on global control parameters.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon">12:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12821" target="_blank">@arXiv 2408.12821</a>
                    <span class="tweet-title">AI Goes Street Smart: Can ChatGPT-4V and Gemini Pro Navigate Urban Landscapes?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research evaluates the capabilities of ChatGPT-4V and Gemini Pro in analyzing urban environments, specifically Street View Imagery, Built Environment, and Interior, focusing on tasks like object recognition, counting, and measurement. This differs from previous work by examining the models' performance across multiple domains and tasks within a specific urban context.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon">12:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12629" target="_blank">@arXiv 2408.12629</a>
                    <span class="tweet-title">Skeleton-Based Gesture Recognition:  Data-Free Learning Gets a Synthetic Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research focuses on data-free class incremental learning (DFCIL) for skeleton-based gesture recognition, a domain that has received less attention than image-based DFCIL. The paper proposes a novel Synthetic Feature Replay (SFR) algorithm that samples synthetic features from class prototypes to replay old classes and augment new classes, eliminating the need for complex data generation methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon">13:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12781" target="_blank">@arXiv 2408.12781</a>
                    <span class="tweet-title">AI Mastery:  From Supervised to Autonomous, How Humans and Machines Will Work Together</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research introduces the "Model Mastery Lifecycle" framework, which outlines how human-AI interaction should evolve as AI models progress towards mastery in a specific task. This framework differs from previous work by explicitly addressing the changing nature of human and AI roles as AI capabilities improve.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon">13:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12602" target="_blank">@arXiv 2408.12602</a>
                    <span class="tweet-title">Fiber Optic Neural Networks:  Light Speed Computing, No Wires Required!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanjing University of Posts and Telecommunications, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel fiber optical neural network architecture that integrates signal transmission and computation within the optical domain, eliminating the need for signal conversion to the electronic domain. This approach differs from previous work by directly processing communication signals within the fiber itself, potentially leading to significant gains in processing efficiency and power consumption.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon">14:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12970" target="_blank">@arXiv 2408.12970</a>
                    <span class="tweet-title">SUMO:  A Search-Based Uncertainty Estimator for Offline RL,  It's a KNN-dergarten of Knowledge!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Harbin Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This research proposes a new uncertainty estimation method called SUMO for model-based offline reinforcement learning. Unlike previous methods that rely on model ensembles, SUMO uses a search-based approach to measure the cross-entropy between simulated and real dynamics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon">14:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12682" target="_blank">@arXiv 2408.12682</a>
                    <span class="tweet-title">MultiMed:  A Medical AI  Benchmark That's  Got  All  The  Modalities!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces MultiMed, a benchmark dataset designed to evaluate and enable large-scale learning across a wide spectrum of medical modalities and tasks. Unlike previous benchmarks that focus on single modalities or tasks, MultiMed integrates data from ten diverse modalities, including imaging, electrophysiology, molecular data, and text, and structures it into eleven challenging tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon">15:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13146" target="_blank">@arXiv 2408.13146</a>
                    <span class="tweet-title">Change-Point Detection:  A Kernel-Based Scan B-Statistic Takes the Lead!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research reproduces a recently proposed online change-point detection algorithm based on a kernel-based scan B-statistic. The algorithm is distribution-free, making it robust for real-world applications. The paper also explores the use of subsampling techniques to improve the algorithm's efficiency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon">15:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13147" target="_blank">@arXiv 2408.13147</a>
                    <span class="tweet-title">ShapeICP:  No Pose Data? No Problem!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel mesh-based active shape model (ASM) for category-level object pose and shape estimation. Unlike previous methods, it does not rely on pose-annotated data for training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon">16:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12980" target="_blank">@arXiv 2408.12980</a>
                    <span class="tweet-title">MedDec:  Decoding Doctor's Notes, One Decision at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Massachusetts Lowell, MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset called MedDec, specifically designed for extracting and classifying medical decisions from discharge summaries. Unlike previous datasets focused on other aspects of clinical notes, MedDec focuses solely on the decision-making process, providing a valuable resource for understanding how doctors make choices.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon">16:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13231" target="_blank">@arXiv 2408.13231</a>
                    <span class="tweet-title">Fourier Features Go Spherical: A New Way to Approximate Kernels</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces a new family of quadrature rules for approximating the Gaussian measure in higher dimensions, leveraging its isotropy. This approach differs from previous work by using a tensor product of radial and spherical quadrature rules, leading to improved approximation bounds.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon">17:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12825" target="_blank">@arXiv 2408.12825</a>
                    <span class="tweet-title">WSI Classification:  When More is Less, and Less is More!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a semi-weakly supervised learning framework for WSI classification, called SWS-MIL. It differs from previous work by employing adaptive pseudo bag augmentation (AdaPse) to assign labels to unlabeled data based on a confidence threshold. This approach aims to reduce noise introduced by traditional pseudo bag augmentation methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon">17:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12742" target="_blank">@arXiv 2408.12742</a>
                    <span class="tweet-title">Vision Transformers:  Attention, Please!  Reusing Attention for Efficiency</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This research proposes TReX, a framework that reuses the attention output of one encoder in subsequent encoders of a Vision Transformer (ViT) model. This differs from prior work that focused on token pruning or weight sharing, which only partially address the computational overhead of attention blocks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon">17:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12767" target="_blank">@arXiv 2408.12767</a>
                    <span class="tweet-title">Spiking Neural Networks:  Brain-Inspired AI Gets a Memory Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This research explores the intersection of Spiking Neural Networks (SNNs) with In-Memory Computing (IMC) architectures, highlighting the potential for low-power edge computing.  The paper emphasizes the need for a holistic approach to design, considering the interdependencies between algorithms, devices, circuits, and system parameters.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon">18:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12614" target="_blank">@arXiv 2408.12614</a>
                    <span class="tweet-title">Image-Feature Consistency:  A New Trick for Semi-Supervised Learning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This paper introduces feature-level perturbation, a new way to augment data in semi-supervised learning. Unlike previous methods that only perturb images, this approach also alters the features extracted by the model, creating a more diverse and challenging learning environment.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon">18:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12733" target="_blank">@arXiv 2408.12733</a>
                    <span class="tweet-title">SQL-GEN:  Bridging the Dialect Gap with Synthetic Data and Model Merging</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces SQL-GEN, a framework for generating synthetic data tailored to specific SQL dialects. Unlike previous approaches that rely on translating queries across dialects, SQL-GEN leverages dialect-specific tutorials to create training datasets that better capture the unique capabilities of each dialect.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon">19:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13012" target="_blank">@arXiv 2408.13012</a>
                    <span class="tweet-title">Drug-Matching Made Easy: AI Predicts Cancer Treatments from Tiny Samples</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge, The National Institute of Agricultural Botany, ValiRx Plc...</span>
                </div>
                <div class="primary-text">
                    This research uses a machine learning approach to predict drug responses in patient-derived cell cultures, focusing on the cell's functional response to a drug panel rather than relying solely on omics data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon">19:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12622" target="_blank">@arXiv 2408.12622</a>
                    <span class="tweet-title">AI Risk: A Taxonomy  of  All  the  Ways  Things  Can  Go  Wrong</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research differs from previous work by creating a comprehensive database of AI risks extracted from 43 taxonomies, along with two new taxonomies: a Causal Taxonomy and a Domain Taxonomy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon">20:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13114" target="_blank">@arXiv 2408.13114</a>
                    <span class="tweet-title">Neural Networks Get a Spline Makeover: Learning Smooth Activations with a Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel variational framework for training freeform nonlinearities in layered computational architectures, specifically focusing on pointwise nonlinearities. The key innovation lies in the use of a second-order total variation (TV(2)) regularization, which promotes sparsity and leads to solutions that are adaptive nonuniform linear splines. This approach allows for the imposition of slope constraints, enabling the design of stable, monotone, and firmly non-expansive activations, crucial for the proper functioning of signal processing algorithms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon">20:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12636" target="_blank">@arXiv 2408.12636</a>
                    <span class="tweet-title">Wave-LSTM:  Unraveling Cancer's Multi-Scale Secrets with a Deep Learning Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford, Alan Turing Institute, Health Data Research UK</span>
                </div>
                <div class="primary-text">
                    This research introduces Wave-LSTM, a novel approach that analyzes cancer copy number alterations (CNAs) at multiple scales, unlike previous methods that focused on single-scale analysis.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon">21:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13154" target="_blank">@arXiv 2408.13154</a>
                    <span class="tweet-title">CNNs See Spots:  Explaining Breast Cancer Diagnosis with AI's "Seeing Eye"</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge, Tilburg University</span>
                </div>
                <div class="primary-text">
                    This research focuses on the interpretability of CNNs for mammogram classification, specifically exploring how post-hoc techniques like Grad-CAM can reveal the reasoning behind the model's predictions. This differs from previous work that primarily focused on interpretability in numerical breast cancer datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon">21:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12608" target="_blank">@arXiv 2408.12608</a>
                    <span class="tweet-title">Frugal Spiking Neural Network:  A Tiny Brain for Big Data!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Univ. Grenoble Alpes, INSERM, Grenoble Institut Neurosciences...</span>
                </div>
                <div class="primary-text">
                    This research introduces a single-layer Spiking Neural Network (SNN) that uses a unique type of neuron called LTS (Leaky Threshold-Switching) to classify patterns in continuous data streams. Unlike previous SNNs that often rely on multiple layers and complex architectures, this approach is highly frugal, requiring only a handful of neurons.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon">21:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.12732" target="_blank">@arXiv 2408.12732</a>
                    <span class="tweet-title">AI's Got Grain: Segment Anything Model for Hard Drive Design</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Seagate Technology, University of Minnesota</span>
                </div>
                <div class="primary-text">
                    This research explores the use of Meta's Segment Anything Model (SAM) for grain segmentation in hard drive design, a task that has traditionally relied on rule-based models or training neural networks on extensive hand-labeled data. SAM's zero-shot generalization capability allows for segmentation across different imaging conditions with minimal labeled data.
                </div>
            </div>
        </div></div>

    <footer class="player-footer">
        <div id="controls">
            <button id="controllerButton" onclick="scrollToCurrentTweet()">
                <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
            </button>
            <button id="controllerButton" onclick="togglePlayPause()">
                <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                <div id="progressTime">0:00</div>
            </button>
        </div>
        <div id="progressContainer" onclick="seek(event)">
            <div id="progressBar"></div>
            <div id="progressCircle" draggable="true"></div>
        </div>
        <audio id="audioPlayer" src="assets/audio.mp3"></audio>
    </footer>

    <script>
        var audio = document.getElementById('audioPlayer');
        var controllerButton = document.getElementById('controllerButton');
        var playPauseImage = document.getElementById('playPauseImage');
        var progressBar = document.getElementById('progressBar');
        var progressCircle = document.getElementById('progressCircle');
        var progressTime = document.getElementById('progressTime');
        var isDragging = false;

        function scrollToCurrentTweet() {
            // This assumes `start-time-icon` divs have text in format "MM:SS"
            const tweets = document.querySelectorAll('.tweet');
            const currentTime = audio.currentTime;

            console.log("NOODLE", currentTime);

            let targetTweet = null;
            let maxStartTime = -1;

            tweets.forEach((tweet, index) => {
                const timeString = tweet.querySelector('.start-time-icon').textContent;
                const parts = timeString.split(':');

                tweetTime = (parseInt(parts[0]) * 60 + parseInt(parts[1]))
                if (parts.length > 2) {
                    tweetTime = tweetTime * 60 + parseInt(parts[2]) // convert MM:SS to seconds
                }
                if (tweetTime <= currentTime && tweetTime > maxStartTime) {
                    maxStartTime = tweetTime;
                    targetTweet = tweet;
                }
            });

            // If no tweet found that meets the condition, scroll to the top tweet
            if (!targetTweet && tweets.length > 0) {
                targetTweet = tweets[0];
            }

            if (targetTweet) {
                targetTweet.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }

        function togglePlayPause() {
            if (audio.paused) {
                audio.play();
                playPauseImage.src = 'assets/buttonPause.svg';
            } else {
                audio.pause();
                playPauseImage.src = 'assets/buttonPlay.svg';
            }
        }

        audio.addEventListener('timeupdate', function () {
            if (!isDragging) {
                var progress = (audio.currentTime / audio.duration) * 100;
                progressBar.style.width = progress + '%';
                progressCircle.style.left = progress + '%'; // Corrected reference
                updateProgressTime(audio.currentTime);
            }
        });

        audio.addEventListener('ended', function () {
            playPauseImage.src = 'assets/buttonPlay.svg';
            progressBar.style.width = '0%';
            progressCircle.style.left = '0%';
            updateProgressTime(0);
        });

        // Mouse events
        progressCircle.addEventListener('mousedown', function (event) {
            isDragging = true;
            document.addEventListener('mousemove', onMouseMove);
            document.addEventListener('mouseup', onMouseUp);
        });

        // Touch events
        progressCircle.addEventListener('touchstart', function (event) {
            isDragging = true;
            document.addEventListener('touchmove', onTouchMove);
            document.addEventListener('touchend', onTouchEnd);
        });

        function onMouseMove(event) {
            seek(event.clientX);
        }

        function onTouchMove(event) {
            var touch = event.touches[0];
            seek(touch.clientX);
        }

        function onMouseUp() {
            isDragging = false;
            document.removeEventListener('mousemove', onMouseMove);
            document.removeEventListener('mouseup', onMouseUp);
        }

        function onTouchEnd() {
            isDragging = false;
            document.removeEventListener('touchmove', onTouchMove);
            document.removeEventListener('touchend', onTouchEnd);
        }

        function onMouseUp() {
            isDragging = false;
            document.removeEventListener('mousemove', onMouseMove);
            document.removeEventListener('mouseup', onMouseUp);
        }

        progressCircle.addEventListener('dragstart', function (event) {
            event.preventDefault();
        });

        function updateProgressTime(currentTime) {
            var minutes = Math.floor(currentTime / 60);
            var seconds = Math.floor(currentTime % 60);
            if (seconds < 10) {
                seconds = '0' + seconds;
            }
            progressTime.textContent = minutes + ':' + seconds;
        }

        function seek(event) {
            var containerRect = progressContainer.getBoundingClientRect();
            var newTime = ((event.clientX - containerRect.left) / containerRect.width) * audio.duration;
            audio.currentTime = newTime;
            var progress = (audio.currentTime / audio.duration) * 100;
            progressBar.style.width = progress + '%';
            progressCircle.style.left = progress + '%'; // Ensure this reference is consistent
            updateProgressTime(newTime);
        }
    </script>

</body>

</html>