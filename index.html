
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Roboto:wght@300;400;500;700&display=swap">
</head>

<body>
    <div class="container">
        <div class="header">
            <div style="height: 100px;"></div>
            <img src="assets/AppLaunchFrog.gif" alt="Descriptive text about the GIF" style="width: 20%; height: auto;">
            <div style="height: 30px;"></div>
            <div class=" notification-text">Hop right over soon... We're busy getting everything just right and will be
                here
                before you can say 'Ribbit'!
            </div>
            <div style="height: 100px;"></div>
            <div class="header-text">ArXiv AI Papers - Daily Highlights</div>
            <div class="header-text">Fri. Aug 30, 2024</div>
            <div class="subheader-text">Opening music from <a href="https://ikson.com" target="_blank"
                    style="color: black; text-decoration: none;">TELL YOUR STORY by ikson</a></div>
            <div style="height: 30px;"></div>
        </div>
        <div class="tweet" id="tweet0">
            <div class="start-time-icon">00:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16073" target="_blank">@arXiv 2408.16073</a>
                    <span class="tweet-title">AI Personas: Can Robots Replicate Marketing Research?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Inspiration Point Labs, Stanford University, Boston University</span>
                </div>
                <div class="primary-text">
                    This research uses large language models (LLMs) to create AI personas that replicate the responses of human participants in media effects studies. This approach differs from previous work by systematically testing the ability of LLMs to reproduce the results of 133 published experimental findings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon">01:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16032" target="_blank">@arXiv 2408.16032</a>
                    <span class="tweet-title">LLMs Go Shopping: A Data-Efficient AI That Learns From Your Clicks</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, University of California Santa Barbara</span>
                </div>
                <div class="primary-text">
                    This research explores the use of Direct Preference Optimization (DPO) for training reinforcement learning (RL) agents in recommender systems. Unlike previous work that relies on explicit reward models, DPO leverages paired preference trajectories, eliminating the need for complex reward function design.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon">01:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16357" target="_blank">@arXiv 2408.16357</a>
                    <span class="tweet-title">Vision's Secret Code: How to Make AI See Like a Pro</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research introduces the "Law of Vision Representation" in multimodal large language models (MLLMs). It proposes that the performance of an MLLM is directly linked to the cross-modal alignment and correspondence of the vision representation used. This differs from previous work that primarily focused on empirical testing of various vision representations without a deeper understanding of the underlying factors driving performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon">02:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16469" target="_blank">@arXiv 2408.16469</a>
                    <span class="tweet-title">Panoramic Vision:  A Multi-Source Domain Adaptation Recipe for Semantic Segmentation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harbin Institute of Technology, Tsinghua University, Didi Global</span>
                </div>
                <div class="primary-text">
                    This research introduces a new task called Multi-source Domain Adaptation for Panoramic Semantic Segmentation (MSDA4PASS). Unlike previous methods that only used either real pinhole or synthetic panoramic images, MSDA4PASS leverages both to improve the segmentation model's performance on real panoramic images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon">02:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16325" target="_blank">@arXiv 2408.16325</a>
                    <span class="tweet-title">Point Cloud Denoising Gets a Diffusion Bridge:  A New Way to Clean Up 3D Scans!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, Google, Microsoft...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to point cloud denoising by adapting Diffusion Schrödinger bridges, which learn an optimal transport plan between noisy and clean point clouds. Unlike previous methods that predict point-wise displacements or learn noise distributions, this method focuses on finding the best way to move points from the noisy cloud to the clean one.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon">03:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16426" target="_blank">@arXiv 2408.16426</a>
                    <span class="tweet-title">Motion Blur? Not Anymore! New AI Model Captures Human and Camera Movement Like Never Before</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">NVIDIA, Shanghai Jiao Tong University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel control-inpainting motion diffusion prior, which uses a pre-trained motion diffusion model to guide the estimation of global human motion from dynamic camera videos. This approach differs from previous methods that rely solely on learned human motion priors, which often result in oversmoothed motions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon">03:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16005" target="_blank">@arXiv 2408.16005</a>
                    <span class="tweet-title">Inverse Rendering:  It's Not About the Volume, It's About the Worlds!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new approach to inverse rendering called "many-worlds" representation. Instead of modeling a volume, it models a distribution of surfaces, each representing a possible "world." This allows for a simpler and more efficient derivative propagation, eliminating the need for silhouette sampling and reducing the cost of optimization steps.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon">04:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16737" target="_blank">@arXiv 2408.16737</a>
                    <span class="tweet-title">Smaller, Weaker, Yet Better: Training LLMs with Budget-Friendly Data</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Los Angeles, Google</span>
                </div>
                <div class="primary-text">
                    This research explores the compute-optimality of using weaker, cheaper language models (WC) to generate synthetic training data for LLMs, challenging the common practice of relying on stronger, more expensive models (SE).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon">04:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16633" target="_blank">@arXiv 2408.16633</a>
                    <span class="tweet-title">Warehouse Robots Get Smart: Deep Learning Makes Picking a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, Northwestern University, Northern Arizona University</span>
                </div>
                <div class="primary-text">
                    This research distinguishes itself by integrating deep learning and reinforcement learning into automated picking systems, enhancing their adaptability to complex warehouse environments. Unlike previous work that relied heavily on static algorithms, this study utilizes a dynamic approach to optimize picking efficiency and accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon">05:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16517" target="_blank">@arXiv 2408.16517</a>
                    <span class="tweet-title">Continual Learning Gets a Tune-Up:  AutoVCL Adapts to the Task at Hand!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research introduces AutoVCL, an extension of the GVCL model that uses task heuristics to adjust the optimization process. Unlike previous work that uses fixed hyperparameters, AutoVCL dynamically adapts to the difficulty and similarity of incoming tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon">05:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16314" target="_blank">@arXiv 2408.16314</a>
                    <span class="tweet-title">Visual Grounding:  When Objects Get Lost in the Crowd, AI Needs a New Map!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research tackles the challenge of visual grounding when multiple objects of the same category are present in an image. It proposes two novel approaches: injecting semantic prior information from text queries and introducing a relation-sensitive data augmentation method. This differs from previous work by focusing on enhancing the model's understanding of both object semantics and spatial relationships.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon">06:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16219" target="_blank">@arXiv 2408.16219</a>
                    <span class="tweet-title">Training-Free Video Temporal Grounding:  No Data, No Problem!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research proposes a training-free approach to video temporal grounding, which means it doesn't rely on specific datasets for training. Instead, it leverages the capabilities of large language models (LLMs) and vision-language models (VLMs) to understand and localize events in videos.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon">06:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16126" target="_blank">@arXiv 2408.16126</a>
                    <span class="tweet-title">Speech Separation:  From Lab to Life, One Simulated Conversation at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC San Diego, Adobe</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel data simulation pipeline called AC-SIM, which incorporates both acoustic and content variations during the training of speech separators. This approach aims to align separators to a diverse range of real-world speech separation scenarios, unlike previous work that often focused on specific acoustic environments.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon">06:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16393" target="_blank">@arXiv 2408.16393</a>
                    <span class="tweet-title">Finding the Sweet Spot: Balancing Diversity and Quality in Optimization</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sorbonne University, Leiden University, University of Adelaide</span>
                </div>
                <div class="primary-text">
                    This research explores a new perspective on diversity in optimization, focusing on finding a fixed number of solutions with a minimum pairwise distance while maximizing their average quality. It differs from existing approaches like evolutionary diversity optimization, quality diversity, and multimodal optimization by explicitly considering the trade-off between diversity and fitness.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon">07:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16204" target="_blank">@arXiv 2408.16204</a>
                    <span class="tweet-title">Micro-batch Clipping:  A Gradient-Trimming Trick That Makes AI Models Smarter</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research delves into the mechanism behind micro-batch clipping, a gradient clipping method, and proposes that it works by adaptively suppressing unhelpful training samples, which they call "draggers." This differs from previous work that focused on micro-batch clipping's memory efficiency in differential privacy settings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon">07:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16181" target="_blank">@arXiv 2408.16181</a>
                    <span class="tweet-title">Inventory Control:  A Minibatch-SGD Meta-Policy for When You Just Can't Order Enough!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new meta-policy for inventory systems that uses minibatch stochastic gradient descent (SGD). Unlike previous methods that focus on bounding the difference between target and current inventory levels, this approach aims to control the frequency of infeasible target inventory levels by using minibatches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon">08:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16246" target="_blank">@arXiv 2408.16246</a>
                    <span class="tweet-title">Sparsity Saves the Day: New CiM Architecture Makes Deep Learning More Efficient</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Keio University, University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel probabilistic approximate computation (PAC) method that leverages statistical techniques to approximate multiply-and-accumulation (MAC) operations in compute-in-memory (CiM) systems. Unlike traditional approximate techniques, PAC focuses on bit-level sparsity, reducing data transfer between main memory and CiM banks, which is a major contributor to power consumption.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon">08:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16506" target="_blank">@arXiv 2408.16506</a>
                    <span class="tweet-title">Pose-Perfect: Training-Free Animation That's All You Need!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a training-free augmentation strategy for pose-guided video generation, which avoids the need for large video datasets and expensive GPU resources. The key innovation lies in a dual alignment method that decouples identity information from pose information, enabling more accurate and consistent animations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon">08:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16478" target="_blank">@arXiv 2408.16478</a>
                    <span class="tweet-title">Depth Perception:  How a Little Depth Can Make Semantic Segmentation a Whole Lot Better</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">TUMunich, ETH Zurich, Munich Center for Machine Learning...</span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel approach to unsupervised domain adaptation (UDA) for semantic segmentation by leveraging depth information. Unlike previous methods that use depth as an auxiliary task or multi-task learning, this research treats depth as a complementary modality to RGB images, focusing on a uni-directional refinement of RGB features using depth.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon">09:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16445" target="_blank">@arXiv 2408.16445</a>
                    <span class="tweet-title">Image Matching: A Love-Hate Relationship with Reality</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research comprehensively evaluates 20 state-of-the-art image matching methods, including 8 introduced in 2024, using both in-domain and out-of-domain datasets. It also investigates the impact of edge detection as a pre-processing step and examines the limitations of the mAA metric.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon">09:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16577" target="_blank">@arXiv 2408.16577</a>
                    <span class="tweet-title">Multimodal Data:  It's Not Just About the Sum of Its Parts!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London, Imperial College London, University of Bristol</span>
                </div>
                <div class="primary-text">
                    This research extends the Probability of Necessary and Sufficient Causes (PNS) framework to multimodal data, addressing the challenges of identifying causal features across different modalities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon">10:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16247" target="_blank">@arXiv 2408.16247</a>
                    <span class="tweet-title">Anno-incomplete Multi-dataset Detection: When Your Data's Got More Gaps Than Swiss Cheese!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research tackles the problem of "Anno-incomplete Multi-dataset Detection," where multiple datasets have incomplete annotations for certain object categories. It proposes a novel branch-interactive multi-task detector with an attention-based feature interactor to mine relations among categories from different datasets. This approach differs from previous work by explicitly addressing the issue of incomplete annotations in multi-dataset object detection.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon">10:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16542" target="_blank">@arXiv 2408.16542</a>
                    <span class="tweet-title">LLMs for Speech Recognition:  A Speedy, Synchronous Duet</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM, Indian Institute of Technology Bombay</span>
                </div>
                <div class="primary-text">
                    This research proposes SALSA, a novel approach for integrating LLMs with ASR systems. Unlike previous methods that rely on expensive fine-tuning or second-pass rescoring, SALSA couples the decoder layers of the ASR and LLM models synchronously, advancing both decoders in tandem. This approach significantly reduces training overhead and decoding latency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon">11:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16030" target="_blank">@arXiv 2408.16030</a>
                    <span class="tweet-title">Snoring Sounds: A Deep Dive into Airway Collapse with AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research distinguishes itself by focusing on classifying snoring sounds associated with multi-level obstructions in OSA patients, a common occurrence that previous studies have largely overlooked.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon">11:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16118" target="_blank">@arXiv 2408.16118</a>
                    <span class="tweet-title">AI-Powered Weather Forecasts: Can Robots Predict the Rain?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research explores the use of reinforcement learning (RL) algorithms to dynamically adjust parameters in climate models, offering a new approach to the long-standing challenge of parameterization in numerical weather prediction (NWP) systems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon">12:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16634" target="_blank">@arXiv 2408.16634</a>
                    <span class="tweet-title">AI Art's New Cop:  Reinforcement Learning Fights Copyright Infringement!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanyang Technological University, Sony</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel copyright protection method for text-to-image diffusion models, using reinforcement learning to minimize the generation of infringing content. Unlike previous methods that focus on specific tasks or lack standardized metrics, this approach incorporates a copyright metric aligned with legal standards and utilizes a multi-step decision-making process to guide the model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon">12:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16061" target="_blank">@arXiv 2408.16061</a>
                    <span class="tweet-title">3D Reconstruction Gets a Memory Boost:  No More Optimization Blues!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research introduces a spatial memory component to 3D reconstruction, allowing for online, incremental reconstruction without the need for optimization-based alignment. This differs from previous methods that relied on pairwise image comparisons and subsequent optimization steps.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon">13:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16221" target="_blank">@arXiv 2408.16221</a>
                    <span class="tweet-title">Speech Dysfluency Modeling Gets a Scalable Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research proposes a new approach to speech dysfluency modeling that utilizes articulatory gestures as a scalable forced aligner. Unlike previous methods that relied on hand-crafted features or struggled with scalability, this approach leverages the power of large language models and introduces a Connectionist Subsequence Aligner (CSA) for more accurate and efficient dysfluency detection.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon">13:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16379" target="_blank">@arXiv 2408.16379</a>
                    <span class="tweet-title">Graph Neural Networks Get a Physics Makeover: Forecasting Spatio-Temporal Data with TG-PhyNN!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sorbonne University, TotalEnergies</span>
                </div>
                <div class="primary-text">
                    This research introduces TG-PhyNN, a framework that incorporates physical constraints into Graph Neural Networks (GNNs) for forecasting spatio-temporal data. Unlike previous work that relies solely on data-driven approaches, TG-PhyNN leverages physical laws to guide the learning process, resulting in more accurate and interpretable predictions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon">14:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16725" target="_blank">@arXiv 2408.16725</a>
                    <span class="tweet-title">Mini-Omni: The Language Model That Can Hear, Talk, and Think in Real Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces Mini-Omni, a language model that can understand and generate speech in real-time, unlike previous models that relied on separate text-to-speech systems. The paper proposes a novel parallel text and audio generation method that allows the model to simultaneously output both text and audio tokens, enabling real-time interaction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon">14:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16018" target="_blank">@arXiv 2408.16018</a>
                    <span class="tweet-title">AI Detectives:  LLMs Crack the Code on Analog Trojan Horses!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Arizona State University, NVIDIA Corporation</span>
                </div>
                <div class="primary-text">
                    This research proposes SPICED, a framework that uses Large Language Models (LLMs) to detect and localize syntactical bugs and analog Trojans in circuit netlists. Unlike previous methods that rely on hardware modifications, SPICED operates entirely in the software domain, eliminating the need for additional circuitry.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon">14:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16147" target="_blank">@arXiv 2408.16147</a>
                    <span class="tweet-title">Forget Robots, Let's Talk Brains: Cognitive Models Predict Engagement Better Than AI!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, Harvard University</span>
                </div>
                <div class="primary-text">
                    This research uses a cognitive model based on Instance-Based Learning Theory (IBLT) to predict individual engagement in recommendations, unlike previous work that relied on general time-series forecasters like LSTMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon">15:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16190" target="_blank">@arXiv 2408.16190</a>
                    <span class="tweet-title">Flowing with Style:  New Method Tracks Objects to Uncover Hidden Flow Dynamics</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Caltech</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method for estimating flow gradients from sparse trajectory data, leveraging deep vision networks and Lagrangian gradient regression (LGR). This approach differs from previous methods by directly analyzing the motion of groups of objects, rather than relying on traditional optical flow or multiple object tracking techniques.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon">15:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16028" target="_blank">@arXiv 2408.16028</a>
                    <span class="tweet-title">LLMs: Not Just for Code, But for Bug-Hunting Too!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research reframes vulnerability detection as an anomaly detection problem, leveraging the fact that LLMs are trained on massive amounts of bug-free code. Instead of trying to learn bug patterns from sparse data, the authors propose using LLMs to identify code that deviates from their learned representation of correct code.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon">16:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16170" target="_blank">@arXiv 2408.16170</a>
                    <span class="tweet-title">CardBench:  A Database of Queries That Will Make Your Head Spin!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces CardBench, a benchmark for learned cardinality estimation in relational databases. Unlike previous benchmarks, CardBench includes a diverse set of 20 real-world databases and thousands of queries, enabling researchers to train and test pre-trained cardinality estimation models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon">16:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16704" target="_blank">@arXiv 2408.16704</a>
                    <span class="tweet-title">One-Shot Video Magic: Turning Text and a Single Clip into Movie Scenes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to text-to-video synthesis that utilizes a pre-trained depth-conditioned text-to-image model and fine-tunes it with a single text-video pair. This differs from previous methods that require extensive training on large datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon">17:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.16481" target="_blank">@arXiv 2408.16481</a>
                    <span class="tweet-title">No More Guesswork: Deep Learning Solves Sodium MRI Quality Assessment</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel deep learning-based no-reference image quality assessment (NR-IQA) metric called the Model Specialization Metric (MSM). Unlike existing NR-IQA metrics, MSM does not rely on subjective human opinions or objective labels, which can be difficult or impossible to obtain for sodium MRI. Instead, MSM measures the difference between the input image and the model's prediction to evaluate image quality.
                </div>
            </div>
        </div></div>

    <footer class="player-footer">
        <div id="controls">
            <button id="controllerButton" onclick="scrollToCurrentTweet()">
                <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
            </button>
            <button id="controllerButton" onclick="togglePlayPause()">
                <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                <div id="progressTime">0:00</div>
            </button>
        </div>
        <div id="progressContainer" onclick="seek(event)">
            <div id="progressBar"></div>
            <div id="progressCircle" draggable="true"></div>
        </div>
        <audio id="audioPlayer" src="assets/audio.mp3"></audio>
    </footer>

    <script>
        var audio = document.getElementById('audioPlayer');
        var controllerButton = document.getElementById('controllerButton');
        var playPauseImage = document.getElementById('playPauseImage');
        var progressBar = document.getElementById('progressBar');
        var progressCircle = document.getElementById('progressCircle');
        var progressTime = document.getElementById('progressTime');
        var isDragging = false;

        function scrollToCurrentTweet() {
            // This assumes `start-time-icon` divs have text in format "MM:SS"
            const tweets = document.querySelectorAll('.tweet');
            const currentTime = audio.currentTime;

            console.log("NOODLE", currentTime);

            let targetTweet = null;
            let maxStartTime = -1;

            tweets.forEach((tweet, index) => {
                const timeString = tweet.querySelector('.start-time-icon').textContent;
                const parts = timeString.split(':');

                tweetTime = (parseInt(parts[0]) * 60 + parseInt(parts[1]))
                if (parts.length > 2) {
                    tweetTime = tweetTime * 60 + parseInt(parts[2]) // convert MM:SS to seconds
                }
                if (tweetTime <= currentTime && tweetTime > maxStartTime) {
                    maxStartTime = tweetTime;
                    targetTweet = tweet;
                }
            });

            // If no tweet found that meets the condition, scroll to the top tweet
            if (!targetTweet && tweets.length > 0) {
                targetTweet = tweets[0];
            }

            if (targetTweet) {
                targetTweet.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }

        function togglePlayPause() {
            if (audio.paused) {
                audio.play();
                playPauseImage.src = 'assets/buttonPause.svg';
            } else {
                audio.pause();
                playPauseImage.src = 'assets/buttonPlay.svg';
            }
        }

        audio.addEventListener('timeupdate', function () {
            if (!isDragging) {
                var progress = (audio.currentTime / audio.duration) * 100;
                progressBar.style.width = progress + '%';
                progressCircle.style.left = progress + '%'; // Corrected reference
                updateProgressTime(audio.currentTime);
            }
        });

        audio.addEventListener('ended', function () {
            playPauseImage.src = 'assets/buttonPlay.svg';
            progressBar.style.width = '0%';
            progressCircle.style.left = '0%';
            updateProgressTime(0);
        });

        // Mouse events
        progressCircle.addEventListener('mousedown', function (event) {
            isDragging = true;
            document.addEventListener('mousemove', onMouseMove);
            document.addEventListener('mouseup', onMouseUp);
        });

        // Touch events
        progressCircle.addEventListener('touchstart', function (event) {
            isDragging = true;
            document.addEventListener('touchmove', onTouchMove);
            document.addEventListener('touchend', onTouchEnd);
        });

        function onMouseMove(event) {
            seek(event.clientX);
        }

        function onTouchMove(event) {
            var touch = event.touches[0];
            seek(touch.clientX);
        }

        function onMouseUp() {
            isDragging = false;
            document.removeEventListener('mousemove', onMouseMove);
            document.removeEventListener('mouseup', onMouseUp);
        }

        function onTouchEnd() {
            isDragging = false;
            document.removeEventListener('touchmove', onTouchMove);
            document.removeEventListener('touchend', onTouchEnd);
        }

        function onMouseUp() {
            isDragging = false;
            document.removeEventListener('mousemove', onMouseMove);
            document.removeEventListener('mouseup', onMouseUp);
        }

        progressCircle.addEventListener('dragstart', function (event) {
            event.preventDefault();
        });

        function updateProgressTime(currentTime) {
            var minutes = Math.floor(currentTime / 60);
            var seconds = Math.floor(currentTime % 60);
            if (seconds < 10) {
                seconds = '0' + seconds;
            }
            progressTime.textContent = minutes + ':' + seconds;
        }

        function seek(event) {
            var containerRect = progressContainer.getBoundingClientRect();
            var newTime = ((event.clientX - containerRect.left) / containerRect.width) * audio.duration;
            audio.currentTime = newTime;
            var progress = (audio.currentTime / audio.duration) * 100;
            progressBar.style.width = progress + '%';
            progressCircle.style.left = progress + '%'; // Ensure this reference is consistent
            updateProgressTime(newTime);
        }
    </script>

</body>

</html>