<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Roboto:wght@300;400;500;700&display=swap">
</head>

<body>
    <div class="container">
        <div class="header">
            <div style="height: 100px;"></div>
            <img src="assets/AppLaunchFrog.gif" alt="Descriptive text about the GIF" style="width: 20%; height: auto;">
            <div style="height: 30px;"></div>
            <div class=" notification-text">Hop right over soon... We're busy getting everything just right and will be
                here
                before you can say 'Ribbit'!
            </div>
            <div style="height: 100px;"></div>
            <div class="header-text">ArXiv AI Papers - Daily Highlights</div>
            <div class="header-text">Tue. Jul 09, 2024</div>
            <div class="subheader-text">Opening music from <a href="https://ikson.com" target="_blank"
                    style="color: black; text-decoration: none;">TELL YOUR STORY by ikson</a></div>
            <div style="height: 30px;"></div>
        </div>
        <div class="tweet" id="tweet0">
            <div class="start-time-icon">00:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06183" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06183</a>
                    <span class="tweet-title">Learning Rate Tuners: Don't Be Greedy, Be Stable!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research investigates the interplay between learning rate tuning and curvature dynamics during
                    deep learning training. It challenges the traditional focus on minimizing loss at each step and
                    proposes a new method, Curvature Dynamics Aware Tuning (CDAT), that prioritizes long-term curvature
                    stabilization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon">01:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05872" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05872</a>
                    <span class="tweet-title">Scaling Up, Scaling Down: How to Train Giant Neural Networks Without
                        Breaking Them</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research explores a broader space of parameterizations for neural networks, explicitly
                    quantifying the contribution of alignment between parameters and data. It relaxes key assumptions
                    made in previous work, leading to new theoretical results and a more general understanding of width
                    scaling.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon">01:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05986" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05986</a>
                    <span class="tweet-title">Satellite Images: A New Lens on Child Poverty</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford, University College London, University of
                        Copenhagen</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset, KidSat, which pairs satellite imagery with high-quality
                    survey data on child poverty. This dataset is specifically designed to benchmark satellite feature
                    representations for predicting child poverty, a task that has not been extensively explored in
                    previous research.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon">02:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05921" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05921</a>
                    <span class="tweet-title">Tracking Any Point in 3D: A New Benchmark for Seeing the World in
                        Motion</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark, TAPVid-3D, for evaluating 3D point tracking models. Unlike
                    previous benchmarks that focus on 2D tracking, TAPVid-3D provides real-world videos with 3D
                    annotations, allowing researchers to assess the performance of models in understanding and
                    predicting 3D motion.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon">02:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05342" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05342</a>
                    <span class="tweet-title">Vision-Language Models: Mind the Knowledge Gap, Not the Parameters!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, SmartMore, CUHK...</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel Distribution-aware Interference-free Knowledge Integration (DIKI)
                    framework for continual learning of Vision-Language Models (VLMs). DIKI addresses the issue of
                    "forward forgetting" where VLMs lose their pre-trained knowledge when adapting to new tasks. Unlike
                    previous methods that rely on heavy computation or external data, DIKI injects new knowledge into a
                    frozen VLM backbone using a fully residual mechanism, minimizing interference with the pre-trained
                    knowledge.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon">03:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06174" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06174</a>
                    <span class="tweet-title">Deepfakes: The Never-Ending Game of Cat and Mouse</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington</span>
                </div>
                <div class="primary-text">
                    This research paper provides a comprehensive overview of deepfake video generation and detection
                    techniques, focusing on the evolution of datasets used to train and evaluate these models. It
                    highlights the importance of robust, diverse, and frequently updated datasets to enhance detection
                    accuracy and generalizability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon">03:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04952" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04952</a>
                    <span class="tweet-title">VLMs: Geolocation Gurus, Privacy Guardians?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Georgia Institute of Technology, CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark, GPTGEOCHAT, specifically designed to evaluate the ability
                    of vision language models (VLMs) to moderate geolocation conversations with users. This differs from
                    previous work that focused on data memorization attacks or inference attacks in traditional LLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon">04:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05224" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05224</a>
                    <span class="tweet-title">Climate Models: Learning to See the Big Picture (and Avoid Going
                        Crazy)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Chicago</span>
                </div>
                <div class="primary-text">
                    This research introduces the concept of "receptive field" (RF) as a metric to predict the stability
                    of machine learning (ML) models used in climate simulations. Previous work often relied on
                    trial-and-error methods to address instability, but this study provides a more systematic approach.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon">04:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06101" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06101</a>
                    <span class="tweet-title">Garment Dynamics Get a Transformer Makeover: Mesh-Aware AI Learns to
                        Dress!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, Adobe Research</span>
                </div>
                <div class="primary-text">
                    This research models garment dynamics by focusing on local interactions between the garment and the
                    body, rather than relying on global features. This allows for generalization to unseen garment and
                    body geometries.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon">05:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05650" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05650</a>
                    <span class="tweet-title">Net Fragments: A New Way to See the World, One Tiny Piece at a
                        Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Zurich University of Applied Sciences, ETH Zurich, Goethe University
                        Frankfurt</span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel architecture called "Dynamic Net Architecture" (DNA) that uses
                    self-organizing networks to learn robust visual representations. Unlike traditional artificial
                    neural networks (ANNs), DNA relies on dynamic lateral connections within a single cortical area,
                    allowing for the integration of local and global features.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon">05:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06167" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06167</a>
                    <span class="tweet-title">CNN Training: Delayed Shrinking for Faster, More Accurate Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Georgia Institute of Technology, Cisco Research, Meta</span>
                </div>
                <div class="primary-text">
                    This research proposes a new technique called Delayed ϵ-Shrinking (DϵpS) for once-for-all training
                    of Convolutional Neural Networks (CNNs). Unlike previous methods that either shrink the full model
                    too early or too late, DϵpS delays the shrinking process until the full model is partially trained,
                    leading to faster training and improved accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon">06:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05483" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05483</a>
                    <span class="tweet-title">Recurrent Models: Learning to Forget, Remembering to Repeat!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, University at Buffalo</span>
                </div>
                <div class="primary-text">
                    This research explores the impact of data order on the performance of recurrent language models
                    (LLMs) for in-context learning. Unlike previous work that focused on improving memory selection
                    mechanisms, this paper demonstrates that the order in which information is presented to the model
                    significantly affects its ability to recall and utilize relevant information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon">06:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06112" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06112</a>
                    <span class="tweet-title">LLMs Get a Brain: Bi-Directional Reasoning Makes AI Decisions
                        Smarter</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">East China Normal University, Microsoft Research</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach called BIDDER, which enhances the decision-making
                    capabilities of LLMs by incorporating bi-directional deliberation reasoning. Unlike traditional
                    methods that rely solely on past information, BIDDER considers both historical context and potential
                    future outcomes, enabling LLMs to make more informed and rational decisions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon">07:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05338" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05338</a>
                    <span class="tweet-title">Auditing AI: A Three-Layered Cake of Oversight</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Oxford Internet Institute, Yale University, Harvard Law
                        School...</span>
                </div>
                <div class="primary-text">
                    This research proposes a three-layered approach to auditing generative AI systems, focusing on
                    governance, model, and application audits, which complements and informs each other. This approach
                    is distinct from previous work that primarily focused on single-layer audits.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon">07:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05416" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05416</a>
                    <span class="tweet-title">SAM's Got Your Back: Cross-Prompting for Semi-Supervised Medical Image
                        Segmentation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel cross-prompting consistency method with the Segment Anything Model
                    (SAM) for semi-supervised medical image segmentation. Unlike previous work that uses SAM as a static
                    component, this method fine-tunes SAM within the SSL pipeline and leverages its prompting mechanism
                    to effectively learn from unlabeled data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon">08:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05412" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05412</a>
                    <span class="tweet-title">Landmark Detection with One Shot: Foundation Models to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel framework for one-shot landmark detection in medical images that
                    utilizes pre-trained foundation models as feature extractors, eliminating the need for extensive
                    unlabeled data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon">08:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06079" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06079</a>
                    <span class="tweet-title">Text-to-Image Diffusion: One Shot, Multi-Scale, and Super-Fast!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This paper introduces a layered U-Net architecture for text-to-image diffusion models, allowing for
                    simultaneous image synthesis at multiple resolution scales. This differs from previous methods that
                    either operate in low-dimensional latent space or rely on cascaded models for high-resolution
                    generation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon">08:58</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04842" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04842</a>
                    <span class="tweet-title">AI Judges for Art: Is Your Text-to-Image Model Really a Good
                        Artist?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UNC-Chapel Hill, University of Chicago, Stanford University...</span>
                </div>
                <div class="primary-text">
                    This research introduces MJ-BENCH, a benchmark specifically designed to evaluate the feedback
                    provided by multimodal judges for text-to-image generation models. Unlike previous work that focuses
                    on evaluating the generation capabilities of these models, MJ-BENCH assesses their ability to
                    provide accurate and unbiased feedback across various perspectives, including alignment, safety,
                    image quality, and bias.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon">09:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04945" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04945</a>
                    <span class="tweet-title">Privacy-Preserving Stats: U-Statistics Get a Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC San Diego, University of Cambridge, University of Texas at
                        Austin</span>
                </div>
                <div class="primary-text">
                    This research focuses on privately estimating a parameter using U-statistics, a common tool in
                    statistical inference. Unlike previous work that focused on discrete data and simple privacy
                    mechanisms, this paper proposes a new algorithm that achieves nearly optimal private error for both
                    non-degenerate and degenerate U-statistics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon">10:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04969" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04969</a>
                    <span class="tweet-title">EVA-Score: Summarization's New Informativeness Detective!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new evaluation metric called EVA-Score, which focuses on the
                    informativeness of long-form summaries. Unlike previous metrics that rely on similarity, EVA-Score
                    uses atomic fact chain generation and document-level relation extraction to assess how much
                    information a summary actually captures.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon">10:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05530" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05530</a>
                    <span class="tweet-title">Robots Learn to Do Tasks with Just a Point and a Word!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Michigan, University of Washington</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for robot planning that uses both language and gestures to
                    control video generation, which is more intuitive and effective than previous language-only methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon">10:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05330" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05330</a>
                    <span class="tweet-title">Proxy Experiments: A Cheaper Way to Find Cause and Effect</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne, Technical University of
                        Munich</span>
                </div>
                <div class="primary-text">
                    This paper presents new algorithms for designing proxy experiments, which are experiments conducted
                    on variables that are easier and cheaper to manipulate than the main target variable. These
                    algorithms are significantly more efficient than previous methods, particularly when dealing with
                    multiple districts in the causal graph.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon">11:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06152" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06152</a>
                    <span class="tweet-title">Uni-ELF: Electrolyte Design Gets a Supercharged AI Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">DP Technology, Peking University, AI for Science Institute...</span>
                </div>
                <div class="primary-text">
                    This research introduces Uni-ELF, a multi-level representation learning framework for electrolyte
                    design. Unlike previous methods that rely on traditional machine learning models, Uni-ELF utilizes a
                    two-stage pretraining approach to capture intricate molecular and mixture-level information,
                    enhancing its predictive capabilities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon">11:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05878" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05878</a>
                    <span class="tweet-title">Transformers Go Hierarchical: Super-Resolution Gets a Speed Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zürich, Shanghai Jiao Tong University</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new approach to image super-resolution (SR) using hierarchical transformers.
                    Unlike previous methods that rely on fixed-size windows, this research uses expanding windows to
                    capture multi-scale features, improving performance while maintaining efficiency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon">12:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05463" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05463</a>
                    <span class="tweet-title">Tired of LLMs Spouting the Same Old Stuff? ReBase to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, Peking University, MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces Retrieval Based Distillation (ReBase), a method that retrieves data from
                    online sources and transforms it into domain-specific data for training specialized models. Unlike
                    previous methods that rely solely on LLMs to generate synthetic data, ReBase leverages the diversity
                    and quality of existing datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon">12:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05910" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05910</a>
                    <span class="tweet-title">Traffic Accidents: Scene Graphs to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, University of Trento, Bosch</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method for traffic accident classification that leverages scene
                    graphs to capture the essential features of an accident. Unlike previous work that focuses on
                    knowledge graphs for broader traffic monitoring, this study specifically uses scene graphs to model
                    relationships between objects involved in accidents.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon">12:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04898" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04898</a>
                    <span class="tweet-title">Incentivizing Truth-Tellers: A New Trick for Online Mechanism
                        Design</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Wisconsin–Madison</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new approach to designing incentive-compatible online mechanisms by
                    leveraging a weaker notion of differential privacy. Unlike previous work, this method applies to
                    general mechanism design problems beyond auctions and achieves a regret bound that scales
                    logarithmically with the size of the mechanism class.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon">13:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05625" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05625</a>
                    <span class="tweet-title">Predicting New User Events: A Causal Inference Approach to Unmasking the
                        Hidden Patterns</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Los Alamos National Laboratory, Carnegie Mellon University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel framework for predicting user events for new users without requiring
                    their category information. It utilizes causal inference by treating user event history as an
                    "intervention" and user category as a "confounder," enabling unbiased prediction across diverse user
                    behaviors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon">13:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05862" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05862</a>
                    <span class="tweet-title">Point Cloud Pre-training: Masked Autoencoders Get a Contrastive
                        Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Pisa, University of Trento, FBK...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method called Point-CMAE that integrates contrastive learning into
                    the masked autoencoder (MAE) framework for point cloud pre-training. Unlike previous approaches that
                    relied on extensive data augmentation, Point-CMAE leverages the inherent contrastive properties of
                    MAE by randomly masking input tokens twice to generate contrastive input pairs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon">14:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05229" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05229</a>
                    <span class="tweet-title">Pre-trained Models Get a Continual Learning Makeover: HiDe-PET to the
                        Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a unified framework for continual learning (CL) with pre-trained models
                    (PTMs) and parameter-efficient tuning (PET) techniques. It decomposes the CL objective into three
                    hierarchical components: within-task prediction (WTP), task-identity inference (TII), and
                    task-adaptive prediction (TAP). This framework is different from previous work because it explicitly
                    optimizes these components through a novel approach called Hierarchical Decomposition PET
                    (HiDe-PET).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon">14:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06153" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06153</a>
                    <span class="tweet-title">LLMs: Code Wizards or Bug-Making Machines?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Fudan University, Peking University, Huazhong University of Science and
                        Technology</span>
                </div>
                <div class="primary-text">
                    This research goes beyond just measuring how well LLMs generate code, focusing on the types of bugs
                    they produce and proposing a method to fix them without retraining.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon">15:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04910" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04910</a>
                    <span class="tweet-title">Arabic Dialects: It's Not Just One Label, It's a Multi-Label Party!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Mohamed bin Zayed University of Artificial Intelligence, CMU,
                        NYU</span>
                </div>
                <div class="primary-text">
                    This research introduces a multi-label approach to Arabic dialect identification, acknowledging that
                    a sentence can belong to multiple dialects, unlike previous single-label methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon">15:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04783" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04783</a>
                    <span class="tweet-title">Privacy-Preserving Density Estimation: A Stable List Decoding
                        Approach</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">McMaster University, Google</span>
                </div>
                <div class="primary-text">
                    This research introduces a new notion of stability called "stable list decoding" and demonstrates
                    its effectiveness in designing differentially private density estimators. This definition is weaker
                    than global stability and is related to the concepts of replicability and list replicability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon">16:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05010" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05010</a>
                    <span class="tweet-title">Vision Transformers on a Diet: PRANCE Makes Models Slimmer, Smarter, and
                        Faster!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, The Chinese University of Hong Kong</span>
                </div>
                <div class="primary-text">
                    This research introduces PRANCE, a framework that jointly optimizes both the model architecture and
                    the number of tokens used in Vision Transformers (ViTs). Unlike previous methods that focus on
                    either architecture or data optimization, PRANCE dynamically adjusts both aspects based on the
                    complexity of each input image, leading to more efficient inference.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon">16:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06087" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06087</a>
                    <span class="tweet-title">Convolutional Kernels Get a Mathematical Makeover: Introducing the
                        Analytic Convolutional Layer!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces the Analytic Convolutional Layer (ACL), a new type of convolutional layer
                    that uses mathematical functions to model convolution kernels. Unlike previous approaches that rely
                    on static kernels or kernel banks, ACLs allow for adaptive updates of the kernel parameters during
                    training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon">17:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05364" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05364</a>
                    <span class="tweet-title">Tabular Data's New BFF: Prototype Learning Makes Deep Models
                        Smarter</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Jilin University, University of Oxford, Chinese University of Hong
                        Kong...</span>
                </div>
                <div class="primary-text">
                    This paper introduces PTARL, a framework that uses prototype learning to improve deep tabular
                    models. Unlike previous methods that focus on individual data samples, PTARL leverages global data
                    structure information by projecting representations into a prototype-based projection space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon">17:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06135" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06135</a>
                    <span class="tweet-title">ANOLE: The Open-Source Multimodal Model That Can Actually Draw!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Generative AI Research Lab (GAIR)</span>
                </div>
                <div class="primary-text">
                    This research introduces ANOLE, an open-source large multimodal model (LMM) that can generate both
                    images and text in an interleaved sequence. Unlike previous LMMs, ANOLE is natively multimodal,
                    meaning it was trained on both text and image data from the start. This eliminates the need for
                    adapters or separate diffusion models for image generation, making it more efficient and
                    streamlined.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon">18:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05370" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05370</a>
                    <span class="tweet-title">Semi-Supervised Learning: When Labels Get a Little Too Biased, SEVAL Steps
                        In!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford, University of Sheffield</span>
                </div>
                <div class="primary-text">
                    This research introduces SEVAL, a method for improving pseudo-labeling in semi-supervised learning
                    (SSL) by learning refinement and threshold adjustment parameters from a partition of the training
                    dataset. This approach differs from previous work by directly optimizing these parameters based on
                    validation data, rather than relying on heuristics or uncalibrated model confidence.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon">18:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06027" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06027</a>
                    <span class="tweet-title">Prompt Engineering Gets a Data-Efficient Makeover: LLMs Learn to Write
                        Their Own Prompts!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to automatic prompt engineering (APE) by training LLMs on a
                    curated dataset of prompt-complementary pairs. This differs from previous APE models that rely
                    heavily on human-labeled data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon">19:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06023" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06023</a>
                    <span class="tweet-title">LLMs Learn to Think Like Us: Distilling System 2 into System 1</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta FAIR</span>
                </div>
                <div class="primary-text">
                    This research explores a novel technique called "System 2 Distillation" where a large language model
                    (LLM) is trained to mimic the reasoning process of a more complex System 2 model, but without
                    generating intermediate reasoning steps. This differs from previous work that focused on distilling
                    the outputs of a separate, larger model into a smaller one.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon">19:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04806" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04806</a>
                    <span class="tweet-title">Neural Networks: Two-Sample Tests Get a Time-Traveling Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Brown University, Duke University, University of California San
                        Diego</span>
                </div>
                <div class="primary-text">
                    This research analyzes the training time needed for a neural network two-sample test to detect
                    differences between datasets. It differs from previous work by focusing on the short-time regime and
                    approximating the neural network dynamics with the neural tangent kernel (NTK).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon">20:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06190" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06190</a>
                    <span class="tweet-title">SuperFlow: LiDAR's Time-Traveling Trick for Better 3D Vision</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanjing University of Aeronautics and Astronautics, Shanghai AI
                        Laboratory, National University of Singapore...</span>
                </div>
                <div class="primary-text">
                    This research introduces SuperFlow, a framework that leverages consecutive LiDAR-camera pairs to
                    establish spatiotemporal pretraining objectives. Unlike previous methods that focus on single LiDAR
                    scans, SuperFlow incorporates temporal cues from consecutive scans, enhancing the model's
                    understanding of dynamic environments.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon">20:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05796" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05796</a>
                    <span class="tweet-title">Prostate Cancer Grading: A Poisson's Paradise for Predicting Gleason
                        Groups</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Beijing University of Posts and Telecommunications, University College
                        London</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel Poisson Ordinal Network (PON) for directly estimating Gleason groups
                    from MRI scans. Unlike previous methods that rely on numerical ground-truth labels, PON leverages
                    Poisson encoding and focal loss to capture the inherent order and dependency between Gleason groups.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon">21:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05000" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05000</a>
                    <span class="tweet-title">LoRA-GA: Giving Large Language Models a Speed Boost with a Clever
                        Trick!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research focuses on improving the initialization method of LoRA, a popular technique for
                    fine-tuning large language models. Unlike previous work that initializes LoRA randomly, this paper
                    proposes a novel method called LoRA-GA that aligns the gradients of the low-rank matrix product with
                    those of full fine-tuning at the first step.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon">21:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05423" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05423</a>
                    <span class="tweet-title">AIED: From Drill & Practice to Pro-Active Responsibility</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research paper argues that the AIED field needs to move beyond its traditional focus on
                    improving learning outcomes and engage more actively with broader issues of responsible AI,
                    particularly in relation to data privacy and the potential for bias in algorithms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon">21:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05282" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05282</a>
                    <span class="tweet-title">UltraEdit: Image Editing Gets a Massive Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Beijing Academy of Artificial Intelligence, Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces ULTRAEDIT, a new dataset for instruction-based image editing. Unlike
                    previous datasets that relied heavily on text-to-image models, ULTRAEDIT uses real images as anchors
                    to mitigate biases and generate more diverse editing samples. It also incorporates region-based
                    editing, allowing for more fine-grained control over the editing process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon">22:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05789" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05789</a>
                    <span class="tweet-title">Hyperparameter Tuning: A New Way to Tame the Wild West of Action
                        Spaces</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Freiburg</span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark called "Piecewise Linear" that simulates the complexities
                    of high-dimensional action spaces in dynamic algorithm configuration (DAC). It also proposes
                    sequential policies as a way to manage these complexities, where actions are selected one dimension
                    at a time, taking into account the interdependence of different dimensions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon">22:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05591" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05591</a>
                    <span class="tweet-title">Transformers Get a Convolutional Makeover: Attention, It's Local
                        Now!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Michigan, UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This paper introduces Convolution-Augmented Transformer (CAT), a new architecture that combines
                    convolutional filters with attention layers. Unlike previous hybrid architectures, CAT can provably
                    solve the associative recall (AR) and copying tasks using a single layer while also enjoying
                    guaranteed length generalization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon">23:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04753" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04753</a>
                    <span class="tweet-title">Sleep Depth Index: A Deep Dive into Your Zzz's!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research proposes a deep learning method to annotate a continuous sleep depth index using
                    existing sleep staging labels, unlike previous methods that rely solely on EEG data or manual
                    feature extraction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon">23:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06048" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06048</a>
                    <span class="tweet-title">Braille to Chinese: A Vision-Braille AI Makes College Dreams Come
                        True!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research fine-tunes a multilingual T5 model for Braille-to-Chinese translation, incorporating a
                    curriculum learning strategy to improve accuracy. Unlike previous work, it focuses on real-world
                    Braille usage, including tone omission, and creates a large-scale Chinese Braille dataset.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon">24:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06057" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06057</a>
                    <span class="tweet-title">BoN-ing Up on Efficiency: A New Way to Align Language Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new method for aligning language models to human preferences called
                    variational BoN (vBoN). Unlike traditional methods that rely on sampling multiple outputs and
                    selecting the best one, vBoN fine-tunes the language model to mimic the behavior of the Best-of-N
                    algorithm during inference.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon">24:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05385" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05385</a>
                    <span class="tweet-title">Neural Network Harmony: Merging Models with a Little Help from CCA</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Montreal, Concordia University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new model merging algorithm called CCA Merge, which uses Canonical
                    Correlation Analysis to align features from multiple neural networks. Unlike previous methods that
                    rely on permutations, CCA Merge allows for more flexible alignment by finding linear combinations of
                    features that maximize correlations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon">25:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05336" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05336</a>
                    <span class="tweet-title">Taxing Times: AI Wants to Make Equality a Reality (But Will It
                        Work?)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford, Princeton University, Stanford University</span>
                </div>
                <div class="primary-text">
                    This research explores the use of AI systems to optimize tax policy, specifically focusing on
                    reducing economic inequality. It distinguishes itself from previous work by framing this application
                    as a continuation and intensification of Weberian rationalization, highlighting the potential for
                    both benefits and tensions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon">25:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04958" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04958</a>
                    <span class="tweet-title">Shuffling the Deck: A New Flow for Better Image Generation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">South China University of Technology, Tsinghua University, Columbia
                        University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new Shuffle operation within the multi-scale architecture of Normalizing
                    Flows. Unlike previous methods that simply split latent variables, this operation assigns weights to
                    each channel feature map based on its information content, adaptively shuffling the latent variables
                    before splitting them.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon">26:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05858" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05858</a>
                    <span class="tweet-title">LLMs on Your Phone? No Problem! New System Makes It Fast!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Beijing University of Posts and
                        Telecommunications</span>
                </div>
                <div class="primary-text">
                    This research presents mllm-NPU, a novel system that efficiently leverages on-device Neural
                    Processing Units (NPUs) for accelerating the inference of large language models (LLMs). Unlike
                    previous work that primarily focused on speeding up the text generation stage, mllm-NPU tackles the
                    prefill stage, which is often the bottleneck in on-device LLM inference.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon">27:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06176" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06176</a>
                    <span class="tweet-title">Contour-Weighted Loss: Giving Segmentation Boundaries a Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new compound loss function for image segmentation that incorporates a
                    contour-weighted cross-entropy loss and a separable dice loss. This approach aims to address the
                    issue of data imbalance between intra- and inter-class regions, which is common in medical image
                    segmentation. Unlike previous methods that focus on improving network architecture, this paper
                    focuses on enhancing the loss function to handle data imbalance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon">27:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05125" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05125</a>
                    <span class="tweet-title">FedLuck: When Gradient Compression Meets Local Updating, AFL Gets a Speed
                        Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Southern University of Science and Technology,
                        Harbin Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This research introduces FedLuck, an AFL framework that jointly optimizes local updating frequency
                    and gradient compression rate, unlike previous approaches that focused on only one of these factors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon">27:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06084" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06084</a>
                    <span class="tweet-title">Synthetic Scenes, Real Results: 3D Vision-Language Pretraining Gets a
                        Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, UC Los Angeles</span>
                </div>
                <div class="primary-text">
                    This research introduces SynVL3D, a synthetic dataset for 3D vision-language pretraining. Unlike
                    previous datasets, SynVL3D offers 10,000 indoor scenes with 1 million descriptions, providing more
                    scene diversity and fine-grained annotations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon">28:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04871" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04871</a>
                    <span class="tweet-title">Layer-Wise Learning Rates: Giving Each Neuron Its Own Pace!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel layer-wise learning scheme that adjusts learning parameters per layer
                    based on the differences in the Jacobian/Attention/Hessian of the output activations with respect to
                    the network parameters. This differs from previous methods that calculate the cumulative differences
                    of all matched features and back-propagate that loss through all layers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon">28:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04973" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04973</a>
                    <span class="tweet-title">LogicVista: AI's New IQ Test - Can Your Robot Pass?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Los Angeles, Yale University</span>
                </div>
                <div class="primary-text">
                    This research introduces LogicVista, a benchmark specifically designed to evaluate the logical
                    reasoning capabilities of multimodal large language models (MLLMs) in visual contexts. Unlike
                    previous benchmarks that focus on object recognition or mathematical reasoning, LogicVista assesses
                    a wider range of reasoning skills, including inductive, deductive, numerical, spatial, and
                    mechanical reasoning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon">29:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05008" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05008</a>
                    <span class="tweet-title">Point Cloud Completion Gets a Spherical Makeover: T-CorresNet's
                        Template-Guided Approach</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel point cloud completion method called T-CorresNet, which utilizes a
                    spherical template to guide the generation of a coarse complete template. This differs from previous
                    methods that rely solely on the input point cloud for completion.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon">29:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05761" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05761</a>
                    <span class="tweet-title">Unmasking the Mystery: How AI's "Gut Feeling" Reveals Hidden Clues in
                        Brain Scans</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Lausanne, Lausanne University Hospital, University of
                        Applied Sciences Western Switzerland...</span>
                </div>
                <div class="primary-text">
                    This research explores the interpretability of uncertainty values in deep learning models for
                    segmenting cortical lesions in multiple sclerosis. Unlike previous work that focused on using
                    uncertainty to assess prediction reliability, this study delves into understanding what these
                    uncertainty values actually mean and how they can be used to gain insights into model behavior.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon">29:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05966" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05966</a>
                    <span class="tweet-title">Bellman Equations Go Continuous: A High-Order Discretization for Policy
                        Evaluation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, University of California San Diego</span>
                </div>
                <div class="primary-text">
                    This research introduces a new class of algorithms for policy evaluation in continuous-time
                    diffusion processes. Unlike previous work, these algorithms use high-order discretization schemes to
                    achieve more accurate approximations of the value function.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon">30:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05649" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05649</a>
                    <span class="tweet-title">Graph Neural Networks Get a Random Makeover: GRASS Grows Smarter with
                        Rewiring and Attention</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University</span>
                </div>
                <div class="primary-text">
                    This paper introduces GRASS, a novel GNN architecture that combines message passing, graph rewiring,
                    and Graph Transformers. Unlike previous work, GRASS rewires the input graph by superimposing a
                    random regular graph, enhancing long-range information propagation while preserving structural
                    features. It also employs a unique additive attention mechanism tailored for graph-structured data,
                    providing a graph inductive bias while remaining computationally efficient.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon">30:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05767" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05767</a>
                    <span class="tweet-title">Ultrasound Reconstruction Gets a Flexible Makeover: Deep Learning Tackles
                        Soft Tissue Motion</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel co-optimization algorithm for reconstructing 3D ultrasound images
                    without a tracker. Unlike previous methods that focused solely on rigid transformations, this
                    approach incorporates nonrigid deformation estimation, accounting for soft tissue movement during
                    scanning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon">31:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05237" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05237</a>
                    <span class="tweet-title">Privacy for the Last Iterate: DP-SGD Gets a New Lease on Life!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research focuses on the privacy of the final output of DP-SGD, a common privacy-preserving
                    machine learning algorithm. Unlike previous work, it analyzes DP-SGD without assuming the loss
                    function is convex or Lipschitz continuous, making it more applicable to real-world scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon">31:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05763" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05763</a>
                    <span class="tweet-title">Distributing the Love: A New Observer for Quasilinear Systems That's Fast,
                        Robust, and Homogeneous!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">French Institute for Research in Computer Science and Automation,
                        Beihang University</span>
                </div>
                <div class="primary-text">
                    This paper introduces a distributed observer for quasilinear systems that achieves finite or
                    fixed-time stability. Unlike previous work focusing on linear systems, this approach utilizes
                    generalized homogeneity to design observers that converge in a finite time, even in the presence of
                    bounded perturbations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet67">
            <div class="start-time-icon">31:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05600" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05600</a>
                    <span class="tweet-title">AI Artist: A Multimodal LLM That Can Paint, Edit, and Fix Its Own
                        Mistakes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Peking University, Huawei...</span>
                </div>
                <div class="primary-text">
                    This research proposes a unified image generation and editing system called GenArtist, which uses a
                    multimodal large language model (MLLM) as an AI agent to coordinate and manage the entire process.
                    Unlike previous methods that focus on specific tasks, GenArtist can handle a wide range of
                    generation and editing tasks, including complex text prompts and multi-step instructions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet68">
            <div class="start-time-icon">32:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06159" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06159</a>
                    <span class="tweet-title">Infrared-Visible Image Fusion: A Three-Branch Tango for Better
                        Vision!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Jilin University, Peking University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel three-branch framework for infrared-visible image fusion. The key
                    innovation lies in the use of a Graph Reasoning Module (GR) to model high-level cross-modality
                    relations and extract low-level details as complementary information in the encoder. This approach
                    differs from previous methods by incorporating a more comprehensive feature extraction strategy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet69">
            <div class="start-time-icon">32:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04738" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04738</a>
                    <span class="tweet-title">Brainwaves on a Budget: New AI Makes ERP Decoding Cheaper and
                        Faster!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Beijing University of Post and Telecommunications, Tsinghua University,
                        Southern University of Science and Technology</span>
                </div>
                <div class="primary-text">
                    This research proposes a contrastive learning framework for ERP detection, which aims to extract
                    subject-invariant components of ERP signals. This approach differs from previous work by focusing on
                    minimizing inter-subject variability in latent space, rather than relying solely on handcrafted
                    features or traditional algorithms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet70">
            <div class="start-time-icon">33:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05608" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05608</a>
                    <span class="tweet-title">Anonymizing Conversations: Making Your Chat Secretly Secret!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Singapore Institute of Technology, National University of Singapore,
                        National Institute of Informatics</span>
                </div>
                <div class="primary-text">
                    This research focuses on anonymizing conversations with multiple speakers, a task not explored in
                    previous work. It proposes a cascaded system that combines speaker diarization with single-speaker
                    anonymization techniques to protect privacy while preserving the conversational structure.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet71">
            <div class="start-time-icon">33:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04900" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04900</a>
                    <span class="tweet-title">Newsvendor's Dilemma: SAA's Local Advantage for Inventory Costs</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research extends the analysis of Sample Average Approximation (SAA) for data-driven newsvendor
                    problems to include general convex inventory costs, going beyond the previously studied linear
                    costs. It also establishes the optimality of SAA under both global and local strong convexity
                    conditions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet72">
            <div class="start-time-icon">34:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04787" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04787</a>
                    <span class="tweet-title">LLMs Learn to Think Recursively: Breaking Down Problems Like a Pro!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Washington University in St. Louis, UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method called Re-Tuning, which trains large language models (LLMs) to
                    solve compositional tasks recursively. Unlike previous approaches that rely on scratchpad prompting
                    or in-context learning, Re-Tuning breaks down problems into smaller subproblems, solves them
                    independently, and then combines the results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet73">
            <div class="start-time-icon">34:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06187" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06187</a>
                    <span class="tweet-title">Tired of Finetuning? JeDi's Got Your Personalized Images!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Johns Hopkins University, TTI-Chicago, NVIDIA Research</span>
                </div>
                <div class="primary-text">
                    This paper introduces JeDi, a finetuning-free personalized text-to-image generation model. Unlike
                    previous methods that rely on encoding reference images into a compact feature space, JeDi directly
                    trains a joint-image diffusion model to learn the distribution of multiple related text-image pairs
                    sharing a common subject.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet74">
            <div class="start-time-icon">35:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05040" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05040</a>
                    <span class="tweet-title">Code Less, Learn More: Pruning Code Datasets for Efficient LLMs</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nvidia</span>
                </div>
                <div class="primary-text">
                    This research focuses on data pruning specifically for fine-tuning large language models (LLMs) for
                    code generation. Unlike previous work that primarily focused on general instruction-following tasks,
                    this paper explores the unique characteristics of synthetic code datasets and proposes a scalable
                    and effective pruning strategy tailored for them.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet75">
            <div class="start-time-icon">35:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04844" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04844</a>
                    <span class="tweet-title">Point Clouds Get a Geometry Makeover: Neural Varifolds to the
                        Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Southampton, University of Cambridge, Université Claude
                        Bernard Lyon 1</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new way to represent the geometry of point clouds using neural varifolds.
                    Unlike previous methods that focus on point positions, this approach incorporates both position and
                    tangent spaces, providing a more nuanced understanding of surface geometry.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet76">
            <div class="start-time-icon">36:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05682" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05682</a>
                    <span class="tweet-title">LLMs Learn from Mistakes: A Teacher-Student Framework for Smarter
                        AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach called Retrieved In-Context Principles (RICP) that utilizes
                    a teacher model to analyze student model mistakes and generate principles to prevent similar errors.
                    Unlike previous methods that use fixed principles, RICP provides both task-level and question-level
                    principles, enhancing customization and error coverage.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet77">
            <div class="start-time-icon">36:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05437" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05437</a>
                    <span class="tweet-title">AI Tutors Learn to Code: Prompt Engineering Makes LLMs Smarter</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research focuses on categorizing prompt engineering strategies for LLMs in computer programming
                    education, specifically exploring how different strategies impact the models' ability to solve
                    problems beyond their inherent capabilities. It goes beyond simply using LLMs for code generation
                    and delves into how to optimize their use for different educational needs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet78">
            <div class="start-time-icon">37:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04877" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04877</a>
                    <span class="tweet-title">AI-Powered Catalyst Hunt: Dashing to a Better Future for Hydrogen!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Chicago</span>
                </div>
                <div class="primary-text">
                    This research introduces a multi-stage machine learning approach for discovering and optimizing
                    acidic oxygen evolution reaction (OER) catalysts. Unlike traditional trial-and-error methods, this
                    approach systematically narrows the exploration space using domain knowledge and active learning,
                    minimizing reliance on subjective intuition.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet79">
            <div class="start-time-icon">37:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06168" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06168</a>
                    <span class="tweet-title">Grasping Under Occlusion: A New Benchmark for Robotic Hands That Don't Get
                        Lost in the Clutter!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Technical University of Munich, Peking University, University of
                        Oxford...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark dataset, TARGO, specifically designed to evaluate the
                    performance of robotic grasping models in cluttered environments with varying levels of occlusion.
                    Unlike previous datasets, TARGO focuses on the impact of occlusion on target-driven grasping, where
                    the robot must identify and grasp a specific object amidst other objects.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet80">
            <div class="start-time-icon">38:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06124" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06124</a>
                    <span class="tweet-title">Tree-mendous Images: Clustering Helps Diffusion Models Generate Better
                        Pictures</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research combines hierarchical clustering with diffusion models to generate images. Previous
                    work used VAEs for clustering, but this approach leverages the strengths of both techniques.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet81">
            <div class="start-time-icon">38:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06107" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06107</a>
                    <span class="tweet-title">Filtering After Shading: A Texture-Filtering Revolution?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">NVIDIA, Shiokara–Engawa Research</span>
                </div>
                <div class="primary-text">
                    This research proposes filtering textures after shading, rather than the traditional approach of
                    filtering before shading. This method is shown to be more accurate, especially when dealing with
                    textures that have a non-linear effect on the final rendered image.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet82">
            <div class="start-time-icon">38:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05339" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05339</a>
                    <span class="tweet-title">AI Ethics: From Principles to Practice - Lessons from AstraZeneca</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research focuses on the practical challenges of implementing AI governance within a large
                    organization, drawing on a case study of AstraZeneca's experience. It goes beyond simply outlining
                    ethical principles and explores the concrete steps needed to operationalize them.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet83">
            <div class="start-time-icon">39:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05341" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05341</a>
                    <span class="tweet-title">AI Ethics: Switch, Ladder, or Matrix? Picking the Right Tool for the
                        Job</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research paper examines how different models for classifying AI systems can be used to
                    operationalize AI governance frameworks. It identifies three distinct approaches: the Switch, the
                    Ladder, and the Matrix, each with its own strengths and weaknesses.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet84">
            <div class="start-time-icon">39:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05674" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05674</a>
                    <span class="tweet-title">KITA: The AI Assistant That Doesn't Hallucinate (and Actually Gets Things
                        Done)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces KITA, a framework for building task-oriented conversational agents that use
                    programmable policies to ensure reliable and consistent responses. Unlike traditional LLMs, KITA
                    avoids generating unfounded responses ("hallucinations") by grounding its answers in hybrid
                    knowledge sources.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet85">
            <div class="start-time-icon">40:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06076" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06076</a>
                    <span class="tweet-title">Deep Learning's Secret: Simple Features Rule the World!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Brown University, Université de Toulouse, Google DeepMind</span>
                </div>
                <div class="primary-text">
                    This research introduces a new metric for quantifying feature complexity based on V-information,
                    which considers the computational effort required to extract a feature. This metric is used to
                    analyze the complexities of features learned by a ResNet50 model trained on ImageNet.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet86">
            <div class="start-time-icon">40:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05734" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05734</a>
                    <span class="tweet-title">Chatbots Get a Grammar Lesson: Can AI Reason Like Humans?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Handong Global University</span>
                </div>
                <div class="primary-text">
                    This study investigates the ability of conversational chatbots powered by large language models
                    (LLMs) to understand and characterize predicate symmetry, a cognitive linguistic function
                    traditionally believed to be an inherent human trait. Unlike previous work that focused on
                    fine-tuning LLMs for specific tasks, this research explores the potential of LLMs to learn new tasks
                    from prompts alone, a paradigm known as in-context learning (ICL).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet87">
            <div class="start-time-icon">40:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05261" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05261</a>
                    <span class="tweet-title">Convexity on a Curveball: New Framework Makes Optimization More
                        Geometric</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, MIT</span>
                </div>
                <div class="primary-text">
                    This paper extends the Disciplined Convex Programming (DCP) framework to handle optimization
                    problems on curved spaces, specifically Cartan-Hadamard manifolds. This is a significant departure
                    from traditional DCP, which focuses on Euclidean spaces.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet88">
            <div class="start-time-icon">41:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04942" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04942</a>
                    <span class="tweet-title">Safe Robots Learn from Past Mistakes: Offline Training for Online
                        Safety!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach called FOSP (Fine-tuning Offline Safe Policy through World
                    Models) that combines offline training with online fine-tuning to enhance safety in robotic tasks.
                    Unlike previous methods that focus solely on online safety, FOSP leverages a pre-trained policy and
                    world model to ensure safety during online exploration.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet89">
            <div class="start-time-icon">41:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05975" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05975</a>
                    <span class="tweet-title">LLaMAX: Giving LLMs a Multilingual Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Purple Mountain Observatory, Nanjing University, CMU</span>
                </div>
                <div class="primary-text">
                    This research focuses on enhancing the translation capabilities of LLMs for low-resource languages
                    by conducting extensive multilingual continual pre-training. Unlike previous work that primarily
                    focused on fine-tuning, this study explores the impact of vocabulary expansion and data augmentation
                    during the pre-training phase.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet90">
            <div class="start-time-icon">42:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05983" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05983</a>
                    <span class="tweet-title">Face Recognition's New Secret Weapon: Explaining AI's Decisions with
                        Pixel-Perfect Maps!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for explaining how AI-based face recognition systems make
                    decisions. Unlike previous work that focused on individual images, this study focuses on explaining
                    the similarities and differences between pairs of images, addressing both face verification and
                    identification scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet91">
            <div class="start-time-icon">42:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06141" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06141</a>
                    <span class="tweet-title">Pose Prediction: Conformalizing the Chaos of Human Movement</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for learning 3D human poses from 2D keypoints by leveraging a
                    conditional distribution with a diffusion model. Unlike previous work that focuses on
                    single-hypothesis predictions, this method generates and aggregates multiple 3D pose hypotheses,
                    effectively integrating conformal prediction into the learning process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet92">
            <div class="start-time-icon">43:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05238" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05238</a>
                    <span class="tweet-title">LiDAR Tracking Gets a Makeover: Part-to-Part Motion Modeling for Point
                        Clouds</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Hangzhou Dianzi University, Shanghai Jiao Tong University, Hanyang
                        University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel tracking framework called P2P that directly infers the relative
                    motion of a target from consecutive LiDAR point clouds. Unlike previous methods that rely on
                    appearance matching or segmentation, P2P leverages part-to-part motion modeling to capture
                    fine-grained information changes between frames, enhancing motion cues for more accurate tracking.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet93">
            <div class="start-time-icon">43:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05732" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05732</a>
                    <span class="tweet-title">Transformers: The New Fairytale of Bias Removal!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Freiburg</span>
                </div>
                <div class="primary-text">
                    This research introduces FairPFN, a transformer-based approach to counterfactual fairness that
                    eliminates the need for prior knowledge of the causal model. Unlike previous methods that rely on
                    domain knowledge and approximate causal discovery techniques, FairPFN is pre-trained on synthetic
                    data to directly remove the causal effects of protected attributes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet94">
            <div class="start-time-icon">44:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04925" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04925</a>
                    <span class="tweet-title">MOOCs Recommendations Get a Brain Boost: LLMs + RAG = Smarter Course
                        Choices</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces RAMO, a course recommender system that uses Retrieval-Augmented Generation
                    (RAG) to enhance the capabilities of large language models (LLMs) in providing personalized
                    recommendations, particularly for new users. This approach addresses the "cold start" problem, which
                    is a common challenge in traditional recommender systems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet95">
            <div class="start-time-icon">44:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05138" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05138</a>
                    <span class="tweet-title">LLMs in Software: A Vortex of Integration Defects!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">East China Normal University, University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research focuses on the integration challenges of RAG-enhanced LLMs in software systems, unlike
                    previous work that primarily focused on improving LLM and RAG algorithms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet96">
            <div class="start-time-icon">44:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06116" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06116</a>
                    <span class="tweet-title">H&E-ing for Help: AI Learns to Spot Cells on Basic Stains</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Vanderbilt University, Johns Hopkins University, National Institutes of
                        Health...</span>
                </div>
                <div class="primary-text">
                    This research uses a novel approach to classify cell subtypes on standard H&E stained slides by
                    leveraging information from more complex Multiplexed Immunofluorescence (MxIF) images. This differs
                    from previous work that focused on classifying cells on H&E using only H&E data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet97">
            <div class="start-time-icon">45:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05687" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05687</a>
                    <span class="tweet-title">Aerial Images, Lane Graphs, and Transformers: A Love Triangle for
                        Self-Driving Cars</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Freiburg</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to generating lane graphs from aerial imagery using
                    transformer models. Unlike previous methods that rely on vehicle-mounted sensors, this approach
                    leverages the broader context provided by aerial images to predict lane connectivity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet98">
            <div class="start-time-icon">45:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05398" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05398</a>
                    <span class="tweet-title">Fairness Fix: A New Method to Make Student Models Less Biased</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sorbonne University</span>
                </div>
                <div class="primary-text">
                    This research introduces a post-processing method that improves the fairness of predictive student
                    models without sacrificing accuracy. Unlike previous work that focuses on evaluating fairness, this
                    method actively mitigates unfairness by adjusting the model's output.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet99">
            <div class="start-time-icon">46:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06053" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06053</a>
                    <span class="tweet-title">Quantum Operators Get a Local Makeover: Deep Learning Goes Small to
                        Predict Big Things</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new deep learning model called SLEM (Strictly Localized Equivariant
                    Message-Passing) for predicting quantum operators. Unlike previous methods that rely on iterative
                    updates that expand the receptive field, SLEM uses a strictly localized scheme to construct
                    representations of quantum operators, leading to improved data efficiency and scalability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet100">
            <div class="start-time-icon">46:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06004" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06004</a>
                    <span class="tweet-title">LLMs: Seeing is Believing, But Can They Think?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">KAIST, Amazon, Allen Institute for AI...</span>
                </div>
                <div class="primary-text">
                    This research explores the precursory inferences for theory of mind (ToM) in LLMs by evaluating
                    their perception inference and perception-to-belief inference capabilities. Unlike previous work
                    that focuses on the accuracy of LLM responses to ToM questions, this study delves into the
                    underlying reasoning processes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet101">
            <div class="start-time-icon">47:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05965" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05965</a>
                    <span class="tweet-title">AI Video Safety: Is Your Dream Video a Nightmare in the Making?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Chinese Academy of Sciences, Tsinghua University, RealAI...</span>
                </div>
                <div class="primary-text">
                    This research introduces T2VSafetyBench, a new benchmark specifically designed to evaluate the
                    safety of text-to-video models. Unlike previous benchmarks that focused primarily on video
                    generation quality, T2VSafetyBench considers 12 critical aspects of video generation safety,
                    including pornography, violence, gore, and temporal risk.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet102">
            <div class="start-time-icon">47:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04974" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04974</a>
                    <span class="tweet-title">Reinforcement Learning Goes Social: How Agents Learn to Share the State of
                        the World</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research extends the MAOPAC algorithm, originally designed for fully observable environments,
                    to handle partially observable settings. It does this by incorporating social learning strategies to
                    estimate the global state in a fully decentralized manner. Unlike many existing algorithms, the
                    proposed method does not necessitate transition models for state estimation, thereby classifying it
                    as a model-free reinforcement learning algorithm.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet103">
            <div class="start-time-icon">47:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04811" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04811</a>
                    <span class="tweet-title">Deep Learning Gets a Makeover: Ditch the Replay Buffer, Embrace
                        LayerNorm!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Polytechnic University of Catalonia, University of Oxford, Barcelona
                        Supercomputing Center...</span>
                </div>
                <div class="primary-text">
                    This research explores the use of LayerNorm in temporal difference (TD) learning algorithms,
                    demonstrating that it can stabilize training without the need for a target network or replay buffer,
                    which are common techniques used in deep Q-learning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet104">
            <div class="start-time-icon">48:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05218" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05218</a>
                    <span class="tweet-title">Rotation Angle's Got a Secret: How Self-Supervised Learning Plays
                        Hide-and-Seek with Datasets</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research explores the impact of rotation angle in self-supervised pre-training (SSP) on learned
                    features, demonstrating that the relationship between rotation angle and feature learning is
                    dataset-dependent and not always monotonic. This differs from previous work by focusing on the
                    interplay between rotation angle and dataset characteristics, rather than simply exploring the
                    effectiveness of SSP in general.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet105">
            <div class="start-time-icon">48:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05132" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05132</a>
                    <span class="tweet-title">Karma's Got Your Back: A Fairer Way to Price Roads</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research proposes a non-monetary system called "Karma" to address equity issues in congestion
                    pricing, where lower-income groups are disproportionately affected by tolls. Unlike previous work on
                    Karma, this paper provides a formal game-theoretic model and a software framework for simulating
                    Karma economies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet106">
            <div class="start-time-icon">49:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04889" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04889</a>
                    <span class="tweet-title">Outsmarting the Learning Curve: How to Game the System When Others Are
                        Learning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research explores how an agent can maximize its utility in a multi-agent environment by
                    anticipating the behavior of other agents who are using online learning algorithms. It differs from
                    previous work by focusing on the optimizer's ability to plan ahead and exploit the learner's
                    sub-optimal strategies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet107">
            <div class="start-time-icon">49:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06189" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06189</a>
                    <span class="tweet-title">Self-Training Video Models: From Captions to Diving Scores!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This paper introduces Video-STaR, a self-training method for Large Vision-Language Models (LVLMs)
                    that utilizes any labeled video dataset for instruction tuning. Unlike previous work that relies on
                    generating question-answer pairs from video captions, Video-STaR leverages existing video labels as
                    weak supervision, enabling the use of diverse datasets for training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet108">
            <div class="start-time-icon">50:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06056" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06056</a>
                    <span class="tweet-title">Robots Learn to Avoid "Stranger Danger" Pedestrians</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research focuses on improving the ability of reinforcement learning (RL) based social robot
                    navigation policies to handle unpredictable pedestrian behavior, a common problem in real-world
                    scenarios. The authors propose modifications to the training process, model architecture, and reward
                    function to enable the robot to identify and respond to unpredictable pedestrians.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet109">
            <div class="start-time-icon">50:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06098" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06098</a>
                    <span class="tweet-title">Unmasking Injustice: How Words Can Reveal Hidden Bias in Text</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Illinois at Chicago, IBM</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel framework that combines three NLP models to automatically detect
                    testimonial, character, and framing injustices in text. Unlike previous work, this framework focuses
                    on identifying epistemological biases, which are subtle word choices that can erode or assert a
                    person's credibility as a knower.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet110">
            <div class="start-time-icon">51:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05079" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05079</a>
                    <span class="tweet-title">Building Blocks: A Playful Tool to Sculpt Architectural Dreams in Latent
                        Space</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research presents a system called Form Forge that allows users to directly manipulate
                    individual latent variables within a StyleGAN model, offering a more granular approach to exploring
                    the latent space of architectural forms compared to previous methods that often rely on projected
                    navigation landmarks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet111">
            <div class="start-time-icon">51:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04926" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04926</a>
                    <span class="tweet-title">LiDAR Tracking: Attention, Please! New Method Outperforms the
                        Crowd!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research proposes two novel methods to improve LiDAR-based tracking-by-attention (TBA) methods:
                    track sampling augmentation and confidence-based query propagation. These methods address the
                    performance gap between TBA and tracking-by-detection (TBD) methods, which have traditionally been
                    more successful in LiDAR-based tracking.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet112">
            <div class="start-time-icon">51:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04920" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04920</a>
                    <span class="tweet-title">Big Data, Small Memory? qlty Makes Deep Learning in Scientific Imaging a
                        Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Lawrence Berkeley National Laboratory</span>
                </div>
                <div class="primary-text">
                    This paper introduces qlty, a toolkit designed to handle large volumetric datasets in scientific
                    imaging by using tensor management techniques. qlty offers methods for subsampling, cleaning, and
                    stitching large-scale spatial data, enabling effective training and inference even in
                    resource-limited environments. This approach differs from previous work by focusing on the specific
                    challenges of handling large datasets in scientific imaging, particularly in segmentation tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet113">
            <div class="start-time-icon">52:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.04885" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.04885</a>
                    <span class="tweet-title">AI-Powered Venture Capital: Can LLMs Spot the Next Unicorn?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, Vela Partners</span>
                </div>
                <div class="primary-text">
                    This research explores the use of large language models (LLMs) to predict startup success based on
                    founder characteristics, going beyond traditional features and leveraging LLM prompting techniques
                    to extract insights from limited data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet114">
            <div class="start-time-icon">52:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06015" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06015</a>
                    <span class="tweet-title">Gene Perturbation Simulation: A New Tool for Causal Structure
                        Learning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research introduces CausalRegNet, a new simulation framework for generating realistic gene
                    perturbation data. Unlike previous methods, CausalRegNet incorporates context-specific properties
                    and scales well to large datasets, making it suitable for benchmarking causal structure learning
                    algorithms in real-world settings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet115">
            <div class="start-time-icon">52:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05467" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05467</a>
                    <span class="tweet-title">IBM's AI Supercomputers: Vela and Blue Vela - A Cloud and On-Premise
                        Powerhouse!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research paper details the design and implementation of IBM's Vela and Blue Vela AI
                    infrastructure, providing a detailed technical overview of the hardware and software components used
                    to support large-scale AI model training. Unlike previous work that often focuses on model
                    architectures or training algorithms, this paper delves into the infrastructure aspects, offering
                    insights into the challenges and solutions for building and operating high-performance AI systems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet116">
            <div class="start-time-icon">53:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.05919" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.05919</a>
                    <span class="tweet-title">Trusting AI? Let's Play a Game!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Workday</span>
                </div>
                <div class="primary-text">
                    This research proposes a quantitative framework for measuring trust in AI and ML systems, going
                    beyond qualitative principles and focusing on the actual operation of these systems. It introduces a
                    trust game model to analyze the dynamic between providers and users, quantifying trust through
                    multiple iterations and defining a minimum trust threshold.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet117">
            <div class="start-time-icon">53:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06157" target="_blank"
                        style="text-decoration: none;">@arXiv 2407.06157</a>
                    <span class="tweet-title">LLMs Time Travel: New AI Can Pinpoint Actions in Videos</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research explores the use of multimodal large language models (LLMs) for temporal activity
                    localization, a task that involves identifying the specific time intervals of actions within a
                    video. Unlike previous methods that rely on specialized architectures and task-specific training,
                    this approach leverages the broad knowledge base and reasoning capabilities of LLMs.
                </div>
            </div>
        </div>
    </div>

    <footer class="player-footer">
        <div id="controls">
            <button id="controllerButton" onclick="scrollToCurrentTweet()">
                <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
            </button>
            <button id="controllerButton" onclick="togglePlayPause()">
                <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                <div id="progressTime">0:00</div>
            </button>
        </div>
        <div id="progressContainer" onclick="seek(event)">
            <div id="progressBar"></div>
            <div id="progressCircle" draggable="true"></div>
        </div>
        <audio id="audioPlayer" src="assets/audio.mp3"></audio>
    </footer>

    <script>
        var audio = document.getElementById('audioPlayer');
        var controllerButton = document.getElementById('controllerButton');
        var playPauseImage = document.getElementById('playPauseImage');
        var progressBar = document.getElementById('progressBar');
        var progressCircle = document.getElementById('progressCircle');
        var progressTime = document.getElementById('progressTime');
        var isDragging = false;

        function scrollToCurrentTweet() {
            // This assumes `start-time-icon` divs have text in format "MM:SS"
            const tweets = document.querySelectorAll('.tweet');
            const currentTime = audio.currentTime;

            console.log("NOODLE", currentTime);

            let targetTweet = null;
            let maxStartTime = -1;

            tweets.forEach((tweet, index) => {
                const timeString = tweet.querySelector('.start-time-icon').textContent;
                const parts = timeString.split(':');

                tweetTime = (parseInt(parts[0]) * 60 + parseInt(parts[1]))
                if (parts.length > 2) {
                    tweetTime = tweetTime * 60 + parseInt(parts[2]) // convert MM:SS to seconds
                }
                if (tweetTime <= currentTime && tweetTime > maxStartTime) {
                    maxStartTime = tweetTime;
                    targetTweet = tweet;
                }
            });

            // If no tweet found that meets the condition, scroll to the top tweet
            if (!targetTweet && tweets.length > 0) {
                targetTweet = tweets[0];
            }

            if (targetTweet) {
                targetTweet.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }

        function togglePlayPause() {
            if (audio.paused) {
                audio.play();
                playPauseImage.src = 'assets/buttonPause.svg';
            } else {
                audio.pause();
                playPauseImage.src = 'assets/buttonPlay.svg';
            }
        }

        audio.addEventListener('timeupdate', function () {
            if (!isDragging) {
                var progress = (audio.currentTime / audio.duration) * 100;
                progressBar.style.width = progress + '%';
                progressCircle.style.left = progress + '%'; // Corrected reference
                updateProgressTime(audio.currentTime);
            }
        });

        audio.addEventListener('ended', function () {
            playPauseImage.src = 'assets/buttonPlay.svg';
            progressBar.style.width = '0%';
            progressCircle.style.left = '0%';
            updateProgressTime(0);
        });

        // Mouse events
        progressCircle.addEventListener('mousedown', function (event) {
            isDragging = true;
            document.addEventListener('mousemove', onMouseMove);
            document.addEventListener('mouseup', onMouseUp);
        });

        // Touch events
        progressCircle.addEventListener('touchstart', function (event) {
            isDragging = true;
            document.addEventListener('touchmove', onTouchMove);
            document.addEventListener('touchend', onTouchEnd);
        });

        function onMouseMove(event) {
            seek(event.clientX);
        }

        function onTouchMove(event) {
            var touch = event.touches[0];
            seek(touch.clientX);
        }

        function onMouseUp() {
            isDragging = false;
            document.removeEventListener('mousemove', onMouseMove);
            document.removeEventListener('mouseup', onMouseUp);
        }

        function onTouchEnd() {
            isDragging = false;
            document.removeEventListener('touchmove', onTouchMove);
            document.removeEventListener('touchend', onTouchEnd);
        }

        function onMouseUp() {
            isDragging = false;
            document.removeEventListener('mousemove', onMouseMove);
            document.removeEventListener('mouseup', onMouseUp);
        }

        progressCircle.addEventListener('dragstart', function (event) {
            event.preventDefault();
        });

        function updateProgressTime(currentTime) {
            var minutes = Math.floor(currentTime / 60);
            var seconds = Math.floor(currentTime % 60);
            if (seconds < 10) {
                seconds = '0' + seconds;
            }
            progressTime.textContent = minutes + ':' + seconds;
        }

        function seek(event) {
            var containerRect = progressContainer.getBoundingClientRect();
            var newTime = ((event.clientX - containerRect.left) / containerRect.width) * audio.duration;
            audio.currentTime = newTime;
            var progress = (audio.currentTime / audio.duration) * 100;
            progressBar.style.width = progress + '%';
            progressCircle.style.left = progress + '%'; // Ensure this reference is consistent
            updateProgressTime(newTime);
        }
    </script>

</body>

</html>