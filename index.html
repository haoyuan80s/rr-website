
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Roboto:wght@300;400;500;700&display=swap">
</head>

<body>
    <div class="container">
        <div class="header">
            <div style="height: 100px;"></div>
            <img src="assets/AppLaunchFrog.gif" alt="Descriptive text about the GIF" style="width: 20%; height: auto;">
            <div style="height: 30px;"></div>
            <div class=" notification-text">Hop right over soon... We're busy getting everything just right and will be
                here
                before you can say 'Ribbit'!
            </div>
            <div style="height: 100px;"></div>
            <div class="header-text">ArXiv AI Papers - Daily Highlights</div>
            <div class="header-text">Thu. Jul 18, 2024</div>
            <div class="subheader-text">Opening music from <a href="https://ikson.com" target="_blank"
                    style="color: black; text-decoration: none;">TELL YOUR STORY by ikson</a></div>
            <div style="height: 30px;"></div>
        </div>
        <div class="tweet" id="tweet0">
            <div class="start-time-icon">00:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12306" target="_blank">@arXiv 2407.12306</a>
                    <span class="tweet-title">Splatfacto-W:  Nerf's New Trick for Making Wild Images Look Real-Time Awesome!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    Splatfacto-W adapts 3D Gaussian Splatting (3DGS) for handling unconstrained image collections, unlike previous methods that relied on 2D models or implicit color prediction, which slowed down rendering.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon">01:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12783" target="_blank">@arXiv 2407.12783</a>
                    <span class="tweet-title">SMooDi:  Text-to-Motion with a Dash of Style!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Northeastern University, Google</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for generating stylized motion sequences using a pre-trained text-to-motion model. Unlike previous methods that either generate motion of various content or transfer style from one sequence to another, SMooDi can rapidly generate motion across a broad range of content and diverse styles.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon">01:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12061" target="_blank">@arXiv 2407.12061</a>
                    <span class="tweet-title">Robots Need Context: New Study Shows AI Struggles with Ambiguous Instructions</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, Meta</span>
                </div>
                <div class="primary-text">
                    This research introduces Situated Instruction Following (SIF), a new benchmark for evaluating embodied AI agents' ability to understand and act upon instructions given in real-world contexts. Unlike previous benchmarks that focus on abstract or overly detailed instructions, SIF emphasizes the inherent ambiguity and evolving nature of human communication.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon">02:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12108" target="_blank">@arXiv 2407.12108</a>
                    <span class="tweet-title">Private Prediction:  LLMs Get a Privacy Makeover, Generating Thousands of Synthetic Texts!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces a new private prediction method for generating synthetic text using large language models (LLMs). Unlike previous methods that focused on making the model itself private, this approach only requires the output synthetic data to be differentially private. This allows for the generation of thousands of high-quality synthetic data points, significantly expanding the potential applications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon">02:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12771" target="_blank">@arXiv 2407.12771</a>
                    <span class="tweet-title">Hashtag Hype: Network and Identity Fuel Twitter's Cultural Explosion</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Michigan</span>
                </div>
                <div class="primary-text">
                    This research goes beyond studying the impact of just network or identity on hashtag diffusion. It investigates how these two factors work together to influence the spread of hashtags on Twitter.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon">02:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12751" target="_blank">@arXiv 2407.12751</a>
                    <span class="tweet-title">Scaling Up Bayesian Learning: A Monte Carlo Marathon</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework for stochastic gradient MCMC algorithms, which are particularly useful for Bayesian learning in large-scale datasets. The key innovation lies in the use of control variates to reduce the variance of the gradient estimator, leading to more efficient and scalable inference. This approach differs from previous work by explicitly addressing the challenges of high dimensionality and computational complexity in Bayesian models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon">03:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12164" target="_blank">@arXiv 2407.12164</a>
                    <span class="tweet-title">Subject-Driven Image Generation:  A Preference-Based Approach to Fine-Tuning Diffusion Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google, University of Waterloo</span>
                </div>
                <div class="primary-text">
                    This research introduces a new reward function called the λ-Harmonic reward function, which enables early stopping during training and accelerates the fine-tuning process for subject-driven text-to-image generation. Unlike previous methods that rely on extensive negative samples or complex text-embedding optimization, this approach leverages preference labels derived from the reward function to achieve text-image alignment by fine-tuning only the UNet component.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon">03:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12679" target="_blank">@arXiv 2407.12679</a>
                    <span class="tweet-title">Goldfish Swims Through Long Videos, Leaving Other Models in the Dust!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">King Abdullah University of Science and Technology, Harvard University, The Swiss AI Lab IDSIA</span>
                </div>
                <div class="primary-text">
                    This research introduces Goldfish, a framework for understanding arbitrarily long videos. Unlike previous methods that struggle with lengthy content, Goldfish uses a retrieval mechanism to identify the most relevant video clips before answering questions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon">04:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12034" target="_blank">@arXiv 2407.12034</a>
                    <span class="tweet-title">Transformers: Not Just Fancy N-Grams, But Maybe They Should Be!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This paper investigates how well transformer-based language models (LLMs) can be approximated by simple N-gram rules, which are statistical patterns derived from the training data. Unlike previous work that focuses on individual neurons or in-context learning, this study examines the overall model behavior as a black box.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon">04:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12288" target="_blank">@arXiv 2407.12288</a>
                    <span class="tweet-title">Machine Learning's New Secret Weapon: Information Theory!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research proposes a theoretical framework for machine learning that leverages Bayesian statistics and information theory to characterize the performance of an optimal Bayesian learner. Unlike existing analyses that weaken with increasing data complexity, this framework provides accurate insights across diverse machine learning settings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon">04:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12185" target="_blank">@arXiv 2407.12185</a>
                    <span class="tweet-title">Deep Learning Goes on a Diet: Satisficing Exploration for Smarter AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Google</span>
                </div>
                <div class="primary-text">
                    This paper extends the concept of "satisficing" exploration, previously explored in multi-armed bandit problems, to deep reinforcement learning. It introduces a new algorithm, Blahut-Arimoto RVF, that allows agents to learn near-optimal solutions without needing to explore the entire state space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon">05:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12178" target="_blank">@arXiv 2407.12178</a>
                    <span class="tweet-title">Stop Exploiting, Start Exploring: Why AI Needs to Embrace Perpetual Curiosity</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research explores a new type of decision-making problem where optimal behavior requires continuous exploration, even as the agent gains knowledge. This contrasts with traditional models where exploration eventually tapers off in favor of exploitation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon">05:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12393" target="_blank">@arXiv 2407.12393</a>
                    <span class="tweet-title">LLMs Get a Personality Makeover:  From Bland Bots to Characterful Chatters</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new training approach called PersLLM that integrates psychology-grounded principles of personality into LLMs, aiming to create more realistic and consistent personalities compared to previous methods that focused on superficial linguistic styles.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon">06:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12516" target="_blank">@arXiv 2407.12516</a>
                    <span class="tweet-title">Spiking Neural Networks Get a Zeroth-Order Makeover: Training Without Backpropagation!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new method called Online Pseudo-Zeroth-Order (OPZO) training for spiking neural networks (SNNs). Unlike traditional methods that rely on backpropagation, OPZO uses a single forward pass with noise injection and direct top-down feedback signals for training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon">06:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12505" target="_blank">@arXiv 2407.12505</a>
                    <span class="tweet-title">Reinforcement Learning Gets a Gravity Makeover:  New Framework Makes Multi-Agent Systems Smarter!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework called Subequivariant Hierarchical Neural Networks (SHNN) for multi-entity reinforcement learning in 3D environments. SHNN leverages the concept of subequivariance, which is a relaxed form of equivariance that accounts for gravitational effects, to reduce the complexity of the state space. This is different from previous work that either used hand-crafted local reference frames or did not consider the effects of gravity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon">07:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12276" target="_blank">@arXiv 2407.12276</a>
                    <span class="tweet-title">Visual Context Makes CLIP See Anomalies Better!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Chinese Academy of Sciences, University of Technology Sydney, Hangzhou Dianzi University...</span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel visual context prompting model (VCP-CLIP) for zero-shot anomaly segmentation (ZSAS). Unlike previous methods that rely on product-specific text prompts, VCP-CLIP utilizes visual context to activate CLIP's anomalous semantic perception ability. It introduces two modules: Pre-VCP and Post-VCP, which incorporate global and fine-grained image features into the text prompts, respectively.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon">07:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12259" target="_blank">@arXiv 2407.12259</a>
                    <span class="tweet-title">LLMs are Secretly Doing Gradient Descent: In-Context Probing as a Data Valuation Shortcut</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This paper explores the connection between in-context probing (ICP) and influence functions, two methods for data valuation. It proposes that ICP implicitly performs gradient descent, making it a cost-effective proxy for influence functions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon">08:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12176" target="_blank">@arXiv 2407.12176</a>
                    <span class="tweet-title">GPT-4V: Can't Even Write a Radiology Report Yet!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Chicago, University of Michigan</span>
                </div>
                <div class="primary-text">
                    This research systematically evaluates GPT-4V's ability to generate radiology reports, going beyond case studies and qualitative analysis. It decomposes the task into image reasoning and report synthesis, revealing GPT-4V's struggles with interpreting chest X-rays.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon">08:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12370" target="_blank">@arXiv 2407.12370</a>
                    <span class="tweet-title">Time Travel for Graphs: How Much Past Matters for Predicting the Future?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Conservatoire National des Arts et Métiers, Sorbonne University, National Institute of Applied Sciences of Rouen...</span>
                </div>
                <div class="primary-text">
                    This research focuses on the "temporal receptive field" in dynamic graph learning, which refers to the amount of past data a model considers when making predictions. Unlike previous work that primarily focused on optimizing the temporal encoding process, this study systematically analyzes the impact of different temporal receptive field sizes on model performance across various datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon">09:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12312" target="_blank">@arXiv 2407.12312</a>
                    <span class="tweet-title">Skeleton Mix-Up:  Shapley Value Makes Action Recognition Smarter!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces Shap-Mix, a novel method for long-tailed skeleton-based action recognition. Unlike previous methods, Shap-Mix utilizes Shapley value to estimate the saliency of different body parts, guiding the mixing of data samples to improve the model's ability to learn from scarce data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon">09:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12207" target="_blank">@arXiv 2407.12207</a>
                    <span class="tweet-title">6D Object Pose Estimation: No CAD Models, Just a Few Snapshots!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, The University of Queensland</span>
                </div>
                <div class="primary-text">
                    This research proposes a pipeline for 6D object pose estimation that doesn't require CAD models, unlike many existing methods. Instead, it uses a neural implicit surface representation (NeuS2) trained on a small set of real images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon">10:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12318" target="_blank">@arXiv 2407.12318</a>
                    <span class="tweet-title">Dynamic Games:  Information Compression for Smarter Strategies</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Michigan</span>
                </div>
                <div class="primary-text">
                    This paper introduces two new notions of information states, Mutually Suﬃcient Information (MSI) and Unilaterally Suﬃcient Information (USI), for dynamic games with asymmetric information. These concepts are based on strategy-independent compression maps, which differ from previous work that often relies on strategy-dependent maps.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon">10:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12307" target="_blank">@arXiv 2407.12307</a>
                    <span class="tweet-title">Hand Reconstruction:  Knowledge is Power, Uncertainty is Key!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Rensselaer, IBM</span>
                </div>
                <div class="primary-text">
                    This research introduces a weakly-supervised method for 3D hand reconstruction that leverages hand knowledge from biomechanics, functional anatomy, and physics. Unlike previous methods that rely solely on data-driven priors or heuristic constraints, this approach systematically incorporates these foundational insights into the training process. Additionally, the paper explicitly models the uncertainty inherent in image observations, improving the robustness of the model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon">10:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12277" target="_blank">@arXiv 2407.12277</a>
                    <span class="tweet-title">Visual Question Answering Gets a Multimodal Makeover: Reranking for the Win!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, Google, University of Massachusetts</span>
                </div>
                <div class="primary-text">
                    This research introduces a multi-modal reranker module to improve the ranking of knowledge candidates in knowledge-intensive visual question answering (KI-VQA) systems. Unlike previous work that relies on uni-modal retrieval, this approach leverages both visual and textual information from the question and knowledge candidates for more accurate relevance score modeling.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon">11:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12622" target="_blank">@arXiv 2407.12622</a>
                    <span class="tweet-title">GEBD Models:  Faster Than a Speeding Bullet, More Accurate Than a Laser Beam!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Xi’an Jiaotong University, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research focuses on improving the efficiency of Generic Event Boundary Detection (GEBD) models by re-examining their architecture. The authors propose a new baseline model, BasicGEBD, which achieves comparable performance to more complex models. They then systematically "modernize" each component of BasicGEBD, resulting in a family of EfficientGEBD models that achieve state-of-the-art performance with significantly faster inference speeds.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon">11:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12491" target="_blank">@arXiv 2407.12491</a>
                    <span class="tweet-title">Building a BEV Perception System: Drag-and-Drop Your Way to Self-Driving!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a hierarchical perception system for autonomous driving, which uses a library of pre-trained modules that can be combined and customized to create different perception models. This approach differs from previous work by emphasizing modularity and reusability, streamlining the development process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon">12:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12773" target="_blank">@arXiv 2407.12773</a>
                    <span class="tweet-title">AI Detects Cancer Mitosis:  A Deep Learning Framework That's Got Your Back (and Your Cells)!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London, Royal National Orthopaedic Hospital, University Hospital Basel...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel two-stage framework for detecting mitotic figures in cancer cells. Unlike previous methods that rely solely on bounding boxes, this approach incorporates nuclei contours, which significantly improves the accuracy of detection.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon">12:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12404" target="_blank">@arXiv 2407.12404</a>
                    <span class="tweet-title">Steering Language Models: A Wild Ride with Unreliable Vectors</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research delves into the reliability and generalizability of steering vectors, a technique for adjusting language model behavior at inference time. Unlike previous work that primarily focused on in-distribution performance, this study investigates both in-distribution reliability and out-of-distribution generalization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon">12:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12784" target="_blank">@arXiv 2407.12784</a>
                    <span class="tweet-title">LLM Agents on Trial:  How a Tiny Trigger Can Hijack Your AI Assistant</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Chicago</span>
                </div>
                <div class="primary-text">
                    This research focuses on backdoor attacks against LLM agents that use retrieval-augmented generation (RAG) systems. Unlike previous work that targeted LLMs or RAG systems individually, this paper proposes a novel attack, AGENTPOISON, that specifically targets the memory or knowledge base of these agents.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon">13:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12399" target="_blank">@arXiv 2407.12399</a>
                    <span class="tweet-title">Simplifying Data's Topology: A Practical Solver for a Knotty Problem</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sorbonne University, University of Arizona</span>
                </div>
                <div class="primary-text">
                    This research introduces a practical solver for topological simplification, focusing on optimizing the cancellation of "non-signal" persistence pairs while preserving "signal" pairs. Unlike previous methods, this approach is not restricted to persistence pairs involving extrema, allowing it to address a broader class of topological features, particularly saddle pairs in three-dimensional scalar data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon">13:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12229" target="_blank">@arXiv 2407.12229</a>
                    <span class="tweet-title">Laugh Now, Cry Later: AI Makes Speech Sound More Human</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National Taiwan University, Microsoft Corporation</span>
                </div>
                <div class="primary-text">
                    This research introduces EmoCtrl-TTS, a text-to-speech model that can generate speech with varying emotions and non-verbal vocalizations (NVs) like laughter and crying. Unlike previous models that focused on controlling emotions at the utterance level, EmoCtrl-TTS can mimic the emotional changes within a single utterance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon">14:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12128" target="_blank">@arXiv 2407.12128</a>
                    <span class="tweet-title">TTA's New Trick: Aligning Data to Save the Day!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Concordia University, University of Toronto, Beijing Jiaotong University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel Distribution Alignment (DA) loss for Test-Time Adaptation (TTA) that aligns test-time feature distributions with the source distributions, addressing the challenges posed by label shifts across online data batches. Unlike previous methods that adapt the model to the test data, this approach aligns the test data to the source model, ensuring compatibility and preventing degradation from conflicting optimization objectives.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon">14:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12718" target="_blank">@arXiv 2407.12718</a>
                    <span class="tweet-title">Slim Down Your Diffusion Models: One-Step Generation with Rectified Flow</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, University of Texas at Austin</span>
                </div>
                <div class="primary-text">
                    This research focuses on training smaller, more efficient one-step diffusion models by combining model size reduction with the rectified flow framework. Unlike previous work, it addresses the challenges of initialization mismatch and underperformance during distillation for smaller models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon">15:12</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12435" target="_blank">@arXiv 2407.12435</a>
                    <span class="tweet-title">Human-Object Interactions:  A Fine-Grained Look at the Little Things</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The Chinese University of Hong Kong  State Key Laboratory of General Artificial Intelligence  BIGAI  Institute for AI  Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset called Semantic-HOI, which provides fine-grained descriptions of human-object interactions at the state level, unlike previous datasets that only offer coarse-grained descriptions of the entire interaction process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon">15:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12680" target="_blank">@arXiv 2407.12680</a>
                    <span class="tweet-title">AI Detectives: Busting Bias in Medical Textbooks!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Connecticut, Worcester Polytechnic Institute, University of Oxford...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel dataset, BRICC, specifically designed to identify and annotate instances of bias in medical educational materials. It then uses this dataset to train AI models for detecting bias in medical text, going beyond previous work that focused on general bias detection in other domains.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon">16:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12543" target="_blank">@arXiv 2407.12543</a>
                    <span class="tweet-title">Models Think Like Us? New Test Checks If AI's Got the Right Brain Wiring!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces "abstraction alignment," a method to assess how well a machine learning model's understanding of concepts aligns with human understanding. Unlike previous work that analyzes concepts in isolation, this method examines the relationships between concepts, revealing how the model structures its knowledge.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon">16:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12275" target="_blank">@arXiv 2407.12275</a>
                    <span class="tweet-title">Transformers Can't Compose? Bottleneck to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, Google, University of Montreal</span>
                </div>
                <div class="primary-text">
                    This research investigates the ability of transformers to generalize compositionally in an in-context learning setting. Unlike previous work that focused on gradient-based meta-learning, this study explores the limitations of transformers in this context and proposes a novel architectural solution.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon">17:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12322" target="_blank">@arXiv 2407.12322</a>
                    <span class="tweet-title">Skeleton Action Recognition:  When Transformers Get a Frequency Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, Microsoft, University of North Carolina at Charlotte...</span>
                </div>
                <div class="primary-text">
                    This research introduces a Frequency-aware Mixed Transformer (FreqMixFormer) for skeleton action recognition. Unlike previous transformer-based approaches that rely solely on spatial features, FreqMixFormer incorporates frequency features, enabling it to better distinguish subtle movements.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon">17:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12197" target="_blank">@arXiv 2407.12197</a>
                    <span class="tweet-title">Soft Robots Get a Sixth Sense:  Predicting the Future with Multi-Modal Perception</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Sant’Anna School of Advanced Studies, University College London</span>
                </div>
                <div class="primary-text">
                    This research builds upon previous work using generative models for soft robot perception by introducing modality-specific encoders and decoders, allowing for late fusion and early decoding capabilities. This approach enables a deeper understanding of the latent representation and its influence on sensory prediction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon">18:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12508" target="_blank">@arXiv 2407.12508</a>
                    <span class="tweet-title">LLMs: The New Search Party for Videos!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Korea Advanced Institute of Science and Technology, UC Berkeley, Seoul National University</span>
                </div>
                <div class="primary-text">
                    This research introduces MERLIN, a novel training-free pipeline that leverages LLMs for iterative feedback learning to refine query embeddings in text-video retrieval. Unlike previous work, MERLIN focuses on addressing the discrepancy between user queries and the content retrieved, enhancing alignment between queries and video content through a dynamic question answering process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon">18:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12117" target="_blank">@arXiv 2407.12117</a>
                    <span class="tweet-title">Training a 7B LLM with 1 Million Tokens? No Problem! Memo to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Tencent</span>
                </div>
                <div class="primary-text">
                    This research proposes Memo, a novel LLM training framework that tackles the memory challenges of long context training by introducing a fine-grained activation recomputation and swapping mechanism. This approach differs from previous work by optimizing memory usage at both the tensor and token levels, minimizing redundant computation and communication overhead.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon">18:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12391" target="_blank">@arXiv 2407.12391</a>
                    <span class="tweet-title">LLM Serving:  A  Survey  of  How  to  Make  AI  Models  Run  Faster  and  Cheaper</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Northeastern University, MIT</span>
                </div>
                <div class="primary-text">
                    This survey focuses on system-level enhancements for LLM serving, specifically those that improve performance and efficiency without altering the core LLM decoding mechanisms. It excludes studies that modify LLM decoding algorithms and focuses on research published after 2023.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon">19:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12051" target="_blank">@arXiv 2407.12051</a>
                    <span class="tweet-title">DNA's Secret Code:  Unlocking the Language of Life with Sparse Recovery</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, University of California  Berkeley</span>
                </div>
                <div class="primary-text">
                    This research proposes Dy-mer, a DNA representation scheme that leverages sparse recovery to capture recurring patterns in DNA sequences, known as K-mers, and represent them as basis vectors. This approach differs from previous methods by explicitly incorporating the semantic structure of DNA, making the representations more explainable and robust.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon">19:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12753" target="_blank">@arXiv 2407.12753</a>
                    <span class="tweet-title">Vision Transformers Get a Makeover:  LookupViT Compresses Images for Faster Inference!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google DeepMind, Ludwig Maximilian University of Munich</span>
                </div>
                <div class="primary-text">
                    LookupViT introduces a novel vision transformer block that compresses information from higher-resolution tokens to a fixed number of tokens. This approach differs from previous work by focusing on intrinsic compression within the architecture rather than post-processing techniques.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon">20:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12781" target="_blank">@arXiv 2407.12781</a>
                    <span class="tweet-title">Taming Transformers: Giving Video AI a 3D Camera Lens</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, Vector Institute, Snap Inc....</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for controlling camera movement in video generation models that are based on transformers. Unlike previous approaches that focused on U-Net architectures, this method specifically addresses the challenges of controlling camera movement in transformer-based models, which process spatial and temporal information jointly.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon">20:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.11979" target="_blank">@arXiv 2407.11979</a>
                    <span class="tweet-title">Clustering Students:  A New Way to See Who's Really Learning!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Federal University of Minas Gerais, École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research introduces a new clustering pipeline called Interpret3C that uses interpretable neural networks to select features for each student individually, rather than using a single set of features for everyone. This allows for more nuanced and accurate clustering, as it takes into account the unique learning patterns of each student.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon">21:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12281" target="_blank">@arXiv 2407.12281</a>
                    <span class="tweet-title">LLMs on a Diet: How Data Poisoning Makes Them Go Bonkers!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, IBM</span>
                </div>
                <div class="primary-text">
                    This research focuses on data poisoning attacks against large language models (LLMs) specifically for natural language generation (NLG) tasks, which have been less explored than attacks on classification tasks. The paper introduces new metrics to evaluate the success and stealthiness of these attacks and explores the effectiveness of various trigger designs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon">21:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12074" target="_blank">@arXiv 2407.12074</a>
                    <span class="tweet-title">LoRA's Secret Weapon: Unmasking the Intrinsic Dimension for Better AI Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research delves into the intrinsic dimension of LoRA updates, a factor previously overlooked, and proposes a method to enhance it for improved generalization performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon">21:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.11977" target="_blank">@arXiv 2407.11977</a>
                    <span class="tweet-title">AI Chatbots Get a Personality Makeover:  LLMs Learn to Play the Part</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge, King’s College London</span>
                </div>
                <div class="primary-text">
                    This research explores the challenges of integrating personas into Large Language Models (LLMs) used for conversational agents. Unlike previous work that focused on embedding static personality traits, this paper emphasizes the need for consistent and contextually appropriate personas across multiple interactions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon">22:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12613" target="_blank">@arXiv 2407.12613</a>
                    <span class="tweet-title">AI Helps Journalists Decode the Murmurs of the Internet</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research introduces AudienceView, a tool that uses large language models (LLMs) to help journalists analyze and understand audience feedback on YouTube. Unlike previous work that focuses on qualitative analysis, AudienceView aims to provide a more automated and accessible approach to sensemaking from textual data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon">22:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12165" target="_blank">@arXiv 2407.12165</a>
                    <span class="tweet-title">AI Agents for Autonomous Clouds:  Building a Framework for Fault-Free Computing</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft, University of California  Berkeley, University of Illinois Urbana-Champaign...</span>
                </div>
                <div class="primary-text">
                    This research proposes a standardized framework, AIOpsLab, for building, evaluating, and improving AI agents designed for cloud operations. Unlike previous work that focuses on specific solutions or uses proprietary services and datasets, AIOpsLab aims to provide a generic, reproducible, and scalable benchmark for evaluating AIOps agents.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon">23:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12777" target="_blank">@arXiv 2407.12777</a>
                    <span class="tweet-title">Human 3D Models From Just 3 Photos: A New Way to Render Reality!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, Meta Reality Labs</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for rendering 3D human models from sparse views, using a technique called "Generalizable Human Gaussians" (GHG). Unlike previous methods that rely on dense input views or per-subject optimization, GHG leverages a human template model and a 2D UV space to learn Gaussian parameters, enabling accurate and photorealistic rendering from just a few input images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon">23:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12210" target="_blank">@arXiv 2407.12210</a>
                    <span class="tweet-title">Self-Supervised Learning:  Probing for the Right Metric!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">California Institute of Technology, ETH Zurich, Swiss Data Science Center...</span>
                </div>
                <div class="primary-text">
                    This research investigates the correlation between different evaluation protocols used to assess the quality of representations learned through self-supervised learning (SSL). It compares the performance of various SSL methods across eleven datasets and analyzes how well in-domain metrics predict out-of-domain performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon">23:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12131" target="_blank">@arXiv 2407.12131</a>
                    <span class="tweet-title">Kilkari's Got a New Trick: AI Helps Moms Hear the Message!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google, Harvard University</span>
                </div>
                <div class="primary-text">
                    This research focuses on a larger-scale mHealth program, Kilkari, and utilizes non-Markovian Time-Series Bandits (TSB) to optimize interventions, unlike previous work that relied on Markovian approaches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon">24:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.11973" target="_blank">@arXiv 2407.11973</a>
                    <span class="tweet-title">AI-Powered Calls: Can They Make Moms Smarter?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google, Harvard University, CMU</span>
                </div>
                <div class="primary-text">
                    This study investigates the impact of AI-scheduled interventions on health knowledge and behavior in a maternal health program, going beyond previous work that focused solely on increased listenership.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon">24:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12707" target="_blank">@arXiv 2407.12707</a>
                    <span class="tweet-title">TTS Gets a Score: New Benchmark Measures Synthetic Speech Quality Like Never Before!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The University of Edinburgh</span>
                </div>
                <div class="primary-text">
                    This research proposes a new benchmark for evaluating Text-to-Speech (TTS) systems by measuring the distance between the distribution of real and synthetic speech. Unlike previous methods that rely on subjective human ratings or single-factor metrics, this approach considers multiple factors like prosody, speaker identity, and intelligibility.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon">25:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12389" target="_blank">@arXiv 2407.12389</a>
                    <span class="tweet-title">CHILDES Gets a Makeover: AI Makes Language Learning Research Easier!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research uses AI and machine learning to automatically analyze morphosyntactic features in child language corpora, making crosslinguistic comparisons more efficient and consistent. This approach differs from previous methods that relied on manual coding or language-specific tools.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon">25:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12287" target="_blank">@arXiv 2407.12287</a>
                    <span class="tweet-title">Human Activity Recognition:  A Privacy-Preserving Deep Dive with Deep Clustering!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research proposes CDFL, a federated learning framework for human activity recognition that addresses data heterogeneity by using deep clustering to select a representative subset of privacy-preserved images for server training. This approach differs from previous work by focusing on image selection rather than client clustering.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon">26:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12064" target="_blank">@arXiv 2407.12064</a>
                    <span class="tweet-title">X-Ray Vision:  A Language Model That Can See and Diagnose!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research proposes a unified framework for a large vision-language model that can perform both localization and classification tasks on medical images, specifically chest X-rays. This approach differs from previous work by integrating multiple visual encoders, rather than relying on a single encoder.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon">26:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12309" target="_blank">@arXiv 2407.12309</a>
                    <span class="tweet-title">MEDFuse:  Unmasking the Secrets of EHR Data with LLMs and Masked Lab Tests!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Michigan, National Yang Ming Chiao Tung University, MIT...</span>
                </div>
                <div class="primary-text">
                    This research proposes MEDFuse, a novel framework that integrates structured lab test data and unstructured clinical notes using embeddings from fine-tuned LLMs and masked lab-test modeling. This approach differs from previous work by explicitly addressing the challenge of integrating modality-specific and modality-shared information, which is crucial for accurate disease prediction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon">26:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12282" target="_blank">@arXiv 2407.12282</a>
                    <span class="tweet-title">Chip Placement with Diffusion:  A Generative Model for Placing Macros Like a Pro!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research uses a diffusion model to place macros simultaneously, unlike previous methods that rely on reinforcement learning, which is slow and sequential.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon">27:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.11974" target="_blank">@arXiv 2407.11974</a>
                    <span class="tweet-title">AI Explains Glaucoma Referrals, But Can't Beat Itself!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Johns Hopkins University, Friedrich-Alexander-Universität</span>
                </div>
                <div class="primary-text">
                    This research investigates the impact of explainable AI on optometrists' referral decisions for glaucoma patients, comparing the performance of AI models with and without explanations to a control group without AI support.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon">27:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12234" target="_blank">@arXiv 2407.12234</a>
                    <span class="tweet-title">Solving PDEs with a Meta-Model:  It's Like Recycling Your Math Homework!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Duke University, Morgan Stanley, University of Minnesota</span>
                </div>
                <div class="primary-text">
                    This research proposes a meta-learning framework for solving parabolic partial differential equations (PDEs) by reusing existing Monte Carlo samples across different parameter settings. This approach differs from previous work by leveraging importance sampling techniques to avoid the need for resampling for each new parameter set.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon">28:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12739" target="_blank">@arXiv 2407.12739</a>
                    <span class="tweet-title">Sketch Your City: AI Turns Doodles into 3D Masterpieces</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London, Niantic, University of Surrey</span>
                </div>
                <div class="primary-text">
                    This research introduces GroundUp, a novel sketch-based 3D city massing tool that leverages both top-down and perspective sketches to infer building geometries. Unlike previous methods that rely on point clouds or photogrammetry, GroundUp utilizes a diffusion model to reconstruct plausible building shapes from sparse user-provided sketches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon">28:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12687" target="_blank">@arXiv 2407.12687</a>
                    <span class="tweet-title">AI Tutor Gets a Makeover: LearnLM-Tutor Aims for Better Teaching</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research focuses on developing a generative AI tutor that is specifically fine-tuned for pedagogical tasks, unlike previous work that primarily relies on prompting.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon">29:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12690" target="_blank">@arXiv 2407.12690</a>
                    <span class="tweet-title">AI's Wild West: Taming the Tech with Innovation and Regulation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This research proposes a framework that combines technical innovation with smart regulation to address the risks associated with AI, unlike previous work that focused solely on one or the other.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon">29:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12620" target="_blank">@arXiv 2407.12620</a>
                    <span class="tweet-title">AI to the Rescue: Can Language Models Save Dying Languages?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM Research, University of São Paulo</span>
                </div>
                <div class="primary-text">
                    This research proposes a new AI development cycle specifically tailored for endangered Indigenous languages. It emphasizes community engagement and data sovereignty, prioritizing the community's needs and control over the data. This approach contrasts with traditional AI development cycles that often rely on large, publicly available datasets.
                </div>
            </div>
        </div></div>

    <footer class="player-footer">
        <div id="controls">
            <button id="controllerButton" onclick="scrollToCurrentTweet()">
                <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
            </button>
            <button id="controllerButton" onclick="togglePlayPause()">
                <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                <div id="progressTime">0:00</div>
            </button>
        </div>
        <div id="progressContainer" onclick="seek(event)">
            <div id="progressBar"></div>
            <div id="progressCircle" draggable="true"></div>
        </div>
        <audio id="audioPlayer" src="assets/audio.mp3"></audio>
    </footer>

    <script>
        var audio = document.getElementById('audioPlayer');
        var controllerButton = document.getElementById('controllerButton');
        var playPauseImage = document.getElementById('playPauseImage');
        var progressBar = document.getElementById('progressBar');
        var progressCircle = document.getElementById('progressCircle');
        var progressTime = document.getElementById('progressTime');
        var isDragging = false;

        function scrollToCurrentTweet() {
            // This assumes `start-time-icon` divs have text in format "MM:SS"
            const tweets = document.querySelectorAll('.tweet');
            const currentTime = audio.currentTime;

            console.log("NOODLE", currentTime);

            let targetTweet = null;
            let maxStartTime = -1;

            tweets.forEach((tweet, index) => {
                const timeString = tweet.querySelector('.start-time-icon').textContent;
                const parts = timeString.split(':');

                tweetTime = (parseInt(parts[0]) * 60 + parseInt(parts[1]))
                if (parts.length > 2) {
                    tweetTime = tweetTime * 60 + parseInt(parts[2]) // convert MM:SS to seconds
                }
                if (tweetTime <= currentTime && tweetTime > maxStartTime) {
                    maxStartTime = tweetTime;
                    targetTweet = tweet;
                }
            });

            // If no tweet found that meets the condition, scroll to the top tweet
            if (!targetTweet && tweets.length > 0) {
                targetTweet = tweets[0];
            }

            if (targetTweet) {
                targetTweet.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }

        function togglePlayPause() {
            if (audio.paused) {
                audio.play();
                playPauseImage.src = 'assets/buttonPause.svg';
            } else {
                audio.pause();
                playPauseImage.src = 'assets/buttonPlay.svg';
            }
        }

        audio.addEventListener('timeupdate', function () {
            if (!isDragging) {
                var progress = (audio.currentTime / audio.duration) * 100;
                progressBar.style.width = progress + '%';
                progressCircle.style.left = progress + '%'; // Corrected reference
                updateProgressTime(audio.currentTime);
            }
        });

        audio.addEventListener('ended', function () {
            playPauseImage.src = 'assets/buttonPlay.svg';
            progressBar.style.width = '0%';
            progressCircle.style.left = '0%';
            updateProgressTime(0);
        });

        // Mouse events
        progressCircle.addEventListener('mousedown', function (event) {
            isDragging = true;
            document.addEventListener('mousemove', onMouseMove);
            document.addEventListener('mouseup', onMouseUp);
        });

        // Touch events
        progressCircle.addEventListener('touchstart', function (event) {
            isDragging = true;
            document.addEventListener('touchmove', onTouchMove);
            document.addEventListener('touchend', onTouchEnd);
        });

        function onMouseMove(event) {
            seek(event.clientX);
        }

        function onTouchMove(event) {
            var touch = event.touches[0];
            seek(touch.clientX);
        }

        function onMouseUp() {
            isDragging = false;
            document.removeEventListener('mousemove', onMouseMove);
            document.removeEventListener('mouseup', onMouseUp);
        }

        function onTouchEnd() {
            isDragging = false;
            document.removeEventListener('touchmove', onTouchMove);
            document.removeEventListener('touchend', onTouchEnd);
        }

        function onMouseUp() {
            isDragging = false;
            document.removeEventListener('mousemove', onMouseMove);
            document.removeEventListener('mouseup', onMouseUp);
        }

        progressCircle.addEventListener('dragstart', function (event) {
            event.preventDefault();
        });

        function updateProgressTime(currentTime) {
            var minutes = Math.floor(currentTime / 60);
            var seconds = Math.floor(currentTime % 60);
            if (seconds < 10) {
                seconds = '0' + seconds;
            }
            progressTime.textContent = minutes + ':' + seconds;
        }

        function seek(event) {
            var containerRect = progressContainer.getBoundingClientRect();
            var newTime = ((event.clientX - containerRect.left) / containerRect.width) * audio.duration;
            audio.currentTime = newTime;
            var progress = (audio.currentTime / audio.duration) * 100;
            progressBar.style.width = progress + '%';
            progressCircle.style.left = progress + '%'; // Ensure this reference is consistent
            updateProgressTime(newTime);
        }
    </script>

</body>

</html>