
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Roboto:wght@300;400;500;700&display=swap">
</head>

<body>
    <div class="container">
        <div class="header">
            <div style="height: 100px;"></div>
            <img src="assets/AppLaunchFrog.gif" alt="Descriptive text about the GIF" style="width: 20%; height: auto;">
            <div style="height: 30px;"></div>
            <div class=" notification-text">Hop right over soon... We're busy getting everything just right and will be
                here
                before you can say 'Ribbit'!
            </div>
            <div style="height: 100px;"></div>
            <div class="header-text">ArXiv AI Papers - Daily Highlights</div>
            <div class="header-text">Mon. Jul 22, 2024</div>
            <div class="subheader-text">Opening music from <a href="https://ikson.com" target="_blank"
                    style="color: black; text-decoration: none;">TELL YOUR STORY by ikson</a></div>
            <div style="height: 30px;"></div>
        </div>
        <div class="tweet" id="tweet0">
            <div class="start-time-icon">00:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14435" target="_blank">@arXiv 2407.14435</a>
                    <span class="tweet-title">Jumpin' ReLU: Sparser, More Faithful Language Model Decompositions</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google DeepMind</span>
                </div>
                <div class="primary-text">
                    This paper introduces JumpReLUSAE, a new type of sparse autoencoder (SAE) that uses a JumpReLU activation function. This modification allows for more faithful reconstructions of language model activations at a given sparsity level compared to previous SAE architectures like Gated and TopK SAEs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon">01:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14482" target="_blank">@arXiv 2407.14482</a>
                    <span class="tweet-title">ChatQA2:  Llama3's Long-Context Leap to GPT-4 Turbo Level!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nvidia</span>
                </div>
                <div class="primary-text">
                    This research focuses on bridging the gap between open-access LLMs and proprietary models like GPT-4 Turbo in terms of long-context understanding and retrieval-augmented generation (RAG) capabilities. The authors achieve this by extending the context window of Llama3-70B-base from 8K to 128K tokens and implementing a three-stage instruction tuning process. This approach differs from previous work by focusing on real-world long-context understanding tasks and combining long-context LLMs with a state-of-the-art long-context retriever for RAG.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon">01:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14309" target="_blank">@arXiv 2407.14309</a>
                    <span class="tweet-title">Asking the Right Questions: How AI Can Make Reading More Engaging</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research focuses on the use of "guiding questions" within written text, specifically those explicitly posed by the author to engage readers. Unlike previous work on Question Under Discussion (QUD) frameworks, which analyze implicit questions to understand discourse relationships, this study examines the impact of explicit questions on human reading comprehension.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon">02:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14044" target="_blank">@arXiv 2407.14044</a>
                    <span class="tweet-title">Code Optimization: Can We Have Our Cake and Eat It Too?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces ECCO, a benchmark for evaluating code efficiency that focuses on maintaining functional correctness while optimizing for runtime and memory usage. Unlike previous benchmarks, ECCO uses a cloud-hosted code execution engine to ensure reliable and reproducible results across different hardware specifications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon">02:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14474" target="_blank">@arXiv 2407.14474</a>
                    <span class="tweet-title">Radiology Reports:  "What If"  We Swap Patches?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, University of Technology Sydney, Mohamed bin Zayed University of Artificial Intelligence</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to radiology report generation by incorporating counterfactual explanations. Unlike previous methods that rely on direct captioning or incorporating medical knowledge, this framework learns non-spurious visual representations by contrasting representations between factual and counterfactual images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon">03:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13998" target="_blank">@arXiv 2407.13998</a>
                    <span class="tweet-title">RAG-QA Arena: Where LLMs Go to Battle for the Best Answer</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google, UC Santa Barbara, Amazon</span>
                </div>
                <div class="primary-text">
                    This research introduces LFRQA, a new dataset for evaluating long-form question answering systems. Unlike previous datasets that focus on short, extractive answers, LFRQA provides human-written, coherent long-form answers that integrate information from multiple documents.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon">03:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14095" target="_blank">@arXiv 2407.14095</a>
                    <span class="tweet-title">Playing Games Without Playing: How Our Brains Simulate Fun</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research explores how people quickly evaluate novel games without extensive experience, focusing on the mental simulations they use rather than optimal gameplay. It contrasts with previous work that emphasizes expert-level play and extensive search.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon">04:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14192" target="_blank">@arXiv 2407.14192</a>
                    <span class="tweet-title">Legal AI Gets a Knowledge Update:  LeKUBE Benchmarks the Latest Legal LLMs</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces LeKUBE, a benchmark specifically designed to evaluate knowledge update methods for legal LLMs. Unlike existing benchmarks that focus on general domains, LeKUBE addresses the unique challenges of updating legal knowledge, such as the nuanced application of new legal knowledge and the complexity of legal regulations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon">04:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13930" target="_blank">@arXiv 2407.13930</a>
                    <span class="tweet-title">Radar-Vision Fusion:  A 4D Tensor Takes on Human Pose Estimation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National Cheng Kung University, University of Washington</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset, RT-Pose, which includes calibrated 4D radar tensors, LiDAR point clouds, and RGB images. This multi-modal approach is unique because it leverages the raw 4D radar tensor, preserving more information than traditional point cloud methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon">05:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13937" target="_blank">@arXiv 2407.13937</a>
                    <span class="tweet-title">Seeing Double: How Cameras and Radars Team Up to Track Objects in 3D</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, Cisco Systems</span>
                </div>
                <div class="primary-text">
                    This research focuses on enhancing 3D object tracking by fusing data from cameras and radars at the tracking stage, rather than earlier in the detection process. This approach allows for more robust and accurate tracking, especially in challenging conditions like bad weather.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon">05:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14094" target="_blank">@arXiv 2407.14094</a>
                    <span class="tweet-title">Recommender Systems: When Algorithms Become Echo Chambers</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, ByteDance</span>
                </div>
                <div class="primary-text">
                    This research introduces a new model called "user-creator feature dynamics" to capture the dual influence of recommender systems on both users and creators. Unlike previous work that focused on either user or creator influence, this study considers both effects simultaneously.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon">06:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13943" target="_blank">@arXiv 2407.13943</a>
                    <span class="tweet-title">LLMs Play Werewolf:  A New Arena for AI Social Skills</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces Werewolf Arena, a novel framework for evaluating LLMs through the lens of the classic social deduction game, Werewolf. Unlike previous work that focused on improving agent performance, this study uses Werewolf as a proving ground to evaluate the relative skills of LLMs by having them play against each other.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon">06:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14346" target="_blank">@arXiv 2407.14346</a>
                    <span class="tweet-title">Search Engines Got a New Trick Up Their Sleeve: Context is King!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach called "Augmented Unity" that leverages query context signals from web search results and large language models to improve keyword retrieval in sponsored search. This differs from previous work by incorporating a dynamic cache to store and retrieve contextual information, enhancing query understanding, particularly for short and ambiguous queries.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon">06:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14430" target="_blank">@arXiv 2407.14430</a>
                    <span class="tweet-title">Deep Learning Models That Can Actually Think Outside the Box!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley, VinUniversity</span>
                </div>
                <div class="primary-text">
                    This research explores the extrapolation capabilities of implicit deep learning models, which are distinguished by their adaptability in layer depth and incorporation of feedback within their computational graph. Unlike traditional feed-forward models, implicit models allow information to propagate both forwardly and backwardly through closed-form feedback loops.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon">07:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14402" target="_blank">@arXiv 2407.14402</a>
                    <span class="tweet-title">LLMs: The New Autopilots for Your Cloud?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanjing University, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research explores the feasibility of using LLMs to create self-managing microservice systems, a concept known as Autonomic Computing. Unlike previous approaches that relied on rule-based systems, this study proposes a hierarchical multi-agent framework where LLMs handle tasks like monitoring, analysis, and issue mitigation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon">07:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14111" target="_blank">@arXiv 2407.14111</a>
                    <span class="tweet-title">Byzantine Workers, Be Gone! New Algorithm Makes Distributed Learning Corruption-Proof</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National University of Singapore</span>
                </div>
                <div class="primary-text">
                    This research focuses on the impact of adversarial corruptions on distributed gradient descent algorithms, a problem that has received limited attention in previous work. The paper proposes a novel algorithm, RDGD, that utilizes a modified lazy mirror descent approach to mitigate the effects of these corruptions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon">07:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13976" target="_blank">@arXiv 2407.13976</a>
                    <span class="tweet-title">PlacidDreamer:  Text-to-3D Harmony, No More Color Wars!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Kuaishou Technology</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new framework called PlacidDreamer that addresses two limitations in text-to-3D generation: conflicting optimization directions and over-saturation in score distillation. It achieves this by harmonizing initialization, multi-view generation, and text-conditioned generation with a single multi-view diffusion model and a novel score distillation algorithm.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon">08:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14121" target="_blank">@arXiv 2407.14121</a>
                    <span class="tweet-title">Seismic Fault Detection:  SAM's Got Your Back (and Your Subsurface)!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research adapts the Segment Anything Model (SAM), a pre-trained computer vision model, for seismic fault detection. Unlike previous methods that rely heavily on labeled seismic data, this approach leverages SAM's knowledge from natural images, requiring less training data and achieving faster convergence.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon">08:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14106" target="_blank">@arXiv 2407.14106</a>
                    <span class="tweet-title">Graph Transformers Go Big: TORCHGT Tames the Beast of Large-Scale Graphs!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanyang Technological University, Zhejiang University, Shanghai AI Laboratory...</span>
                </div>
                <div class="primary-text">
                    This research introduces TORCHGT, a system specifically designed for training graph transformers on large-scale graphs. Unlike previous work that struggled with the computational demands of long sequences, TORCHGT leverages graph sparsity and clustering to optimize training efficiency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon">09:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14022" target="_blank">@arXiv 2407.14022</a>
                    <span class="tweet-title">Causal Inference:  Beyond the Binary, It's Getting Complex!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Zhejiang University, Peking University, Emory University</span>
                </div>
                <div class="primary-text">
                    This research focuses on causal inference with complex treatments, going beyond the traditional binary treatment setting (treatment or no treatment) to explore multi-valued, continuous, and bundled treatment options.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon">09:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14081" target="_blank">@arXiv 2407.14081</a>
                    <span class="tweet-title">Graph Classification Gets a Disentangled Makeover: Semi-Supervised Learning Goes Factor-Wise!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of International Business and Economics, UC Los Angeles, Peking University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel framework called DisenSemi for semi-supervised graph classification. Unlike previous approaches that transfer the entire knowledge from the unsupervised model to the supervised one, DisenSemi disentangles the graph representation into distinct latent factors and transfers only the relevant semantics that align well with the supervised task.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon">10:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13796" target="_blank">@arXiv 2407.13796</a>
                    <span class="tweet-title">Jailbreaking LLMs:  A Direct Attack on the Input, No Suffix Needed!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanyang Technological University, University of New South Wales, Huazhong University of Science and Technology...</span>
                </div>
                <div class="primary-text">
                    This research explores a new method for jailbreaking LLMs by directly manipulating the input embedding, eliminating the need for suffixes, which have been the primary focus of previous work.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon">10:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13982" target="_blank">@arXiv 2407.13982</a>
                    <span class="tweet-title">ASR's Got a Case of the Mumbles: How Recording Quality Confounds Racial Bias in Speech Recognition</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, University of Minnesota</span>
                </div>
                <div class="primary-text">
                    This study goes beyond simply noting that automatic speech recognition (ASR) systems perform worse on African American English (AAE) speech. It digs deeper, revealing that the quality of the audio recordings used to train these systems can significantly impact their accuracy, potentially masking or even exaggerating the true extent of racial bias.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon">11:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13775" target="_blank">@arXiv 2407.13775</a>
                    <span class="tweet-title">Drivers vs. Bots:  The "Sweet Spot" of Cooperative Driving Advice</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research delves into driver sentiments towards cooperative real-time advisory (CoRTA) systems, focusing on how these systems influence driver trust and behavior. Unlike previous studies that primarily evaluated system effectiveness or UI design, this paper uses a driving simulator study and qualitative analysis to understand driver preferences for various aspects of the interaction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon">11:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14245" target="_blank">@arXiv 2407.14245</a>
                    <span class="tweet-title">Dataset Distillation:  Stop Stretching Those Trajectories!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Technical University of Munich, University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research addresses the Accumulated Mismatching Problem (AMP) in long-range matching dataset distillation (LDD) methods. Unlike previous LDD methods that use a fixed trajectory length, this paper proposes Automatic Training Trajectories (ATT), which dynamically adjusts the trajectory length to minimize errors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon">12:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14387" target="_blank">@arXiv 2407.14387</a>
                    <span class="tweet-title">Graph Learning Gets a Sonic Makeover: GLAudio Turns Node Features into Waves!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This paper introduces GLAudio, a novel graph learning architecture that propagates node features through the graph network according to the discrete wave equation. Unlike previous methods that rely on the heat equation, GLAudio separates information propagation and processing into distinct steps, potentially mitigating over-smoothing and over-squashing.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon">12:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13863" target="_blank">@arXiv 2407.13863</a>
                    <span class="tweet-title">GANs Gone Wild: Cracking Model Secrets with Intermediate Features</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harbin Institute of Technology, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new method for model inversion attacks, called IF-GMI, which leverages intermediate features within the GAN structure. Unlike previous methods that solely focused on the latent space, IF-GMI explores the rich semantic information encoded in the intermediate layers of the GAN, leading to improved attack accuracy, especially in out-of-distribution scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon">13:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13928" target="_blank">@arXiv 2407.13928</a>
                    <span class="tweet-title">Bias Busters: New Method Makes LLMs More Respectful</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">American University in Cairo</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new framework for mitigating bias in LLMs using Direct Preference Optimization (DPO). Unlike previous approaches like Reinforcement Learning from Human Feedback (RLHF), DPO directly optimizes the model to favor less biased completions without requiring a separate reward model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon">13:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14191" target="_blank">@arXiv 2407.14191</a>
                    <span class="tweet-title">AI Predicts ALS Survival:  Brain Scans Get a "Normative" Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to predicting survival in ALS by combining normative modeling with diffusion autoencoders. This differs from previous work that used either generative or non-generative normative models alone.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon">14:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14506" target="_blank">@arXiv 2407.14506</a>
                    <span class="tweet-title">Chart-ing a New Course: How to Teach LLMs to Read Charts Like Humans</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of British Columbia, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research focuses on the pre-training process for multimodal language models (MLLMs) specifically tailored for chart understanding. Unlike previous work that primarily relies on fine-tuning, this paper explores the impact of incorporating raw data values during pre-training to improve the model's ability to extract underlying numeric values from charts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon">14:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.14352" target="_blank">@arXiv 2407.14352</a>
                    <span class="tweet-title">Flying High, Seeing Low: AI Helps Pilots Avoid Power Line Peril</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research proposes a new deep learning approach to jointly detect power line cables and pylons from images captured by aircraft-mounted cameras. Unlike previous work that focused on either cables or pylons separately, this method uses a single network to detect both, leveraging a modern convolutional architecture and a loss function designed for curvilinear structure delineation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon">15:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13833" target="_blank">@arXiv 2407.13833</a>
                    <span class="tweet-title">Phi-3:  A Language Model That's Safe, Sound, and Ready for Your Smartphone!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research focuses on safety aligning a series of small language models (SLMs) called Phi-3.  The study utilizes a "break-fix" cycle, which involves multiple rounds of safety post-training, red teaming, and vulnerability identification. This iterative approach is distinct from traditional single-round fine-tuning methods.
                </div>
            </div>
        </div></div>

    <footer class="player-footer">
        <div id="controls">
            <button id="controllerButton" onclick="scrollToCurrentTweet()">
                <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
            </button>
            <button id="controllerButton" onclick="togglePlayPause()">
                <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                <div id="progressTime">0:00</div>
            </button>
        </div>
        <div id="progressContainer" onclick="seek(event)">
            <div id="progressBar"></div>
            <div id="progressCircle" draggable="true"></div>
        </div>
        <audio id="audioPlayer" src="assets/audio.mp3"></audio>
    </footer>

    <script>
        var audio = document.getElementById('audioPlayer');
        var controllerButton = document.getElementById('controllerButton');
        var playPauseImage = document.getElementById('playPauseImage');
        var progressBar = document.getElementById('progressBar');
        var progressCircle = document.getElementById('progressCircle');
        var progressTime = document.getElementById('progressTime');
        var isDragging = false;

        function scrollToCurrentTweet() {
            // This assumes `start-time-icon` divs have text in format "MM:SS"
            const tweets = document.querySelectorAll('.tweet');
            const currentTime = audio.currentTime;

            console.log("NOODLE", currentTime);

            let targetTweet = null;
            let maxStartTime = -1;

            tweets.forEach((tweet, index) => {
                const timeString = tweet.querySelector('.start-time-icon').textContent;
                const parts = timeString.split(':');

                tweetTime = (parseInt(parts[0]) * 60 + parseInt(parts[1]))
                if (parts.length > 2) {
                    tweetTime = tweetTime * 60 + parseInt(parts[2]) // convert MM:SS to seconds
                }
                if (tweetTime <= currentTime && tweetTime > maxStartTime) {
                    maxStartTime = tweetTime;
                    targetTweet = tweet;
                }
            });

            // If no tweet found that meets the condition, scroll to the top tweet
            if (!targetTweet && tweets.length > 0) {
                targetTweet = tweets[0];
            }

            if (targetTweet) {
                targetTweet.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }

        function togglePlayPause() {
            if (audio.paused) {
                audio.play();
                playPauseImage.src = 'assets/buttonPause.svg';
            } else {
                audio.pause();
                playPauseImage.src = 'assets/buttonPlay.svg';
            }
        }

        audio.addEventListener('timeupdate', function () {
            if (!isDragging) {
                var progress = (audio.currentTime / audio.duration) * 100;
                progressBar.style.width = progress + '%';
                progressCircle.style.left = progress + '%'; // Corrected reference
                updateProgressTime(audio.currentTime);
            }
        });

        audio.addEventListener('ended', function () {
            playPauseImage.src = 'assets/buttonPlay.svg';
            progressBar.style.width = '0%';
            progressCircle.style.left = '0%';
            updateProgressTime(0);
        });

        // Mouse events
        progressCircle.addEventListener('mousedown', function (event) {
            isDragging = true;
            document.addEventListener('mousemove', onMouseMove);
            document.addEventListener('mouseup', onMouseUp);
        });

        // Touch events
        progressCircle.addEventListener('touchstart', function (event) {
            isDragging = true;
            document.addEventListener('touchmove', onTouchMove);
            document.addEventListener('touchend', onTouchEnd);
        });

        function onMouseMove(event) {
            seek(event.clientX);
        }

        function onTouchMove(event) {
            var touch = event.touches[0];
            seek(touch.clientX);
        }

        function onMouseUp() {
            isDragging = false;
            document.removeEventListener('mousemove', onMouseMove);
            document.removeEventListener('mouseup', onMouseUp);
        }

        function onTouchEnd() {
            isDragging = false;
            document.removeEventListener('touchmove', onTouchMove);
            document.removeEventListener('touchend', onTouchEnd);
        }

        function onMouseUp() {
            isDragging = false;
            document.removeEventListener('mousemove', onMouseMove);
            document.removeEventListener('mouseup', onMouseUp);
        }

        progressCircle.addEventListener('dragstart', function (event) {
            event.preventDefault();
        });

        function updateProgressTime(currentTime) {
            var minutes = Math.floor(currentTime / 60);
            var seconds = Math.floor(currentTime % 60);
            if (seconds < 10) {
                seconds = '0' + seconds;
            }
            progressTime.textContent = minutes + ':' + seconds;
        }

        function seek(event) {
            var containerRect = progressContainer.getBoundingClientRect();
            var newTime = ((event.clientX - containerRect.left) / containerRect.width) * audio.duration;
            audio.currentTime = newTime;
            var progress = (audio.currentTime / audio.duration) * 100;
            progressBar.style.width = progress + '%';
            progressCircle.style.left = progress + '%'; // Ensure this reference is consistent
            updateProgressTime(newTime);
        }
    </script>

</body>

</html>