<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Fresh AI Paper Top Picks</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 80px;"></div>
            <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                Freshest
                Top Picks:
                <span class="highlightNumber" style="font-size: 28px;">75</span> out of <span
                    class="highlightNumber">398</span> arXiv AI
                papers
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="line-height: 1.2;">just released on</div>
                <div id="data-default-date" data-date="2024-08-27"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 10px;">
            </div>
        </div>

        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">
                00:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14400" target="_blank">
                        @arXiv 2408.14400
                    </a>
                    <span class="tweet-title">
                        Solar Power, From Space to Your Roof: Google's Satellite Sunroof Maps the World!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google, University of Toronto
                    </span>
                </div>
                <div class="primary-text">
                    This research expands Google's Solar API to use satellite imagery, enabling global solar potential
                    assessment. Unlike previous work that relied on aerial imagery, this approach tackles the challenges
                    of lower resolution and oblique views from satellites.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">
                01:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14397" target="_blank">
                        @arXiv 2408.14397
                    </a>
                    <span class="tweet-title">
                        Radiology Reports: AI's Got a Knowledge Graph Problem!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard Medical School
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel system called ReXKG, which constructs a comprehensive radiology
                    knowledge graph from medical reports. Unlike previous work that focused on report-to-report
                    comparisons, ReXKG evaluates AI models based on their understanding of radiological knowledge.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">
                01:41
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13915" target="_blank">
                        @arXiv 2408.13915
                    </a>
                    <span class="tweet-title">
                        LLMs: The New Lie Detectors? They're Not Just Chatting, They're Catching Liars!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Princeton University
                    </span>
                </div>
                <div class="primary-text">
                    This research explores a novel bootstrapping framework that utilizes self-generated feedback from
                    LLMs to enhance their reasoning capabilities for lie detection. Unlike previous work that relies on
                    supervised learning or external evaluators, this approach leverages the LLM's own feedback to
                    improve its performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">
                02:00
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13678" target="_blank">
                        @arXiv 2408.13678
                    </a>
                    <span class="tweet-title">
                        Speech Models Learn to Speak Like Us: A Layer-by-Layer Look at Prosody
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research investigates how self-supervised speech models represent suprasegmental features like
                    tone and stress across different layers of the model. It compares the performance of models trained
                    on English and Mandarin, highlighting the role of context in learning language-specific
                    representations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">
                02:27
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14471" target="_blank">
                        @arXiv 2408.14471
                    </a>
                    <span class="tweet-title">
                        Foundation Models: From Outdated to Up-to-Date with a Little Help from Their Friends
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of TÃ¼bingen, Helmholtz Munich, University of Cambridge...
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on continual pretraining for multimodal foundation models, specifically
                    addressing the gap between infrequent, large-scale updates and frequent, sample-level updates. It
                    introduces a new benchmark, FoMo-in-Flux, which emulates real-world deployment scenarios with
                    realistic compute constraints and diverse datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">
                02:52
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13630" target="_blank">
                        @arXiv 2408.13630
                    </a>
                    <span class="tweet-title">
                        Voting Rules Get a Neural Network Makeover: Embeddings Make the Difference!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tulane University, IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the use of tailored embeddings to learn probabilistic voting rules more
                    efficiently and effectively. Unlike previous work, which often relied on large, complex models, this
                    study demonstrates that carefully crafted embeddings can significantly reduce the number of model
                    parameters and improve scalability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">
                03:16
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13674" target="_blank">
                        @arXiv 2408.13674
                    </a>
                    <span class="tweet-title">
                        Text-to-Avatar: From Words to Wondrous 3D Faces!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Chinese University of Hong Kong, Meta Reality Labs
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a text-conditioned generative model called GenCA that creates photorealistic,
                    drivable 3D avatars. Unlike previous methods that rely on extensive data capture or limited
                    expression control, GenCA leverages a two-stage framework to generate avatars from text
                    descriptions, enabling greater flexibility and control.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">
                03:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14339" target="_blank">
                        @arXiv 2408.14339
                    </a>
                    <span class="tweet-title">
                        AI's Got a New Test: Can It Handle a Multi-Concept Image Challenge?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Princeton University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces CONCEPTMIX, a new benchmark for evaluating the compositional capabilities
                    of text-to-image (T2I) models. Unlike previous benchmarks that rely on fixed templates and limited
                    concept combinations, CONCEPTMIX uses GPT-4 to generate diverse and complex prompts, allowing for a
                    more comprehensive and scalable evaluation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">
                04:07
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14419" target="_blank">
                        @arXiv 2408.14419
                    </a>
                    <span class="tweet-title">
                        Charts Can Lie: New Benchmark Tests AI's Ability to Spot Visual Deception
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of WisconsinâMadison, ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark called CHARTOM, which focuses on evaluating multimodal
                    large language models (MLLMs) on their ability to understand the potential for misleadingness in
                    data visualizations, specifically charts. Unlike previous benchmarks that primarily focused on text
                    comprehension, CHARTOM tests visual perception and theory of mind by presenting charts with
                    intentional manipulations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">
                04:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14016" target="_blank">
                        @arXiv 2408.14016
                    </a>
                    <span class="tweet-title">
                        Pixel-Perfect Views: New AI Makes 3D Objects Look Real From Every Angle!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Illinois Urbana-Champaign, Snap Inc., University of Toronto
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel depth-truncated epipolar attention mechanism within the decoder of
                    a latent video diffusion model. This approach allows for cross-view attention at higher resolutions,
                    leading to improved pixel-level alignment in multi-view image generation. Unlike previous methods
                    that rely on coarse alignment, this technique focuses on specific regions of interest, enhancing the
                    accuracy and consistency of the generated images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">
                05:07
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13767" target="_blank">
                        @arXiv 2408.13767
                    </a>
                    <span class="tweet-title">
                        Deep Learning's Secret Weapon: Linear Neural Networks Get a Dynamical Makeover
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tel Aviv University, Princeton University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a dynamical analysis of linear neural networks, focusing on the
                    trajectories of weight matrices during training. This approach differs from previous work that often
                    relies on static analyses of the network's structure.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">
                05:31
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13934" target="_blank">
                        @arXiv 2408.13934
                    </a>
                    <span class="tweet-title">
                        AI Bots Learn to Move Like Pro CS:GO Players - No More Bot-Like Behavior!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, Activision Blizzard, NVIDIA...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a transformer-based movement model trained on a large dataset of
                    professional Counter-Strike: Global Offensive (CS:GO) gameplay. Unlike previous work, this model is
                    designed to be computationally efficient, making it suitable for real-time use in commercial games.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">
                06:01
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14037" target="_blank">
                        @arXiv 2408.14037
                    </a>
                    <span class="tweet-title">
                        Robot Data Diet: How to Feed Your AI for Maximum Performance
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on automatically curating large-scale robotics datasets for imitation learning
                    by using distributionally robust optimization to maximize worst-case performance across all possible
                    downstream domains. This approach differs from previous work by addressing the challenges of
                    applying DRO to robotics datasets, including variability in action spaces and dynamics across
                    different datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">
                06:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13836" target="_blank">
                        @arXiv 2408.13836
                    </a>
                    <span class="tweet-title">
                        PropSAM: Segmenting 3D Medical Objects with a Single Prompt, Like a Boss!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces PropSAM, a model that leverages the continuous flow of information within
                    3D medical structures for segmentation. Unlike previous models that rely on multiple prompts or
                    complex 3D architectures, PropSAM uses a single 2D prompt and a propagation-based approach to
                    achieve accurate volumetric segmentation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">
                06:54
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13533" target="_blank">
                        @arXiv 2408.13533
                    </a>
                    <span class="tweet-title">
                        Is Noise in AI a Bad Thing? Think Again!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research goes beyond the usual assumption that noise is always bad for AI models. It defines
                    seven types of noise in retrieval-augmented generation (RAG) systems and shows that some types of
                    noise can actually improve performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">
                07:19
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13727" target="_blank">
                        @arXiv 2408.13727
                    </a>
                    <span class="tweet-title">
                        Log Parsing Gets a Brain: LLMs Make Log Analysis Easier Than Ever!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Alibaba, Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces LogParser-LLM, a novel log parser that leverages the capabilities of Large
                    Language Models (LLMs) to efficiently parse logs. Unlike previous LLM-based approaches that parse
                    logs line-by-line, LogParser-LLM utilizes a prefix tree to reduce the number of LLM calls, making it
                    more practical for real-world applications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">
                07:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13460" target="_blank">
                        @arXiv 2408.13460
                    </a>
                    <span class="tweet-title">
                        Deep Learning's New Noise-Canceling Trick: A Low-Pass Filter for Privacy
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Illinois at Urbana-Champaign, University of California Los Angeles, University of
                        Southern California
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel signal processing approach to improve differentially private (DP)
                    optimizers. Unlike previous methods that focus on reducing noise in each individual update step,
                    this paper analyzes the gradient updates in the frequency domain, identifying that the gradient
                    signal is concentrated in lower frequencies while the DP noise is spread across all frequencies.
                    This allows for the application of a low-pass filter to effectively suppress the noise and enhance
                    the performance of DP optimizers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">
                08:03
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13464" target="_blank">
                        @arXiv 2408.13464
                    </a>
                    <span class="tweet-title">
                        AI Debate Club: How Talking Bots Can Uncover Bias in News
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to mitigating bias in training data by using multiple Large
                    Language Models (LLMs) to engage in a structured dialogue, thereby uncovering diverse perspectives
                    and challenging human annotations. This differs from previous work that primarily focused on
                    ensemble methods or semi-supervised learning, which may not fully address underlying biases.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">
                08:21
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13933" target="_blank">
                        @arXiv 2408.13933
                    </a>
                    <span class="tweet-title">
                        Mobile LLMs: Tiny Brains, Big Language, No More Phone Meltdowns!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Samsung AI Center
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces MobileQuant, a post-training quantization method that optimizes both
                    weights and activations for large language models (LLMs). Unlike previous methods that focus on
                    weight-only quantization or dynamic per-token quantization, MobileQuant utilizes static per-tensor
                    quantization for activations, making it compatible with existing mobile hardware.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">
                08:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13278" target="_blank">
                        @arXiv 2408.13278
                    </a>
                    <span class="tweet-title">
                        AI's Copyright Conundrum: Can Randomness Save the Day?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, Google, University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research explores randomization techniques to mitigate copyright infringement in AI models,
                    specifically focusing on the concept of Near Access-Freeness (NAF) and its connection to
                    differential privacy. Unlike previous work that primarily focused on output filtering, this paper
                    proposes a more nuanced approach by leveraging the inherent randomness in generative models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">
                09:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13359" target="_blank">
                        @arXiv 2408.13359
                    </a>
                    <span class="tweet-title">
                        Learning Rate: It's Not Just About the Size, It's About the Power!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the relationship between optimal learning rate, batch size, and the number of
                    training tokens, proposing a new learning rate scheduler called PowerLR that is agnostic to these
                    factors. Unlike previous work that focused on transferring hyperparameters from small proxy models
                    to large models, this paper demonstrates that the optimal learning rate follows a power-law
                    relationship with respect to batch size and the number of training tokens.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">
                09:45
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13704" target="_blank">
                        @arXiv 2408.13704
                    </a>
                    <span class="tweet-title">
                        LLMs as NLG Evaluators: Can They Tell the Difference Between Good and Bad Writing?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Texas A&amp;M University, Rice University, Axon Enterprise Inc.
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmarking framework called DHP (Discernment of Hierarchical
                    Perturbation) to assess the ability of LLMs to evaluate the quality of text generated by other LLMs.
                    Unlike previous studies that rely on human scores, DHP uses a hierarchical perturbation approach to
                    create different levels of text quality and then uses statistical tests to measure how well LLMs can
                    distinguish between them.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">
                10:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14086" target="_blank">
                        @arXiv 2408.14086
                    </a>
                    <span class="tweet-title">
                        Stackelberg Games: When Followers Play by the Rules, Leaders Win Big!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This research explores Stackelberg games where the follower uses a "no-regret" learning strategy,
                    meaning they consistently make decisions that minimize their potential for regret. This is a
                    departure from previous work that often assumed followers were either myopic or had specific
                    game-type limitations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">
                10:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14340" target="_blank">
                        @arXiv 2408.14340
                    </a>
                    <span class="tweet-title">
                        Music's New Maestro: Foundation Models Take the Stage!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google, Meta, University of Cambridge...
                    </span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive overview of foundation models for music, highlighting their
                    unique challenges and opportunities compared to traditional methods. It delves into the technical
                    details of pre-training strategies, model architectures, and domain adaptation techniques, while
                    also addressing ethical and social impact considerations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">
                11:04
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13320" target="_blank">
                        @arXiv 2408.13320
                    </a>
                    <span class="tweet-title">
                        Zero-Shot Learning Goes Online: Classifying Images on the Fly!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Alibaba, University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research explores an online zero-shot transfer scenario where images arrive in a random order
                    and must be classified immediately without storing their representations. This differs from previous
                    work that typically relies on offline optimization with access to the entire dataset.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">
                11:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13351" target="_blank">
                        @arXiv 2408.13351
                    </a>
                    <span class="tweet-title">
                        Deep Learning's New Trick: Augmenting Features with Semantic Savvy!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Alibaba, University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel semantic adversarial augmentation (SeA) method for fixed deep
                    features. Unlike previous work that focuses on augmenting input data or intermediate layers, SeA
                    directly manipulates the features extracted from the last layer of a pre-trained model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">
                11:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14468" target="_blank">
                        @arXiv 2408.14468
                    </a>
                    <span class="tweet-title">
                        K-Sort Arena: Ranking AI Models with a Multi-Model Showdown!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Chinese Academy of Sciences, UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces K-Sort Arena, a platform for evaluating visual generative models that uses
                    K-wise comparisons (comparing more than two models at once) instead of the traditional pairwise
                    comparisons. This approach aims to improve efficiency and reliability by gathering richer
                    information from user votes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">
                12:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13585" target="_blank">
                        @arXiv 2408.13585
                    </a>
                    <span class="tweet-title">
                        Sign Language Gets Its Own Massive Translation Test: FLEURS-ASL is Here!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces FLEURS-ASL, a new benchmark for evaluating sign language translation
                    models. Unlike previous benchmarks that focus on single languages or specific domains, FLEURS-ASL is
                    designed to be massively multilingual and multitask, supporting translation between American Sign
                    Language and over 200 other languages.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">
                12:48
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14114" target="_blank">
                        @arXiv 2408.14114
                    </a>
                    <span class="tweet-title">
                        ShapeMamba-EM: Giving 3D EM Images a Makeover with Local Shape Descriptors!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, Peng Cheng Laboratory
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces ShapeMamba-EM, a fine-tuning method for 3D electron microscopy (EM) image
                    segmentation. It differs from previous work by incorporating two novel modules: 3D Mamba Adapters
                    for long-range dependency modeling and a 3D Local Shape Descriptor (LSD) encoder for capturing local
                    morphological features.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">
                13:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13637" target="_blank">
                        @arXiv 2408.13637
                    </a>
                    <span class="tweet-title">
                        Choosing Your Dinner: How to Make Everyone Happy (and Not Just the Majority)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford, Alan Turing Institute, Agency for Science Technology and Research
                    </span>
                </div>
                <div class="primary-text">
                    This paper explores the computational complexity of maximizing welfare in a model of sequential
                    decision-making, where a single alternative is chosen at each round. It differs from previous work
                    by focusing on the compatibility of welfare maximization with strategyproofness and proportionality.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">
                13:44
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13654" target="_blank">
                        @arXiv 2408.13654
                    </a>
                    <span class="tweet-title">
                        LLMs Get a Memory Boost: Symbolic Working Memory Makes Reasoning Smarter!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Southern California, Fudan University, University of Washington...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes augmenting LLMs with an external working memory that stores facts and rules
                    in both natural language and symbolic forms. This allows for more precise tracking of information
                    during multi-step deductive reasoning, addressing the challenge of rule grounding in LLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">
                14:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14023" target="_blank">
                        @arXiv 2408.14023
                    </a>
                    <span class="tweet-title">
                        Video-CCAM: Making LLMs Watch Videos Like Humans Do!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Tencent QQ
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Causal Cross-Attention Masks (CCAMs) within the cross-attention layer of a
                    Video-MLLM. This approach allows the model to process videos with varying frame lengths, unlike
                    previous methods that either downsampled visual features or extended the LLM context size.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">
                14:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14307" target="_blank">
                        @arXiv 2408.14307
                    </a>
                    <span class="tweet-title">
                        AI-Powered 3D Printing: A Chatty Bot Makes Your Prints Perfect!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research uses a pre-trained Large Language Model (LLM) to monitor and control 3D printing,
                    unlike previous work that relied on specialized machine learning models or sensor-based systems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">
                15:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13896" target="_blank">
                        @arXiv 2408.13896
                    </a>
                    <span class="tweet-title">
                        Jailbreaking AI Art: Random Tokens Unleash NSFW Images
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nankai University, Nanyang Technological University, Alibaba Group...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel black-box attack method called RT-Attack, which utilizes random
                    token substitution to bypass defense mechanisms in text-to-image models. Unlike previous work that
                    relies on gradient information, RT-Attack operates without access to the model's internal workings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">
                15:37
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13689" target="_blank">
                        @arXiv 2408.13689
                    </a>
                    <span class="tweet-title">
                        Tracking in Clutter: A Distributed Algorithm That's Not All Talk!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a new method for distributed multi-object tracking that uses a locally
                    maximized evidence lower bound (LM-ELBO) to reduce communication overhead compared to previous
                    consensus-based approaches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">
                16:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13546" target="_blank">
                        @arXiv 2408.13546
                    </a>
                    <span class="tweet-title">
                        Machines with Synesthesia: Boosting Vehicular Networks with Double Dynamics
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, Hong Kong University of Science and Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel deep reinforcement learning (DRL) framework for integrated sensing
                    and communication (ISAC) precoding in vehicular networks. Unlike previous optimization-based
                    methods, this approach leverages various modalities, such as positioning and channel information, to
                    adapt to double dynamics without relying on perfect prior information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">
                16:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13986" target="_blank">
                        @arXiv 2408.13986
                    </a>
                    <span class="tweet-title">
                        LLMs Go on a Road Trip: Predicting Human Mobility with an Agentic Framework
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces AgentMove, a framework that leverages large language models (LLMs) for
                    mobility prediction. Unlike previous LLM-based approaches that directly generate output, AgentMove
                    decomposes the task into sub-tasks and utilizes specialized modules to capture individual and shared
                    mobility patterns, as well as urban structure effects.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">
                17:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13646" target="_blank">
                        @arXiv 2408.13646
                    </a>
                    <span class="tweet-title">
                        Pedestrian Detection: Don't Let Them Get Away With Being Too Tall!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Imperial College London, University College London
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel post-processing method called Mean Height Aided Suppression (MHAS)
                    that leverages the perspective effect common in pedestrian datasets to improve detection accuracy.
                    Unlike previous work that focuses on feature extraction or multi-stage prediction, MHAS utilizes
                    easily available information about pedestrian heights and their relative positions within the image
                    to filter out false positives.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">
                17:34
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13370" target="_blank">
                        @arXiv 2408.13370
                    </a>
                    <span class="tweet-title">
                        Relighting 3D Objects with Gaussian Splats: It's Like Magic, But With Math!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich, Tencent
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a new method for relighting 3D objects represented by Gaussian Splats, using a
                    lighting-dependent appearance model based on bidirectional spherical harmonics. This differs from
                    previous work that relied on surface-based shading models, which struggle with volumetric and fuzzy
                    objects.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">
                17:59
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13461" target="_blank">
                        @arXiv 2408.13461
                    </a>
                    <span class="tweet-title">
                        Vision-Language Models: A Multimodal Attack of the Titans!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft, Deakin University, Commonwealth Scientific and Industrial Research Organisation
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the adversarial robustness of vision-language pretraining (VLP) models,
                    specifically targeting the cross-modal interactions within these models. Unlike previous work that
                    primarily focused on single-modality attacks, this study proposes a novel Joint Multimodal
                    Transformer Feature Attack (JMTFA) that simultaneously perturbs both visual and textual modalities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">
                18:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14279" target="_blank">
                        @arXiv 2408.14279
                    </a>
                    <span class="tweet-title">
                        Point Cloud Reconstruction: Learning Local Patterns for Unseen Shapes
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Wayne State University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes learning local pattern modularization for point cloud reconstruction from
                    unseen classes. Unlike previous methods that rely on global priors, this approach focuses on
                    learning class-agnostic local patterns, which are then customized based on the input image.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">
                18:54
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14211" target="_blank">
                        @arXiv 2408.14211
                    </a>
                    <span class="tweet-title">
                        MagicMan: Conjuring Up 3D Humans From a Single Photo!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Tencent AI Lab, The Hong Kong University of Science and Technology...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces MagicMan, a diffusion model that generates multiple consistent views of a
                    human from a single image, unlike previous methods that often struggle with 3D consistency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">
                19:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13385" target="_blank">
                        @arXiv 2408.13385
                    </a>
                    <span class="tweet-title">
                        Unsupervised Learning: When Masked Images Meet Contrastive Power!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Huazhong University of Science and Technology, Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel unsupervised pretraining method called Masked Image Contrastive
                    Modeling (MICM) that combines the strengths of Masked Image Modeling (MIM) and Contrastive Learning
                    (CL). Unlike previous approaches that focus solely on either MIM or CL, MICM aims to bridge the gap
                    between generalization and discriminability by integrating both techniques.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">
                19:45
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13373" target="_blank">
                        @arXiv 2408.13373
                    </a>
                    <span class="tweet-title">
                        Learning Unknowns from Unknowns: How to Spot a Unicorn in a Sea of Horses
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Huazhong University of Science and Technology, Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for few-shot open-set recognition (FSOR) that generates
                    diversified negative prototypes by learning from the "unknown space" of base classes. Unlike
                    previous methods that rely solely on known class data, this approach leverages the inverse
                    representation of base classes to create more representative negative prototypes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">
                20:11
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13619" target="_blank">
                        @arXiv 2408.13619
                    </a>
                    <span class="tweet-title">
                        Maxwell's Equations Get a Geometric Makeover: Spacetime Algebra Solves the Puzzle!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the impact of different algebraic frameworks on the accuracy of solving
                    Maxwell's equations using Geometric Algebra (GA) networks. The study introduces STAResNet, a ResNet
                    architecture in Spacetime Algebra (STA), and compares its performance to a standard Clifford ResNet
                    in GA.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">
                20:34
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14354" target="_blank">
                        @arXiv 2408.14354
                    </a>
                    <span class="tweet-title">
                        Java Just Got Its Own Issue-Solving Benchmark!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Chinese Academy of Science, Peking University, Huawei Co. Ltd....
                    </span>
                </div>
                <div class="primary-text">
                    This research extends the SWE-bench benchmark, previously focused on Python, to include Java,
                    creating a multilingual platform for evaluating LLMs' ability to resolve software issues.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">
                21:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13770" target="_blank">
                        @arXiv 2408.13770
                    </a>
                    <span class="tweet-title">
                        TranSplat: 3D Reconstruction Gets a Depth-Aware Makeover!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, University of Hong Kong, China Telecom
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces TranSplat, a novel method for 3D reconstruction from sparse views. Unlike
                    previous methods that rely heavily on accurate feature matching, TranSplat utilizes a depth
                    confidence map to guide the matching process, improving reconstruction accuracy in challenging areas
                    like low-texture regions and repetitive patterns.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">
                21:30
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14185" target="_blank">
                        @arXiv 2408.14185
                    </a>
                    <span class="tweet-title">
                        GPT on the Road: AI Navigates Traffic Like a Pro!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Shandong University, Hong Kong University of Science and Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel dynamic path planning framework called DynamicRouteGPT, which
                    utilizes large language models (LLMs) to achieve real-time multi-vehicle navigation. Unlike previous
                    reinforcement learning approaches that focus solely on local optima, DynamicRouteGPT integrates
                    global path optimization with local path adjustments, ensuring vehicles reach their destinations
                    efficiently while adapting to dynamic traffic conditions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">
                22:00
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13724" target="_blank">
                        @arXiv 2408.13724
                    </a>
                    <span class="tweet-title">
                        3D Printing Gets a Grip: New Model Makes Objects Move Like They Should!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Cornell University, University of California Berkeley, Stanford University...
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on generating physically plausible parts for interactable objects, going
                    beyond just shape and appearance. It uses a diffusion-based model with geometric conditioning and
                    physical constraints to ensure the generated parts can move smoothly and realistically.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">
                22:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13355" target="_blank">
                        @arXiv 2408.13355
                    </a>
                    <span class="tweet-title">
                        Keyword Spotting: When Adversaries Become Teachers!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        The University of Texas at Dallas, META AI
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new training strategy for keyword spotting (KWS) models that uses
                    adversarial examples to improve robustness. The key innovation is the use of "datasource-aware
                    disentangled adversarial training," which involves maintaining separate batch normalization (BN)
                    layers for different data sources, including clean audio, augmented audio, and adversarial examples.
                    This approach aims to reduce the mismatch between the original and adversarial data distributions,
                    leading to more robust models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">
                22:59
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14262" target="_blank">
                        @arXiv 2408.14262
                    </a>
                    <span class="tweet-title">
                        AI Still Can't Understand Black English: Self-Supervised Learning Fails to Bridge the Gap
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU, UC Berkeley, Delft University of Technology
                    </span>
                </div>
                <div class="primary-text">
                    This study investigates whether self-supervised learning (SSL) speech models can improve automatic
                    speech recognition (ASR) performance for African American Vernacular English (AAVE) compared to
                    Mainstream American English (MAE). Unlike previous work that focused on data augmentation or dialect
                    classifiers, this research examines the generalization capabilities of SSL models trained on
                    out-of-domain data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">
                23:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14116" target="_blank">
                        @arXiv 2408.14116
                    </a>
                    <span class="tweet-title">
                        Space-Based AI: Training Models with Satellites, Not Servers!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ShanghaiTech University, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a hierarchical learning and computing framework for space-ground integrated
                    networks, focusing on energy-efficient model aggregation through inter-satellite links (ISLs) in LEO
                    mega-constellations. This approach differs from previous work by explicitly addressing the dynamic
                    nature of satellite network topology and the energy constraints of LEO satellites.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">
                23:59
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14090" target="_blank">
                        @arXiv 2408.14090
                    </a>
                    <span class="tweet-title">
                        GPU Communication: Supercomputer Interconnect Speed Dating
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Sapienza University of Rome, University of Trento, Vrije Universiteit Amsterdam...
                    </span>
                </div>
                <div class="primary-text">
                    This research comprehensively characterizes the performance of GPU-to-GPU communication on three
                    different supercomputers, each with unique architectures, and analyzes the performance of various
                    communication APIs and software stacks. This sets it apart from previous work by providing a
                    large-scale, multi-system comparison.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">
                24:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14326" target="_blank">
                        @arXiv 2408.14326
                    </a>
                    <span class="tweet-title">
                        Fetal Brain Mapping: AI Makes It Easier Than Ever!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Boston Childrenâs Hospital, Harvard Medical School, Elmhurst Hospital Center...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces the first machine learning model specifically designed for fetal brain
                    tractography, addressing the unique challenges of low signal quality and rapid brain development in
                    utero.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">
                24:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14472" target="_blank">
                        @arXiv 2408.14472
                    </a>
                    <span class="tweet-title">
                        Humanoid Robots Learn to Walk Like Us, Even on Snow!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Denoising World Model Learning (DWL), a new reinforcement learning
                    framework that enables humanoid robots to master challenging terrains with zero-shot sim-to-real
                    transfer. DWL differs from previous work by incorporating an encoder-decoder architecture to denoise
                    observations and reconstruct the true state, effectively bridging the sim-to-real gap.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">
                25:21
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14267" target="_blank">
                        @arXiv 2408.14267
                    </a>
                    <span class="tweet-title">
                        1-Bit Training: Deep Learning Goes Binary!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Beijing Jiaotong University, Tsinghua University, Huawei
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the possibility of training deep neural networks using only 1-bit precision
                    for weights, activations, and gradients. This is a significant departure from previous work, which
                    typically uses 4-bit or higher precision.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">
                25:48
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13679" target="_blank">
                        @arXiv 2408.13679
                    </a>
                    <span class="tweet-title">
                        Segment Anything, Mesh Anything: Zero-Shot Mesh Part Segmentation with SAMesh
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT, Backflip AI
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes SegmentAnyMesh (SAMesh), a novel zero-shot method for mesh part segmentation
                    that leverages the power of Segment Anything (SAM) for 2D image segmentation. Unlike previous
                    approaches that rely on text descriptions, SAMesh operates directly on multi-view renders of the
                    mesh, enabling it to segment parts without requiring any textual input.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">
                26:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13295" target="_blank">
                        @arXiv 2408.13295
                    </a>
                    <span class="tweet-title">
                        AI for Public Health: Fairness Metrics for Equity-Minded Algorithms
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto, York University, McGill University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the specific types of bias and fairness metrics relevant to measuring
                    equity in population health, unlike previous work that focused on general benefits and risks of AI
                    in healthcare or biases in healthcare data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">
                26:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14348" target="_blank">
                        @arXiv 2408.14348
                    </a>
                    <span class="tweet-title">
                        AI for Wildlife: Can We Trust Robots to Count Zebras?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University College London, Zoological Society of London, Instituto BiotrÃ³picos...
                    </span>
                </div>
                <div class="primary-text">
                    This study goes beyond simply measuring how well AI classifies wildlife images. It investigates how
                    AI's performance impacts the accuracy of ecological metrics like species richness and occupancy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">
                27:05
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14199" target="_blank">
                        @arXiv 2408.14199
                    </a>
                    <span class="tweet-title">
                        Testbed Trouble: A Guide to Tiny Robots and Cars!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        RWTH Aachen University, University of Delaware, Arizona State University...
                    </span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive survey of small-scale testbeds for connected and automated
                    vehicles (CAVs) and robot swarms (RSs), including a detailed online table comparing 22 existing
                    testbeds based on 62 characteristics derived from the sense-plan-act paradigm. This approach differs
                    from previous surveys by offering a more granular and up-to-date analysis of testbed capabilities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">
                27:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13667" target="_blank">
                        @arXiv 2408.13667
                    </a>
                    <span class="tweet-title">
                        Outlier Detection: It's Not Just the Data, It's the Algorithm's Assumptions!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research delves into the sources of algorithmic bias in unsupervised outlier detection,
                    specifically examining the impact of various data biases on the fairness and performance of popular
                    OD models. Unlike previous work that primarily focused on supervised ML, this study investigates the
                    role of data-centric factors in driving unfairness in unsupervised algorithms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">
                27:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13361" target="_blank">
                        @arXiv 2408.13361
                    </a>
                    <span class="tweet-title">
                        Clustering with a Smile: Neural GAMs Make Explanations Easy!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces NeurCAM, a novel approach to interpretable clustering that leverages neural
                    generalized additive models (GAMs) to provide fuzzy cluster membership with additive explanations.
                    Unlike previous methods that rely on decision trees, NeurCAM offers a more scalable and expressive
                    approach to interpretable clustering.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">
                28:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13376" target="_blank">
                        @arXiv 2408.13376
                    </a>
                    <span class="tweet-title">
                        Reinforcement Learning Gets a Categorical Makeover: Task Composition with a Twist!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Universitat PolitÃ¨cnica de Catalunya, The University of Iowa, The University of Texas at Austin
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to compositional reinforcement learning (RL) by leveraging
                    category theory, a branch of mathematics that explores structures and their relationships. This
                    approach differs from previous work by systematically capturing task interdependencies and
                    compositional structures, leading to a principled way of discovering optimal task decompositions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">
                28:42
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13479" target="_blank">
                        @arXiv 2408.13479
                    </a>
                    <span class="tweet-title">
                        Quantum Computing: Drug Discovery's New Secret Weapon?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Rensselaer Polytechnic Institute, University of Illinois Urbana-Champaign, Instituut-Lorentz...
                    </span>
                </div>
                <div class="primary-text">
                    This research paper explores the integration of quantum computing into drug discovery and
                    development, focusing on how quantum technologies might accelerate and enhance various stages of the
                    drug development cycle. It specifically explores the application of quantum computing in addressing
                    challenges related to drug discovery, such as molecular simulation and the prediction of drug-target
                    interactions, as well as the optimization of clinical trial outcomes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon" title="Play from here">
                29:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13885" target="_blank">
                        @arXiv 2408.13885
                    </a>
                    <span class="tweet-title">
                        DAGs in Spacetime: A Neural Network Learns to Warp Reality
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford, McMaster University
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a new type of geometry called "Neural Spacetimes" (NSTs) that can represent
                    directed acyclic graphs (DAGs) as events in a spacetime manifold. Unlike previous work that focused
                    on undirected graphs or causality embedding separately, NSTs can encode both edge weights and
                    directionality in a single representation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon" title="Play from here">
                29:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14013" target="_blank">
                        @arXiv 2408.14013
                    </a>
                    <span class="tweet-title">
                        Color Edge Detection: A Multiscale Fusion of Gradient and Denoising!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Johns Hopkins University, UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new color edge detection method that combines a multiscale gradient fusion
                    strategy with the CBM3D filter for image denoising. This approach differs from previous work by
                    utilizing the XYZ color space and incorporating anisotropic Gaussian directional derivatives for
                    enhanced edge detection.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon" title="Play from here">
                30:11
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13556" target="_blank">
                        @arXiv 2408.13556
                    </a>
                    <span class="tweet-title">
                        Supply Chain Risk? Let's Play "What If?" with Causal Machine Learning!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        PoznaÅ University of Economics and Business, University of Huddersfield, University of
                        Cambridge...
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the use of causal machine learning (CML) for developing supply chain risk
                    intervention models, a departure from traditional machine learning models that focus on identifying
                    correlations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet67">
            <div class="start-time-icon" title="Play from here">
                30:36
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14028" target="_blank">
                        @arXiv 2408.14028
                    </a>
                    <span class="tweet-title">
                        Surgeons, Meet Your New AI Assistant: Text-Guided Surgical Video Generation!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces SurGen, a text-guided diffusion model for surgical video generation, which
                    produces higher resolution and longer duration videos compared to existing methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet68">
            <div class="start-time-icon" title="Play from here">
                31:00
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14435" target="_blank">
                        @arXiv 2408.14435
                    </a>
                    <span class="tweet-title">
                        AI Sees Faces, But Does It See Bias?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        California Institute of Technology, ETH Zurich, Empa
                    </span>
                </div>
                <div class="primary-text">
                    This research uses a synthetic dataset of faces, where attributes like age, gender, and race are
                    systematically varied, allowing for a more controlled and causal analysis of bias in a
                    vision-language model compared to previous studies that relied on observational data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet69">
            <div class="start-time-icon" title="Play from here">
                31:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13912" target="_blank">
                        @arXiv 2408.13912
                    </a>
                    <span class="tweet-title">
                        Splatt3R: 3D Reconstruction From Uncalibrated Images, No Camera Needed!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces Splatt3R, a model that can reconstruct 3D scenes from just two uncalibrated
                    images, unlike previous methods that require known camera parameters or a dense collection of
                    images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet70">
            <div class="start-time-icon" title="Play from here">
                31:45
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14416" target="_blank">
                        @arXiv 2408.14416
                    </a>
                    <span class="tweet-title">
                        Metaverse Meets AI: Hyperdimensional Computing Makes Federated Learning Faster!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Kingâs College London, Nanyang Technological University, Zhejiang University...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel framework called FSL-HDC, which integrates federated split learning
                    (FSL) with hyperdimensional computing (HDC) for training foundation models in the Metaverse. This
                    approach differs from previous work by utilizing HDC's low computational complexity and energy
                    efficiency to address the challenges of training large models on resource-constrained edge devices.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet71">
            <div class="start-time-icon" title="Play from here">
                32:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.13888" target="_blank">
                        @arXiv 2408.13888
                    </a>
                    <span class="tweet-title">
                        SQL Queries: A Neurosymbolic Reasoning Revolution!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge, University of Bristol
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a neurosymbolic architecture for generating SQL queries that leverages
                    external modules to guide the exploration of the solution space. Unlike previous work that relies on
                    custom neural architectures or symbolic reasoning alone, this approach integrates a language model
                    with symbolic modules to improve the performance of smaller, open-source language models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet72">
            <div class="start-time-icon" title="Play from here">
                32:49
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14009" target="_blank">
                        @arXiv 2408.14009
                    </a>
                    <span class="tweet-title">
                        TD3 Gets a Boost: Contrastive Learning Makes Robotic Arms Smarter
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        National Tsing Hua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces an Exploration-Enhanced Contrastive Learning (EECL) module that improves
                    the exploration capabilities of the TD3 algorithm by providing additional rewards for encountering
                    novel states. This differs from previous work by directly balancing exploration and exploitation
                    through a structured reward system.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet73">
            <div class="start-time-icon" title="Play from here">
                33:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14183" target="_blank">
                        @arXiv 2408.14183
                    </a>
                    <span class="tweet-title">
                        Robot Navigation Gets a Social Upgrade: Deep Learning Helps Bots Avoid Collisions with Kids,
                        Bikes, and More!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Lomonosov Moscow State University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new deep reinforcement learning approach for robot navigation that
                    considers the type of entity the robot interacts with, leading to safer and more efficient
                    navigation. Unlike previous methods that treat all obstacles equally, this approach uses
                    entity-specific information to guide the robot's behavior.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet74">
            <div class="start-time-icon" title="Play from here">
                33:40
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.14325" target="_blank">
                        @arXiv 2408.14325
                    </a>
                    <span class="tweet-title">
                        Wide Neural Networks: Sampling Made Easy with a Twist!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich, University of Turin
                    </span>
                </div>
                <div class="primary-text">
                    This paper explores the use of preconditioned Crank-Nicolson (pCN) and Crank-Nicolson Langevin
                    (pCNL) algorithms for sampling from the reparametrized posterior distribution of weights in Bayesian
                    Neural Networks (BNNs) as the network width increases. This approach differs from previous work by
                    focusing on the parameter space behavior of BNNs and proving that the acceptance probabilities of
                    these algorithms approach 1 as the width of the network grows, regardless of stepsize tuning.
                </div>
            </div>
        </div>


        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Listen and learn ^.^</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202408271724_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/privacy.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading privacy.html:', error));
    </script>
</body>

</html>