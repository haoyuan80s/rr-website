<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                    Fresh Picks:
                    <span class="highlightNumber">33</span> out of <span class="highlightNumber">197</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-08-08"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>

        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">
                00:58
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03340" target="_blank">
                        @arXiv 2408.03340
                    </a>
                    <span class="tweet-title">
                        Frame-tastic! Researchers Find the Sweet Spot for Video Frame Sampling in RAG
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research directly compares various video frame sampling methods for use in Video RAG, a
                    retrieval-augmented generation pattern for multi-modal LLMs. Unlike previous work, it focuses on
                    the
                    trade-off between the number of frames sampled and the retrieval recall score, aiming to
                    identify
                    efficient strategies that maintain high retrieval efficacy while reducing storage and processing
                    demands.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">
                01:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03505" target="_blank">
                        @arXiv 2408.03505
                    </a>
                    <span class="tweet-title">
                        Optimus Prime Time: How to Train Multimodal LLMs Faster Than Ever
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University, Bytedance
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes Optimus, a system that schedules encoder computation within the "bubbles"
                    of
                    LLM training, unlike previous methods that treat the entire model as a single unit.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">
                01:42
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03350" target="_blank">
                        @arXiv 2408.03350
                    </a>
                    <span class="tweet-title">
                        Theorem Proving Goes Long: New Benchmark Tests AI's Contextual Reasoning Skills
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Carnegie Mellon University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces miniCTX, a new benchmark for evaluating neural theorem provers. Unlike
                    previous benchmarks, miniCTX focuses on proving theorems that depend on new definitions, lemmas,
                    and
                    other contextual information that wasn't seen during training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">
                02:05
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03567" target="_blank">
                        @arXiv 2408.03567
                    </a>
                    <span class="tweet-title">
                        Turning Exocentric Videos into Egocentric Gold: A New Recipe for Video Understanding
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        FAIR at Meta, UCLA
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a method called Embed to transform exocentric video-language data into a
                    format suitable for egocentric video representation learning. Unlike previous work that focuses
                    on
                    distilling egocentric cues from exocentric data, Embed actively curates and transforms
                    exocentric
                    data to align with the egocentric style.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">
                02:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03717" target="_blank">
                        @arXiv 2408.03717
                    </a>
                    <span class="tweet-title">
                        Infrared Target Detection: Picking the Best, Not Just Hitting the Target!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nanjing University of Science and Technology, University College London, Nankai University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new attention mechanism called SeRank that uses a non-linear Top-K
                    selection process to preserve the most salient features of infrared small targets, preventing
                    their
                    signal from being diluted by background noise. This differs from previous attention mechanisms
                    that
                    often use linear computations, which can inadvertently merge target features with background
                    noise.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">
                03:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03617" target="_blank">
                        @arXiv 2408.03617
                    </a>
                    <span class="tweet-title">
                        Baby Talk: Can AI Learn Language Like a Toddler?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This study investigates whether child-directed speech, the language spoken to children, is
                    uniquely
                    valuable for training language models. It compares the performance of models trained on this
                    data to
                    those trained on synthetic conversations and a heterogeneous blend of datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">
                03:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03405" target="_blank">
                        @arXiv 2408.03405
                    </a>
                    <span class="tweet-title">
                        Bandits with Brains: How to Make Agents Work Together When They're Not All Equal
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a new type of multi-armed bandit problem where agents have different
                    sensitivities to their environment. This means that the rewards they receive for pulling an arm
                    can
                    vary depending on their individual abilities. The paper then proposes a new algorithm,
                    MIN-WIDTH,
                    that accounts for this heterogeneity and helps agents collaborate more effectively.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">
                04:00
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03733" target="_blank">
                        @arXiv 2408.03733
                    </a>
                    <span class="tweet-title">
                        Neural Networks: Quadratically Many Samples, Zero Error?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This paper analyzes the Bayes-optimal error for learning a single-hidden layer neural network
                    with
                    quadratic activation, extending previous work by considering a quadratic number of samples in
                    the
                    input dimension.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">
                04:21
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03657" target="_blank">
                        @arXiv 2408.03657
                    </a>
                    <span class="tweet-title">
                        Ultrasound Gets a Neural Network Makeover: Sharper Images, Clearer Diagnoses!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Technical University of Munich, Helmholtz Zentrum Munich, Stanford University...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method for enhancing ultrasound image resolution by directly
                    working on B-mode images, which are more commonly available than RF data, using a physics-based
                    deconvolution process and Implicit Neural Representations (INRs).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">
                04:46
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03561" target="_blank">
                        @arXiv 2408.03561
                    </a>
                    <span class="tweet-title">
                        LLMs Get a Makeover: Secure Inference Without the Heavy Lifting!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel fine-tuning framework called MARILL that minimizes the use of
                    secure
                    multi-party computation (MPC) during secure inference of large language models (LLMs). Unlike
                    prior
                    work that focuses on low-level approximations, MARILL introduces high-level architectural
                    changes to
                    reduce the number of expensive operations within MPC.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">
                05:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03703" target="_blank">
                        @arXiv 2408.03703
                    </a>
                    <span class="tweet-title">
                        Vision Transformers Go Mobile: CAS-ViT Makes AI Lighter Than Air!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        SenseTime, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces CAS-ViT, a new type of Vision Transformer that uses convolutional additive
                    self-attention to achieve a balance between efficiency and performance. Unlike previous work
                    that
                    focused on refining the self-attention mechanism, CAS-ViT proposes a novel additive similarity
                    function that simplifies the process of capturing global contextual information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">
                05:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03359" target="_blank">
                        @arXiv 2408.03359
                    </a>
                    <span class="tweet-title">
                        LLMs as Preference Machines: A New Way to Rank Things!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces LAMPO, a framework that uses LLMs to make pairwise comparisons for ordinal
                    classification tasks. Unlike previous methods that rely on pointwise predictions, LAMPO
                    leverages
                    the LLM's ability to make relative judgments, addressing limitations like context length
                    constraints
                    and ordering biases.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">
                06:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03819" target="_blank">
                        @arXiv 2408.03819
                    </a>
                    <span class="tweet-title">
                        AI Learns Like Humans: Counterfactual Data Augmentation with a Twist!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Notre Dame, Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to active learning by leveraging Variation Theory, a
                    theory of human concept learning, to guide the generation of counterfactual data. Unlike
                    previous
                    work that relies on black-box methods for generating counterfactual data, this approach uses
                    neuro-symbolic patterns to define concept boundaries and guide the generation of data that
                    varies
                    along specific dimensions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">
                06:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03551" target="_blank">
                        @arXiv 2408.03551
                    </a>
                    <span class="tweet-title">
                        Vanishing Point: The Secret Sauce for 3D Scene Understanding from a Single Image
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Ulsan National Institute of Science and Technology, CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel framework called VPOcc that leverages vanishing points (VPs) to
                    address the information imbalance caused by camera perspective projection in monocular 3D
                    semantic
                    occupancy prediction. Unlike previous methods that rely on implicit strategies, VPOcc explicitly
                    utilizes VPs to guide feature extraction and aggregation, resulting in more accurate 3D scene
                    understanding.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">
                07:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03761" target="_blank">
                        @arXiv 2408.03761
                    </a>
                    <span class="tweet-title">
                        Fetal Ultrasound Gets a Makeover: AI Summarizes Scans, Saves Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces MMSummary, a novel multimodal summary generation system for medical
                    imaging
                    videos, specifically focusing on fetal ultrasound. Unlike previous work that primarily focused
                    on 2D
                    images or text-based summarization, MMSummary generates a concise representation of the video
                    content, including keyframes, text descriptions, and biometric measurements.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">
                07:37
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03538" target="_blank">
                        @arXiv 2408.03538
                    </a>
                    <span class="tweet-title">
                        Light Speed Relighting: How Gaussian Splats Got a Speed Boost
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new method called Precomputed Radiance Transfer of Gaussian Splats
                    (PRTGS) for real-time relighting. Unlike previous methods that relied on ray tracing or ambient
                    occlusion, PRTGS precomputes the complex transfer functions required for shadows and indirect
                    lighting, significantly accelerating the process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">
                08:03
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03573" target="_blank">
                        @arXiv 2408.03573
                    </a>
                    <span class="tweet-title">
                        LLMs on Trial: A New Way to Test Big Language Models with Less Data
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Tokyo, University of Alberta
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces AcTracer, an active testing framework for LLMs that uses both internal
                    and
                    external information from the model to select a small subset of test data for evaluation. This
                    approach differs from previous active testing methods by leveraging the unique characteristics
                    of
                    LLMs, such as their diverse task-handling abilities and large training data sets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">
                08:33
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03569" target="_blank">
                        @arXiv 2408.03569
                    </a>
                    <span class="tweet-title">
                        Bayesian Optimization: A Smart Way to Find the Best Fit for Your Model
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Technical University of Munich, ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new Bayesian optimization approach for finding the maximum a
                    posteriori
                    (MAP) estimate of model parameters. It utilizes rational polynomial chaos expansions (RPCE) as
                    surrogate models, which are particularly well-suited for approximating frequency response
                    functions.
                    The key innovation lies in the use of a sparse Bayesian learning strategy for RPCE, which
                    effectively reduces the number of model evaluations required for MAP estimation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">
                09:07
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03559" target="_blank">
                        @arXiv 2408.03559
                    </a>
                    <span class="tweet-title">
                        Drone Detectives: Hermit Crabs Get Super-Sized with Deep Learning!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Tokyo
                    </span>
                </div>
                <div class="primary-text">
                    This research combines drone imagery with deep learning techniques, specifically
                    Super-Resolution
                    Reconstruction (SRR) and a modified YOLOv8 model called CRAB-YOLO, to improve the accuracy of
                    hermit
                    crab detection in low-resolution images. This approach differs from previous work by
                    incorporating
                    SRR to enhance image quality before object detection, leading to more precise results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">
                09:30
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03564" target="_blank">
                        @arXiv 2408.03564
                    </a>
                    <span class="tweet-title">
                        Underwater Litter: AASS-isted Deep Learning for a Cleaner Sea!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Tokyo
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel Aerial-Aquatic Speedy Scanner (AASS) system for underwater
                    litter
                    monitoring, combining the efficiency of UAVs with the high-resolution imaging capabilities of
                    ROVs.
                    It also explores the impact of different super-resolution reconstruction (SRR) models and
                    magnification factors on underwater litter detection accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">
                09:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03349" target="_blank">
                        @arXiv 2408.03349
                    </a>
                    <span class="tweet-title">
                        Tapis Takes the Wheel: Smart Scheduling for HPC Jobs
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Texas at Austin
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on developing a smart scheduling capability within the Tapis framework,
                    which
                    automatically determines job configurations and dynamically provisions resources, unlike
                    previous
                    approaches that require users to manually specify these parameters.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">
                10:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03345" target="_blank">
                        @arXiv 2408.03345
                    </a>
                    <span class="tweet-title">
                        AI Can't Solve Math's Biggest Mysteries (Yet)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Warwick, University of Paris 1 Pantheon-Sorbonne
                    </span>
                </div>
                <div class="primary-text">
                    This paper revisits a classic argument about the inherent difficulty of proof discovery in
                    mathematics, updating it to consider recent advances in artificial intelligence. It argues that
                    while AI-powered methods have shown promise in solving certain mathematical problems, they are
                    still
                    limited to problems of low logical complexity and rely on brute-force search techniques.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">
                10:40
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03330" target="_blank">
                        @arXiv 2408.03330
                    </a>
                    <span class="tweet-title">
                        Switching Gears: A New Model for Smoothly Navigating Neural Dynamics
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, Caltech, Columbia University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel Gaussian Process Switching Linear Dynamical System (gpSLDS)
                    model.
                    Unlike previous rSLDS models, the gpSLDS uses a custom kernel function to enforce smooth,
                    locally
                    linear dynamics, addressing limitations like artifactual oscillations and providing posterior
                    uncertainty estimates.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">
                11:03
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03685" target="_blank">
                        @arXiv 2408.03685
                    </a>
                    <span class="tweet-title">
                        Deep Learning Gets a Power Boost: New Library Optimizes Energy Storage
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nanyang Technological University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces RL-ADN, an open-source library for optimizing energy storage systems
                    dispatch in active distribution networks. It differs from previous work by incorporating a data
                    augmentation module based on Gaussian Mixture Models and Copula functions, which enhances the
                    performance of deep reinforcement learning algorithms. Additionally, RL-ADN utilizes the Laurent
                    power flow solver, significantly reducing computational time during training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">
                11:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03599" target="_blank">
                        @arXiv 2408.03599
                    </a>
                    <span class="tweet-title">
                        Neural Network Upgrades: Boosting Performance with Activation Function Extensions!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a framework called "extensions" to unify and explain the performance
                    benefits
                    of various activation functions. It introduces novel techniques that create "extensions" of
                    neural
                    networks by manipulating activation functions, leading to improved performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">
                12:00
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03618" target="_blank">
                        @arXiv 2408.03618
                    </a>
                    <span class="tweet-title">
                        LLMs Get Schooled on Logic: New Framework Makes Arguments Less Fallacious
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        École Polytechnique Fédérale de Lausanne
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework called FIPO that uses preference optimization methods
                    to
                    train LLMs to generate logically sound arguments. Unlike previous work, FIPO incorporates a
                    classification loss that specifically penalizes the model for misclassifying fallacy types.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">
                12:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03463" target="_blank">
                        @arXiv 2408.03463
                    </a>
                    <span class="tweet-title">
                        Survival Subgroups: Unmasking the Hidden Heroes (and Villains) of Treatment
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge, University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method for identifying subgroups of patients with different
                    treatment responses in observational time-to-event data. Unlike previous approaches that
                    primarily
                    focus on randomized controlled trials (RCTs), this method leverages routinely collected
                    observational data, addressing the challenges of non-random treatment assignment and censoring.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">
                12:48
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03433" target="_blank">
                        @arXiv 2408.03433
                    </a>
                    <span class="tweet-title">
                        Supervised and Unsupervised: A Segmentation Model's Double Life
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Mines Paris PSL University, EPFL, Swiss Data Science Center
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a new type of diffusion model called a "hybrid diffusion model" that
                    combines
                    supervised and unsupervised pretraining. This differs from previous work that focused on either
                    supervised or unsupervised pretraining alone.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">
                13:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03338" target="_blank">
                        @arXiv 2408.03338
                    </a>
                    <span class="tweet-title">
                        InLUT3D: A Point Cloud Dataset So Big, It'll Make Your Head Spin!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Lodz University of Technology, Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces the InLUT3D dataset, a collection of 3D point clouds from indoor spaces at
                    the
                    Lodz University of Technology. Unlike previous datasets, InLUT3D focuses on real-world indoor
                    environments, offering a more realistic and challenging testbed for scene understanding
                    algorithms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">
                13:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03568" target="_blank">
                        @arXiv 2408.03568
                    </a>
                    <span class="tweet-title">
                        GANs for Image Recognition: Deep Learning Gets a Boost from a Friendly Rivalry!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        New York University, Northwestern University, Northeastern University...
                    </span>
                </div>
                <div class="primary-text">
                    This research compares the performance of image recognition algorithms based on generative
                    adversarial networks (GANs) with traditional methods. The study focuses on the unique advantages
                    of
                    GANs in handling complex images, noise interference, and varying image quality.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">
                13:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03506" target="_blank">
                        @arXiv 2408.03506
                    </a>
                    <span class="tweet-title">
                        LLMs: Quality Over Quantity, 9 Days to AI Superstar!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Pints.aiLabs
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on training a language model with a smaller, carefully curated dataset,
                    prioritizing quality over quantity. This approach contrasts with the trend of using massive
                    datasets
                    for training LLMs, which often require months of compute time.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">
                14:20
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03872" target="_blank">
                        @arXiv 2408.03872
                    </a>
                    <span class="tweet-title">
                        Time Series Forecasting: When Products Gossip, Predictions Improve!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        IBM, MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new Transformer-based forecasting approach that combines cross-series
                    attention with a shared, multi-task network. This differs from previous work that either focused
                    on
                    individual time series or jointly embedded all variables per time point.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">
                14:45
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2408.03408" target="_blank">
                        @arXiv 2408.03408
                    </a>
                    <span class="tweet-title">
                        LLMs: Not Just for Chatbots, They're Building Compilers Now!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research explores using large language models (LLMs) to build compilers for tensor
                    accelerators, a type of hardware that speeds up computations in machine learning and other
                    fields.
                    Unlike previous approaches that rely on hand-crafted rules or search algorithms, this work
                    leverages
                    LLMs' ability to understand and generate code, potentially making compiler development more
                    agile
                    and efficient.
                </div>
            </div>
        </div>


        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202408080755_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>

</html>