
daily_data = {
    "date": "2024-10-04",
    "tweets": [
            {
                "startTime": "00:56",
                "arxivId": "2410.01776",
                "arxivLink": "https://arxiv.org/abs/2410.01776",
                "title": "Climate Modeling Gets a Generative AI Makeover: Faster, More Accurate, and Less Expensive!",
                "institute": "Google",
                "text": "This research proposes a novel approach to downscaling climate model ensembles by combining dynamical downscaling with generative artificial intelligence. This differs from previous work by leveraging the strengths of both physics-based models and generative models to improve the accuracy and efficiency of downscaling.",
                "paper-title": "Dynamical-generative downscaling of climate model ensembles",
                "image-path": "flux_paper_image/2410.01776_1728030595.png"
            },

            {
                "startTime": "01:18",
                "arxivId": "2410.01804",
                "arxivLink": "https://arxiv.org/abs/2410.01804",
                "title": "Ellipsoids: The New Shape of Real-Time 3D Rendering",
                "institute": "UC San Diego, Google",
                "text": "This paper introduces a new method for real-time 3D rendering that uses constant density ellipsoids as primitives, allowing for exact volume rendering without numerical quadrature. This differs from previous methods like 3D Gaussian Splatting (3DGS) which rely on approximations and suffer from popping artifacts.",
                "paper-title": "EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis",
                "image-path": "flux_paper_image/2410.01804_1728030481.png"
            },

            {
                "startTime": "01:46",
                "arxivId": "2410.01796",
                "arxivLink": "https://arxiv.org/abs/2410.01796",
                "title": "Bellman Diffusion: Deep Learning Meets the Real World, One Linear Step at a Time!",
                "institute": "University of Cambridge, Sony",
                "text": "This paper introduces Bellman Diffusion, a novel deep generative model (DGM) framework that addresses the limitations of existing DGMs in Markov Decision Processes (MDPs). Unlike traditional DGMs, Bellman Diffusion maintains linearity by directly modeling gradient and scalar fields, enabling efficient integration with the Bellman equation.",
                "paper-title": "Bellman Diffusion: Generative Modeling as Learning a Linear Operator in the Distribution Space",
                "image-path": "flux_paper_image/2410.01796_1728030588.png"
            },

            {
                "startTime": "02:03",
                "arxivId": "2410.01792",
                "arxivLink": "https://arxiv.org/abs/2410.01792",
                "title": "Can AI Really Think? New Study Shows Even \"Reasoning\" Models Still Have a \"Next Word\" Hangover!",
                "institute": "Yale University, Princeton University",
                "text": "This research investigates whether a new language model, o1, optimized for reasoning, still exhibits the limitations associated with its origins in next-word prediction. The study compares o1's performance to previous LLMs on various tasks, focusing on the influence of output and task probability.",
                "paper-title": "When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1",
                "image-path": "flux_paper_image/2410.01792_1728030566.png"
            },

            {
                "startTime": "02:25",
                "arxivId": "2410.01805",
                "arxivLink": "https://arxiv.org/abs/2410.01805",
                "title": "LLMs Get a Memory Makeover: How Retaining Heads Help Big Models Think Big!",
                "institute": "Tsinghua University, Hong Kong University of Science and Technology",
                "text": "This research introduces a novel training-based approach for long-context LLM inference, called LOCRET, which utilizes retaining heads to predict the causal importance of each cache unit, enabling more accurate eviction within a fixed cache size. This differs from previous methods that rely on static-sized caches or quantization, which often struggle with memory bottlenecks as context length increases.",
                "paper-title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads",
                "image-path": "flux_paper_image/2410.01805_1728030608.png"
            },

            {
                "startTime": "02:49",
                "arxivId": "2410.01779",
                "arxivLink": "https://arxiv.org/abs/2410.01779",
                "title": "Neural Networks: A Ring Circus of Global Optimizers!",
                "institute": "Meta",
                "text": "This research introduces a novel framework called CoGO (Composing Global Optimizers) that leverages algebraic structures within the weight space of 2-layer neural networks to construct global optimizers for reasoning tasks. Unlike previous work, CoGO analyzes the training dynamics and demonstrates that gradient descent solutions match the theoretical constructions.",
                "paper-title": "Composing Global Optimizers to Reasoning Tasks via Algebraic Objects in Neural Nets",
                "image-path": "flux_paper_image/2410.01779_1728030539.png"
            },

            {
                "startTime": "03:13",
                "arxivId": "2410.01767",
                "arxivLink": "https://arxiv.org/abs/2410.01767",
                "title": "Conformal Prediction Gets a Decision-Making Makeover!",
                "institute": "CMU, University of Amsterdam, Microsoft",
                "text": "This research introduces a new framework for conformal prediction that incorporates information about downstream decision problems, allowing for the generation of prediction sets that minimize a user-specified \"decision loss\" while maintaining statistical coverage guarantees. This differs from previous work on conformal prediction, which focused primarily on statistical coverage without considering the utility of the prediction sets for decision-making.",
                "paper-title": "Decision-Focused Uncertainty Quantification",
                "image-path": "flux_paper_image/2410.01767_1728030464.png"
            },

            {
                "startTime": "03:34",
                "arxivId": "2410.01806",
                "arxivLink": "https://arxiv.org/abs/2410.01806",
                "title": "SambaMOTR: Dancing Tracklets to Track Objects Better!",
                "institute": "ETH Zurich, Max Planck Institute for Informatics, INSAIT",
                "text": "This research introduces Samba, a novel set-of-sequences model that synchronizes multiple state-space models to jointly process tracklets. This approach allows for modeling long-range dependencies within tracklets and interdependencies among them, which is a significant improvement over previous methods that only considered individual tracklets.",
                "paper-title": "Samba: Synchronized Set-of-Sequences Modeling for Multiple Object Tracking",
                "image-path": "flux_paper_image/2410.01806_1728030632.png"
            },

            {
                "startTime": "04:04",
                "arxivId": "2410.01756",
                "arxivLink": "https://arxiv.org/abs/2410.01756",
                "title": "Folding Tokens for Better Image Generation: A New Trick for AI Artists",
                "institute": "Carnegie Mellon University, Adobe Research, MBZUAI",
                "text": "This research proposes ImageFolder, a new image tokenizer that uses product quantization to create spatially aligned tokens. Unlike previous methods, ImageFolder folds these tokens during autoregressive modeling, resulting in a shorter sequence length without sacrificing generation quality.",
                "paper-title": "ImageFolder: Autoregressive Image Generation with Folded Tokens",
                "image-path": "flux_paper_image/2410.01756_1728030556.png"
            },

            {
                "startTime": "04:34",
                "arxivId": "2410.01801",
                "arxivLink": "https://arxiv.org/abs/2410.01801",
                "title": "FabricDiffusion: Turning 2D Clothes into 3D Dreams!",
                "institute": "Texas A&M University, CMU, Google",
                "text": "This research introduces FabricDiffusion, a method that transfers fabric textures from a single clothing image to 3D garments of arbitrary shapes. Unlike previous approaches that rely on 2D-to-3D texture mapping or depth-aware inpainting, FabricDiffusion extracts distortion-free, tileable texture materials that are subsequently mapped onto the UV space of the garment.",
                "paper-title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images",
                "image-path": "flux_paper_image/2410.01801_1728030627.png"
            },

            {
                "startTime": "04:56",
                "arxivId": "2410.01791",
                "arxivLink": "https://arxiv.org/abs/2410.01791",
                "title": "DreamGarden: Growing Games From a Single Prompt, One Leaf at a Time!",
                "institute": "NYU, University of Texas at Austin, Microsoft",
                "text": "This research introduces DreamGarden, an AI system that uses a hierarchical planning approach to generate game environments from a single prompt. Unlike previous work that focuses on generating code or assets, DreamGarden orchestrates multiple specialized AI agents to create a complete simulation.",
                "paper-title": "DreamGarden: A Designer Assistant for Growing Games from a Single Prompt",
                "image-path": "flux_paper_image/2410.01791_1728030638.png"
            },

            {
                "startTime": "05:15",
                "arxivId": "2410.01803",
                "arxivLink": "https://arxiv.org/abs/2410.01803",
                "title": "KANs: The Neural Network That Doesn't Get Stuck in a Rut!",
                "institute": "Caltech",
                "text": "This research compares the performance of Kolmogorov-Arnold Networks (KANs) with Multi-Layer Perceptrons (MLPs) in terms of their ability to learn high-frequency components. It finds that KANs are less biased towards low frequencies than MLPs, suggesting they may be better suited for scientific computing applications.",
                "paper-title": "On the expressiveness and spectral bias of KANs",
                "image-path": "flux_paper_image/2410.01803_1728030500.png"
            },

            {
                "startTime": "05:42",
                "arxivId": "2410.01798",
                "arxivLink": "https://arxiv.org/abs/2410.01798",
                "title": "Windowed MAPF: No More Deadlock Drama!",
                "institute": "CMU",
                "text": "This research introduces a new framework called WinC-MAPF, which guarantees completeness for windowed MAPF solvers. Unlike previous windowed approaches that often get stuck in deadlock, WinC-MAPF uses heuristic updates and agent independence to ensure all agents reach their goals.",
                "paper-title": "Windowed MAPF with Completeness Guarantees",
                "image-path": "flux_paper_image/2410.01798_1728030648.png"
            },

            {
                "startTime": "06:01",
                "arxivId": "2410.01771",
                "arxivLink": "https://arxiv.org/abs/2410.01771",
                "title": "Binary Search Gets a Bayesian Makeover: Faster, Smarter, and Less Spammy!",
                "institute": "University of Cambridge",
                "text": "This paper introduces Bayesian Binary Search (BBS), a probabilistic variant of the classic binary search algorithm. Unlike traditional binary search, BBS leverages machine learning techniques to estimate the probability density of the search space, guiding the search process based on the learned distribution.",
                "paper-title": "Bayesian Binary Search",
                "image-path": "flux_paper_image/2410.01771_1728030561.png"
            },

            {
                "startTime": "06:29",
                "arxivId": "2410.01769",
                "arxivLink": "https://arxiv.org/abs/2410.01769",
                "title": "LLMs: They're Not Just Memorizing, They're Getting Smarter (But Still Have a Valley to Cross)",
                "institute": "Harvard University, MIT",
                "text": "This research introduces SCYLLA, a dynamic evaluation framework that measures the generalization abilities of LLMs by assessing their performance on both in-distribution (ID) and out-of-distribution (OOD) data across varying levels of task complexity. This approach helps disentangle generalization from memorization, which is a key distinction from previous work.",
                "paper-title": "Quantifying Generalization Complexity for Large Language Models",
                "image-path": "flux_paper_image/2410.01769_1728030735.png"
            },

            {
                "startTime": "06:52",
                "arxivId": "2410.01763",
                "arxivLink": "https://arxiv.org/abs/2410.01763",
                "title": "Stereotypes Stick: How Social Coordination Creates and Perpetuates Bias",
                "institute": "University of Toronto",
                "text": "This research uses a computational model of social coordination to demonstrate how pre-existing expectations about how others will behave can create a feedback loop that reinforces stereotypes, even in the absence of biased motivations. This approach differs from previous work that focused on explaining stereotypes through biased motivations or cognitive limitations.",
                "paper-title": "Social coordination perpetuates stereotypic expectations and behaviors across generations in deep multi-agent reinforcement learning",
                "image-path": "flux_paper_image/2410.01763_1728030458.png"
            }
    ],
    "stats": {
        "num_pick": 16,
        "num_total": 33,
    },
    "audio": "https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202410041648_audio.mp3"
}