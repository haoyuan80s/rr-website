<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                    Fresh Picks:
                    <span class="highlightNumber">163</span> out of <span class="highlightNumber">696</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-07-19"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>

        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">
                00:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12929" target="_blank">
                        @arXiv 2407.12929
                    </a>
                    <span class="tweet-title">
                        Foundation Models: More Transparent Than a Fishbowl? (Maybe Not)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, Princeton University, Massachusetts Institute of Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research builds upon the Foundation Model Transparency Index (FMTI) v1.0, which assessed
                    the
                    transparency of leading foundation model developers based on publicly available information.
                    FMTI
                    v1.1 differs by directly requesting transparency reports from developers, potentially including
                    information not previously public.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">
                01:16
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12854" target="_blank">
                        @arXiv 2407.12854
                    </a>
                    <span class="tweet-title">
                        Bigger is Better: A Trillion-Token Datastore Makes Language Models Smarter
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research explores a new dimension of scaling language models by focusing on the size of the
                    datastore used at inference time. Unlike previous work that primarily focused on the size of the
                    pretraining data and the number of parameters, this study investigates how increasing the amount
                    of
                    data available to a retrieval-based language model during inference can improve performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">
                01:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12306" target="_blank">
                        @arXiv 2407.12306
                    </a>
                    <span class="tweet-title">
                        Splatfacto-W: Nerf's New Trick for Making Wild Images Look Real-Time Awesome!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    Splatfacto-W adapts 3D Gaussian Splatting (3DGS) for handling unconstrained image collections,
                    unlike previous methods that relied on 2D models or implicit color prediction, which slowed down
                    rendering.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">
                02:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12858" target="_blank">
                        @arXiv 2407.12858
                    </a>
                    <span class="tweet-title">
                        LLMs: Grounded or Just Making Stuff Up?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Oracle Corporation, Microsoft, Google
                    </span>
                </div>
                <div class="primary-text">
                    This survey paper focuses on the practical challenges of grounding large language models (LLMs)
                    in a
                    specific knowledge base, going beyond simply preventing hallucinations. It explores various
                    approaches like retrieval-augmented generation, constrained decoding, and evaluation methods to
                    ensure that every claim made by an LLM can be traced back to a source within the knowledge base.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">
                02:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13692" target="_blank">
                        @arXiv 2407.13692
                    </a>
                    <span class="tweet-title">
                        LLMs: Learning to Be Legible, One Math Problem at a Time!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        OpenAI
                    </span>
                </div>
                <div class="primary-text">
                    This research explores a novel training method for Large Language Models (LLMs) that focuses on
                    making their outputs more legible to humans. Instead of directly optimizing for answer
                    correctness,
                    the study proposes a "checkability training" approach that involves training a smaller LLM
                    verifier
                    to judge the correctness of the larger LLM's solutions. This method is distinct from previous
                    work
                    that primarily focused on optimizing LLMs for accuracy alone.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">
                02:49
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12783" target="_blank">
                        @arXiv 2407.12783
                    </a>
                    <span class="tweet-title">
                        SMooDi: Text-to-Motion with a Dash of Style!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Northeastern University, Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for generating stylized motion sequences using a
                    pre-trained
                    text-to-motion model. Unlike previous methods that either generate motion of various content or
                    transfer style from one sequence to another, SMooDi can rapidly generate motion across a broad
                    range
                    of content and diverse styles.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">
                03:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12061" target="_blank">
                        @arXiv 2407.12061
                    </a>
                    <span class="tweet-title">
                        Robots Need Context: New Study Shows AI Struggles with Ambiguous Instructions
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Carnegie Mellon University, Meta
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Situated Instruction Following (SIF), a new benchmark for evaluating
                    embodied AI agents' ability to understand and act upon instructions given in real-world
                    contexts.
                    Unlike previous benchmarks that focus on abstract or overly detailed instructions, SIF
                    emphasizes
                    the inherent ambiguity and evolving nature of human communication.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">
                03:44
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12108" target="_blank">
                        @arXiv 2407.12108
                    </a>
                    <span class="tweet-title">
                        Private Prediction: LLMs Get a Privacy Makeover, Generating Thousands of Synthetic Texts!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new private prediction method for generating synthetic text using
                    large
                    language models (LLMs). Unlike previous methods that focused on making the model itself private,
                    this approach only requires the output synthetic data to be differentially private. This allows
                    for
                    the generation of thousands of high-quality synthetic data points, significantly expanding the
                    potential applications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">
                04:04
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12835" target="_blank">
                        @arXiv 2407.12835
                    </a>
                    <span class="tweet-title">
                        AI's Echo Chamber: Can Language Models Learn From Their Own Chatter?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, National University of Singapore, University of Minnesota
                    </span>
                </div>
                <div class="primary-text">
                    This research investigates the impact of training large language models (LLMs) on data generated
                    by
                    other LLMs, a phenomenon termed "regurgitative training." Unlike previous work that focused on
                    simpler generative models, this study examines the effects on state-of-the-art LLMs like GPT-4
                    and
                    Llama2, as well as transformer models trained from scratch.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">
                04:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13710" target="_blank">
                        @arXiv 2407.13710
                    </a>
                    <span class="tweet-title">
                        Fairness Toolkit: No More Bias, Just Better Decisions!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces OxonFair, a toolkit that enforces fairness in machine learning models
                    by
                    optimizing user-defined objectives and fairness constraints. Unlike existing toolkits, OxonFair
                    supports NLP and Computer Vision tasks, uses validation data to prevent overfitting, and allows
                    for
                    customization of fairness metrics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">
                05:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13676" target="_blank">
                        @arXiv 2407.13676
                    </a>
                    <span class="tweet-title">
                        Sound Source Localization: When Sight and Sound Get Hitched!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        KAIST, POSTECH, Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the often-overlooked aspect of cross-modal interaction in sound source
                    localization. Unlike previous work that primarily focused on localization performance, this
                    paper
                    introduces a new benchmark, evaluation metrics, and a learning framework that specifically
                    address
                    the ability of models to understand the relationship between audio and visual signals.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">
                05:30
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12771" target="_blank">
                        @arXiv 2407.12771
                    </a>
                    <span class="tweet-title">
                        Hashtag Hype: Network and Identity Fuel Twitter's Cultural Explosion
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Michigan
                    </span>
                </div>
                <div class="primary-text">
                    This research goes beyond studying the impact of just network or identity on hashtag diffusion.
                    It
                    investigates how these two factors work together to influence the spread of hashtags on Twitter.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">
                05:52
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12751" target="_blank">
                        @arXiv 2407.12751
                    </a>
                    <span class="tweet-title">
                        Scaling Up Bayesian Learning: A Monte Carlo Marathon
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework for stochastic gradient MCMC algorithms, which are
                    particularly useful for Bayesian learning in large-scale datasets. The key innovation lies in
                    the
                    use of control variates to reduce the variance of the gradient estimator, leading to more
                    efficient
                    and scalable inference. This approach differs from previous work by explicitly addressing the
                    challenges of high dimensionality and computational complexity in Bayesian models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">
                06:20
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13755" target="_blank">
                        @arXiv 2407.13755
                    </a>
                    <span class="tweet-title">
                        Deep RL's New Trick: Random Rewards for Smarter Exploration!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces Random Latent Exploration (RLE), a new exploration technique that combines
                    the
                    strengths of bonus-based and noise-based exploration strategies. Unlike previous methods, RLE
                    doesn't add noise or bonuses to the rewards, but instead trains the agent to achieve various
                    goals
                    from a random goal space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">
                06:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12164" target="_blank">
                        @arXiv 2407.12164
                    </a>
                    <span class="tweet-title">
                        Subject-Driven Image Generation: A Preference-Based Approach to Fine-Tuning Diffusion Models
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google, University of Waterloo
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new reward function called the Î»-Harmonic reward function, which
                    enables
                    early stopping during training and accelerates the fine-tuning process for subject-driven
                    text-to-image generation. Unlike previous methods that rely on extensive negative samples or
                    complex
                    text-embedding optimization, this approach leverages preference labels derived from the reward
                    function to achieve text-image alignment by fine-tuning only the UNet component.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">
                07:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12679" target="_blank">
                        @arXiv 2407.12679
                    </a>
                    <span class="tweet-title">
                        Goldfish Swims Through Long Videos, Leaving Other Models in the Dust!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        King Abdullah University of Science and Technology, Harvard University, The Swiss AI Lab
                        IDSIA
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Goldfish, a framework for understanding arbitrarily long videos. Unlike
                    previous methods that struggle with lengthy content, Goldfish uses a retrieval mechanism to
                    identify
                    the most relevant video clips before answering questions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">
                07:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12034" target="_blank">
                        @arXiv 2407.12034
                    </a>
                    <span class="tweet-title">
                        Transformers: Not Just Fancy N-Grams, But Maybe They Should Be!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This paper investigates how well transformer-based language models (LLMs) can be approximated by
                    simple N-gram rules, which are statistical patterns derived from the training data. Unlike
                    previous
                    work that focuses on individual neurons or in-context learning, this study examines the overall
                    model behavior as a black box.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">
                08:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12288" target="_blank">
                        @arXiv 2407.12288
                    </a>
                    <span class="tweet-title">
                        Machine Learning's New Secret Weapon: Information Theory!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a theoretical framework for machine learning that leverages Bayesian
                    statistics and information theory to characterize the performance of an optimal Bayesian
                    learner.
                    Unlike existing analyses that weaken with increasing data complexity, this framework provides
                    accurate insights across diverse machine learning settings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">
                08:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13622" target="_blank">
                        @arXiv 2407.13622
                    </a>
                    <span class="tweet-title">
                        Sparse Q-Learning: When Less is More in Reinforcement Learning
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU, UC Los Angeles, Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This paper explores the impact of sparsity in linear function approximation for reinforcement
                    learning. Unlike previous work that struggles with misspecification errors in non-sparse
                    settings,
                    this research demonstrates that leveraging sparsity can overcome the exponential sample
                    complexity
                    barrier.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">
                09:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13278" target="_blank">
                        @arXiv 2407.13278
                    </a>
                    <span class="tweet-title">
                        Deep Time Series Models: A Benchmarking Bonanza!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research distinguishes itself by offering a comprehensive survey of deep time series
                    models,
                    encompassing various analysis tasks and model architectures. It also introduces a benchmark,
                    Time
                    Series Library (TSLib), for fair evaluation of these models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">
                09:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12178" target="_blank">
                        @arXiv 2407.12178
                    </a>
                    <span class="tweet-title">
                        Stop Exploiting, Start Exploring: Why AI Needs to Embrace Perpetual Curiosity
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research explores a new type of decision-making problem where optimal behavior requires
                    continuous exploration, even as the agent gains knowledge. This contrasts with traditional
                    models
                    where exploration eventually tapers off in favor of exploitation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">
                09:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12185" target="_blank">
                        @arXiv 2407.12185
                    </a>
                    <span class="tweet-title">
                        Deep Learning Goes on a Diet: Satisficing Exploration for Smarter AI
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, Google
                    </span>
                </div>
                <div class="primary-text">
                    This paper extends the concept of "satisficing" exploration, previously explored in multi-armed
                    bandit problems, to deep reinforcement learning. It introduces a new algorithm, Blahut-Arimoto
                    RVF,
                    that allows agents to learn near-optimal solutions without needing to explore the entire state
                    space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">
                10:19
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12999" target="_blank">
                        @arXiv 2407.12999
                    </a>
                    <span class="tweet-title">
                        GenAI Safety: Can We Regulate This Wild West?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google, Office of Naval Research, University of Maryland College Park...
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the gap between GenAI policy and technology, examining how regulation
                    can
                    guide technical evolution and how technology can meet regulatory requirements. It analyzes the
                    policy landscape in the EU, China, and the US, highlighting their divergent priorities. The
                    paper
                    also explores lessons from military risk management and discusses the limitations of model
                    alignment, inspection, and watermarking.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">
                10:46
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12393" target="_blank">
                        @arXiv 2407.12393
                    </a>
                    <span class="tweet-title">
                        LLMs Get a Personality Makeover: From Bland Bots to Characterful Chatters
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new training approach called PersLLM that integrates
                    psychology-grounded
                    principles of personality into LLMs, aiming to create more realistic and consistent
                    personalities
                    compared to previous methods that focused on superficial linguistic styles.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">
                11:15
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12516" target="_blank">
                        @arXiv 2407.12516
                    </a>
                    <span class="tweet-title">
                        Spiking Neural Networks Get a Zeroth-Order Makeover: Training Without Backpropagation!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a new method called Online Pseudo-Zeroth-Order (OPZO) training for spiking
                    neural networks (SNNs). Unlike traditional methods that rely on backpropagation, OPZO uses a
                    single
                    forward pass with noise injection and direct top-down feedback signals for training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">
                11:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12505" target="_blank">
                        @arXiv 2407.12505
                    </a>
                    <span class="tweet-title">
                        Reinforcement Learning Gets a Gravity Makeover: New Framework Makes Multi-Agent Systems
                        Smarter!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework called Subequivariant Hierarchical Neural Networks
                    (SHNN)
                    for multi-entity reinforcement learning in 3D environments. SHNN leverages the concept of
                    subequivariance, which is a relaxed form of equivariance that accounts for gravitational
                    effects, to
                    reduce the complexity of the state space. This is different from previous work that either used
                    hand-crafted local reference frames or did not consider the effects of gravity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">
                12:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13759" target="_blank">
                        @arXiv 2407.13759
                    </a>
                    <span class="tweet-title">
                        Street View Dreams: AI Makes Cityscapes Come Alive
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method for generating long-range, consistent street views using
                    an
                    autoregressive video diffusion model. Unlike previous approaches that rely on 3D models or GANs,
                    this method leverages a large-scale dataset of street view imagery and corresponding map data to
                    create realistic and controllable cityscapes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">
                12:34
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13449" target="_blank">
                        @arXiv 2407.13449
                    </a>
                    <span class="tweet-title">
                        Do All Roads Lead to Rome? Generative Image Models Learn Similar Representations!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This research extends previous work on model stitching by including Normalizing Flows and
                    Diffusion
                    Models, and uses probe-based metrics to assess the semantic similarity of representations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">
                13:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12276" target="_blank">
                        @arXiv 2407.12276
                    </a>
                    <span class="tweet-title">
                        Visual Context Makes CLIP See Anomalies Better!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Chinese Academy of Sciences, University of Technology Sydney, Hangzhou Dianzi University...
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel visual context prompting model (VCP-CLIP) for zero-shot anomaly
                    segmentation (ZSAS). Unlike previous methods that rely on product-specific text prompts,
                    VCP-CLIP
                    utilizes visual context to activate CLIP's anomalous semantic perception ability. It introduces
                    two
                    modules: Pre-VCP and Post-VCP, which incorporate global and fine-grained image features into the
                    text prompts, respectively.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">
                13:32
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13709" target="_blank">
                        @arXiv 2407.13709
                    </a>
                    <span class="tweet-title">
                        DPO's Secret Sauce: How Reference Models Spice Up Language Learning
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Yale University, Shanghai Jiao Tong University
                    </span>
                </div>
                <div class="primary-text">
                    This research delves into the often-overlooked role of reference models in Direct Preference
                    Optimization (DPO), a popular method for fine-tuning large language models. The study
                    investigates
                    how the strength of the KL-divergence constraint, which penalizes deviations from the reference
                    model, impacts performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">
                13:55
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12939" target="_blank">
                        @arXiv 2407.12939
                    </a>
                    <span class="tweet-title">
                        Sparse Images, Full Rooms: A Training-Free 3D Scene Completion Trick
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Carnegie Mellon University, National Tsinghua University, National YangMingChiaoTung
                        University...
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes GenRC, a method for completing 3D room models from sparse RGBD images.
                    Unlike
                    previous methods that rely on human-designed text prompts or predefined camera trajectories,
                    GenRC
                    uses a training-free pipeline that leverages a pre-trained diffusion model and a novel technique
                    called Equirectangular-Diffusion (E-Diffusion) to generate a view-consistent panoramic RGBD
                    image.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">
                14:27
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13690" target="_blank">
                        @arXiv 2407.13690
                    </a>
                    <span class="tweet-title">
                        Math Models Get a Difficulty Boost: New Dataset Makes LLMs Smarter!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Hong Kong University of Science and Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new method called Difficulty-Aware Rejection Tuning (DART) for training
                    large language models (LLMs) to solve mathematical problems. Unlike previous methods that focus
                    on
                    easy queries, DART prioritizes difficult queries during data synthesis, leading to more robust
                    models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">
                14:47
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13137" target="_blank">
                        @arXiv 2407.13137
                    </a>
                    <span class="tweet-title">
                        BEV Segmentation Gets a Boost: Mamba's Global Vision for Self-Driving Cars
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, University of Macau
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to BEV segmentation by incorporating the Mamba model,
                    a
                    state-space model known for its efficiency in long-sequence modeling. This differs from previous
                    methods that primarily focused on improving view transformation modules or using convolutional
                    networks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">
                15:10
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13399" target="_blank">
                        @arXiv 2407.13399
                    </a>
                    <span class="tweet-title">
                        KL-Regularization: The Myth, the Magic, and the Ï 2-Fix!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Illinois, Princeton University, University of Wisconsin-Madison...
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces Ï 2-Preference Optimization (Ï PO), a new algorithm for offline language
                    model
                    alignment that uses Ï 2-divergence instead of KL-divergence for regularization. This change
                    implicitly implements the principle of pessimism, which helps to mitigate overoptimization, a
                    common
                    problem in offline alignment methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">
                15:45
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12259" target="_blank">
                        @arXiv 2407.12259
                    </a>
                    <span class="tweet-title">
                        LLMs are Secretly Doing Gradient Descent: In-Context Probing as a Data Valuation Shortcut
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU
                    </span>
                </div>
                <div class="primary-text">
                    This paper explores the connection between in-context probing (ICP) and influence functions, two
                    methods for data valuation. It proposes that ICP implicitly performs gradient descent, making it
                    a
                    cost-effective proxy for influence functions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">
                16:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13621" target="_blank">
                        @arXiv 2407.13621
                    </a>
                    <span class="tweet-title">
                        Privacy-Preserving AI: Neural Tangent Kernel Gets a Privacy Makeover!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Adobe, University of Hong Kong, University of Wisconsin-Madison...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to ensuring differential privacy in Neural Tangent
                    Kernel
                    (NTK) regression. Unlike previous work that focused on adding noise to gradients during
                    training,
                    this study adds noise directly to the NTK matrix itself, preserving its positive semi-definite
                    property.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">
                16:47
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12176" target="_blank">
                        @arXiv 2407.12176
                    </a>
                    <span class="tweet-title">
                        GPT-4V: Can't Even Write a Radiology Report Yet!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Chicago, University of Michigan
                    </span>
                </div>
                <div class="primary-text">
                    This research systematically evaluates GPT-4V's ability to generate radiology reports, going
                    beyond
                    case studies and qualitative analysis. It decomposes the task into image reasoning and report
                    synthesis, revealing GPT-4V's struggles with interpreting chest X-rays.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">
                17:16
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13083" target="_blank">
                        @arXiv 2407.13083
                    </a>
                    <span class="tweet-title">
                        Sounding Like a Human: New Tech Makes Virtual Avatars Sound More Realistic
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Rochester, Meta
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces the concept of "acoustic primitives" to model the sound field generated
                    by
                    a human body. Unlike previous methods that relied on a single high-order ambisonic
                    representation,
                    this approach uses multiple low-order ambisonic spheres attached to the body, allowing for more
                    efficient and accurate near-field sound rendering.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">
                17:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13729" target="_blank">
                        @arXiv 2407.13729
                    </a>
                    <span class="tweet-title">
                        AI Can't Break the Rules: Baba Is You Tests the Limits of Language Models
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark, Baba Is AI, based on the game Baba Is You, to evaluate
                    the
                    ability of large language models (LLMs) to manipulate and combine rules in a dynamic
                    environment.
                    Unlike previous benchmarks, Baba Is AI focuses on compositional generalization, where the rules
                    of
                    the game itself can be changed by the agent.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">
                18:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12370" target="_blank">
                        @arXiv 2407.12370
                    </a>
                    <span class="tweet-title">
                        Time Travel for Graphs: How Much Past Matters for Predicting the Future?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Conservatoire National des Arts et MÃ©tiers, Sorbonne University, National Institute of
                        Applied
                        Sciences of Rouen...
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the "temporal receptive field" in dynamic graph learning, which refers
                    to
                    the amount of past data a model considers when making predictions. Unlike previous work that
                    primarily focused on optimizing the temporal encoding process, this study systematically
                    analyzes
                    the impact of different temporal receptive field sizes on model performance across various
                    datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">
                18:31
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12312" target="_blank">
                        @arXiv 2407.12312
                    </a>
                    <span class="tweet-title">
                        Skeleton Mix-Up: Shapley Value Makes Action Recognition Smarter!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Shap-Mix, a novel method for long-tailed skeleton-based action
                    recognition.
                    Unlike previous methods, Shap-Mix utilizes Shapley value to estimate the saliency of different
                    body
                    parts, guiding the mixing of data samples to improve the model's ability to learn from scarce
                    data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">
                18:54
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12207" target="_blank">
                        @arXiv 2407.12207
                    </a>
                    <span class="tweet-title">
                        6D Object Pose Estimation: No CAD Models, Just a Few Snapshots!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich, The University of Queensland
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a pipeline for 6D object pose estimation that doesn't require CAD models,
                    unlike many existing methods. Instead, it uses a neural implicit surface representation (NeuS2)
                    trained on a small set of real images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">
                19:23
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12794" target="_blank">
                        @arXiv 2407.12794
                    </a>
                    <span class="tweet-title">
                        Query Rewrite Gets a Brain: AI Learns to Optimize SQL Like a Boss!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to query rewrite by combining Equality Saturation with
                    Reinforcement Learning. Unlike previous methods that rely on predefined rule orderings, this
                    system
                    learns to navigate the complex search space of equivalent query plans, leading to more efficient
                    and
                    faster results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">
                19:47
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13094" target="_blank">
                        @arXiv 2407.13094
                    </a>
                    <span class="tweet-title">
                        Video Models: Can They Really Understand What's Happening?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Meta, Johns Hopkins University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new evaluation task called "retrieval from counterfactually augmented
                    data" (RCAD). Unlike standard video-text retrieval, RCAD uses captions that are modified to have
                    the
                    same objects but different actions, forcing models to understand the video's semantics over
                    time.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">
                20:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13739" target="_blank">
                        @arXiv 2407.13739
                    </a>
                    <span class="tweet-title">
                        Granite Code Models: Now with 128K Tokens, They're Not Just Big, They're Long!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a method for scaling the context length of Granite code models from
                    2K/4K
                    to 128K tokens. This is achieved through a combination of continual pretraining and instruction
                    tuning, using a repository-level file packing approach and length-upsampled long-context data.
                    This
                    differs from previous work by focusing on extending the context length of open-source code
                    models,
                    rather than relying on proprietary models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">
                20:37
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12318" target="_blank">
                        @arXiv 2407.12318
                    </a>
                    <span class="tweet-title">
                        Dynamic Games: Information Compression for Smarter Strategies
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Michigan
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces two new notions of information states, Mutually Suï¬cient Information (MSI)
                    and
                    Unilaterally Suï¬cient Information (USI), for dynamic games with asymmetric information. These
                    concepts are based on strategy-independent compression maps, which differ from previous work
                    that
                    often relies on strategy-dependent maps.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">
                21:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12870" target="_blank">
                        @arXiv 2407.12870
                    </a>
                    <span class="tweet-title">
                        Cell Recognition: Beyond Looks, It's All About the Neighborhood!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Sydney, Lawrence Berkeley National Laboratory, University of Maryland...
                    </span>
                </div>
                <div class="primary-text">
                    This research goes beyond just looking at individual cells to understand their type. It focuses
                    on
                    the relationships between cells and their surrounding tissue, which is more robust to variations
                    in
                    how samples are prepared.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">
                21:34
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12874" target="_blank">
                        @arXiv 2407.12874
                    </a>
                    <span class="tweet-title">
                        LLMs Learn to Follow Instructions... By Making Up Their Own Homework!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Carnegie Mellon University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method called SELF-GUIDE, which allows a language model to
                    improve
                    its ability to follow instructions by generating its own training data. This differs from
                    previous
                    work that relies on external data or more powerful "teacher" models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">
                21:58
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12307" target="_blank">
                        @arXiv 2407.12307
                    </a>
                    <span class="tweet-title">
                        Hand Reconstruction: Knowledge is Power, Uncertainty is Key!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Rensselaer, IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a weakly-supervised method for 3D hand reconstruction that leverages
                    hand
                    knowledge from biomechanics, functional anatomy, and physics. Unlike previous methods that rely
                    solely on data-driven priors or heuristic constraints, this approach systematically incorporates
                    these foundational insights into the training process. Additionally, the paper explicitly models
                    the
                    uncertainty inherent in image observations, improving the robustness of the model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">
                22:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13188" target="_blank">
                        @arXiv 2407.13188
                    </a>
                    <span class="tweet-title">
                        Watermarking AI Art: A QR Code in Every Pixel?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new framework called Safe-SD that embeds graphical watermarks directly
                    into
                    the generative process of Stable Diffusion models, unlike previous methods that watermark images
                    after they are generated.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">
                22:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12828" target="_blank">
                        @arXiv 2407.12828
                    </a>
                    <span class="tweet-title">
                        LLMs: Knowledge Editing's Messy Ripple Effects
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Illinois, Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces GradSim, a new indicator that measures the similarity of knowledge
                    storage
                    in LLMs based on gradient cosine similarity. This helps explain why knowledge editing often
                    leads to
                    unexpected ripple effects, where changes to one fact unintentionally affect other related facts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">
                23:21
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13322" target="_blank">
                        @arXiv 2407.13322
                    </a>
                    <span class="tweet-title">
                        Heart Rate Hack: New AI Learns Your Pulse Without Touching You!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        National Tsing Hua University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on test-time adaptation (TTA) for remote photoplethysmography (rPPG)
                    estimation, a technique that adapts a pre-trained model to new data without needing additional
                    training data. This differs from previous work that relied on domain generalization (DG) or
                    domain
                    adaptation (DA) techniques, which require access to source data during training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">
                23:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12277" target="_blank">
                        @arXiv 2407.12277
                    </a>
                    <span class="tweet-title">
                        Visual Question Answering Gets a Multimodal Makeover: Reranking for the Win!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU, Google, University of Massachusetts
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a multi-modal reranker module to improve the ranking of knowledge
                    candidates in knowledge-intensive visual question answering (KI-VQA) systems. Unlike previous
                    work
                    that relies on uni-modal retrieval, this approach leverages both visual and textual information
                    from
                    the question and knowledge candidates for more accurate relevance score modeling.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">
                24:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12622" target="_blank">
                        @arXiv 2407.12622
                    </a>
                    <span class="tweet-title">
                        GEBD Models: Faster Than a Speeding Bullet, More Accurate Than a Laser Beam!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Xiâan Jiaotong University, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on improving the efficiency of Generic Event Boundary Detection (GEBD)
                    models
                    by re-examining their architecture. The authors propose a new baseline model, BasicGEBD, which
                    achieves comparable performance to more complex models. They then systematically "modernize"
                    each
                    component of BasicGEBD, resulting in a family of EfficientGEBD models that achieve
                    state-of-the-art
                    performance with significantly faster inference speeds.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">
                24:31
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12491" target="_blank">
                        @arXiv 2407.12491
                    </a>
                    <span class="tweet-title">
                        Building a BEV Perception System: Drag-and-Drop Your Way to Self-Driving!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a hierarchical perception system for autonomous driving, which uses a
                    library
                    of pre-trained modules that can be combined and customized to create different perception
                    models.
                    This approach differs from previous work by emphasizing modularity and reusability, streamlining
                    the
                    development process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">
                24:55
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13764" target="_blank">
                        @arXiv 2407.13764
                    </a>
                    <span class="tweet-title">
                        Shape of Motion: Dancing 3D Scenes from a Single Video!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley, Google Research
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a method for reconstructing dynamic 3D scenes from a single video by
                    representing the motion of scene elements as a combination of shared motion bases. This differs
                    from
                    previous work that either relies on templates, focuses on quasi-static scenes, or fails to
                    explicitly model 3D motion.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">
                25:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12943" target="_blank">
                        @arXiv 2407.12943
                    </a>
                    <span class="tweet-title">
                        LLMs Gone Wild? New Tool Detects Hallucinations with a Side of Critique!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Shanghai Jiao Tong University, Fudan University, Shanghai Artificial Intelligence
                        Laboratory...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces HALU-J, a hallucination detection model that goes beyond simple
                    classification. It analyzes multiple pieces of evidence, categorizes them, and provides detailed
                    critiques, making it more reliable than previous methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">
                25:41
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12773" target="_blank">
                        @arXiv 2407.12773
                    </a>
                    <span class="tweet-title">
                        AI Detects Cancer Mitosis: A Deep Learning Framework That's Got Your Back (and Your Cells)!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University College London, Royal National Orthopaedic Hospital, University Hospital Basel...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel two-stage framework for detecting mitotic figures in cancer
                    cells.
                    Unlike previous methods that rely solely on bounding boxes, this approach incorporates nuclei
                    contours, which significantly improves the accuracy of detection.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">
                26:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13219" target="_blank">
                        @arXiv 2407.13219
                    </a>
                    <span class="tweet-title">
                        Video Grounding: The Secret Sauce for Long Video Generation?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to long video generation by leveraging multi-sentence
                    video
                    grounding. Unlike previous methods that rely solely on generative models, this approach utilizes
                    a
                    massive video moment retrieval model to find relevant video segments that match text prompts,
                    ensuring temporal consistency and adherence to physical laws.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">
                26:27
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12804" target="_blank">
                        @arXiv 2407.12804
                    </a>
                    <span class="tweet-title">
                        Stop Clicking! New Study Shows How to Curb Over-Reliance on AI
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge, Carnegie Mellon University, Princeton University...
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the use of "frictions" - small modifications to user interfaces - to
                    encourage more thoughtful use of LLMs. Unlike previous work focusing on restricting access or
                    filtering content, this study investigates how selective frictions, based on user expertise, can
                    modulate LLM engagement without significantly impacting accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">
                26:52
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12404" target="_blank">
                        @arXiv 2407.12404
                    </a>
                    <span class="tweet-title">
                        Steering Language Models: A Wild Ride with Unreliable Vectors
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University College London
                    </span>
                </div>
                <div class="primary-text">
                    This research delves into the reliability and generalizability of steering vectors, a technique
                    for
                    adjusting language model behavior at inference time. Unlike previous work that primarily focused
                    on
                    in-distribution performance, this study investigates both in-distribution reliability and
                    out-of-distribution generalization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">
                27:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12982" target="_blank">
                        @arXiv 2407.12982
                    </a>
                    <span class="tweet-title">
                        Forget Big Models, Let's Talk to the Library: Retrieval-Enhanced Machine Learning is Here!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU, University of Massachusetts
                    </span>
                </div>
                <div class="primary-text">
                    This research formalizes the framework of Retrieval-Enhanced Machine Learning (REML) by
                    synthesizing
                    existing studies across various domains, including computer vision, time series prediction, and
                    computational biology. It also bridges the gap between REML and Information Retrieval (IR)
                    research
                    by investigating each component of the REML framework.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">
                27:48
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12996" target="_blank">
                        @arXiv 2407.12996
                    </a>
                    <span class="tweet-title">
                        Deep Learning Ensembles: Sharpness and Diversity, a Love-Hate Relationship!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nankai University, Dartmouth College, University of California San Diego...
                    </span>
                </div>
                <div class="primary-text">
                    This research investigates the interplay between sharpness and diversity in deep ensembles,
                    revealing a trade-off: minimizing sharpness can reduce diversity, potentially hindering ensemble
                    performance. The paper proposes SharpBalance, a novel training approach that addresses this
                    trade-off by balancing sharpness and diversity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">
                28:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12784" target="_blank">
                        @arXiv 2407.12784
                    </a>
                    <span class="tweet-title">
                        LLM Agents on Trial: How a Tiny Trigger Can Hijack Your AI Assistant
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Chicago
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on backdoor attacks against LLM agents that use retrieval-augmented
                    generation
                    (RAG) systems. Unlike previous work that targeted LLMs or RAG systems individually, this paper
                    proposes a novel attack, AGENTPOISON, that specifically targets the memory or knowledge base of
                    these agents.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon" title="Play from here">
                28:48
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13237" target="_blank">
                        @arXiv 2407.13237
                    </a>
                    <span class="tweet-title">
                        LLMs: Not Just for Chatbots, They're Now Helping Robots Learn Faster!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method called LLM-Empowered State Representation (LESR) that
                    utilizes
                    large language models (LLMs) to generate task-related state representations for reinforcement
                    learning (RL) agents. Unlike previous work that relies on extensive sample learning, LESR
                    leverages
                    LLM's knowledge to enhance state representations, leading to improved sample efficiency and
                    performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon" title="Play from here">
                29:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13342" target="_blank">
                        @arXiv 2407.13342
                    </a>
                    <span class="tweet-title">
                        Smoothing Out the Rough Edges: A New Filter for 3D Point Cloud Reconstruction
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel implicit filtering technique for learning neural signed
                    distance
                    functions (SDFs) from 3D point clouds. Unlike previous methods that focus on individual points,
                    this
                    approach leverages the geometric information within the neighborhood of each point to smooth the
                    implicit field while preserving high-frequency details.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon" title="Play from here">
                29:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12883" target="_blank">
                        @arXiv 2407.12883
                    </a>
                    <span class="tweet-title">
                        Reasoning Retrieval: It's Not Just Keywords, It's Logic!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Hong Kong, Princeton University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces BRIGHT, a new benchmark for text retrieval that focuses on queries
                    requiring complex reasoning, unlike previous benchmarks that primarily rely on keyword or
                    semantic
                    matching.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet67">
            <div class="start-time-icon" title="Play from here">
                30:03
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13541" target="_blank">
                        @arXiv 2407.13541
                    </a>
                    <span class="tweet-title">
                        Self-Supervised Learning: Crowded Features Need a Crowd Control!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Chinese Academy of Sciences, Tsinghua University, Hong Kong University of Science and
                        Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research identifies a "crowding problem" in self-supervised learning (SSL) methods, where
                    features from different classes are not distinctly separated. The paper proposes a novel method
                    called Dynamic Semantic Adjuster (DSA) to address this issue by explicitly encouraging the
                    clustering of similar samples and the separation of dissimilar samples. This approach differs
                    from
                    previous SSL methods that primarily focus on aligning features from the same sample.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet68">
            <div class="start-time-icon" title="Play from here">
                30:33
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12844" target="_blank">
                        @arXiv 2407.12844
                    </a>
                    <span class="tweet-title">
                        LLMs: Smarter Than We Thought? A Tiny Benchmark Reveals Their True Potential!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Helmholtz Munich, University of Cambridge
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces "metabench," a sparse benchmark that uses psychometric techniques to
                    identify the most informative items from six existing benchmarks. This approach aims to reduce
                    redundancy and improve efficiency in evaluating LLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet69">
            <div class="start-time-icon" title="Play from here">
                30:54
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13734" target="_blank">
                        @arXiv 2407.13734
                    </a>
                    <span class="tweet-title">
                        Diffusion Models Get a Reinforcement Learning Makeover: Rewarding Results!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Genentech, Princeton University
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the use of reinforcement learning (RL) algorithms to fine-tune diffusion
                    models, focusing on optimizing downstream reward functions. Unlike previous work that primarily
                    focused on standard fine-tuning methods, this paper delves into the unique aspects of RL-based
                    fine-tuning, highlighting its advantages and limitations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet70">
            <div class="start-time-icon" title="Play from here">
                31:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12399" target="_blank">
                        @arXiv 2407.12399
                    </a>
                    <span class="tweet-title">
                        Simplifying Data's Topology: A Practical Solver for a Knotty Problem
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Sorbonne University, University of Arizona
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a practical solver for topological simplification, focusing on
                    optimizing
                    the cancellation of "non-signal" persistence pairs while preserving "signal" pairs. Unlike
                    previous
                    methods, this approach is not restricted to persistence pairs involving extrema, allowing it to
                    address a broader class of topological features, particularly saddle pairs in three-dimensional
                    scalar data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet71">
            <div class="start-time-icon" title="Play from here">
                31:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13185" target="_blank">
                        @arXiv 2407.13185
                    </a>
                    <span class="tweet-title">
                        NeRF's Got Moves: Kalman Filter Makes Dynamic Scenes Sing!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        The University of Tokyo, Kyoto University, Shanghai Artificial Intelligence Laboratory
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces KFD-NeRF, a dynamic neural radiance field that uses a Kalman filter to
                    estimate motion. Unlike previous methods that rely solely on observations or predictions,
                    KFD-NeRF
                    combines both sources of knowledge for more accurate deformation estimations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet72">
            <div class="start-time-icon" title="Play from here">
                32:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12229" target="_blank">
                        @arXiv 2407.12229
                    </a>
                    <span class="tweet-title">
                        Laugh Now, Cry Later: AI Makes Speech Sound More Human
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        National Taiwan University, Microsoft Corporation
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces EmoCtrl-TTS, a text-to-speech model that can generate speech with
                    varying
                    emotions and non-verbal vocalizations (NVs) like laughter and crying. Unlike previous models
                    that
                    focused on controlling emotions at the utterance level, EmoCtrl-TTS can mimic the emotional
                    changes
                    within a single utterance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet73">
            <div class="start-time-icon" title="Play from here">
                32:46
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13696" target="_blank">
                        @arXiv 2407.13696
                    </a>
                    <span class="tweet-title">
                        Benchmarking Benchmarks: A Guide to Getting Agreement Testing Right
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on the methodological inconsistencies in Benchmark Agreement Testing (BAT)
                    and
                    proposes a set of best practices to improve its reliability and validity. Unlike previous work,
                    it
                    analyzes a large dataset of benchmarks and models to demonstrate the impact of different
                    methodological choices on BAT results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet74">
            <div class="start-time-icon" title="Play from here">
                33:07
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13466" target="_blank">
                        @arXiv 2407.13466
                    </a>
                    <span class="tweet-title">
                        Robots Learn New Tricks with Language-Powered Brains!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Volkswagen Group, Technical University of Munich, University of Zurich...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a model-based multi-task reinforcement learning approach that leverages
                    pre-trained language models to extract semantically meaningful task representations. This
                    differs
                    from previous work, which primarily focused on model-free methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet75">
            <div class="start-time-icon" title="Play from here">
                33:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12128" target="_blank">
                        @arXiv 2407.12128
                    </a>
                    <span class="tweet-title">
                        TTA's New Trick: Aligning Data to Save the Day!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Concordia University, University of Toronto, Beijing Jiaotong University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel Distribution Alignment (DA) loss for Test-Time Adaptation (TTA)
                    that
                    aligns test-time feature distributions with the source distributions, addressing the challenges
                    posed by label shifts across online data batches. Unlike previous methods that adapt the model
                    to
                    the test data, this approach aligns the test data to the source model, ensuring compatibility
                    and
                    preventing degradation from conflicting optimization objectives.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet76">
            <div class="start-time-icon" title="Play from here">
                34:04
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13108" target="_blank">
                        @arXiv 2407.13108
                    </a>
                    <span class="tweet-title">
                        Universal Image Super-Resolution: One Model to Rule Them All!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Science and Technology of China, National University of Singapore, Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a universal framework for compressed image super-resolution (CSR), dubbed
                    UCIP, which can handle distortions from various compression codecs, unlike previous works that
                    typically focused on a single codec like JPEG.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet77">
            <div class="start-time-icon" title="Play from here">
                34:46
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13677" target="_blank">
                        @arXiv 2407.13677
                    </a>
                    <span class="tweet-title">
                        Building 3D Shapes with LEGOs: A Transformer-Powered Approach to Part-Aware Generation
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces PASTA, a generative model that uses an autoregressive transformer to
                    create
                    3D shapes as sequences of cuboidal primitives. Unlike previous methods that treat objects
                    holistically, PASTA considers the underlying part-based structure, enabling more control and
                    flexibility in shape generation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet78">
            <div class="start-time-icon" title="Play from here">
                35:10
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13429" target="_blank">
                        @arXiv 2407.13429
                    </a>
                    <span class="tweet-title">
                        Time Series Sleuth: AI Learns to Pick the Best Medical Tests
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich, Helmholtz Zentrum MÃ¼nchen
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to dynamic feature acquisition (DFA) in time series
                    data,
                    specifically focusing on medical time series. Unlike previous work that primarily relied on
                    reinforcement learning, this study utilizes a conditional mutual information (CMI) maximization
                    approach to train an acquirer model end-to-end.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet79">
            <div class="start-time-icon" title="Play from here">
                35:34
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12940" target="_blank">
                        @arXiv 2407.12940
                    </a>
                    <span class="tweet-title">
                        Driving AI Gets a Kinematic Makeover: Simulating Realistic Agents with Fewer Parameters!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Science and Technology of China, Mach Drive, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new autoregressive paradigm for trajectory generation that focuses on
                    modeling the distribution of control actions rather than the trajectory states themselves. This
                    approach reduces the complexity of the task and ensures physical feasibility by directly
                    modeling
                    the cause (control actions) instead of the effect (trajectories).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet80">
            <div class="start-time-icon" title="Play from here">
                36:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12718" target="_blank">
                        @arXiv 2407.12718
                    </a>
                    <span class="tweet-title">
                        Slim Down Your Diffusion Models: One-Step Generation with Rectified Flow
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich, University of Texas at Austin
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on training smaller, more efficient one-step diffusion models by combining
                    model size reduction with the rectified flow framework. Unlike previous work, it addresses the
                    challenges of initialization mismatch and underperformance during distillation for smaller
                    models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet81">
            <div class="start-time-icon" title="Play from here">
                36:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13605" target="_blank">
                        @arXiv 2407.13605
                    </a>
                    <span class="tweet-title">
                        Urban Flow Prediction: When Physics Meets Data, Chaos Gets Reweighted!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Queensland, Beijing University of Posts and Telecommunications, Peking
                        University
                    </span>
                </div>
                <div class="primary-text">
                    This research tackles the issue of physical inconsistency in urban flow prediction models.
                    Unlike
                    previous work that assumes perfect data alignment with physical laws, this paper proposes a
                    novel
                    active sample reweighting framework that accounts for data quality and model uncertainty.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet82">
            <div class="start-time-icon" title="Play from here">
                36:59
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12435" target="_blank">
                        @arXiv 2407.12435
                    </a>
                    <span class="tweet-title">
                        Human-Object Interactions: A Fine-Grained Look at the Little Things
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        The Chinese University of Hong Kong State Key Laboratory of General Artificial Intelligence
                        BIGAI Institute for AI Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset called Semantic-HOI, which provides fine-grained
                    descriptions
                    of human-object interactions at the state level, unlike previous datasets that only offer
                    coarse-grained descriptions of the entire interaction process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet83">
            <div class="start-time-icon" title="Play from here">
                37:33
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13221" target="_blank">
                        @arXiv 2407.13221
                    </a>
                    <span class="tweet-title">
                        Ranking Labels Like a Movie Critic: New AI Learns to Spot the Most Relevant Scenes!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tencent, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method for ranking labels based on their relevance to
                    multimodal
                    inputs, particularly video clips. Unlike previous label ranking methods that focus on
                    single-modality data and object labels, this approach leverages a reinforcement learning
                    framework
                    to learn the relevance of labels in a more nuanced way, considering both visual and textual
                    information.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet84">
            <div class="start-time-icon" title="Play from here">
                38:03
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12859" target="_blank">
                        @arXiv 2407.12859
                    </a>
                    <span class="tweet-title">
                        Chatty Bots Ask the Right Questions: AI Makes Data Exploration Conversational
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        IBM Research India
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a system that generates natural language questions about tabular data,
                    specifically aggregate questions, in a conversational setting. Unlike previous work that focuses
                    on
                    visualizations or single-hop questions, this system leverages interestingness measures and a
                    fine-tuned language model to recommend relevant questions for deeper data exploration.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet85">
            <div class="start-time-icon" title="Play from here">
                38:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13300" target="_blank">
                        @arXiv 2407.13300
                    </a>
                    <span class="tweet-title">
                        ASR Error Correction: When Less is More!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel data filtering method for training error correction (EC) models.
                    Unlike previous work that focuses on discarding pairs with large edit distances, this study
                    introduces two criteria based on linguistic acceptability and inferability from context.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet86">
            <div class="start-time-icon" title="Play from here">
                38:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13095" target="_blank">
                        @arXiv 2407.13095
                    </a>
                    <span class="tweet-title">
                        Zero-Shot Learning: Audio-Visual Recognition Made Easy!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Carnegie Mellon University, University of Wisconsin-Madison
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework for audio-visual generalized zero-shot learning
                    (AVGZSL)
                    that simplifies the process by aligning audio-visual embeddings with transformed text
                    representations. Unlike previous approaches that relied on complex auto-encoders, this method
                    utilizes a single supervised contrastive loss to learn the alignment between audio-visual and
                    textual modalities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet87">
            <div class="start-time-icon" title="Play from here">
                39:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13555" target="_blank">
                        @arXiv 2407.13555
                    </a>
                    <span class="tweet-title">
                        PetFace: A Face-tastic Dataset for Animal ID!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Kyoto University, The University of Tokyo
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a large-scale dataset called PetFace, which contains over 257,000
                    unique
                    animal individuals across 13 families and 319 breeds. This dataset is significantly larger than
                    previous animal face datasets, which typically included less than 100 individuals.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet88">
            <div class="start-time-icon" title="Play from here">
                39:54
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12680" target="_blank">
                        @arXiv 2407.12680
                    </a>
                    <span class="tweet-title">
                        AI Detectives: Busting Bias in Medical Textbooks!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Connecticut, Worcester Polytechnic Institute, University of Oxford...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel dataset, BRICC, specifically designed to identify and annotate
                    instances of bias in medical educational materials. It then uses this dataset to train AI models
                    for
                    detecting bias in medical text, going beyond previous work that focused on general bias
                    detection in
                    other domains.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet89">
            <div class="start-time-icon" title="Play from here">
                40:22
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12543" target="_blank">
                        @arXiv 2407.12543
                    </a>
                    <span class="tweet-title">
                        Models Think Like Us? New Test Checks If AI's Got the Right Brain Wiring!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces "abstraction alignment," a method to assess how well a machine learning
                    model's understanding of concepts aligns with human understanding. Unlike previous work that
                    analyzes concepts in isolation, this method examines the relationships between concepts,
                    revealing
                    how the model structures its knowledge.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet90">
            <div class="start-time-icon" title="Play from here">
                40:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12275" target="_blank">
                        @arXiv 2407.12275
                    </a>
                    <span class="tweet-title">
                        Transformers Can't Compose? Bottleneck to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich, Google, University of Montreal
                    </span>
                </div>
                <div class="primary-text">
                    This research investigates the ability of transformers to generalize compositionally in an
                    in-context learning setting. Unlike previous work that focused on gradient-based meta-learning,
                    this
                    study explores the limitations of transformers in this context and proposes a novel
                    architectural
                    solution.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet91">
            <div class="start-time-icon" title="Play from here">
                41:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13068" target="_blank">
                        @arXiv 2407.13068
                    </a>
                    <span class="tweet-title">
                        Krait Strikes: Backdoor Attack Targets Graph Prompt Tuning
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Pittsburgh, CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the vulnerability of graph prompt tuning to backdoor attacks, a novel
                    attack
                    vector that disguises malicious triggers as benign prompts. Unlike previous work focusing on
                    traditional GNNs, this study specifically investigates the unique challenges posed by graph
                    prompt
                    tuning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet92">
            <div class="start-time-icon" title="Play from here">
                41:39
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12322" target="_blank">
                        @arXiv 2407.12322
                    </a>
                    <span class="tweet-title">
                        Skeleton Action Recognition: When Transformers Get a Frequency Boost!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU, Microsoft, University of North Carolina at Charlotte...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a Frequency-aware Mixed Transformer (FreqMixFormer) for skeleton action
                    recognition. Unlike previous transformer-based approaches that rely solely on spatial features,
                    FreqMixFormer incorporates frequency features, enabling it to better distinguish subtle
                    movements.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet93">
            <div class="start-time-icon" title="Play from here">
                42:09
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12197" target="_blank">
                        @arXiv 2407.12197
                    </a>
                    <span class="tweet-title">
                        Soft Robots Get a Sixth Sense: Predicting the Future with Multi-Modal Perception
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        SantâAnna School of Advanced Studies, University College London
                    </span>
                </div>
                <div class="primary-text">
                    This research builds upon previous work using generative models for soft robot perception by
                    introducing modality-specific encoders and decoders, allowing for late fusion and early decoding
                    capabilities. This approach enables a deeper understanding of the latent representation and its
                    influence on sensory prediction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet94">
            <div class="start-time-icon" title="Play from here">
                42:34
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12508" target="_blank">
                        @arXiv 2407.12508
                    </a>
                    <span class="tweet-title">
                        LLMs: The New Search Party for Videos!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Korea Advanced Institute of Science and Technology, UC Berkeley, Seoul National University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces MERLIN, a novel training-free pipeline that leverages LLMs for
                    iterative
                    feedback learning to refine query embeddings in text-video retrieval. Unlike previous work,
                    MERLIN
                    focuses on addressing the discrepancy between user queries and the content retrieved, enhancing
                    alignment between queries and video content through a dynamic question answering process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet95">
            <div class="start-time-icon" title="Play from here">
                43:05
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13751" target="_blank">
                        @arXiv 2407.13751
                    </a>
                    <span class="tweet-title">
                        Stock Market Similarity: A Time Machine for Your Portfolio?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford, Ulsan National Institute of Science and Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces SimStock, a novel temporal self-supervised learning framework that
                    combines
                    techniques from self-supervised learning (SSL) and temporal domain generalization to learn
                    robust
                    and informative representations of financial time series data. This approach differs from
                    previous
                    work by explicitly addressing the non-stationary nature of financial markets, enabling the model
                    to
                    adapt to distribution shifts and generalize well to future time periods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet96">
            <div class="start-time-icon" title="Play from here">
                43:34
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12877" target="_blank">
                        @arXiv 2407.12877
                    </a>
                    <span class="tweet-title">
                        LLMs Get a Peer Review: New Framework Makes AI Writing Better
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft, Indian Institute of Technology Kharagpur
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel evaluation framework for NLG, called ReFeR, which uses LLMs as
                    evaluators and feedback providers in a system akin to academic peer review. This approach
                    differs
                    from previous work by incorporating a multi-dimensional evaluation schema and generating
                    constructive feedback for model refinement.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet97">
            <div class="start-time-icon" title="Play from here">
                43:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12117" target="_blank">
                        @arXiv 2407.12117
                    </a>
                    <span class="tweet-title">
                        Training a 7B LLM with 1 Million Tokens? No Problem! Memo to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, Tencent
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes Memo, a novel LLM training framework that tackles the memory challenges
                    of
                    long context training by introducing a fine-grained activation recomputation and swapping
                    mechanism.
                    This approach differs from previous work by optimizing memory usage at both the tensor and token
                    levels, minimizing redundant computation and communication overhead.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet98">
            <div class="start-time-icon" title="Play from here">
                44:21
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12820" target="_blank">
                        @arXiv 2407.12820
                    </a>
                    <span class="tweet-title">
                        LLM's Memory Woes? PQCache to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, Carnegie Mellon University, Beijing Institute of Technology...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes PQCache, a novel system-algorithm co-designed method that leverages
                    Product
                    Quantization (PQ) to manage the Key-Value Cache (KVCache) in long-context LLM inference. Unlike
                    previous methods that either compromise model quality or introduce high latency, PQCache aims to
                    achieve both effectiveness and efficiency by employing PQ for approximate retrieval of relevant
                    key-value pairs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet99">
            <div class="start-time-icon" title="Play from here">
                44:55
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12391" target="_blank">
                        @arXiv 2407.12391
                    </a>
                    <span class="tweet-title">
                        LLM Serving: A Survey of How to Make AI Models Run Faster and Cheaper
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Northeastern University, MIT
                    </span>
                </div>
                <div class="primary-text">
                    This survey focuses on system-level enhancements for LLM serving, specifically those that
                    improve
                    performance and efficiency without altering the core LLM decoding mechanisms. It excludes
                    studies
                    that modify LLM decoding algorithms and focuses on research published after 2023.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet100">
            <div class="start-time-icon" title="Play from here">
                45:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12051" target="_blank">
                        @arXiv 2407.12051
                    </a>
                    <span class="tweet-title">
                        DNA's Secret Code: Unlocking the Language of Life with Sparse Recovery
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, University of California Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes Dy-mer, a DNA representation scheme that leverages sparse recovery to
                    capture
                    recurring patterns in DNA sequences, known as K-mers, and represent them as basis vectors. This
                    approach differs from previous methods by explicitly incorporating the semantic structure of
                    DNA,
                    making the representations more explainable and robust.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet101">
            <div class="start-time-icon" title="Play from here">
                45:45
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13372" target="_blank">
                        @arXiv 2407.13372
                    </a>
                    <span class="tweet-title">
                        One Model to Rule Them All: Image Restoration Gets a Unified Makeover!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Pisa, University of Trento, University of WÃ¼rzburg...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a unified approach to image restoration, leveraging inherent similarities
                    across various degradations. Unlike previous methods that require specific modules for each
                    degradation, this model uses a single architecture with a novel gated degradation adaptation
                    mechanism to handle any type of image degradation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet102">
            <div class="start-time-icon" title="Play from here">
                46:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12753" target="_blank">
                        @arXiv 2407.12753
                    </a>
                    <span class="tweet-title">
                        Vision Transformers Get a Makeover: LookupViT Compresses Images for Faster Inference!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google DeepMind, Ludwig Maximilian University of Munich
                    </span>
                </div>
                <div class="primary-text">
                    LookupViT introduces a novel vision transformer block that compresses information from
                    higher-resolution tokens to a fixed number of tokens. This approach differs from previous work
                    by
                    focusing on intrinsic compression within the architecture rather than post-processing
                    techniques.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet103">
            <div class="start-time-icon" title="Play from here">
                46:44
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13264" target="_blank">
                        @arXiv 2407.13264
                    </a>
                    <span class="tweet-title">
                        Underwater Noise? No Problem! New Denoising Techniques Dive Deep.
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nanyang Technological University, National University of Singapore, Harbin Institute of
                        Technology...
                    </span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive review of recent advancements in underwater acoustic
                    signal
                    denoising, including a taxonomy of techniques and a discussion of the unique challenges
                    associated
                    with this field. It also highlights the need for more robust denoising techniques that can adapt
                    to
                    the dynamic underwater acoustic environment.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet104">
            <div class="start-time-icon" title="Play from here">
                47:07
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13722" target="_blank">
                        @arXiv 2407.13722
                    </a>
                    <span class="tweet-title">
                        H-Consistency Bounds Get a Boost: New Tools for Better Learning Guarantees
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        NYU, Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework for deriving H-consistency bounds by relaxing the
                    assumption that the lower bound of the surrogate loss conditional regret is a convex function of
                    the
                    target conditional regret. This allows for the derivation of tighter bounds in various
                    scenarios,
                    including multi-class classification under Tsybakov noise conditions and bipartite ranking.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet105">
            <div class="start-time-icon" title="Play from here">
                47:33
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13746" target="_blank">
                        @arXiv 2407.13746
                    </a>
                    <span class="tweet-title">
                        Multi-Label Learning: A New Loss Function That's Not Hamming Around!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        NYU, Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel multi-label logistic loss function that accounts for label
                    correlations and benefits from label-independent H-consistency bounds, unlike previous work that
                    relied on binary relevance methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet106">
            <div class="start-time-icon" title="Play from here">
                48:00
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13732" target="_blank">
                        @arXiv 2407.13732
                    </a>
                    <span class="tweet-title">
                        Deferring to Experts: A New Loss Function That's Actually Consistent
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        NYU, Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new family of surrogate loss functions for learning to defer,
                    parameterized by a non-increasing function. The paper establishes their realizable H-consistency
                    properties under mild conditions and proves that several of these surrogate losses benefit from
                    H-consistency bounds for cost functions based on classification error and general cost
                    functions,
                    which also imply their Bayes-consistency. This research not only resolves an open question posed
                    in
                    previous work but also lays the groundwork for comparing various consistency notions in learning
                    to
                    defer and standard classification.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet107">
            <div class="start-time-icon" title="Play from here">
                48:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13666" target="_blank">
                        @arXiv 2407.13666
                    </a>
                    <span class="tweet-title">
                        Confidence Intervals Get a Data-Driven Makeover: No More Asymptotic Assumptions!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        RWTH Aachen University, Harvard University, Technical University of Munich...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a data-driven approach to adjust confidence intervals for
                    high-dimensional
                    regression problems. Unlike previous methods that rely on asymptotic assumptions, this approach
                    explicitly accounts for the remainder term in the estimation error, providing more accurate and
                    reliable uncertainty quantification in finite-sample regimes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet108">
            <div class="start-time-icon" title="Play from here">
                48:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13575" target="_blank">
                        @arXiv 2407.13575
                    </a>
                    <span class="tweet-title">
                        Debiasing the LASSO: Sampling Without Replacement Makes a Big Difference!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        RWTH Aachen University, Harvard University, Technical University Munich...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for improving the debiased LASSO estimator by leveraging a
                    reweighted sampling without replacement scheme. This approach differs from previous work that
                    relied
                    on sampling with replacement, which can lead to redundant samples and less accurate results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet109">
            <div class="start-time-icon" title="Play from here">
                49:23
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13522" target="_blank">
                        @arXiv 2407.13522
                    </a>
                    <span class="tweet-title">
                        Indic Languages Get Their Own Question-Answering Test: LLMs Face the Challenge!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Indian Institute of Technology Bombay, IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces INDICQABENCHMARK, a multilingual benchmark for evaluating the
                    question-answering capabilities of LLMs in 11 major Indian languages. Unlike previous
                    benchmarks, it
                    includes a diverse range of domains and both extractive and abstractive question-answering
                    tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet110">
            <div class="start-time-icon" title="Play from here">
                49:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12781" target="_blank">
                        @arXiv 2407.12781
                    </a>
                    <span class="tweet-title">
                        Taming Transformers: Giving Video AI a 3D Camera Lens
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto, Vector Institute, Snap Inc....
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for controlling camera movement in video generation models
                    that are based on transformers. Unlike previous approaches that focused on U-Net architectures,
                    this
                    method specifically addresses the challenges of controlling camera movement in transformer-based
                    models, which process spatial and temporal information jointly.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet111">
            <div class="start-time-icon" title="Play from here">
                50:25
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.11979" target="_blank">
                        @arXiv 2407.11979
                    </a>
                    <span class="tweet-title">
                        Clustering Students: A New Way to See Who's Really Learning!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Federal University of Minas Gerais, Ãcole Polytechnique FÃ©dÃ©rale de Lausanne
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new clustering pipeline called Interpret3C that uses interpretable
                    neural
                    networks to select features for each student individually, rather than using a single set of
                    features for everyone. This allows for more nuanced and accurate clustering, as it takes into
                    account the unique learning patterns of each student.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet112">
            <div class="start-time-icon" title="Play from here">
                50:48
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13771" target="_blank">
                        @arXiv 2407.13771
                    </a>
                    <span class="tweet-title">
                        Model Merging: Training-Free Domain Adaptation for Scene Understanding
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Chinese Academy of Sciences, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research explores a novel approach to multi-target domain adaptation for scene
                    understanding
                    models. Unlike previous methods that require simultaneous access to data from all target
                    domains,
                    this paper proposes a training-free model merging technique that integrates models independently
                    adapted to distinct domains.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet113">
            <div class="start-time-icon" title="Play from here">
                51:25
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13642" target="_blank">
                        @arXiv 2407.13642
                    </a>
                    <span class="tweet-title">
                        Diffusion Models: Giving 3D Scenes a Semantic Makeover!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Carnegie Mellon University, Google DeepMind, Google...
                    </span>
                </div>
                <div class="primary-text">
                    This research uses text-to-image diffusion models, which are typically used for image
                    generation, to
                    perform open-vocabulary 3D semantic segmentation. This approach differs from previous methods
                    that
                    relied on contrastively trained models like CLIP.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet114">
            <div class="start-time-icon" title="Play from here">
                51:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13076" target="_blank">
                        @arXiv 2407.13076
                    </a>
                    <span class="tweet-title">
                        LoRa Networks: Matching Up for Energy Efficiency!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Sun Yat-sen University, Nanyang Technological University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on optimizing energy efficiency in multi-gateway LoRa networks by jointly
                    allocating transmission parameters like channel, spreading factor, and transmission power.
                    Unlike
                    previous work, it considers the impact of imperfect spreading factor orthogonality and capture
                    effects, making the model more realistic.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet115">
            <div class="start-time-icon" title="Play from here">
                52:26
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13431" target="_blank">
                        @arXiv 2407.13431
                    </a>
                    <span class="tweet-title">
                        Polynomial Predictions: Making Self-Driving Cars Smarter (and Less Greedy)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Continental AG, Freie UniversitÃ¤t Berlin, University of Freiburg...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to trajectory prediction for autonomous driving by
                    using
                    polynomial representations for both input and output data. This differs from previous work that
                    primarily relies on sequence-based representations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet116">
            <div class="start-time-icon" title="Play from here">
                52:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12281" target="_blank">
                        @arXiv 2407.12281
                    </a>
                    <span class="tweet-title">
                        LLMs on a Diet: How Data Poisoning Makes Them Go Bonkers!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        CMU, IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on data poisoning attacks against large language models (LLMs)
                    specifically
                    for natural language generation (NLG) tasks, which have been less explored than attacks on
                    classification tasks. The paper introduces new metrics to evaluate the success and stealthiness
                    of
                    these attacks and explores the effectiveness of various trigger designs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet117">
            <div class="start-time-icon" title="Play from here">
                53:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12967" target="_blank">
                        @arXiv 2407.12967
                    </a>
                    <span class="tweet-title">
                        Sampling Convex Bodies: A RÃ©nyi-Infinity Journey with d3 Queries!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Georgia Institute of Technology, University of Toronto
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new algorithm called the Proximal sampler, which achieves convergence
                    in
                    the RÃ©nyi-infinity divergence for both uniform and truncated Gaussian distributions on convex
                    bodies. This is a stronger metric than previously used in the field, and the algorithm achieves
                    this
                    without any overhead in query complexity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet118">
            <div class="start-time-icon" title="Play from here">
                53:41
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13292" target="_blank">
                        @arXiv 2407.13292
                    </a>
                    <span class="tweet-title">
                        Whispering Secrets: How Phoneme-Based Pre-Training Makes Low-Resource Speech Recognition
                        Sing!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the effectiveness of different pre-training methods for low-resource
                    speech
                    recognition, focusing on the Iu Mien language. It specifically investigates the use of a
                    weakly-supervised phoneme-based multilingual pre-training model called Whistle, comparing its
                    performance to other pre-training approaches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet119">
            <div class="start-time-icon" title="Play from here">
                54:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12074" target="_blank">
                        @arXiv 2407.12074
                    </a>
                    <span class="tweet-title">
                        LoRA's Secret Weapon: Unmasking the Intrinsic Dimension for Better AI Models
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research delves into the intrinsic dimension of LoRA updates, a factor previously
                    overlooked,
                    and proposes a method to enhance it for improved generalization performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet120">
            <div class="start-time-icon" title="Play from here">
                54:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.11977" target="_blank">
                        @arXiv 2407.11977
                    </a>
                    <span class="tweet-title">
                        AI Chatbots Get a Personality Makeover: LLMs Learn to Play the Part
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge, Kingâs College London
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the challenges of integrating personas into Large Language Models (LLMs)
                    used
                    for conversational agents. Unlike previous work that focused on embedding static personality
                    traits,
                    this paper emphasizes the need for consistent and contextually appropriate personas across
                    multiple
                    interactions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet121">
            <div class="start-time-icon" title="Play from here">
                54:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12613" target="_blank">
                        @arXiv 2407.12613
                    </a>
                    <span class="tweet-title">
                        AI Helps Journalists Decode the Murmurs of the Internet
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces AudienceView, a tool that uses large language models (LLMs) to help
                    journalists analyze and understand audience feedback on YouTube. Unlike previous work that
                    focuses
                    on qualitative analysis, AudienceView aims to provide a more automated and accessible approach
                    to
                    sensemaking from textual data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet122">
            <div class="start-time-icon" title="Play from here">
                55:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12165" target="_blank">
                        @arXiv 2407.12165
                    </a>
                    <span class="tweet-title">
                        AI Agents for Autonomous Clouds: Building a Framework for Fault-Free Computing
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft, University of California Berkeley, University of Illinois Urbana-Champaign...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a standardized framework, AIOpsLab, for building, evaluating, and
                    improving
                    AI agents designed for cloud operations. Unlike previous work that focuses on specific solutions
                    or
                    uses proprietary services and datasets, AIOpsLab aims to provide a generic, reproducible, and
                    scalable benchmark for evaluating AIOps agents.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet123">
            <div class="start-time-icon" title="Play from here">
                55:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13480" target="_blank">
                        @arXiv 2407.13480
                    </a>
                    <span class="tweet-title">
                        Self-Driving Cars Get a Safety Upgrade: New AI Predicts Risky Moves!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Nanyang Technological University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on trajectory prediction for autonomous vehicles in safety-critical
                    scenarios,
                    unlike previous work that primarily focused on normal driving conditions. The paper proposes a
                    risk-aware framework that incorporates risk information into the prediction process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet124">
            <div class="start-time-icon" title="Play from here">
                56:08
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12777" target="_blank">
                        @arXiv 2407.12777
                    </a>
                    <span class="tweet-title">
                        Human 3D Models From Just 3 Photos: A New Way to Render Reality!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Carnegie Mellon University, Meta Reality Labs
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for rendering 3D human models from sparse views, using a
                    technique called "Generalizable Human Gaussians" (GHG). Unlike previous methods that rely on
                    dense
                    input views or per-subject optimization, GHG leverages a human template model and a 2D UV space
                    to
                    learn Gaussian parameters, enabling accurate and photorealistic rendering from just a few input
                    images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet125">
            <div class="start-time-icon" title="Play from here">
                56:35
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12210" target="_blank">
                        @arXiv 2407.12210
                    </a>
                    <span class="tweet-title">
                        Self-Supervised Learning: Probing for the Right Metric!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        California Institute of Technology, ETH Zurich, Swiss Data Science Center...
                    </span>
                </div>
                <div class="primary-text">
                    This research investigates the correlation between different evaluation protocols used to assess
                    the
                    quality of representations learned through self-supervised learning (SSL). It compares the
                    performance of various SSL methods across eleven datasets and analyzes how well in-domain
                    metrics
                    predict out-of-domain performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet126">
            <div class="start-time-icon" title="Play from here">
                56:57
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12131" target="_blank">
                        @arXiv 2407.12131
                    </a>
                    <span class="tweet-title">
                        Kilkari's Got a New Trick: AI Helps Moms Hear the Message!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google, Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on a larger-scale mHealth program, Kilkari, and utilizes non-Markovian
                    Time-Series Bandits (TSB) to optimize interventions, unlike previous work that relied on
                    Markovian
                    approaches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet127">
            <div class="start-time-icon" title="Play from here">
                57:29
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13765" target="_blank">
                        @arXiv 2407.13765
                    </a>
                    <span class="tweet-title">
                        Probing Language Models: Are They Secretly Causal Concept Wizards?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a formal framework called "latent causal probing" to analyze whether
                    language
                    models (LLMs) learn underlying causal relationships from their training data. It differs from
                    previous work by explicitly incorporating causal models into the probing process, allowing for a
                    more rigorous analysis of the LLM's ability to learn latent causal concepts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet128">
            <div class="start-time-icon" title="Play from here">
                58:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.11973" target="_blank">
                        @arXiv 2407.11973
                    </a>
                    <span class="tweet-title">
                        AI-Powered Calls: Can They Make Moms Smarter?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google, Harvard University, CMU
                    </span>
                </div>
                <div class="primary-text">
                    This study investigates the impact of AI-scheduled interventions on health knowledge and
                    behavior in
                    a maternal health program, going beyond previous work that focused solely on increased
                    listenership.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet129">
            <div class="start-time-icon" title="Play from here">
                58:31
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13706" target="_blank">
                        @arXiv 2407.13706
                    </a>
                    <span class="tweet-title">
                        Tiny Drones, Big Brains: GAP9Shield Packs AI Punch for Nano-UAVs
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces the GAP9Shield, a module designed for nano-drones, which integrates a
                    high-definition camera, a WiFi-BLE module, and a 5D ranging subsystem. This differs from
                    previous
                    work by combining these features into a single, lightweight, and energy-efficient package.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet130">
            <div class="start-time-icon" title="Play from here">
                59:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.13594" target="_blank">
                        @arXiv 2407.13594
                    </a>
                    <span class="tweet-title">
                        Neural Networks: They're Not Just Black Boxes Anymore!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Wisconsin-Madison, Carnegie Mellon University, Center for AI Safety
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a set of axioms to formally define and evaluate mechanistic
                    interpretability,
                    a technique for understanding how neural networks work. This approach differs from previous work
                    by
                    providing a standardized framework for judging the validity of mechanistic interpretations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet131">
            <div class="start-time-icon" title="Play from here">
                59:37
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.12707" target="_blank">
                        @arXiv 2407.12707
                    </a>
                    <span class="tweet-title">
                        TTS Gets a Score: New Benchmark Measures Synthetic Speech Quality Like Never Before!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        The University of Edinburgh
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new benchmark for evaluating Text-to-Speech (TTS) systems by measuring
                    the
                    distance between the distribution of real and synthetic speech. Unlike previous methods that
                    rely on
                    subjective human ratings or single-factor metrics, this approach considers multiple factors like
                    prosody, speaker identity, and intelligibility.
                </div>
            </div>
        </div>


        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202407191446_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>

</html>