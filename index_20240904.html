
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Fresh AI Paper Top Picks</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 80px;"></div>
            <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                Freshest
                Top Picks:
                <span class="highlightNumber" style="font-size: 28px;">135</span> out of <span
                    class="highlightNumber">734</span> arXiv AI
                papers
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="line-height: 1.2;">just released on</div>
                <div id="data-default-date" data-date="2024-09-04"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 10px;">
            </div>
        </div>


        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">00:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00610" target="_blank">@arXiv 2409.00610</a>
                    <span class="tweet-title">Protein Function Prediction Gets a Graph-Based Makeover: Region Proposals to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, City University of Hong Kong</span>
                </div>
                <div class="primary-text">
                    This research introduces ProteinRPN, a novel model for protein function prediction that utilizes a graph-based region proposal network to identify and refine functional regions within protein residue graphs. This approach differs from previous methods by incorporating domain-specific knowledge and multi-stage refinement, leading to more accurate and structurally coherent predictions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">01:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00342" target="_blank">@arXiv 2409.00342</a>
                    <span class="tweet-title">AI Image Generation Gets a Policy Makeover: AdaNAT Learns to Adapt!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, National University of Singapore</span>
                </div>
                <div class="primary-text">
                    This research introduces AdaNAT, a method that uses a learnable policy network to automatically configure the generation policy for token-based image generation, unlike previous methods that rely on manually designed rules.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">01:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00252" target="_blank">@arXiv 2409.00252</a>
                    <span class="tweet-title">Dataset Creators Spill the Tea: 7 Tips for Building Better Data</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Southern California, Microsoft Research</span>
                </div>
                <div class="primary-text">
                    This research focuses on the perspectives of dataset creators, a group often overlooked in discussions about responsible machine learning. It presents seven recommendations for improving dataset creation practices based on interviews with 18 leading dataset creators.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">02:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02060" target="_blank">@arXiv 2409.02060</a>
                    <span class="tweet-title">Open-Source Language Model:  It's Not Just About the Weights, It's About the Whole Shebang!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Allen Institute for AI, Contextual AI, University of Washington...</span>
                </div>
                <div class="primary-text">
                    This research introduces OLMOE, a fully open-source Mixture-of-Experts (MoE) language model. Unlike previous MoE models, which often only release model weights, OLMOE provides access to training data, code, and logs, making it a more transparent and accessible resource for researchers.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">02:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01704" target="_blank">@arXiv 2409.01704</a>
                    <span class="tweet-title">OCR 2.0:  A Unified Model for All Your Optical Character Needs!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">StepFun, Megvii Technology, University of Chinese Academy of Sciences...</span>
                </div>
                <div class="primary-text">
                    This research proposes a "General OCR Theory" and a model called GOT, which aims to unify various OCR tasks into a single end-to-end model. Unlike traditional OCR systems that rely on multiple modules, GOT uses a single encoder-decoder architecture, reducing complexity and maintenance costs. It also differs from large vision-language models (LVLMs) by focusing on pure OCR tasks, using a smaller number of parameters, and prioritizing high compression rates for optical characters.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">03:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00276" target="_blank">@arXiv 2409.00276</a>
                    <span class="tweet-title">Hacking the System: New Research Cracks the Code on Adversarial Attacks in Non-linear Systems</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research focuses on the exact recovery guarantees for a non-smooth estimator in the context of parameterized non-linear system identification under adversarial attacks. It differs from previous work by providing a more general and stronger analysis of the necessary and sufficient conditions for optimality and uniqueness of solutions, leading to improved sample complexity bounds.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">03:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01247" target="_blank">@arXiv 2409.01247</a>
                    <span class="tweet-title">AI's Chatty Side: How Much Talk Does It Take to Get a Bot to Go Bad?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge, Spanish National Research Council, Universitat Polit`ecnica de Val`encia</span>
                </div>
                <div class="primary-text">
                    This research introduces two new metrics, Conversational Length and Conversational Complexity, to assess the risk of eliciting harmful outputs from LLMs. Unlike previous work that focuses on individual prompts, this study examines the dynamics of extended conversations and the effort required to steer LLMs towards harmful content.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">03:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01890" target="_blank">@arXiv 2409.01890</a>
                    <span class="tweet-title">Stale Embeddings? No Problem! Corrector Networks to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel approach to training dense retrieval models by using a small parametric "corrector network" to adjust stale cached target embeddings. This method avoids the computationally expensive process of re-embedding targets during training, which is a common practice in previous work.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">04:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00358" target="_blank">@arXiv 2409.00358</a>
                    <span class="tweet-title">Decoding Dialects: How a Low-Rank Adapter Makes AI Understand Your Accent</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of New South Wales, Google</span>
                </div>
                <div class="primary-text">
                    This research extends dialect adaptation techniques, previously applied to encoder models, to decoder models. It introduces a novel architecture called LORDD, which utilizes task-specific and dialect-specific adapters to improve the performance of decoder models on target word prediction tasks in Indian English.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">04:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01011" target="_blank">@arXiv 2409.01011</a>
                    <span class="tweet-title">Ancient Chinese Scripts Get a Multi-Modal Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a multi-modal multi-granularity tokenizer specifically designed for analyzing ancient Chinese scripts, particularly the Chu bamboo slip (CBS) script. Unlike previous work that focuses on character-level tokenization, this tokenizer breaks down characters into sub-character components, potentially revealing more information about the semantics and phonetics of the text.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">04:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00138" target="_blank">@arXiv 2409.00138</a>
                    <span class="tweet-title">AI Chatbots:  Spilling Secrets Without a Whisper?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Northeastern University, Harvard University</span>
                </div>
                <div class="primary-text">
                    This research focuses on evaluating the privacy awareness of language models (LMs) in action, specifically when they are used as agents to assist with communication tasks. Unlike previous work that primarily used probing questions, this study constructs a framework called PrivacyLens to evaluate LMs in a more realistic scenario where they interact with tools like email and calendars.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">05:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01156" target="_blank">@arXiv 2409.01156</a>
                    <span class="tweet-title">Video's Got Talent: New Technique Merges Redundant Frames for Faster Text-Video Retrieval</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">JD.com, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research tackles the issue of temporal redundancy in video data, which is often overlooked in efficient text-video retrieval methods. The authors propose a novel Temporal Token Merging (TempMe) framework that progressively merges similar tokens across frames, reducing computational overhead while improving performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">05:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00800" target="_blank">@arXiv 2409.00800</a>
                    <span class="tweet-title">Speech Recognition:  Discrete vs. Continuous -  Who Wins the Token Race?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Tencent AI Lab, The Chinese University of Hong Kong</span>
                </div>
                <div class="primary-text">
                    This research systematically compares discrete and continuous speech representations within Large Language Model (LLM)-based Automatic Speech Recognition (ASR), a topic not extensively explored before.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">06:07</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02108" target="_blank">@arXiv 2409.02108</a>
                    <span class="tweet-title">Deep Shadows: A Deep Dive into Shadow Detection, Removal, and Generation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Shanghai Artificial Intelligence Laboratory, Chinese University of Hong Kong, Adobe Research</span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive survey of shadow analysis in the deep learning era, covering tasks, deep models, datasets, evaluation metrics, and result comparisons. It also includes a cross-dataset generalization study and explores the relationship between model size/speed and performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">06:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00588" target="_blank">@arXiv 2409.00588</a>
                    <span class="tweet-title">Diffusion Policy Gets a Fine-Tuning Boost:  RL Meets Diffusion for Robot Superpowers!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Princeton University, Massachusetts Institute of Technology, Toyota Research Institute...</span>
                </div>
                <div class="primary-text">
                    This paper introduces Diffusion Policy Policy Optimization (DPPO), a new framework for fine-tuning diffusion-based policies using policy gradient methods from reinforcement learning. Unlike previous work that focused on off-policy Q-learning or weighted regression, DPPO leverages the unique properties of diffusion models to achieve more efficient and stable training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">06:56</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01141" target="_blank">@arXiv 2409.01141</a>
                    <span class="tweet-title">LLMs Get a Speed Boost: Duplex Makes Big Language Models Run Faster and Cooler</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Seoul National University, Samsung</span>
                </div>
                <div class="primary-text">
                    This research proposes Duplex, a device that integrates two types of processing units to accelerate large language models (LLMs). Unlike previous heterogeneous systems that duplicate MoE layers, Duplex uses a combination of high-Op/B and low-Op/B processors to efficiently handle the varying computational demands of different layers within LLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">07:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01071" target="_blank">@arXiv 2409.01071</a>
                    <span class="tweet-title">VideoLLaMB:  Remembering the Whole Movie, One Scene at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Beijing Academy of Artificial Intelligence, UC Santa Cruz, Peking University</span>
                </div>
                <div class="primary-text">
                    This paper introduces VideoLLaMB, a framework that uses recurrent memory tokens within bridge layers to encode entire video sequences, preserving semantic continuity and enhancing model performance across various tasks. This approach differs from previous work that often relies on video compression strategies, which can lead to the loss of critical visual cues.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">07:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00250" target="_blank">@arXiv 2409.00250</a>
                    <span class="tweet-title">Medical Report Generation:  From Text to Tags, It's a Multi-Label Thing!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">NYU, University of Technology Sydney, Stanford University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new approach to medical report generation by framing it as a multi-label classification problem. Instead of generating text directly, the model focuses on identifying and classifying key medical concepts from images, simplifying the process and potentially improving accuracy. This differs from previous methods that relied heavily on complex sequence generation models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">08:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01007" target="_blank">@arXiv 2409.01007</a>
                    <span class="tweet-title">AI Debate Club:  Unlocking Wisdom Through Contentious Conversations</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces EVINCE, a framework that uses conditional statistics and information theory to quantify and moderate adversarial dialogues between LLMs. This approach aims to improve prediction accuracy, robustness, and stability in LLMs by balancing diverse perspective exploration with strong prior exploitation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">08:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01927" target="_blank">@arXiv 2409.01927</a>
                    <span class="tweet-title">Web Agents: Planning is the Real Bottleneck, Not Grounding!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research breaks down web agents into two components: planning and grounding. It then isolates these components in experiments to determine which one is the main bottleneck in web agent performance. This approach differs from previous work that often treats web agents as black boxes and focuses on end-to-end evaluations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">09:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00951" target="_blank">@arXiv 2409.00951</a>
                    <span class="tweet-title">Robots Learn New Tricks with AI-Powered Image Makeovers!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel framework for semantically controllable data augmentation in robot learning. Unlike traditional data augmentation techniques that focus on minor variations like color adjustments, this approach leverages pre-trained image-text generative models to introduce diverse, realistic, and semantically meaningful alterations to robot data. This allows robots to learn from a wider range of experiences, improving their ability to generalize to unseen real-world scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">09:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01652" target="_blank">@arXiv 2409.01652</a>
                    <span class="tweet-title">Robot Choreographer:  New AI Makes Robots Dance to Your Commands!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces Relational Keypoint Constraints (ReKep), a new way to represent robotic manipulation tasks. Unlike previous methods that rely on rigid-body transformations or require extensive training data, ReKep uses semantically meaningful 3D keypoints and Python functions to define constraints, enabling more flexible and adaptable robot behaviors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">10:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01420" target="_blank">@arXiv 2409.01420</a>
                    <span class="tweet-title">Neural Networks Get a Coding Makeover: Erasure Codes for Faster Inference!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM, CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a method to apply erasure coding to neural networks, a technique previously used for linear computations. The key difference is that the authors leverage the diagonal Fisher information matrix to create a coded model that approximates a linear combination of the outputs of multiple neural networks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">10:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01369" target="_blank">@arXiv 2409.01369</a>
                    <span class="tweet-title">Imitation Game:  Teaching Language Models to Think Like Humans with Inverse Reinforcement Learning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google DeepMind, Cohere</span>
                </div>
                <div class="primary-text">
                    This research explores the use of inverse reinforcement learning (IRL) for fine-tuning large language models (LLMs). Unlike traditional maximum likelihood estimation (MLE), which focuses on predicting the next token, IRL aims to optimize the entire sequence generation process by extracting rewards and directly optimizing actions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">10:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00133" target="_blank">@arXiv 2409.00133</a>
                    <span class="tweet-title">LLMs in Medicine: From Zero to Hero (With a Little Fine-Tuning)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Xinxiang Medical University, Henan Province, Shanghai AI Laboratory...</span>
                </div>
                <div class="primary-text">
                    This research stands out by providing a comprehensive analysis of LLMs across various biomedical fields, including genomics, clinical practice, and drug discovery. It goes beyond specific applications or model architectures, offering a broader perspective on the current landscape, challenges, and future prospects of LLMs in biomedicine.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">11:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01199" target="_blank">@arXiv 2409.01199</a>
                    <span class="tweet-title">Video Compression Gets a 3D Makeover:  How Omni-dimensional VAEs Are Revolutionizing Video Generation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces OD-VAE, a variational autoencoder (VAE) that compresses videos in both the spatial and temporal dimensions. Unlike previous VAEs used in latent video diffusion models (LVDMs), which only compress spatially, OD-VAE leverages the temporal redundancy in video frames to achieve more concise latent representations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">11:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01072" target="_blank">@arXiv 2409.01072</a>
                    <span class="tweet-title">Rainy Day Blues? This AI Can Still See the Road!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Wuhan University, Nanyang Technological University, National Tsing Hua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new online domain adaptation framework called RODASS, which dynamically detects domain shifts and adjusts hyperparameters to minimize training costs and error propagation. Unlike previous methods, RODASS incorporates a Dynamic Ambiguous Patch Mask (DAP Mask) strategy to mitigate error accumulation in ambiguous classes, enhancing the model's robustness against external noise.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">12:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02048" target="_blank">@arXiv 2409.02048</a>
                    <span class="tweet-title">Single Image, Endless Views:  Taming Diffusion Models for 3D Scene Magic</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Tencent AI Lab, The Chinese University of Hong Kong...</span>
                </div>
                <div class="primary-text">
                    This research proposes ViewCrafter, a novel view synthesis method that combines the power of video diffusion models with point cloud representations. Unlike previous methods that rely on dense multi-view captures, ViewCrafter can generate high-fidelity novel views from single or sparse images, enabling precise camera pose control and consistent view generation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">12:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01055" target="_blank">@arXiv 2409.01055</a>
                    <span class="tweet-title">Outpainting Videos:  From Tiny to Titanic,  No Memory Limits!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tencent, HKUST, USTC...</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for video outpainting that uses spatial windows to overcome GPU memory limitations, allowing for higher-resolution outpainting with extensive content generation. Unlike previous methods that rely on single-shot outpainting, this approach breaks down the task into smaller, manageable sub-tasks, enabling the generation of larger, more detailed videos.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">12:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00141" target="_blank">@arXiv 2409.00141</a>
                    <span class="tweet-title">Battery Health Check: Graphing the Way to Longer Life</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Singapore University of Technology and Design, Chongqing University, Nanyang Technological University</span>
                </div>
                <div class="primary-text">
                    This research uses a graph convolutional network (GCN) to estimate the state of health (SOH) of lithium-ion batteries, incorporating inter-cycle degradation information through a graph structure. Unlike previous methods that focus on individual cycles, this approach captures the temporal dynamics of battery degradation across multiple cycles.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">13:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01524" target="_blank">@arXiv 2409.01524</a>
                    <span class="tweet-title">LLMs Learn to Catch Their Own Math Mistakes: A Step-by-Step Self-Correction Revolution!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Zhejiang University, Peking University, Meituan...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to self-correction in LLMs, focusing on spontaneous step-level correction during mathematical reasoning. Unlike previous methods that rely on post-hoc generation or external tools, this study equips LLMs with the ability to identify and correct errors as they occur during the inference process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">13:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00839" target="_blank">@arXiv 2409.00839</a>
                    <span class="tweet-title">Unlocking the Black Box: How Entropy Loss Makes 3D Object Detection Smarter</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Mohamed bin Zayed University of Artificial Intelligence, Beijing University of Posts and Telecommunications, Tianjin University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel loss function called "Entropy Loss" that aims to improve the interpretability of 3D object detection networks. Unlike previous methods that focus on architectural design, this approach leverages principles from communication theory to quantify and control information flow within the network.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">14:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01086" target="_blank">@arXiv 2409.01086</a>
                    <span class="tweet-title">Fashion AI Gets a Texture Makeover: New Model Makes Clothes Look Real!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Shenzhen Technology University, Shenzhen University, Carnegie Mellon University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for fashion image editing called DPDEdit, which uses multiple types of information, including text, human poses, and garment textures, to create more realistic and detailed clothing edits. Unlike previous methods that rely solely on text descriptions, DPDEdit incorporates actual texture images to guide the generation process, resulting in more accurate and visually appealing results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">14:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01661" target="_blank">@arXiv 2409.01661</a>
                    <span class="tweet-title">NeRFs Get a Privacy Makeover:  How to Share 3D Scenes Without Sharing Your Home!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Zhejiang University, Chinese University of Hong Kong, Nanyang Technological University</span>
                </div>
                <div class="primary-text">
                    This research introduces SplitNeRF, a framework for training NeRF models collaboratively without sharing private scene data. It differs from previous work by focusing on privacy concerns specific to NeRF training, where the model itself can reveal sensitive information about the scene.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">15:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01151" target="_blank">@arXiv 2409.01151</a>
                    <span class="tweet-title">Hallucination Busting:  New Metric Unmasks Image Understanding Woes in AI</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, Beijing Academy of Artificial Intelligence</span>
                </div>
                <div class="primary-text">
                    This research introduces a parameter-free representation alignment metric (Pfram) to assess the quality of image representations in multimodal large language models (MLLMs). Unlike previous work that evaluates MLLMs as a whole, Pfram isolates the impact of image understanding on object hallucination.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">15:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00879" target="_blank">@arXiv 2409.00879</a>
                    <span class="tweet-title">Soft MoE:  More Experts, Less Confusion, More Power!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University</span>
                </div>
                <div class="primary-text">
                    This research investigates the implicit biases of Soft Mixture of Experts (MoE) models, specifically focusing on the impact of the number of experts on representation power and expert specialization. Unlike previous work that primarily focused on computational aspects of MoE, this paper delves into the architectural implications of Soft MoE.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">15:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00618" target="_blank">@arXiv 2409.00618</a>
                    <span class="tweet-title">YOLOO:  Multi-Modal Tracking Without the Multi-Modal Fuss!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanjing University of Aeronautics and Astronautics, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new approach to multi-modal 3D object tracking called YOLOO. Unlike previous methods that require processing data from multiple sources (like images and point clouds) during inference, YOLOO learns a unified representation from all modalities during training, allowing it to track objects efficiently using only point cloud data during inference.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">16:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01989" target="_blank">@arXiv 2409.01989</a>
                    <span class="tweet-title">Electrolyte Design:  A Data-Driven Recipe for Better Batteries</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research uses a data-driven approach to optimize electrolyte formulations for a novel interhalogen battery, considering not only electrolyte composition but also cathode loading and separator type. This differs from previous work that focused on optimizing electrolyte composition alone.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">16:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00088" target="_blank">@arXiv 2409.00088</a>
                    <span class="tweet-title">LLMs on Your Phone?  It's Happening!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta, University of North Texas</span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive review of the challenges and solutions for deploying large language models (LLMs) on edge devices, focusing on efficient architectures, model compression techniques, and hardware acceleration strategies. It differs from previous work by offering a detailed analysis of the evolution of on-device LLMs, including a timeline of key model releases and a discussion of the advantages and limitations of on-device inference compared to cloud-based approaches.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">17:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00162" target="_blank">@arXiv 2409.00162</a>
                    <span class="tweet-title">Rewarding LLMs with Language: A New Recipe for Alignment</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel sequence-to-sequence (seq2seq) reward modeling method for RLHF. Unlike traditional methods that rely on scalar feedback, this approach leverages language feedback, providing richer and more fine-grained information to the reward model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">17:28</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00331" target="_blank">@arXiv 2409.00331</a>
                    <span class="tweet-title">WikiCausal:  Building a Knowledge Graph of Causes and Effects, One Wikipedia Article at a Time!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research introduces a new corpus and evaluation framework specifically designed for building causal knowledge graphs from text. Unlike previous work that focused on smaller, manually curated datasets or low-level tasks, this framework allows for evaluating the quality of end-to-end causal extraction solutions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">17:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01421" target="_blank">@arXiv 2409.01421</a>
                    <span class="tweet-title">CSG Shapes Get a Differentiable Makeover:  Rasterization Makes CAD Editing a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Edinburgh, University College London</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for rendering CSG models in a differentiable manner, which allows for the optimization of shape parameters. Unlike previous approaches that rely on complex mesh processing or analytical SDF expressions, this method leverages CSG rasterization and anti-aliasing to achieve differentiability.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">18:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00028" target="_blank">@arXiv 2409.00028</a>
                    <span class="tweet-title">Pupil Power: Holograms That See Like You Do!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, University of North Carolina at Chapel Hill</span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework for generating 3D holograms that dynamically adjust their depth-of-field based on the viewer's pupil size. Unlike previous methods that either assume a fixed pupil size or struggle with speckle noise, this approach uses an adjustable deformable convolutional layer to create realistic defocus effects.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">18:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00851" target="_blank">@arXiv 2409.00851</a>
                    <span class="tweet-title">Time's Up!  Text-to-Audio Models Can't Tell Time (But We Can Help)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford, University of Tübingen</span>
                </div>
                <div class="primary-text">
                    This research delves into the temporal understanding capabilities of text-to-audio retrieval models, focusing on how they handle the order of events in audio descriptions. Unlike previous work that primarily used CNN-based audio encoders, this study investigates the performance of transformer-based encoders, which are known for their ability to handle temporal data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">19:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00558" target="_blank">@arXiv 2409.00558</a>
                    <span class="tweet-title">LLM Director:  Composing 3D Videos with a Textual Script</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Science and Technology of China, Microsoft, Shanghai Jiao Tong University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to text-to-video generation by composing individual concepts in 3D space. Unlike previous methods that implicitly learn concepts in 2D, this approach explicitly represents each concept in 3D, allowing for more flexible control and interaction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">19:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01588" target="_blank">@arXiv 2409.01588</a>
                    <span class="tweet-title">Reinforcement Learning:  City Planners' New Best Friend for Facility Placement</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a knowledge-informed reinforcement learning (RL) method for solving the large-scale urban facility location problem (FLP). Unlike previous approaches that rely heavily on local search, this method leverages a graph neural network (GNN) to guide the selection of edges on a swap graph, significantly accelerating the solution generation process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">19:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01931" target="_blank">@arXiv 2409.01931</a>
                    <span class="tweet-title">Force Fields: From Slow and Steady to Fast and Furious</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">New York University, Asahi Kasei Pharma Corporation, Open Molecular Software Foundation...</span>
                </div>
                <div class="primary-text">
                    This research focuses on the design space between molecular mechanics (MM) and machine learning (ML) force fields, exploring the speed-accuracy tradeoff. It argues that the accuracy of MLFFs is no longer a limiting factor, but their speed is, and proposes a new generation of MLFFs that are faster, more stable, and more generalizable than current models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">20:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01990" target="_blank">@arXiv 2409.01990</a>
                    <span class="tweet-title">Shrinking LLMs: How to Make Big Language Models Tiny and Speedy</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Wisconsin-Madison</span>
                </div>
                <div class="primary-text">
                    This research explores systematic design in model compression, focusing on optimizing memory usage and improving KV cache eviction techniques for LLMs. Unlike previous work that primarily focused on quantization, knowledge distillation, and pruning, this paper delves into the architectural level of LLM deployment, aiming to make them more efficient and scalable.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">20:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01306" target="_blank">@arXiv 2409.01306</a>
                    <span class="tweet-title">Neural Networks: The New Electron Density Detectives!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel method called NERD (Neural Electron Real-space Density) to extract accurate electron densities from real-space many-electron wave functions. Unlike previous methods that rely on basis sets or struggle with multi-modal densities, NERD uses a neural network trained with score matching and noise-contrastive estimation to capture both local and global features of the density.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">20:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02038" target="_blank">@arXiv 2409.02038</a>
                    <span class="tweet-title">LLMs Go to Work:  Enterprise Data Makes Them Sweat!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, Amazon, University of Washington</span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark dataset called BEAVER, specifically designed for evaluating text-to-SQL models on real-world enterprise data warehouses. Unlike existing benchmarks that rely on publicly available data, BEAVER uses anonymized data from actual enterprise systems, capturing the complexity and unique characteristics of enterprise data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">21:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01992" target="_blank">@arXiv 2409.01992</a>
                    <span class="tweet-title">QueryCheetah: Hunting Privacy Vulnerabilities with Speed and Finesse</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Imperial College London, École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research introduces QueryCheetah, a method for automated discovery of privacy attacks against query-based systems (QBSs). Unlike previous methods, QueryCheetah uses a fast local-search technique to explore a wider range of query syntaxes, resulting in a significant speed-up.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">21:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01666" target="_blank">@arXiv 2409.01666</a>
                    <span class="tweet-title">RAG's Back, Baby! Long-Context LLMs Get a Retrieval Boost</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nvidia</span>
                </div>
                <div class="primary-text">
                    This paper challenges the notion that long-context LLMs have rendered RAG obsolete. It proposes an order-preserving RAG mechanism that outperforms long-context LLMs by focusing on relevant information within a long document.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">22:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00844" target="_blank">@arXiv 2409.00844</a>
                    <span class="tweet-title">LLMs Get Report Cards:  Grading AI with Human-Readable Summaries</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for evaluating large language models (LLMs) called "Report Cards." Unlike traditional quantitative benchmarks, Report Cards are human-interpretable natural language summaries of model behavior for specific skills or topics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">22:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00316" target="_blank">@arXiv 2409.00316</a>
                    <span class="tweet-title">OMR:  From  Perfect  to  Imperfect  –  A  New  Way  to  Read  Music!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington</span>
                </div>
                <div class="primary-text">
                    This research tackles the challenge of imperfect object detection in optical music recognition (OMR) by introducing a training pipeline that directly trains the notation assembly model on the output of an object detector. This approach differs from previous work that assumed perfect detection output.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">23:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01502" target="_blank">@arXiv 2409.01502</a>
                    <span class="tweet-title">Avatar-Powered Video Magic:  Turning Text Prompts into Realistic Human Videos!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Arizona State University, University of Washington</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for generating human videos by conditioning video diffusion models on controlled 3D avatar rendering. This approach combines the photorealism of 2D methods with the controllability of 3D avatar-based approaches, addressing limitations of both.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">23:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00486" target="_blank">@arXiv 2409.00486</a>
                    <span class="tweet-title">Sounding Objects:  A Multi-Scale, Multi-Instance Approach to Visual Sound Localization</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, MBZUAI, Xiaohongshu</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel framework called M2VSL that utilizes multi-scale visual features to align audio-visual representations at multiple levels. This differs from previous methods that primarily focused on global audio and single-scale visual features.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">23:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00845" target="_blank">@arXiv 2409.00845</a>
                    <span class="tweet-title">2D to 3D:  Bridging the Gap with Relational Distillation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, Mila-Quebec AI Institute</span>
                </div>
                <div class="primary-text">
                    This research focuses on the structural mismatch between 2D and 3D representations in autonomous driving data distillation. It proposes a relational distillation framework that enforces intra-modal and cross-modal constraints to bridge this gap, leading to improved performance in zero-shot and few-shot 3D semantic segmentation tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">24:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01367" target="_blank">@arXiv 2409.01367</a>
                    <span class="tweet-title">Fairness on Graphs:  A New Framework That's Not Afraid to Be Bottlenecked!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">South China University of Technology, Hong Kong Polytechnic University, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research proposes a new framework called GRAFair, which uses a variational graph auto-encoder to learn fair representations on graphs. Unlike previous adversarial learning methods, GRAFair achieves fairness in a stable manner without relying on adversarial training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">24:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00124" target="_blank">@arXiv 2409.00124</a>
                    <span class="tweet-title">AI for Wireless:  Can ChatGPT Decode Your Phone Calls?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Rensselaer Polytechnic Institute</span>
                </div>
                <div class="primary-text">
                    This research explores using large language models (LLMs) for wireless symbol detection, specifically focusing on in-context learning (ICL) without any training or fine-tuning. This differs from previous work that primarily relied on traditional deep neural networks (DNNs) which require extensive training data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">25:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01411" target="_blank">@arXiv 2409.01411</a>
                    <span class="tweet-title">Multi-Agent Networks:  Learning to Talk, Learning to Listen, Learning to Win!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Michigan</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel algorithm, Anaconda, that allows multi-agent networks to dynamically adjust their communication topology to optimize performance during collaborative tasks. Unlike previous methods that rely on fixed network structures, Anaconda enables agents to choose their communication partners based on the task at hand, balancing the trade-off between decision speed and accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">25:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01676" target="_blank">@arXiv 2409.01676</a>
                    <span class="tweet-title">Diffusion Models:  The New Way to Spot Faulty Machines</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, École Polytechnique Fédérale de Lausanne</span>
                </div>
                <div class="primary-text">
                    This research proposes a classifier-free diffusion model for health indicator derivation in rotating machines. Unlike previous methods that model the entire data distribution, this approach focuses on generating healthy samples and comparing them to real-time data to identify anomalies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">25:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00597" target="_blank">@arXiv 2409.00597</a>
                    <span class="tweet-title">Stance Wars:  New Dataset Makes Multimodal Conversation Analysis a Real Fight!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Shenzhen Technology University, Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset, MmMtCSD, specifically designed for multimodal multi-turn conversation stance detection. Unlike previous datasets, MmMtCSD focuses on capturing the nuances of stance within conversational threads, including both text and images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">26:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01595" target="_blank">@arXiv 2409.01595</a>
                    <span class="tweet-title">DiT-tastic Videos:  AI Makes Driving Scenes Look Real!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harbin Institute of Technology, Li Auto Inc., Tsinghua University...</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel DiT-based framework for generating temporally and multi-view consistent videos, specifically designed for autonomous driving scenarios. Unlike previous methods, it incorporates bird's-eye view layouts and scene text for precise control, addressing limitations in resolution, aspect ratios, and object inconsistencies.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">27:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00101" target="_blank">@arXiv 2409.00101</a>
                    <span class="tweet-title">EEG Signals:  The New Language LLMs Can Speak!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Shanghai Jiao Tong University, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research proposes NeuroLM, a multi-task foundation model for EEG signal processing that leverages the capabilities of Large Language Models (LLMs) by treating EEG signals as a foreign language. Unlike previous work that requires individual fine-tuning for each downstream task, NeuroLM can perform multiple tasks within a single model through instruction tuning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon" title="Play from here">27:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00894" target="_blank">@arXiv 2409.00894</a>
                    <span class="tweet-title">Over-parameterized Models:  Learning to Adapt Like a Chameleon!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research explores how over-parameterization in sequence models can dynamically adjust the eigenvalues of a kernel, leading to improved adaptivity and generalization compared to traditional fixed-kernel methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet65">
            <div class="start-time-icon" title="Play from here">27:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01315" target="_blank">@arXiv 2409.01315</a>
                    <span class="tweet-title">NeuralBIM Gets a Multi-Frequency Makeover: Solving Inverse Scattering Problems with a Deep Learning Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a multi-frequency Neural Born Iterative Method (NeuralBIM) for solving inverse scattering problems. Unlike previous single-frequency approaches, this method leverages information from multiple frequencies to improve accuracy and efficiency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet66">
            <div class="start-time-icon" title="Play from here">28:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00083" target="_blank">@arXiv 2409.00083</a>
                    <span class="tweet-title">Brain-Computer Interface Gets a Brain: On-Device Learning for Wearable EEG!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, Tsinghua University, Slovak Academy of Sciences...</span>
                </div>
                <div class="primary-text">
                    This research focuses on on-device learning for EEG-based motor imagery brain-computer interfaces (BCIs), which allows for real-time adaptation to individual users' brain signals without needing to send data to a cloud or server. This is different from previous work that relied on offline training and transfer learning, which can be time-consuming and require significant computational resources.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet67">
            <div class="start-time-icon" title="Play from here">28:50</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00997" target="_blank">@arXiv 2409.00997</a>
                    <span class="tweet-title">DataSculpt:  LLMs Get a Long-Context Makeover!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces DataSculpt, a framework that tackles the challenge of constructing long-context training data for LLMs by framing it as a multi-objective combinatorial optimization problem. Unlike previous approaches that focus on single objectives or heuristic methods, DataSculpt considers multiple objectives like relevance, homogeneity, integrity, and computational efficiency, leading to a more balanced and effective data organization strategy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet68">
            <div class="start-time-icon" title="Play from here">29:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00744" target="_blank">@arXiv 2409.00744</a>
                    <span class="tweet-title">LiDAR Odometry Gets a Memory Boost: Deep Learning Learns from the Past!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Shanghai Jiao Tong University, University of Cambridge, UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel deep sequence LiDAR odometry method called DSLO, which leverages inconsistent spatio-temporal propagation. Unlike previous methods that focus on two adjacent frames, DSLO incorporates historical motion information to improve accuracy and robustness.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet69">
            <div class="start-time-icon" title="Play from here">29:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00349" target="_blank">@arXiv 2409.00349</a>
                    <span class="tweet-title">Toddler Tantrums: New Dataset Helps AI Understand Tiny Tykes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington</span>
                </div>
                <div class="primary-text">
                    This research introduces ToddlerAct, a new dataset specifically designed for action recognition in toddlers, addressing the lack of toddler-specific data in existing datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet70">
            <div class="start-time-icon" title="Play from here">30:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01082" target="_blank">@arXiv 2409.01082</a>
                    <span class="tweet-title">Image Retrieval Gets a Confidence Boost: Evidential Transformers Take the Lead!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, Texas A&M University</span>
                </div>
                <div class="primary-text">
                    This paper introduces the Evidential Transformer, a new approach to image retrieval that incorporates uncertainty quantification. Unlike previous methods that rely on deterministic neural networks, this model explicitly models uncertainty, providing a more robust and informative framework for retrieval tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet71">
            <div class="start-time-icon" title="Play from here">30:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00244" target="_blank">@arXiv 2409.00244</a>
                    <span class="tweet-title">Deep Learning Gets a Data Assimilation Makeover: TorchDA Makes It Easy!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École des Ponts ParisTech</span>
                </div>
                <div class="primary-text">
                    This research introduces TorchDA, a Python package that integrates deep learning models into data assimilation workflows. Unlike existing tools, TorchDA allows users to seamlessly incorporate custom neural networks for state transition and observation functions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet72">
            <div class="start-time-icon" title="Play from here">30:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00237" target="_blank">@arXiv 2409.00237</a>
                    <span class="tweet-title">Wildfire Forecasting Gets a Speed Boost: Deep Learning to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École des Ponts ParisTech</span>
                </div>
                <div class="primary-text">
                    This research proposes two deep learning models, CAE-LSTM and ConvLSTM, to act as surrogates for the JULES-INFERNO wildfire model, significantly reducing the computational time required for global wildfire prediction. This approach differs from previous work by focusing on global-scale prediction and utilizing a combination of convolutional and recurrent neural networks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet73">
            <div class="start-time-icon" title="Play from here">31:26</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00147" target="_blank">@arXiv 2409.00147</a>
                    <span class="tweet-title">Math-tastic Vision: A Language Model That Can See and Solve Problems!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, ByteDance, University of Electronic Science and Technology of China</span>
                </div>
                <div class="primary-text">
                    This research introduces MultiMath-7B, a multimodal large language model (MLLM) that combines visual and mathematical reasoning. Unlike previous work that focused solely on text-based math problems or geometric problem solving, MultiMath-7B can handle a wider range of mathematical tasks involving images, such as function plots and scientific charts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet74">
            <div class="start-time-icon" title="Play from here">31:47</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01821" target="_blank">@arXiv 2409.01821</a>
                    <span class="tweet-title">Visual Prompting vs. Linear Probing: A Likelihood-Based Showdown!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National Tsing Hua University, Dartmouth College, IBM Research...</span>
                </div>
                <div class="primary-text">
                    This research introduces a log-likelihood ratio (LLR) method to assess the effectiveness of visual prompting (VP) versus linear probing (LP) for adapting pre-trained models to new tasks. Unlike previous work that focused on model selection, this paper focuses on method selection, providing a way to predict which approach will perform better on a given dataset.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet75">
            <div class="start-time-icon" title="Play from here">32:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00315" target="_blank">@arXiv 2409.00315</a>
                    <span class="tweet-title">Chatbots Need a Memory Makeover: How Long is Too Long for a Chat History?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research investigates the impact of context length on the performance of Transformer-based open-domain dialog models. Unlike previous work that focused on the absolute performance of models, this study examines how varying the length of the dialog history during training and testing affects the model's ability to generate coherent responses.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet76">
            <div class="start-time-icon" title="Play from here">32:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00099" target="_blank">@arXiv 2409.00099</a>
                    <span class="tweet-title">LiCoNet's Got Your Back:  Custom Keyword Spotting with a Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">META AI</span>
                </div>
                <div class="primary-text">
                    This research introduces a hardware-efficient framework for customized keyword spotting (KWS) using LiCoNet, a streaming convolution network. The novelty lies in the use of spectral-temporal graph attentive pooling (GAP) and a hybrid loss function to improve word embedding learning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet77">
            <div class="start-time-icon" title="Play from here">33:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00286" target="_blank">@arXiv 2409.00286</a>
                    <span class="tweet-title">Sports-Obsessed AI: Tiny Model, Big Sports Knowledge!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">NYU, CMU, Cornell University...</span>
                </div>
                <div class="primary-text">
                    This research focuses on developing a small, domain-specific language model trained exclusively on sports data. Unlike previous work that relies on large, general-purpose models, this study explores the potential of optimizing model structure for specific domains.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet78">
            <div class="start-time-icon" title="Play from here">33:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00815" target="_blank">@arXiv 2409.00815</a>
                    <span class="tweet-title">Speech Separation:  A New Way to Untangle the Cocktail Party Problem</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Kyoto University, Meta</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method called "overlapped encoding separation" (EncSep) to improve the performance of serialized output training (SOT) in multi-speaker automatic speech recognition (ASR).  EncSep utilizes the CTC-Attention hybrid loss, which is typically difficult to apply in SOT-based ASR due to the serialized nature of the training labels.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet79">
            <div class="start-time-icon" title="Play from here">33:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00297" target="_blank">@arXiv 2409.00297</a>
                    <span class="tweet-title">Quantized Networks:  Universal Approximators, Even With Rounding Errors!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Korea Institute for Advanced Study, Korea University</span>
                </div>
                <div class="primary-text">
                    This research investigates the expressive power of quantized neural networks under fixed-point arithmetic, considering rounding errors during calculations. Unlike previous work that focused on exact operations, this study analyzes the impact of rounding on the networks' ability to approximate functions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet80">
            <div class="start-time-icon" title="Play from here">34:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00753" target="_blank">@arXiv 2409.00753</a>
                    <span class="tweet-title">Traffic Pressure:  A Multi-Hop Approach to Smarter Perimeter Control</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research introduces a new metric called "multi-hop pressure" to improve perimeter control in traffic networks. Unlike previous methods that only consider immediate downstream traffic, multi-hop pressure accounts for congestion further down the road, providing a more comprehensive view of traffic conditions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet81">
            <div class="start-time-icon" title="Play from here">34:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01216" target="_blank">@arXiv 2409.01216</a>
                    <span class="tweet-title">VR Point Clouds:  Less Data, More Action!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">City University of Hong Kong, Southeast University, ETH Zurich...</span>
                </div>
                <div class="primary-text">
                    This research introduces ESP-PCT, a new framework that focuses on identifying the most important parts of point cloud data generated by millimeter-wave sensors in VR applications. This approach differs from previous methods that process the entire point cloud, leading to improved accuracy and efficiency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet82">
            <div class="start-time-icon" title="Play from here">34:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01584" target="_blank">@arXiv 2409.01584</a>
                    <span class="tweet-title">Artful Explanations: Can AI Understand Art in Different Languages?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo, Nara Institute of Science and Technology</span>
                </div>
                <div class="primary-text">
                    This research focuses on evaluating the ability of large-scale vision language models (LVLMs) to generate explanations for artworks in multiple languages. Unlike previous work that primarily relied on machine translation, this study created a dataset without translation, ensuring cultural nuances are considered.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet83">
            <div class="start-time-icon" title="Play from here">35:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00458" target="_blank">@arXiv 2409.00458</a>
                    <span class="tweet-title">Predicting the Future with Sparse Data: A Voronoi Tessellation Tale</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École des Ponts ParisTech</span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework called DSOVT, which uses Voronoi tessellation to interpolate sparse and time-varying data in dynamical systems. This approach differs from previous work by directly integrating physics constraints into the training process of the deep learning models, enhancing the accuracy and robustness of predictions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet84">
            <div class="start-time-icon" title="Play from here">35:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01690" target="_blank">@arXiv 2409.01690</a>
                    <span class="tweet-title">CLIP Goes to the Museum: Taming AI for Fine-Grained Art Understanding</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research adapts CLIP, a powerful vision-language model, to understand museum exhibits in a structured, tabular format. Unlike previous work that focuses on general image-text understanding, this study specifically targets the nuanced details and relationships within museum data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet85">
            <div class="start-time-icon" title="Play from here">36:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00966" target="_blank">@arXiv 2409.00966</a>
                    <span class="tweet-title">Graph Matching: When Correlation Meets Community Detection, It Gets Tricky!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This paper investigates the detection problem for correlated stochastic block models, focusing on the power and limitations of low-degree polynomial tests. It establishes a sharp computational transition for these tests, showing that they can distinguish correlated block models from independent Erd˝os-R´enyi graphs if and only if the subsampling probability exceeds a certain threshold. This threshold is determined by the minimum of two thresholds: the correlation detection threshold for Erd˝os-R´enyi graphs and the Kesten-Stigum threshold for community detection in block models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet86">
            <div class="start-time-icon" title="Play from here">36:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01154" target="_blank">@arXiv 2409.01154</a>
                    <span class="tweet-title">Flu Forecasting:  Neural Networks Get a Grip on Uncertainty</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research introduces two new frameworks for forecasting infectious disease prevalence using neural networks. The first framework incorporates Bayesian layers to produce uncertainty estimates, while the second framework uses neural ordinary differential equations to bridge the gap between mechanistic compartmental models and neural networks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet87">
            <div class="start-time-icon" title="Play from here">36:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01362" target="_blank">@arXiv 2409.01362</a>
                    <span class="tweet-title">Time Series Detective: Unmasking Temporal Patterns with Sparse Kernels</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT, University of Central Florida, McGill University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to learning convolutional kernels from time series data by formulating the problem as a sparse regression with non-negativity constraints. This differs from previous work by automatically learning kernels, reducing human bias, and enhancing interpretability through sparsity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet88">
            <div class="start-time-icon" title="Play from here">37:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01184" target="_blank">@arXiv 2409.01184</a>
                    <span class="tweet-title">Brain Surgery, But Make It AI: A Challenge to Automate Pituitary Tumor Removal</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London</span>
                </div>
                <div class="primary-text">
                    This research focuses on workflow recognition in endoscopic pituitary surgery, a unique task compared to other minimally invasive surgeries due to the smaller working space and higher frequency of instrument and step switching.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet89">
            <div class="start-time-icon" title="Play from here">37:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01175" target="_blank">@arXiv 2409.01175</a>
                    <span class="tweet-title">Logit Scaling: A Simple Trick to Spot Fake News in AI!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Belgrade, ML Collective, Google DeepMind</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method called Logit Scaling (LTS) for out-of-distribution (OOD) detection. Unlike previous methods, LTS doesn't require access to the model's training data or any additional training steps.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet90">
            <div class="start-time-icon" title="Play from here">38:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01545" target="_blank">@arXiv 2409.01545</a>
                    <span class="tweet-title">Speech Enhancement Gets a Noise Makeover: GANs Learn to Mimic Real-World Sounds</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National Taiwan Normal University, Academia Sinica, United-Link Co.  Ltd.</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel data simulation method for speech enhancement that leverages noise embeddings extracted from target-domain data. Unlike previous approaches that focus on replicating overall spectral characteristics, this method explicitly models and incorporates the intricate, fine-grained features of the noise.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet91">
            <div class="start-time-icon" title="Play from here">38:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00041" target="_blank">@arXiv 2409.00041</a>
                    <span class="tweet-title">Waveform Noise:  The Secret Code of Line Access</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, University of Sydney, University of Oxford...</span>
                </div>
                <div class="primary-text">
                    This research focuses on detecting line-access events in critical care by analyzing the noise artifacts in high-frequency blood pressure waveform data, rather than removing them as is typically done.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet92">
            <div class="start-time-icon" title="Play from here">39:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01037" target="_blank">@arXiv 2409.01037</a>
                    <span class="tweet-title">Cartoon Capers: New Dataset Unmasks Metaphor and Sarcasm in Memes!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark dataset called NYK-MS, which focuses on understanding metaphor and sarcasm in cartoon-caption pairs. Unlike previous datasets that primarily rely on text or memes, NYK-MS leverages the rich visual and textual information present in cartoons.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet93">
            <div class="start-time-icon" title="Play from here">39:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01281" target="_blank">@arXiv 2409.01281</a>
                    <span class="tweet-title">LLMs Get a Speed Boost:  Prefixing Their Way to Faster Reasoning!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Shanghai Jiao Tong University, Microsoft</span>
                </div>
                <div class="primary-text">
                    This paper introduces "path-consistency," a method that leverages the confidence of answers generated in earlier branches to identify the prefix of the most promising path. This approach dynamically guides the generation of subsequent branches, reducing both errors and redundancies from random or less useful sampling in self-consistency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet94">
            <div class="start-time-icon" title="Play from here">39:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01089" target="_blank">@arXiv 2409.01089</a>
                    <span class="tweet-title">Deep Learning on Mobile Devices:  A Framework That's Always Ready to Adapt!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National Technical University of Athens, Samsung</span>
                </div>
                <div class="primary-text">
                    This research introduces CARIn, a framework that optimizes deep learning (DL) applications on mobile devices by generating a set of execution configurations that anticipate and adapt to runtime fluctuations. This differs from previous work that typically focuses on finding a single optimal configuration.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet95">
            <div class="start-time-icon" title="Play from here">40:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00137" target="_blank">@arXiv 2409.00137</a>
                    <span class="tweet-title">AI Jailbreak:  Multi-Turn Attacks Are the New Escape Route!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of California  Berkeley, Stanford University, Georgia Institute of Technology...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset and framework for studying multi-turn jailbreak attacks on large language models (LLMs). Unlike previous work that focused on single-turn attacks, this study explores how malicious instructions can be spread across multiple prompts, potentially bypassing safety measures.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet96">
            <div class="start-time-icon" title="Play from here">40:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01447" target="_blank">@arXiv 2409.01447</a>
                    <span class="tweet-title">Zero-Sum Games: When AI Learns to Play Nice (and Win!)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Purdue University, University of Maryland, Caltech...</span>
                </div>
                <div class="primary-text">
                    This research focuses on developing independent learning dynamics for two-player zero-sum games, both matrix and stochastic, with provable last-iterate finite-sample guarantees. Unlike previous work, this study achieves polynomial sample complexity for finding a Nash equilibrium in both settings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet97">
            <div class="start-time-icon" title="Play from here">41:10</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00853" target="_blank">@arXiv 2409.00853</a>
                    <span class="tweet-title">Robots Learn to Farm, Code, and Communicate: A New AI Simulation Evolves Culture</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research differs from previous work by focusing on the evolution of higher-level cognitive abilities, such as cultural accumulation, in an open-ended simulation. Instead of modeling low-level processes like physics or chemistry, the researchers created an environment where agents can interact with programmable robots, allowing them to develop complex behaviors like tool use and communication.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet98">
            <div class="start-time-icon" title="Play from here">41:34</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02026" target="_blank">@arXiv 2409.02026</a>
                    <span class="tweet-title">LLMs on a Diet:  How to Slim Down Your Language Model Without Losing Its Smarts</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new method for compressing large language models (LLMs) by quantizing their weights using a convex optimization framework. Unlike previous methods that rely on heuristics or approximations, this approach aims to find the optimal bit depth for each weight matrix to minimize prediction accuracy loss.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet99">
            <div class="start-time-icon" title="Play from here">42:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00015" target="_blank">@arXiv 2409.00015</a>
                    <span class="tweet-title">AI's Got a License to Learn: Dynamic Certification for Responsible Robots</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Universitat Politècnica de Catalunya, University of California  San Diego, The University of Texas at Austin</span>
                </div>
                <div class="primary-text">
                    This paper proposes a dynamic certification framework for embodied AI systems, which differs from traditional static certification by allowing for continuous evaluation and adjustment of the system's operational envelope based on real-world performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet100">
            <div class="start-time-icon" title="Play from here">42:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00084" target="_blank">@arXiv 2409.00084</a>
                    <span class="tweet-title">LLMs: Gastroenterology's New BFFs?  (But They Can't See!)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Icahn School of Medicine at Mount Sinai, University of Texas Health, Virginia Hospital Center...</span>
                </div>
                <div class="primary-text">
                    This study evaluates the performance of large language models (LLMs) and vision-language models (VLMs) on gastroenterology board exams, focusing on the impact of model configurations, prompt engineering, and the integration of visual data. It differs from previous work by specifically examining the performance of VLMs on image-containing questions, a previously unexplored area in medical reasoning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet101">
            <div class="start-time-icon" title="Play from here">42:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00112" target="_blank">@arXiv 2409.00112</a>
                    <span class="tweet-title">GPT-4 as Therapist: Can AI Really Help You Solve Your Problems?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, Dartmouth College</span>
                </div>
                <div class="primary-text">
                    This study explores the use of prompt engineering to improve the performance of Large Language Models (LLMs) in delivering Problem-Solving Therapy (PST), focusing on the symptom identification and goal-setting phases. Unlike previous work that focused on fine-tuning LLMs for specific tasks, this research investigates the potential of prompt engineering as a more efficient and resource-friendly approach.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet102">
            <div class="start-time-icon" title="Play from here">43:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00103" target="_blank">@arXiv 2409.00103</a>
                    <span class="tweet-title">LLMs:  Causal Reasoning's  New  Consistency  Crisis?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Polytechnique Fédérale de Lausanne, University of Waterloo</span>
                </div>
                <div class="primary-text">
                    This research introduces the concept of "causal epistemic consistency" to evaluate how well LLMs can differentiate between subtle variations in causal reasoning. It goes beyond simply identifying the existence of a causal relationship and focuses on the nuanced impact of intermediate factors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet103">
            <div class="start-time-icon" title="Play from here">43:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01293" target="_blank">@arXiv 2409.01293</a>
                    <span class="tweet-title">Chaos to Clarity: A New Method for Taming Dynamical Systems</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This research builds upon the MAGI method for analyzing dynamical systems, introducing a novel approach called PilotMAGI (pMAGI) that significantly improves numerical stability and accuracy in parameter inference and trajectory reconstruction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet104">
            <div class="start-time-icon" title="Play from here">44:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02069" target="_blank">@arXiv 2409.02069</a>
                    <span class="tweet-title">Brushing Up on AI: How a Smart Toothbrush Learned to Motivate Patients</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, Imperial College London, University of Michigan...</span>
                </div>
                <div class="primary-text">
                    This research focuses on deploying an online reinforcement learning algorithm as part of a mHealth intervention in a registered clinical trial, addressing the unique challenges of this setting.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet105">
            <div class="start-time-icon" title="Play from here">44:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00438" target="_blank">@arXiv 2409.00438</a>
                    <span class="tweet-title">Financial News:  Not Just Chatter, It's a Hypergraph!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge, Durham University</span>
                </div>
                <div class="primary-text">
                    This research uses a geometric hypergraph attention network (GHAN) to analyze the impact of financial news on stock prices. Unlike traditional graph-based models, hypergraphs can capture complex, multi-entity interactions, such as the simultaneous impact of a single news event on multiple stocks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet106">
            <div class="start-time-icon" title="Play from here">45:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00696" target="_blank">@arXiv 2409.00696</a>
                    <span class="tweet-title">Rating LLMs:  A Fairer, Cheaper, and More Nuanced Approach</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This paper introduces POLYRATING, a new rating system for LLMs that accounts for biases in human and LLM-based evaluations. Unlike previous systems, POLYRATING models shared features and biases, allowing for more accurate and nuanced comparisons of model performance across different tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet107">
            <div class="start-time-icon" title="Play from here">45:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00717" target="_blank">@arXiv 2409.00717</a>
                    <span class="tweet-title">Teaching AI to Play Nice: How Human Feedback Can Tame Multi-Agent Chaos</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, Harvard University</span>
                </div>
                <div class="primary-text">
                    This research focuses on multi-agent reinforcement learning from human feedback (MARLHF), a new area that explores how to train multiple AI agents to cooperate or compete effectively based on human preferences. Unlike previous work that focused on single-agent RLHF, this paper investigates the unique challenges and requirements for training multiple agents simultaneously.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet108">
            <div class="start-time-icon" title="Play from here">45:48</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00129" target="_blank">@arXiv 2409.00129</a>
                    <span class="tweet-title">Minishogi's State-Space: A 2.38 x 10^18 Position Puzzle!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research estimates the number of reachable positions in Minishogi using a statistical sampling method, unlike previous studies that focused on precise calculations or strict bounds.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet109">
            <div class="start-time-icon" title="Play from here">46:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00391" target="_blank">@arXiv 2409.00391</a>
                    <span class="tweet-title">Depressed? Don't Worry, Your Voice Can Tell Us!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, San Diego State University</span>
                </div>
                <div class="primary-text">
                    This research introduces two new models, DAAMAudioCNNLSTM and DAAMAudioTransformer, which use a Density Adaptive Attention Mechanism (DAAM) to focus on the most informative parts of speech signals for depression detection. This approach differs from previous work by dynamically weighting features based on their importance, leading to more accurate and explainable results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet110">
            <div class="start-time-icon" title="Play from here">46:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00729" target="_blank">@arXiv 2409.00729</a>
                    <span class="tweet-title">Can We Pinpoint What Makes LLMs Say What They Say?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This paper introduces the concept of "context attribution," which aims to identify the specific parts of the context that cause a language model to generate a particular statement. Unlike previous work that focuses on teaching models to generate citations, this method directly identifies the sources the model actually uses.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet111">
            <div class="start-time-icon" title="Play from here">47:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01712" target="_blank">@arXiv 2409.01712</a>
                    <span class="tweet-title">GWAS Gets a Speed Boost: Mixed Precision Makes Epistasis Analysis a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">King Abdullah University of Science and Technology, MIT, Saint Louis University...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel mixed-precision approach for Kernel Ridge Regression (KRR) in Genome-Wide Association Studies (GWAS). Unlike previous methods that rely on single precision, this approach leverages multiple precisions, including INT8 and FP8, to accelerate computations while maintaining accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet112">
            <div class="start-time-icon" title="Play from here">47:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00163" target="_blank">@arXiv 2409.00163</a>
                    <span class="tweet-title">Deep Learning Predicts Esophageal Cancer Recurrence:  A New Recipe for Personalized Treatment</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research uses deep neural networks to predict disease-free survival and overall survival in patients with esophageal cancer after surgery. Unlike previous studies that relied on single-center data, this study utilizes a large, multicenter dataset, making the findings more generalizable.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet113">
            <div class="start-time-icon" title="Play from here">47:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01477" target="_blank">@arXiv 2409.01477</a>
                    <span class="tweet-title">Reinforcement Learning's New Trick:  Zeroth-Order Gradient Magic</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This paper introduces a new actor-critic algorithm called Compatible Policy Gradient (CPG) that uses a zeroth-order approximation of the action-value gradient. This approach avoids the need for precise gradient computations, which is a common challenge in deterministic policy gradient (DPG) methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet114">
            <div class="start-time-icon" title="Play from here">48:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01564" target="_blank">@arXiv 2409.01564</a>
                    <span class="tweet-title">Spiking Neural Networks Get a Temporal Boost: Key-Residual Frames for Action Recognition!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University</span>
                </div>
                <div class="primary-text">
                    This research proposes ReSpike, a hybrid neural network architecture that combines the strengths of Artificial Neural Networks (ANNs) and Spiking Neural Networks (SNNs) for action recognition. Unlike previous work that primarily focused on static image tasks, ReSpike leverages the temporal information in video clips by decomposing them into key frames (for spatial information) and residual frames (for temporal information).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet115">
            <div class="start-time-icon" title="Play from here">48:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.02078" target="_blank">@arXiv 2409.02078</a>
                    <span class="tweet-title">Political Text Analysis: Tiny Models, Big Results!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Princeton University, The Pennsylvania State University, Louisiana State University</span>
                </div>
                <div class="primary-text">
                    This research introduces two open-source language models, Political DEBATE Large and Base, specifically trained for zero-shot and few-shot classification of political text. Unlike previous work that relies on large, proprietary models, these models are significantly smaller and more efficient, yet achieve comparable or even better performance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet116">
            <div class="start-time-icon" title="Play from here">49:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01668" target="_blank">@arXiv 2409.01668</a>
                    <span class="tweet-title">One-Shot Voice Conversion:  A Pure Transformer Trick for Mimicking Voices</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Xiangtan University, Peking University</span>
                </div>
                <div class="primary-text">
                    This research proposes Pureformer-VC, a voice conversion framework that uses Conformer and Zipformer blocks for disentangled encoding and style transfer. Unlike previous methods, it relies on a pure transformer architecture and incorporates a triplet loss for discriminative training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet117">
            <div class="start-time-icon" title="Play from here">49:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00993" target="_blank">@arXiv 2409.00993</a>
                    <span class="tweet-title">LLMs Learn to Gossip: How AI Agents Develop Social Norms Through Chat</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research explores the emergence of social norms in LLMs through natural language interactions, going beyond simple choices like "cooperate" or "defect" and allowing for more complex strategies and discussions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet118">
            <div class="start-time-icon" title="Play from here">50:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00313" target="_blank">@arXiv 2409.00313</a>
                    <span class="tweet-title">Sketchy Business: Training-Free Diffusion Model Makes Images Look Like You Drew Them!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This paper proposes a training-free method for incorporating sketches as guidance in image generation using pre-trained diffusion models. Unlike previous methods that rely on additional training or fine-tuning, this approach leverages the cross-attention maps of diffusion models to track the layout and structure features of sketches, enabling the generation of images that closely resemble the input sketch.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet119">
            <div class="start-time-icon" title="Play from here">50:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01138" target="_blank">@arXiv 2409.01138</a>
                    <span class="tweet-title">Fake Satellite Images: Can You Tell the Real Deal From the Deep Fake?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Berlin University of Applied Sciences, Einstein Center Digital Future, Princeton University</span>
                </div>
                <div class="primary-text">
                    This research focuses on generating synthetic satellite imagery of rare objects, specifically nuclear power plants. Unlike previous work, it investigates the effectiveness of using both textual and image input to guide the generation process, and it compares the results of different models with human perception.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet120">
            <div class="start-time-icon" title="Play from here">50:46</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00301" target="_blank">@arXiv 2409.00301</a>
                    <span class="tweet-title">Driving in the Dark? No Problem! New AI Can See Through Rain, Snow, and Even Tunnels!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a new framework called ContextVLM that uses vision-language models (VLMs) to detect driving contexts, such as weather, lighting, and traffic conditions, in autonomous vehicles. Unlike traditional supervised learning approaches, ContextVLM leverages zero-shot and few-shot learning, reducing the need for large, class-balanced, and context-annotated datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet121">
            <div class="start-time-icon" title="Play from here">51:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01574" target="_blank">@arXiv 2409.01574</a>
                    <span class="tweet-title">Parallel Tempering Gets a Brain: Policy Gradients for Optimal MCMC</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel reinforcement learning approach to optimize parallel tempering MCMC. Unlike previous adaptive methods that focus on achieving uniform acceptance rates, this work explores alternative reward formulations and proposes a new metric based on the distance between swapped states.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet122">
            <div class="start-time-icon" title="Play from here">51:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00768" target="_blank">@arXiv 2409.00768</a>
                    <span class="tweet-title">Super-Resolution:  Low-Res Images, High-Res Results?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National Institute of Advanced Industrial Science and Technology, Keio University, University of Tsukuba...</span>
                </div>
                <div class="primary-text">
                    This research challenges the common assumption that high-resolution images are essential for training image super-resolution (SR) models. The authors propose a new dataset, DiverSeg, constructed from low-resolution images, demonstrating that SR models can achieve comparable or even better performance than those trained on high-resolution datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet123">
            <div class="start-time-icon" title="Play from here">52:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00393" target="_blank">@arXiv 2409.00393</a>
                    <span class="tweet-title">Neural Networks Learn to Control, But Can They Stay Stable?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach called Lyapunov-NODE Control (L-NODEC) for learning control policies in continuous-time optimal control problems. Unlike previous methods, L-NODEC incorporates a Lyapunov condition into the learning process, ensuring the stability of the controlled system.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet124">
            <div class="start-time-icon" title="Play from here">52:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01985" target="_blank">@arXiv 2409.01985</a>
                    <span class="tweet-title">Denoising Without a Clue: How to Clean Up Noisy Images When You Don't Know the Noise Level</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CNRS, ENS de Lyon, University of Edinburgh...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new self-supervised learning method called UNSURE, which can denoise images without requiring knowledge of the noise level. Unlike previous methods that either assume full knowledge of the noise distribution or rely on cross-validation techniques, UNSURE leverages the concept of divergence-free estimators, which are less constrained and more expressive.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet125">
            <div class="start-time-icon" title="Play from here">52:53</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00269" target="_blank">@arXiv 2409.00269</a>
                    <span class="tweet-title">Your Brain, My Brain:  How AI Can Tell If We Think Alike</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces a new method for measuring subjective similarity between human-written and AI-generated content. Unlike previous approaches that rely on large datasets and machine learning, this method uses a cognitive model to predict individual preferences and biases.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet126">
            <div class="start-time-icon" title="Play from here">53:15</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00025" target="_blank">@arXiv 2409.00025</a>
                    <span class="tweet-title">Power Quality Problems?  Vision Transformers to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Connecticut, Concordia University, University of Toronto...</span>
                </div>
                <div class="primary-text">
                    This research uses a Vision Transformer (ViT) model to classify power quality disturbances (PQDs), a novel approach compared to previous methods that relied on hand-crafted features or convolutional neural networks (CNNs).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet127">
            <div class="start-time-icon" title="Play from here">53:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01217" target="_blank">@arXiv 2409.01217</a>
                    <span class="tweet-title">Low-Resource Languages Get a Voice:  Multilingual TTS Makes a Splash!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Mohammed V University in Rabat, International University of Rabat, University of Leeds...</span>
                </div>
                <div class="primary-text">
                    This research explores the use of multilingual pre-training for Text-to-Speech (TTS) systems in low-resource languages. Unlike previous work that primarily focused on monolingual transfer learning, this study investigates the benefits of leveraging data from multiple languages, including those from different language families.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet128">
            <div class="start-time-icon" title="Play from here">54:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01245" target="_blank">@arXiv 2409.01245</a>
                    <span class="tweet-title">Safe Exploration in RL:  Consecutive Cost Steps Tell the Tale!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Freiburg</span>
                </div>
                <div class="primary-text">
                    This research introduces a new metric called Expected Maximum Consecutive Cost steps (EMCC) to evaluate safe exploration in reinforcement learning (RL) algorithms. Unlike previous metrics that focus on overall cost, EMCC considers the severity of unsafe actions based on their consecutive occurrences during training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet129">
            <div class="start-time-icon" title="Play from here">54:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00606" target="_blank">@arXiv 2409.00606</a>
                    <span class="tweet-title">Style Transfer Showdown: Neural Networks vs. Patchwork Art</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Illinois, Washington University in St. Louis, Georgia Institute of Technology...</span>
                </div>
                <div class="primary-text">
                    This research compares traditional style transfer methods, which rely on stitching together image patches, with a modern deep-learning approach that uses segmentation to isolate foreground objects and apply style transfer only to the background.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet130">
            <div class="start-time-icon" title="Play from here">55:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00721" target="_blank">@arXiv 2409.00721</a>
                    <span class="tweet-title">Chatbots Go to the Polls: Who Would ChatGPT Vote For?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Czech University of Life Sciences Prague</span>
                </div>
                <div class="primary-text">
                    This study distinguishes itself by focusing on the political bias of ChatGPT and Gemini in the context of the 2024 European Parliament elections, using a standardized prompt across all 27 EU member states.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet131">
            <div class="start-time-icon" title="Play from here">55:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00869" target="_blank">@arXiv 2409.00869</a>
                    <span class="tweet-title">Robot Butler: Deep Learning Makes Tabletop Tidying a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of California  Berkeley, University of Michigan, Massachusetts Institute of Technology...</span>
                </div>
                <div class="primary-text">
                    This research focuses on using deep learning to estimate the orientation of objects on a tabletop, which is a unique application compared to previous work that primarily focused on object recognition and 3D pose estimation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet132">
            <div class="start-time-icon" title="Play from here">55:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.01407" target="_blank">@arXiv 2409.01407</a>
                    <span class="tweet-title">Cosmological Data Fusion:  Flowing Through the Nuisance Parameters</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research proposes using normalising flows to emulate marginal posterior distributions of cosmological parameters, allowing for efficient combination of constraints from independent datasets without increasing the dimensionality of the parameter space. This differs from previous work that often relies on sampling over the full joint parameter space, which can be computationally expensive.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet133">
            <div class="start-time-icon" title="Play from here">56:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00128" target="_blank">@arXiv 2409.00128</a>
                    <span class="tweet-title">Can AI Pass the Psych 101 Test? LLMs Put to the Test in a Big Way!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research stands out by replicating a large number of psychological experiments using GPT-4, a powerful language model, as a simulated participant. It goes beyond previous studies that focused on specific experiments or assessments, providing a broader understanding of LLMs' capabilities in replicating human responses across diverse psychological contexts.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet134">
            <div class="start-time-icon" title="Play from here">56:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.00608" target="_blank">@arXiv 2409.00608</a>
                    <span class="tweet-title">TinyAgent: Siri on Steroids, But It Lives on Your Mac!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley, ICSI</span>
                </div>
                <div class="primary-text">
                    This research focuses on training smaller language models (SLMs) for function calling, a key component of agentic systems, and deploying them locally on devices like Macbooks. This differs from previous work that primarily focused on large models requiring cloud infrastructure.
                </div>
            </div>
        </div>

        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Listen and learn ^.^</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202409042340_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>    
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/privacy.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading privacy.html:', error));
    </script>
</body>
</html>