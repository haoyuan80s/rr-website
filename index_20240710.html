<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - AI Paper Picks of the Day</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 80px;"></div>
            <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                Freshest
                Top Picks:
                <span class="highlightNumber" style="font-size: 28px;">65</span> out of <span
                    class="highlightNumber">290</span> arXiv AI
                papers
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="line-height: 1.2;">just released on</div>
                <div id="data-default-date" data-date="2024-07-10"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 10px;">
            </div>
        </div>

        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">
                00:51
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07018" target="_blank">
                        @arXiv 2407.07018
                    </a>
                    <span class="tweet-title">
                        Can AI Read Your Diary to Predict Treatment Effects?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto, Vector Institute, Meta
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method called NATURAL, which uses large language models (LLMs) to
                    estimate causal effects from unstructured text data. Unlike previous approaches that rely on
                    structured data, NATURAL leverages the ability of LLMs to extract conditional distributions from
                    text, enabling the estimation of treatment effects without manual data structuring.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">
                01:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06380" target="_blank">
                        @arXiv 2407.06380
                    </a>
                    <span class="tweet-title">
                        Data Diet: How to Build the Perfect Language Model Menu
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Nvidia
                    </span>
                </div>
                <div class="primary-text">
                    This research systematically analyzes the entire pipeline of constructing pretraining datasets for
                    language models, going beyond just data curation and exploring a wider range of data sources and
                    languages.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">
                01:31
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06460" target="_blank">
                        @arXiv 2407.06460
                    </a>
                    <span class="tweet-title">
                        Making AI Forget: A Six-Way Test for Machine Unlearning
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington, Princeton University, University of Southern California...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new benchmark called MUSE to evaluate machine unlearning algorithms for
                    language models. MUSE considers six key properties, including verbatim memorization, knowledge
                    memorization, privacy leakage, utility preservation, scalability, and sustainability. This
                    comprehensive approach goes beyond previous benchmarks that focused on specific tasks like question
                    answering.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">
                02:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06842" target="_blank">
                        @arXiv 2407.06842
                    </a>
                    <span class="tweet-title">
                        Chatting Your Way to a 3D Makeover: New AI Edits Scenes with Text Prompts
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Beihang University, Google, Megvii...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to 3D scene editing by decoupling the 2D editing process
                    from the 3D reconstruction process. This allows for flexible integration of various visual models,
                    unlike previous methods that relied on intricate pipelines combining specific 2D and 3D models.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">
                02:30
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06946" target="_blank">
                        @arXiv 2407.06946
                    </a>
                    <span class="tweet-title">
                        Can AI Recognize Itself? A New Test for Language Model Self-Awareness
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        École Polytechnique Fédérale de Lausanne
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to assess self-recognition in language models (LMs) using
                    model-generated "security questions." Unlike previous work that relies on internal model parameters
                    or output probabilities, this method only requires model outputs, making it externally administrable
                    and scalable.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">
                02:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07082" target="_blank">
                        @arXiv 2407.07082
                    </a>
                    <span class="tweet-title">
                        Can AI Learn to Be a Better Teacher for AI?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the use of learned optimization in reinforcement learning (RL). Unlike
                    previous work that focuses on improving specific aspects of RL, this paper proposes a meta-learning
                    approach to train an optimizer that can handle multiple challenges in RL simultaneously, including
                    non-stationarity, plasticity loss, and exploration.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">
                03:21
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07038" target="_blank">
                        @arXiv 2407.07038
                    </a>
                    <span class="tweet-title">
                        Climate Change Chat: A New Model Decodes Disagreements on Reddit
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research uses Graph Attention Networks (GATs) to analyze comment-reply pairs on Reddit,
                    focusing on climate change discussions. Unlike previous work that primarily relies on textual
                    features, this model incorporates both textual embeddings and sentiment scores, capturing the
                    intricate interactions and sentiment dynamics within these conversations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">
                03:49
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06723" target="_blank">
                        @arXiv 2407.06723
                    </a>
                    <span class="tweet-title">
                        Captions Get a Graph Makeover: Images Described with Links and Relationships!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Apple Inc., University of Washington, National Tsing Hua University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new way to annotate images using a graph-based captioning (GBC) format.
                    Unlike traditional scene graphs, GBC uses plain text descriptions for each node, allowing for more
                    flexibility and intuitiveness.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">
                04:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07074" target="_blank">
                        @arXiv 2407.07074
                    </a>
                    <span class="tweet-title">
                        SLAM Goes Decentralized: Hyperion's Continuous-Time GBP Framework
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich, Imperial College London, University of Cyprus
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel continuous-time Gaussian Belief Propagation (GBP) framework, called
                    Hyperion, for decentralized probabilistic inference in Continuous-Time SLAM (CTSLAM). Unlike
                    traditional centralized NLLS optimization approaches, Hyperion enables distributed and asynchronous
                    state estimation across multiple agents.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">
                04:48
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06334" target="_blank">
                        @arXiv 2407.06334
                    </a>
                    <span class="tweet-title">
                        Double-Ended Synthesis Planning: A Chemical Search Party!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT, Georgia Institute of Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to computer-aided synthesis planning (CASP) that
                    incorporates starting material constraints. Unlike previous methods that assume any building block
                    can be used, this method specifically targets desired starting materials, making it more relevant to
                    real-world scenarios.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">
                05:11
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07061" target="_blank">
                        @arXiv 2407.07061
                    </a>
                    <span class="tweet-title">
                        Internet of Agents: LLMs Team Up to Solve Problems Like a Boss!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes the Internet of Agents (IoA), a framework that allows diverse, third-party
                    agents to collaborate on tasks. Unlike previous multi-agent systems, IoA supports distributed agents
                    and dynamic communication, enabling agents to form teams and manage conversations autonomously.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">
                05:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06325" target="_blank">
                        @arXiv 2407.06325
                    </a>
                    <span class="tweet-title">
                        Sparsity Saves the Day: How Compressive Sensing Makes Microservices Management a Breeze
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Texas A&amp;M University, University of Texas at Austin
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces CONGO, an algorithm that leverages sparsity in gradient information to
                    optimize resource allocation in microservices. Unlike previous methods that rely on full gradient
                    information, CONGO uses compressive sensing to estimate gradients with fewer samples, leading to
                    faster convergence and reduced computational cost.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">
                06:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07090" target="_blank">
                        @arXiv 2407.07090
                    </a>
                    <span class="tweet-title">
                        Ray Tracing Radiance Fields: Faster Than a Speeding Splat!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        NVIDIA, University of Toronto, Vector Institute
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a new method for ray tracing particle-based scene representations, specifically
                    focusing on 3D Gaussian Splatting. Unlike previous work that relied on rasterization, this approach
                    leverages specialized GPU ray tracing hardware for more efficient rendering.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">
                06:33
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06863" target="_blank">
                        @arXiv 2407.06863
                    </a>
                    <span class="tweet-title">
                        AI's Got Culture: New Benchmark Tests If Text-to-Image Models Can Tell a Samosa from a Sushi
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Google
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark called CUBE to evaluate the cultural competence of
                    text-to-image models. Unlike previous benchmarks that focus on realism and faithfulness, CUBE
                    assesses the models' ability to accurately represent cultural artifacts and avoid stereotypical
                    depictions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">
                06:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07087" target="_blank">
                        @arXiv 2407.07087
                    </a>
                    <span class="tweet-title">
                        AI's Got a Copycat Problem: New Benchmark Catches Literal and Non-Literal Copying
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington, Cornell University, Allen Institute for AI
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces COPYBENCH, a benchmark that evaluates both literal and non-literal copying
                    of copyrighted text by language models. Previous work primarily focused on literal copying,
                    neglecting the more nuanced forms of plagiarism that can occur.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">
                07:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06464" target="_blank">
                        @arXiv 2407.06464
                    </a>
                    <span class="tweet-title">
                        Sidewalk Sleuths: New Dataset Helps Us See the City Through Pedestrian Eyes
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of São Paulo, University of Illinois Chicago, Massachusetts Institute of Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new multimodal dataset called SideSeeing, which combines video footage,
                    sensor data, and GPS information to provide a comprehensive understanding of sidewalk accessibility.
                    Unlike previous datasets that focus on specific aspects of sidewalks, SideSeeing offers a more
                    holistic view by integrating multiple data sources.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">
                07:56
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06518" target="_blank">
                        @arXiv 2407.06518
                    </a>
                    <span class="tweet-title">
                        V2X Communication: Graph Neural Networks to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Jiangnan University, Tsinghua University, Shanghai Jiao Tong University...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for resource allocation in V2X communication by integrating
                    Graph Neural Networks (GNN) with Deep Reinforcement Learning (DRL). Unlike previous work that
                    primarily focuses on centralized resource allocation, this approach enables distributed
                    decision-making, allowing vehicles to make independent resource allocation decisions based on local
                    observations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">
                08:27
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07088" target="_blank">
                        @arXiv 2407.07088
                    </a>
                    <span class="tweet-title">
                        Spacecraft Dating: How to Make Sure Your Satellite Doesn't Get Lost in Space!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University, Hebrew University of Jerusalem, IT University of Copenhagen...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel design-for-verification approach that modifies the training loop of
                    deep reinforcement learning (DRL) controllers to make them more verification-friendly. It also
                    utilizes formal verification techniques, specifically k-induction, to ensure the correctness of the
                    training process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">
                08:58
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07059" target="_blank">
                        @arXiv 2407.07059
                    </a>
                    <span class="tweet-title">
                        Brain-Like Models: Are We Measuring the Right Things?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT, NYU, University of Tübingen
                    </span>
                </div>
                <div class="primary-text">
                    This research directly optimizes synthetic datasets to become more similar to real neural
                    recordings, allowing for a deeper understanding of how different similarity measures prioritize
                    various aspects of the data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">
                09:23
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06683" target="_blank">
                        @arXiv 2407.06683
                    </a>
                    <span class="tweet-title">
                        Mapping the Future: How BEV Features Speed Up Self-Driving Cars
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto, Nvidia, Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes directly using internal features from online map estimation models for
                    behavior prediction, rather than relying solely on decoded map elements. This approach allows for
                    tighter integration between mapping and prediction, potentially leading to faster inference times
                    and more accurate predictions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">
                09:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06249" target="_blank">
                        @arXiv 2407.06249
                    </a>
                    <span class="tweet-title">
                        LLMs Learn New Tricks: A Benchmark for Teaching Code Models API Updates
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Texas at Austin
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces CodeUpdateArena, a benchmark specifically designed to evaluate how well
                    large language models (LLMs) can incorporate new API function updates into their knowledge. Unlike
                    previous benchmarks that focus on general code generation, CodeUpdateArena focuses on the ability of
                    LLMs to adapt to evolving APIs, a crucial aspect for real-world code generation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">
                10:20
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07093" target="_blank">
                        @arXiv 2407.07093
                    </a>
                    <span class="tweet-title">
                        Binary Brains: LLMs Go Full 1-Bit, No Pretraining Needed!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Mohamed bin Zayed University of Artificial Intelligence, CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research presents a novel method for training fully binarized large language models (LLMs) from
                    scratch, without relying on pre-trained models. It utilizes an autoregressive distillation loss
                    function to guide the training process, achieving comparable performance to full-precision LLMs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">
                10:42
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06800" target="_blank">
                        @arXiv 2407.06800
                    </a>
                    <span class="tweet-title">
                        Whisper's New Language Trick: Teaching an Old Dog New Sounds!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on adding new languages to pre-trained Automatic Speech Recognition (ASR)
                    models without sacrificing performance on existing languages. It compares different fine-tuning
                    methods, including a novel approach called Soft Language Code Tuning (SLCT), and explores the use of
                    Elastic Weight Consolidation (EWC) to mitigate catastrophic forgetting.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">
                11:07
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06756" target="_blank">
                        @arXiv 2407.06756
                    </a>
                    <span class="tweet-title">
                        Fourier Features: Fast Learners, Slow Thinkers?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University College London, DeepMind
                    </span>
                </div>
                <div class="primary-text">
                    This research investigates the frequency of representations learned by periodic activation functions
                    in deep reinforcement learning (RL) algorithms. Unlike previous work that focused on either low or
                    high frequency representations, this study empirically shows that periodic activations consistently
                    converge to high frequencies regardless of their initialisation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">
                11:28
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07086" target="_blank">
                        @arXiv 2407.07086
                    </a>
                    <span class="tweet-title">
                        LLMs Get Minds of Their Own: AI Agents Learn to Read Opponents' Minds
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Stanford University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to multi-agent reinforcement learning (MARL) by leveraging
                    large language models (LLMs) to create agents that can infer the strategies of other agents. This
                    differs from previous work by incorporating a "Theory of Mind" module that generates, evaluates, and
                    refines hypotheses about other agents' goals and actions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">
                11:55
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06323" target="_blank">
                        @arXiv 2407.06323
                    </a>
                    <span class="tweet-title">
                        LLMs Gone Wild? New Guardrails Tame Toxic Text with Synthetic Data!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        IBM
                    </span>
                </div>
                <div class="primary-text">
                    This research tackles the "use-mention" distinction problem in social bias detection for LLMs.
                    Unlike previous work, it focuses on generating synthetic data to train guardrail models specifically
                    to differentiate between harmful use and harmless mention of biased language.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">
                12:18
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07055" target="_blank">
                        @arXiv 2407.07055
                    </a>
                    <span class="tweet-title">
                        Folding Embryos: AI Predicts Cell Rearrangements Before They Happen!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Massachusetts Institute of Technology, Northeastern University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a dual-graph data structure to represent multicellular systems, enabling a
                    geometric deep learning model to predict cell rearrangements during embryogenesis. This approach
                    differs from previous work by unifying granular and foam-like physical pictures within a single
                    framework.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">
                12:44
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06628" target="_blank">
                        @arXiv 2407.06628
                    </a>
                    <span class="tweet-title">
                        IMU-Powered Action Recognition: When Your Phone Knows You're Doing the Macarena
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Tokyo
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for action recognition that integrates motion data from
                    body-worn IMUs with egocentric video. The key innovation lies in the use of a multimodal masked
                    autoencoder (MAE) for self-supervised pretraining, which leverages the natural correlation between
                    visual and motion signals. This approach addresses the scarcity of labeled multimodal data and
                    allows the model to learn strong representations from unlabeled data. Additionally, the paper
                    introduces a graph-based IMU modeling technique to capture the collaborative dynamics of multiple
                    IMU devices placed across the body.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">
                13:16
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06939" target="_blank">
                        @arXiv 2407.06939
                    </a>
                    <span class="tweet-title">
                        Robots in Homes: A New Challenge for Open-Vocabulary Manipulation!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        FAIR Georgia Tech Hello Robot Inc Carnegie Mellon Bielefeld University Ukrainian Catholic
                        University Bosch Center for AI IIITA IIT ISM Dhanbad IIITM CNAEIT
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark task for robotics called Open-Vocabulary Mobile
                    Manipulation (OVMM), which focuses on robots finding and manipulating objects in previously unseen
                    environments. This differs from previous work that typically assumes a closed world with known
                    object categories.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">
                13:47
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06866" target="_blank">
                        @arXiv 2407.06866
                    </a>
                    <span class="tweet-title">
                        ChatGPT's Got a Bias: How Your Fandom Affects Its Answers
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University
                    </span>
                </div>
                <div class="primary-text">
                    This paper examines the biases in the guardrails of large language models (LLMs), specifically
                    focusing on how contextual information about the user influences the model's likelihood to refuse a
                    request. This is distinct from previous work that primarily focused on biases in the model's output
                    rather than its refusal mechanisms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">
                14:12
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06813" target="_blank">
                        @arXiv 2407.06813
                    </a>
                    <span class="tweet-title">
                        AI Diplomat Richelieu: Self-Evolving Agent Masters Diplomacy Without Human Help!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, Beijing Information Science and Technology University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces Richelieu, an AI agent that learns to play Diplomacy through self-play,
                    unlike previous methods that relied heavily on human data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">
                14:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.07071" target="_blank">
                        @arXiv 2407.07071
                    </a>
                    <span class="tweet-title">
                        LLMs Hallucinating? LookbackLens to the Rescue!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        MIT
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on detecting and mitigating contextual hallucinations in LLMs, a type of error
                    where the model generates content inconsistent with the provided context. Unlike previous work that
                    primarily focuses on hallucinations arising from the model's internal knowledge, this paper
                    leverages attention maps to identify and address these contextual errors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">
                14:58
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06233" target="_blank">
                        @arXiv 2407.06233
                    </a>
                    <span class="tweet-title">
                        AI: The New Social Theorist?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a framework for AI-driven social theory, where AI models are used to test and
                    generate new social theories based on their predictive power. This approach differs from previous
                    work by emphasizing the need for AI systems to possess specific capabilities like semanticization,
                    transferability, and generativity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">
                15:24
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06304" target="_blank">
                        @arXiv 2407.06304
                    </a>
                    <span class="tweet-title">
                        Video Generation Gets a Visual Makeover: New Model Learns from Images!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Snap Inc., UC Merced, Carnegie Mellon University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new approach to video generation that incorporates visual information
                    during the training process. Unlike previous models that rely solely on text-based prompts, this
                    model leverages a retrieval method to pair multimodal in-context examples with text prompts,
                    enabling it to generate videos grounded in both text and images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">
                15:47
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06529" target="_blank">
                        @arXiv 2407.06529
                    </a>
                    <span class="tweet-title">
                        Financial Fraud Fighters: GNN-CL's Got Your Back!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Columbia University, Rutgers University, College of William &amp; Mary...
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new model called GNN-CL for financial fraud detection. It combines graph
                    neural networks (GNN), convolutional neural networks (CNN), and long short-term memory (LSTM)
                    networks. The key innovation is the use of reinforcement learning to dynamically adjust the weights
                    assigned to central nodes in the GNN, which helps to mitigate the problem of feature weakening that
                    can occur when aggregating information from neighboring nodes.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">
                16:25
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06886" target="_blank">
                        @arXiv 2407.06886
                    </a>
                    <span class="tweet-title">
                        Embodied AI: From Couch Potato to Kitchen King!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Hong Kong, Peking University
                    </span>
                </div>
                <div class="primary-text">
                    This research provides a comprehensive survey of Embodied AI, focusing on the advancements made
                    possible by Multi-modal Large Models (MLMs) and World Models (WMs), which were not fully explored in
                    previous surveys.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">
                16:54
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06235" target="_blank">
                        @arXiv 2407.06235
                    </a>
                    <span class="tweet-title">
                        AI Auditing: From Balance Sheets to Bots, It's Time to Get Real!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford
                    </span>
                </div>
                <div class="primary-text">
                    This research goes beyond just looking at the technical aspects of AI systems and also examines the
                    governance structures of the organizations that design and deploy them.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">
                17:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06677" target="_blank">
                        @arXiv 2407.06677
                    </a>
                    <span class="tweet-title">
                        Transformers Get a Makeover: Dynamic Modules for Better Language Models
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Peking University, Renmin University of China, Tsinghua University...
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new architecture called Mixture-of-Modules (MoM) that breaks the
                    traditional depth-ordered structure of Transformers. Instead of processing tokens sequentially from
                    shallow to deep layers, MoM allows for dynamic selection and assembly of modules, creating a more
                    flexible and efficient computation graph.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">
                17:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06833" target="_blank">
                        @arXiv 2407.06833
                    </a>
                    <span class="tweet-title">
                        CryoET Segmentation Gets a Prompt-Based Makeover: No Training Required!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Carnegie Mellon University, University of Alabama at Birmingham
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a training-free approach for segmenting CryoET tomograms using prompt-based
                    methods. Unlike previous work that relied on supervised training or template matching, this method
                    leverages existing 2D foundation models and a novel prompt-based 3D segmentation pipeline.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">
                18:11
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06512" target="_blank">
                        @arXiv 2407.06512
                    </a>
                    <span class="tweet-title">
                        Moon-derful Data: A New Dataset for Lunar Rover Adventures!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Chinese Academy of Sciences, Chinese Academy of Sciences
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces LuSNAR, a multi-task, multi-scene, and multi-label lunar dataset. Unlike
                    previous datasets focused on single tasks, LuSNAR provides comprehensive data for evaluating
                    autonomous perception and navigation systems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">
                18:31
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06935" target="_blank">
                        @arXiv 2407.06935
                    </a>
                    <span class="tweet-title">
                        Federated Learning Gets a Speed Boost with Hamiltonian Monte Carlo
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ByteDance Inc, Purdue University, Morgan Stanley
                    </span>
                </div>
                <div class="primary-text">
                    This paper introduces a new Bayesian federated learning algorithm called FA-HMC, which uses
                    Hamiltonian Monte Carlo (HMC) for parameter estimation and uncertainty quantification. Unlike
                    previous work that relied on stochastic gradient Langevin dynamics (SGLD), FA-HMC leverages the
                    second-order nature of HMC, which has been shown to be more computationally efficient in various
                    settings.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">
                19:02
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06536" target="_blank">
                        @arXiv 2407.06536
                    </a>
                    <span class="tweet-title">
                        Two-Stage Evolution: A Multi-Objective Optimization Algorithm That's Twice as Nice!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Xi'an Jiaotong University, Nanyang Technological University, University of Science and
                        Technology of China
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel two-stage evolutionary framework for multi-objective optimization
                    (TEMOF). Unlike traditional approaches, TEMOF divides the optimization process into two distinct
                    stages, enhancing the search capability of the population by selectively choosing parents from both
                    the primary population and an external archive.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">
                19:52
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06549" target="_blank">
                        @arXiv 2407.06549
                    </a>
                    <span class="tweet-title">
                        Ads Relevance: One Model to Rule Them All!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel multi-faceted attention model that uses task ID encoding to improve
                    the generalization capability of a single model for multi-task ads relevance. This approach differs
                    from previous work by treating the multi-task problem as a language task, enabling the model to
                    learn from all tasks while only needing to see one task at a time during inference.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">
                20:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06938" target="_blank">
                        @arXiv 2407.06938
                    </a>
                    <span class="tweet-title">
                        RodinHD: Avatar Generation Gets a Hair-Raising Upgrade!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Science and Technology of China, Tsinghua University, Microsoft Research Asia
                    </span>
                </div>
                <div class="primary-text">
                    This paper tackles the issue of "catastrophic forgetting" in 3D avatar generation, which occurs when
                    the model forgets details from previously trained avatars. To address this, the authors introduce a
                    novel data scheduling strategy called "task replay" and a weight consolidation regularization term.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">
                20:50
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06984" target="_blank">
                        @arXiv 2407.06984
                    </a>
                    <span class="tweet-title">
                        Stereo Vision: Seeing Objects in 3D, Even Through Glass!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, Tencent
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces CODERS, a one-stage approach for object detection, pose estimation, and
                    reconstruction from stereo images. Unlike previous methods that rely on two-stage frameworks or
                    point cloud data, CODERS leverages stereo information to directly estimate object properties,
                    including shape, in an end-to-end manner.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">
                21:16
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06312" target="_blank">
                        @arXiv 2407.06312
                    </a>
                    <span class="tweet-title">
                        Koopman Learning: When Data Can't Save You
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Cambridge
                    </span>
                </div>
                <div class="primary-text">
                    This paper establishes fundamental limits on Koopman learning, proving that certain spectral
                    properties of Koopman operators cannot be learned from trajectory data, even with unlimited data and
                    perfect measurements. This is achieved by embedding abrupt spectral changes into the system
                    dynamics, effectively creating adversarial systems that prevent algorithmic convergence.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">
                21:41
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06703" target="_blank">
                        @arXiv 2407.06703
                    </a>
                    <span class="tweet-title">
                        HERMES: A Neural Network That Predicts Protein Mutations Like a Boss!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Washington
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces HERMES, a 3D rotationally equivariant neural network model for mutational
                    effect and stability prediction. Unlike previous work, HERMES is pre-trained to predict amino acid
                    propensity from its surrounding 3D structure, making it more versatile and adaptable for various
                    predictive objectives.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">
                22:05
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06823" target="_blank">
                        @arXiv 2407.06823
                    </a>
                    <span class="tweet-title">
                        DJs, Meet Your New Sidekick: AI Learns to Drop the Beat (and Cue Points)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        ETH Zurich
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a novel method for cue point estimation in electronic dance music (EDM)
                    tracks using a pre-trained object detection transformer, fine-tuned on a new dataset of manually
                    annotated cue points. Unlike previous methods, this approach does not rely on low-level musical
                    information analysis.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">
                22:25
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06234" target="_blank">
                        @arXiv 2407.06234
                    </a>
                    <span class="tweet-title">
                        US vs. EU AI Laws: A Tale of Two Acts (and What They Can Learn From Each Other)
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Oxford, University College London, University of Bologna
                    </span>
                </div>
                <div class="primary-text">
                    This research compares the US Algorithmic Accountability Act of 2022 (US AAA) with the EU Artificial
                    Intelligence Act (EU AIA), highlighting their similarities and differences in scope, approach, and
                    enforcement mechanisms. It goes beyond simply describing the acts, offering insights into how each
                    can learn from the other to improve their effectiveness.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">
                22:53
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06483" target="_blank">
                        @arXiv 2407.06483
                    </a>
                    <span class="tweet-title">
                        Language Models: Can They Handle a Multi-Task Makeover?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Virginia, Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces the concept of "composable interventions" for language models. Unlike
                    previous work that focuses on individual interventions, this study examines how multiple
                    interventions interact when applied sequentially to the same model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">
                23:14
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06250" target="_blank">
                        @arXiv 2407.06250
                    </a>
                    <span class="tweet-title">
                        Fairness in Eye Scans: A Pointy-Headed Approach to Better Diagnosis
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Chinese Academy of Sciences, Tsinghua University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a novel Point-Image Diffusion method for medical image synthesis. Unlike
                    previous methods that directly generate images, this approach first generates segmentation masks
                    using 3D point clouds, which allows for more precise control over the boundaries of the synthesized
                    images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">
                23:42
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06541" target="_blank">
                        @arXiv 2407.06541
                    </a>
                    <span class="tweet-title">
                        Malicious Bots Can't Stop This New Optimization Algorithm!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University, Bar-Ilan University, Arizona State University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces a new distributed optimization algorithm called Resilient Projected
                    Push-Pull (RP3) that achieves geometric convergence rates in expectation even in the presence of
                    malicious agents. Unlike previous work, RP3 leverages stochastic inter-agent trust values and
                    gradient tracking to achieve this resilience.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">
                24:10
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06567" target="_blank">
                        @arXiv 2407.06567
                    </a>
                    <span class="tweet-title">
                        AI Stock Whisperer: FINCON's Multi-Agent System Makes Market Moves
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Yale University
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces FINCON, a multi-agent system that uses a manager-analyst hierarchy to
                    process financial information and make trading decisions. Unlike previous systems, FINCON
                    incorporates a risk control component that updates investment beliefs based on performance,
                    enhancing decision-making.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">
                24:37
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06783" target="_blank">
                        @arXiv 2407.06783
                    </a>
                    <span class="tweet-title">
                        Poisson Learning: From Spikes to Smoothness with Measure Data
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Würzburg, University of Minnesota, University of Bonn
                    </span>
                </div>
                <div class="primary-text">
                    This research proves convergence rates for Poisson Learning, a semi-supervised learning algorithm,
                    to its continuum limit, which is a Poisson equation with measure data. This is a novel contribution
                    as previous work focused on proving convergence for smoother data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">
                25:17
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06516" target="_blank">
                        @arXiv 2407.06516
                    </a>
                    <span class="tweet-title">
                        VQA-Diff: Turning Car Pics into 3D Assets with a Chatty AI!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Toronto, Huawei, York University
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes VQA-Diff, a framework that leverages the zero-shot prediction ability of
                    Visual Question Answering (VQA) models and the structure and appearance generation capabilities of
                    Diffusion Models to generate 3D vehicle assets from single images. Unlike previous methods that rely
                    solely on image RGB information, VQA-Diff incorporates real-world knowledge from a Large Language
                    Model (LLM) to understand the characteristics of vehicles, enabling robust zero-shot prediction for
                    novel view rendering.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">
                25:43
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06798" target="_blank">
                        @arXiv 2407.06798
                    </a>
                    <span class="tweet-title">
                        AI-Generated Legal Docs: Can Lawyers Tell the Difference?
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Masaryk University, CMU
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on how lawyers perceive legal documents when they believe those documents were
                    generated by AI, specifically ChatGPT. Previous research has explored the use of LLMs in legal
                    tasks, but this study uniquely examines the impact of perceived AI authorship on document
                    evaluation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">
                26:04
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06910" target="_blank">
                        @arXiv 2407.06910
                    </a>
                    <span class="tweet-title">
                        Microsoft Sellers Get Smart: AI Recommends the Perfect Content for Every Deal!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Microsoft
                    </span>
                </div>
                <div class="primary-text">
                    This research focuses on recommending content at the opportunity level, the lowest granularity in a
                    CRM system, which is more relevant for sellers than previous approaches that focused on broader
                    recommendations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">
                26:23
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06888" target="_blank">
                        @arXiv 2407.06888
                    </a>
                    <span class="tweet-title">
                        ReLU's Quadratic Constraints: A Complete Set, Finally!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University of Michigan, University of Illinois
                    </span>
                </div>
                <div class="primary-text">
                    This paper presents a complete set of quadratic constraints (QCs) for the repeated ReLU function,
                    which is commonly used in neural networks. This is different from previous work that only provided
                    partial sets of QCs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet58">
            <div class="start-time-icon" title="Play from here">
                26:48
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06576" target="_blank">
                        @arXiv 2407.06576
                    </a>
                    <span class="tweet-title">
                        Virtual Personas Get Backstories: LLMs Now Have Lives!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        UC Berkeley
                    </span>
                </div>
                <div class="primary-text">
                    This research introduces "Anthology," a method for conditioning LLMs to specific virtual personas by
                    using open-ended life narratives, or "backstories," as a prompt prefix. This differs from previous
                    work that relied on explicit demographic information or question-answer pairs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet59">
            <div class="start-time-icon" title="Play from here">
                27:13
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06496" target="_blank">
                        @arXiv 2407.06496
                    </a>
                    <span class="tweet-title">
                        Privacy Amplification: It's Our Loss, Not Yours!
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        University College London
                    </span>
                </div>
                <div class="primary-text">
                    This research investigates whether privacy guarantees in a popular machine learning algorithm,
                    DP-SGD, can be improved when only the final trained model is released, unlike the current analysis
                    that assumes all intermediate steps are revealed. Unlike previous work that focused on constrained
                    loss functions, this study explores the possibility of privacy amplification for general loss
                    functions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet60">
            <div class="start-time-icon" title="Play from here">
                27:38
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06964" target="_blank">
                        @arXiv 2407.06964
                    </a>
                    <span class="tweet-title">
                        Vision Transformers Get a Memory Makeover: A Disentangled Approach to Efficient Tuning
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Tsinghua University, ShanghaiTech University, National University of Singapore
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new method called Synthesized Query Tuning (SynQT) for adapting pre-trained
                    Vision Transformers to downstream tasks. Unlike previous methods that insert new structures into the
                    pre-trained model, SynQT separates task-specific learning from pre-trained knowledge utilization,
                    resulting in a more memory-efficient training process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet61">
            <div class="start-time-icon" title="Play from here">
                28:06
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06608" target="_blank">
                        @arXiv 2407.06608
                    </a>
                    <span class="tweet-title">
                        Deep Learning Gets a Makeover: Image Reconstruction with Learned Attentive Regularizers
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        École Polytechnique Fédérale de Lausanne, Chemnitz University of Technology
                    </span>
                </div>
                <div class="primary-text">
                    This research proposes a new regularization scheme for image reconstruction that leverages deep
                    learning while maintaining interpretability. Unlike previous deep learning-based models, this scheme
                    is based on a series of convex problems, making it easier to analyze theoretically. The key
                    innovation lies in the use of learned masks to refine the regularization strength spatially, making
                    the model progressively attentive to image structure.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet62">
            <div class="start-time-icon" title="Play from here">
                28:31
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06765" target="_blank">
                        @arXiv 2407.06765
                    </a>
                    <span class="tweet-title">
                        Linearity's Little Helper: A Generalization Bound for Nearly-Linear Networks
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        École Polytechnique Fédérale de Lausanne
                    </span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel generalization bound for neural networks that are close to being linear.
                    Unlike previous work, this bound can be evaluated before training, making it a-priori.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet63">
            <div class="start-time-icon" title="Play from here">
                29:01
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06226" target="_blank">
                        @arXiv 2407.06226
                    </a>
                    <span class="tweet-title">
                        Quantum Computing: Brain-Boosting PSP Diagnosis
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Derozio Memorial College
                    </span>
                </div>
                <div class="primary-text">
                    This research explores the use of quantum machine learning (QML) for classifying brain networks
                    affected by Progressive Supranuclear Palsy (PSP), a neurological disorder. Unlike previous studies
                    that primarily relied on classical machine learning techniques, this paper investigates the
                    potential of QML to improve classification accuracy and efficiency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet64">
            <div class="start-time-icon" title="Play from here">
                29:25
            </div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2407.06295" target="_blank">
                        @arXiv 2407.06295
                    </a>
                    <span class="tweet-title">
                        Teaching Cells to Build: A New Way to Engineer Morphogenesis
                    </span>
                </div>
                <div class="institute-line">
                    <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg" />
                    <span class="institute-text">
                        Harvard University, University of Lausanne
                    </span>
                </div>
                <div class="primary-text">
                    This research uses automatic differentiation to learn the rules governing cell interactions and
                    genetic networks that lead to complex developmental outcomes. This approach differs from previous
                    work by directly optimizing over the parameters of the physical model, rather than relying on
                    manually crafted rules.
                </div>
            </div>
        </div>


        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Listen and learn ^.^</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202407102335_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/privacy.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading privacy.html:', error));
    </script>
</body>

</html>