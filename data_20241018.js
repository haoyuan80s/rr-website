
daily_data = {
    "date": "2024-10-18",
    "tweets": [
        
        {
            "startTime": "00:54",
            "arxivId": "2410.13211",
            "arxivLink": "https://arxiv.org/abs/2410.13211",
            "title": "Predicting the Unpredictable: How to Estimate the Odds of Rare Events in Language Models",
            "institute": "Harvard University",
            "text": "This research explores a new approach to estimating the probability of rare events in language models, focusing on activation extrapolation methods that don't rely on finding explicit examples of these events. This differs from previous work that primarily focused on importance sampling methods, which often require searching for rare events, making them less effective when these events are extremely rare.",
            "paper-title": "Estimating the Probabilities of Rare Outputs in Language Models",
            "image-path": "flux_paper_image/2410.13211_1729280767.png"
        },

        {
            "startTime": "01:25",
            "arxivId": "2410.13863",
            "arxivLink": "https://arxiv.org/abs/2410.13863",
            "title": "Scaling Up Text-to-Image: Continuous Tokens Are the New Black!",
            "institute": "Google DeepMind, MIT",
            "text": "This paper explores the scaling behavior of autoregressive models for text-to-image generation, focusing on the impact of using continuous tokens instead of discrete tokens. It also investigates the effect of different token generation orders (random vs. raster).",
            "paper-title": "Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens",
            "image-path": "flux_paper_image/2410.13863_1729280520.png"
        },

        {
            "startTime": "01:49",
            "arxivId": "2410.12881",
            "arxivLink": "https://arxiv.org/abs/2410.12881",
            "title": "Math-Chatty LLMs: Talking Their Way to Smarter Reasoning",
            "institute": "CMU, Nvidia",
            "text": "This research proposes a novel method for generating synthetic dialogue data that focuses on mathematical reasoning. Unlike previous approaches that simply rephrase text, this method creates conversations between simulated participants with varying levels of knowledge, encouraging deeper analysis and step-by-step problem-solving.",
            "paper-title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs",
            "image-path": "flux_paper_image/2410.12881_1729280506.png"
        },

        {
            "startTime": "02:18",
            "arxivId": "2410.13787",
            "arxivLink": "https://arxiv.org/abs/2410.13787",
            "title": "LLMs Can Learn About Themselves: A Peek Inside the Mind of a Language Model",
            "institute": "UCSanDiego, TruthfulAI, StanfordUniversity...",
            "text": "This research investigates whether LLMs can acquire knowledge about themselves through introspection, a process that goes beyond simply imitating their training data. The study defines introspection as the ability to access facts about oneself that are not contained in or derivable from training data.",
            "paper-title": "Looking Inward: Language Models Can Learn About Themselves by Introspection",
            "image-path": "flux_paper_image/2410.13787_1729281515.png"
        },

        {
            "startTime": "02:45",
            "arxivId": "2410.12799",
            "arxivLink": "https://arxiv.org/abs/2410.12799",
            "title": "Ads Personalization: A Doubly Robust Approach to Balancing Revenue and Engagement",
            "institute": "Meta",
            "text": "This research proposes a streamlined framework for personalized ad supply that leverages information from data collection policies through doubly robust learning. This approach differs from previous work by focusing on modeling the long-term causal effects of ad-load changes, which is crucial for balancing revenue and user engagement.",
            "paper-title": "Ads Supply Personalization via Doubly Robust Learning",
            "image-path": "flux_paper_image/2410.12799_1729281173.png"
        },

        {
            "startTime": "03:04",
            "arxivId": "2410.12982",
            "arxivLink": "https://arxiv.org/abs/2410.12982",
            "title": "Flash Inference: Making AI Models Think Faster Than Ever!",
            "institute": "Harvard University",
            "text": "This paper proposes a new method for speeding up inference in long convolution sequence models (LCSMs), achieving near-linear time complexity. Unlike previous approaches that focus on approximating the model, this method provides exact inference, making it more accurate and applicable to a wider range of models.",
            "paper-title": "Flash Inference: Near Linear Time Inference for Long Convolution Sequence Models and Beyond",
            "image-path": "flux_paper_image/2410.12982_1729280873.png"
        },

        {
            "startTime": "03:22",
            "arxivId": "2410.12869",
            "arxivLink": "https://arxiv.org/abs/2410.12869",
            "title": "LLMs: Not So Smart After All? New Method Uses Weak Evaluators to Get Better Results!",
            "institute": "Hong Kong University of Science and Technology, University of Washington",
            "text": "This research introduces a new method called GED (Preference Graph Ensemble and Denoise) that uses multiple weaker LLMs to evaluate other LLMs' outputs. This approach aims to overcome the problem of inconsistent preferences that can arise when using a single, powerful LLM as an evaluator.",
            "paper-title": "Language Model Preference Evaluation with Multiple Weak Evaluators",
            "image-path": "flux_paper_image/2410.12869_1729280981.png"
        },

        {
            "startTime": "03:44",
            "arxivId": "2410.13720",
            "arxivLink": "https://arxiv.org/abs/2410.13720",
            "title": "Movie Magic: AI Makes Personalized, Edited Videos with a Snap of Your Fingers!",
            "institute": "Meta",
            "text": "This research introduces MovieGen, a set of foundation models that can generate high-quality videos and audio, personalize videos based on a user's image, and precisely edit videos using text instructions. Unlike previous work, MovieGen is trained on a massive dataset of images, videos, and audio, enabling it to learn complex relationships between these modalities.",
            "paper-title": "Movie Gen: A Cast of Media Foundation Models",
            "image-path": "flux_paper_image/2410.13720_1729281430.png"
        },

        {
            "startTime": "04:07",
            "arxivId": "2410.13027",
            "arxivLink": "https://arxiv.org/abs/2410.13027",
            "title": "Diffusion Models Learn to Dance: Generating Realistic 3D Trajectories",
            "institute": "Stanford University",
            "text": "This paper introduces geometric trajectory diffusion models (GeoTDM), a novel approach for modeling the temporal distribution of 3D geometric trajectories. Unlike previous methods that focus on static structures, GeoTDM captures the dynamic nature of physical systems by incorporating temporal information into the diffusion process.",
            "paper-title": "Geometric Trajectory Diffusion Models",
            "image-path": "flux_paper_image/2410.13027_1729280225.png"
        },

        {
            "startTime": "04:29",
            "arxivId": "2410.13826",
            "arxivLink": "https://arxiv.org/abs/2410.13826",
            "title": "Unmasking Model Skills: How AI's Hidden Talents Reveal Surprising Strengths",
            "institute": "University of Maryland, Microsoft",
            "text": "This research proposes a novel method for automatically identifying the underlying skills required to solve evaluation instances by analyzing model-generated rationales. This approach differs from previous work that relies on manual annotations or surface-level attributes.",
            "paper-title": "Unearthing Skill-Level Insights for Understanding Trade-Offs of Foundation Models",
            "image-path": "flux_paper_image/2410.13826_1729280047.png"
        },

        {
            "startTime": "05:02",
            "arxivId": "2410.13851",
            "arxivLink": "https://arxiv.org/abs/2410.13851",
            "title": "Robots Learn to See Themselves: Differentiable Rendering for Vision-Based Control",
            "institute": "Columbia University, Stanford University",
            "text": "This research introduces a new method called \"differentiable robot rendering\" that allows the visual appearance of a robot to be directly linked to its control parameters. This is different from previous work that relied on separate models for visual appearance and control.",
            "paper-title": "Differentiable Robot Rendering",
            "image-path": "flux_paper_image/2410.13851_1729282460.png"
        },

        {
            "startTime": "05:25",
            "arxivId": "2410.13848",
            "arxivLink": "https://arxiv.org/abs/2410.13848",
            "title": "Janus: Two Faces, One Multimodal Mastermind!",
            "institute": "DeepSeek-AI, The University of Hong Kong, Peking University",
            "text": "This paper introduces Janus, a multimodal model that uses separate visual encoders for understanding and generation, unlike previous models that relied on a single encoder for both tasks.",
            "paper-title": "Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation",
            "image-path": "flux_paper_image/2410.13848_1729281470.png"
        },

        {
            "startTime": "05:56",
            "arxivId": "2410.13837",
            "arxivLink": "https://arxiv.org/abs/2410.13837",
            "title": "Reward Design on Autopilot: AI Learns to Shape Rewards for Faster Learning",
            "institute": "MIT, Boston University",
            "text": "This paper introduces ORSO, an algorithm that frames reward shaping selection as an online model selection problem. Unlike traditional methods that fully evaluate each shaping reward function, ORSO uses principled exploration strategies to automatically identify promising shaping reward functions without human intervention.",
            "paper-title": "ORSO: Accelerating Reward Design via Online Reward Selection and Policy Optimization",
            "image-path": "flux_paper_image/2410.13837_1729280929.png"
        },

        {
            "startTime": "06:17",
            "arxivId": "2410.12877",
            "arxivLink": "https://arxiv.org/abs/2410.12877",
            "title": "Steering Language Models: A New Way to Make Them Follow Instructions!",
            "institute": "ETH Zurich, Microsoft",
            "text": "This research explores a new method for improving instruction-following in language models by directly manipulating their internal representations, or activations, during inference. Unlike previous work that focused on high-level concepts like sentiment or style, this paper targets more specific, verifiable instructions related to output format, length, and word inclusion/exclusion.",
            "paper-title": "Improving Instruction-Following in Language Models through Activation Steering",
            "image-path": "flux_paper_image/2410.12877_1729281278.png"
        },

        {
            "startTime": "06:37",
            "arxivId": "2410.13267",
            "arxivLink": "https://arxiv.org/abs/2410.13267",
            "title": "Music Across Languages: A New System Makes Music Retrieval Multilingual!",
            "institute": "Peking University, Tsinghua University",
            "text": "This research introduces CLaMP 2, a music information retrieval system that supports 101 languages, unlike previous systems that primarily focused on English. It uses a large language model (LLM) to refine the multilingual corpus, reducing noise and balancing language distribution.",
            "paper-title": "CLaMP 2: Multimodal Music Information Retrieval Across 101 Languages Using Large Language Models",
            "image-path": "flux_paper_image/2410.13267_1729282137.png"
        },

        {
            "startTime": "07:05",
            "arxivId": "2410.13621",
            "arxivLink": "https://arxiv.org/abs/2410.13621",
            "title": "Cancer Segmentation Gets a Prompt Boost: SAM Learns from Weak Labels!",
            "institute": "Google, Naver",
            "text": "This research proposes a novel approach to weakly supervised cancer segmentation by combining class activation maps (CAM) with the Segment Anything Model (SAM). Unlike previous methods that rely on image-level labels or multiple-instance learning, this approach leverages the zero-shot capabilities of SAM and enhances its performance through explicit visual prompting and a pixel-level entropy-based prompting module.",
            "paper-title": "Enhanced Prompt-leveraged Weakly Supervised Cancer Segmentation based on Segment Anything",
            "image-path": "flux_paper_image/2410.13621_1729280157.png"
        },

        {
            "startTime": "07:31",
            "arxivId": "2410.12972",
            "arxivLink": "https://arxiv.org/abs/2410.12972",
            "title": "LLMs Can't Follow Simple Instructions: A New Benchmark Shows Just How Badly",
            "institute": "IBM",
            "text": "This research introduces a new benchmark for evaluating instruction-following abilities in large language models (LLMs). Unlike previous benchmarks, this one focuses on knowledge tasks and includes instructions that are conditional on the correct answer.",
            "paper-title": "Evaluating the Instruction-following Abilities of Language Models using Knowledge Tasks",
            "image-path": "flux_paper_image/2410.12972_1729280439.png"
        },

        {
            "startTime": "07:54",
            "arxivId": "2410.12921",
            "arxivLink": "https://arxiv.org/abs/2410.12921",
            "title": "Two-Sample Tests Get a Credal Makeover: Epistemic Ignorance Meets Hypothesis Testing!",
            "institute": "CISPA Helmholtz Center for Information Security, University College London, Inria...",
            "text": "This paper introduces a new framework for comparing credal sets, which are sets of probability distributions that represent epistemic uncertainty. Unlike traditional two-sample tests that compare precise distributions, these credal tests allow for reasoning about equality, inclusion, intersection, and mutual exclusivity of credal sets.",
            "paper-title": "Credal Two-Sample Tests of Epistemic Ignorance",
            "image-path": "flux_paper_image/2410.12921_1729282166.png"
        },

        {
            "startTime": "08:20",
            "arxivId": "2410.13817",
            "arxivLink": "https://arxiv.org/abs/2410.13817",
            "title": "Robot Learns to Open Doors (and Dishwashers!) with Just One Demo",
            "institute": "ETH Zurich",
            "text": "This research proposes a new approach to training robots for multi-contact loco-manipulation tasks, using a single demonstration and an adaptive phase dynamics model to guide the reinforcement learning process. This differs from previous work that often relies on multiple demonstrations or task-specific reward designs.",
            "paper-title": "Guided Reinforcement Learning for Robust Multi-Contact Loco-Manipulation",
            "image-path": "flux_paper_image/2410.13817_1729279817.png"
        },

        {
            "startTime": "08:42",
            "arxivId": "2410.13714",
            "arxivLink": "https://arxiv.org/abs/2410.13714",
            "title": "Generative AI: Can We Make It, But Not Understand It?",
            "institute": "University of California Berkeley, Google Research",
            "text": "This research formalizes the concept of \"generation in the limit\" from previous work by Kleinberg and Mullainathan, using the lens of statistical learning theory. It introduces a new combinatorial dimension called the Closure dimension to characterize uniform generatability, which is a stronger notion of generation than previously studied.",
            "paper-title": "Generation through the lens of learning theory",
            "image-path": "flux_paper_image/2410.13714_1729280207.png"
        },

        {
            "startTime": "08:59",
            "arxivId": "2410.13857",
            "arxivLink": "https://arxiv.org/abs/2410.13857",
            "title": "LLMs: Math Whizzes or Number Nincompoops? Precision Matters!",
            "institute": "Peking University, Huawei Noah\u2019s Ark Lab",
            "text": "This research delves into the impact of numerical precision on LLMs' arithmetic abilities, focusing on how the number of bits used to represent numbers affects their performance on tasks like addition and multiplication. Unlike previous work that treated entire numbers as single tokens, this study examines the impact of tokenizing individual digits, aligning with modern LLM practices.",
            "paper-title": "How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMs",
            "image-path": "flux_paper_image/2410.13857_1729280494.png"
        },

        {
            "startTime": "09:19",
            "arxivId": "2410.13735",
            "arxivLink": "https://arxiv.org/abs/2410.13735",
            "title": "Conformal Prediction Gets a Makeover: Ranking Samples for Tighter Uncertainty Sets",
            "institute": "University of Southern California, Carnegie Mellon University",
            "text": "This research introduces a new method for probabilistic conformal prediction (PCP) called PCP-VCR. Unlike traditional PCP, which uses a single scalar value to measure non-conformity, PCP-VCR uses a vectorized non-conformity score that ranks generated samples based on their empirical density. This allows for more flexible and efficient prediction sets.",
            "paper-title": "Optimizing Probabilistic Conformal Prediction with Vectorized Non-Conformity Scores",
            "image-path": "flux_paper_image/2410.13735_1729280868.png"
        },

        {
            "startTime": "09:57",
            "arxivId": "2410.13832",
            "arxivLink": "https://arxiv.org/abs/2410.13832",
            "title": "Panoramic Videos: From Casual Pans to Immersive Experiences!",
            "institute": "University of Washington, Google DeepMind, Weitzmann Institute of Science",
            "text": "This research introduces a method for synthesizing panoramic videos from casually captured panning videos, addressing the limitations of existing methods that struggle with dynamic scenes. The key innovation lies in adapting generative video models to complete the unknown regions of the panoramic video volume, overcoming the challenge of limited spatio-temporal context windows.",
            "paper-title": "VidPanos: Generative Panoramic Videos from Casual Panning Videos",
            "image-path": "flux_paper_image/2410.13832_1729282903.png"
        },

        {
            "startTime": "10:15",
            "arxivId": "2410.12885",
            "arxivLink": "https://arxiv.org/abs/2410.12885",
            "title": "Alexa, Is My Brain Still Working? New Study Uses Voice Assistants to Detect Early Cognitive Decline",
            "institute": "University of Massachusetts Boston, CMU, University of North Carolina at Chapel Hill",
            "text": "This research stands out by using longitudinal data collected from voice assistant interactions to detect cognitive decline. Unlike previous studies that analyze speech at specific time points, this study tracks changes in speech patterns over time, potentially offering a more nuanced understanding of cognitive changes.",
            "paper-title": "Exploiting Longitudinal Speech Sessions via Voice Assistant Systems for Early Detection of Cognitive Decline",
            "image-path": "flux_paper_image/2410.12885_1729279914.png"
        },

        {
            "startTime": "10:41",
            "arxivId": "2410.13530",
            "arxivLink": "https://arxiv.org/abs/2410.13530",
            "title": "3D Scene Generation: From Noise to Room-Scale Reality with Gaussian Diffusion!",
            "institute": "Technical University of Munich, Meta Reality Labs Zurich",
            "text": "This paper introduces L3DG, a novel approach for generating 3D scenes using a latent 3D Gaussian diffusion model. Unlike previous methods that operate directly on the 3D representation, L3DG leverages a compressed latent space of 3D Gaussians, enabling efficient generation of large-scale scenes.",
            "paper-title": "L3DG: Latent 3D Gaussian Diffusion",
            "image-path": "flux_paper_image/2410.13530_1729280745.png"
        },

        {
            "startTime": "11:05",
            "arxivId": "2410.13083",
            "arxivLink": "https://arxiv.org/abs/2410.13083",
            "title": "FedCAP: FL's New BFF for Handling Data Drama and Bad Actors",
            "institute": "University of Texas at Dallas, Microsoft, Lehigh University...",
            "text": "This paper proposes FedCAP, a federated learning framework that tackles both data heterogeneity and Byzantine attacks. FedCAP introduces a model update calibration mechanism to distinguish malicious updates from benign ones in non-IID settings. It also uses a customized aggregation rule to promote collaboration among similar clients and accelerate the deterioration of malicious models.",
            "paper-title": "FedCAP: Robust Federated Learning via Customized Aggregation and Personalization",
            "image-path": "flux_paper_image/2410.13083_1729280692.png"
        },

        {
            "startTime": "11:39",
            "arxivId": "2410.12793",
            "arxivLink": "https://arxiv.org/abs/2410.12793",
            "title": "AI in Healthcare: A Survey of CTSA Institutions Reveals the Good, the Bad, and the LLMs",
            "institute": "Cornell University, Columbia University, University of Florida...",
            "text": "This research focuses on the infrastructure for generative AI (GenAI) within the Clinical and Translational Science Award (CTSA) program, a nationwide consortium of medical research institutions. It differs from previous work by examining the perspectives of institutional leaders, specifically focusing on their decision-making processes, governance structures, and ethical considerations regarding GenAI adoption.",
            "paper-title": "Environment Scan of Generative AI Infrastructure for Clinical and Translational Science",
            "image-path": "flux_paper_image/2410.12793_1729279801.png"
        },

        {
            "startTime": "12:02",
            "arxivId": "2410.13835",
            "arxivLink": "https://arxiv.org/abs/2410.13835",
            "title": "Attention Heads Go MIA: Why LLMs Get Lost in the Sauce",
            "institute": "UC Berkeley",
            "text": "This paper delves into the \"extreme-token phenomena\" observed in LLMs, specifically attention sinks, value-state drains, and residual-state peaks. Unlike previous work that focused on describing these phenomena, this research proposes a mechanistic explanation for their emergence, attributing them to an \"active-dormant\" mechanism in attention heads.",
            "paper-title": "Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs",
            "image-path": "flux_paper_image/2410.13835_1729281984.png"
        },

        {
            "startTime": "12:27",
            "arxivId": "2410.13770",
            "arxivLink": "https://arxiv.org/abs/2410.13770",
            "title": "Noisy Data, Big Changes: How Diffusion Models Uncover Hidden Structure",
            "institute": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne",
            "text": "This research uses forward-backward experiments in diffusion models to probe the latent hierarchical structure of data, going beyond previous work that focused on images and did not explore the geometrical structure of changes.",
            "paper-title": "Probing the Latent Hierarchical Structure of Data via Diffusion Models",
            "image-path": "flux_paper_image/2410.13770_1729280890.png"
        },

        {
            "startTime": "12:46",
            "arxivId": "2410.12928",
            "arxivLink": "https://arxiv.org/abs/2410.12928",
            "title": "DreamCraft3D++: 3D Modeling on Speed, With a Dash of Magic!",
            "institute": "Tsinghua University, Zhejiang University",
            "text": "This paper introduces DreamCraft3D++, an extension of DreamCraft3D that utilizes a feed-forward multi-plane based reconstruction model to replace the time-consuming geometry sculpting optimization, resulting in a 1000x speedup.",
            "paper-title": "DreamCraft3D++: Efficient Hierarchical 3D Generation with Multi-Plane Reconstruction Model",
            "image-path": "flux_paper_image/2410.12928_1729282257.png"
        },

        {
            "startTime": "13:11",
            "arxivId": "2410.12918",
            "arxivLink": "https://arxiv.org/abs/2410.12918",
            "title": "Decentralized Learning Gets a Speed Boost with Model Fragmentation",
            "institute": "EPFL, \u00c9cole Polytechnique",
            "text": "This research introduces DivShare, a novel asynchronous decentralized learning algorithm that fragments models into smaller pieces before sending them to other nodes. This approach differs from previous work by addressing the challenge of communication stragglers, which are nodes with slower communication speeds.",
            "paper-title": "Boosting Asynchronous Decentralized Learning with Model Fragmentation",
            "image-path": "flux_paper_image/2410.12918_1729281935.png"
        },

        {
            "startTime": "13:35",
            "arxivId": "2410.13780",
            "arxivLink": "https://arxiv.org/abs/2410.13780",
            "title": "Matrix Multiplication: A Quantization Quest for Speed!",
            "institute": "Hebrew University of Jerusalem, MIT",
            "text": "This paper focuses on compressing matrices for faster matrix multiplication, a key operation in machine learning. Unlike traditional compression methods that aim to reconstruct the original matrices, this work focuses on compressing matrices in a way that minimizes the error in their product.",
            "paper-title": "Optimal Quantization for Matrix Multiplication",
            "image-path": "flux_paper_image/2410.13780_1729281309.png"
        },

        {
            "startTime": "14:01",
            "arxivId": "2410.13246",
            "arxivLink": "https://arxiv.org/abs/2410.13246",
            "title": "LLMs: Fact-Checking Their Confidence One Atomic Claim at a Time!",
            "institute": "University of Cambridge, Tencent",
            "text": "This research introduces \"atomic calibration,\" a new approach to evaluating the confidence of LLMs in long-form text generation. Unlike previous methods that focus on the overall confidence of a response, atomic calibration breaks down responses into individual factual claims and assesses the confidence of each claim separately.",
            "paper-title": "Atomic Calibration of LLMs in Long-Form Generations",
            "image-path": "flux_paper_image/2410.13246_1729280215.png"
        },

        {
            "startTime": "14:32",
            "arxivId": "2410.12822",
            "arxivLink": "https://arxiv.org/abs/2410.12822",
            "title": "Unlocking Video Diffusion: A World Model Without the Keys!",
            "institute": "Microsoft Research",
            "text": "This research proposes a novel approach called AVID, which adapts pretrained video diffusion models to action-conditioned world models without requiring access to the pretrained model's parameters. This differs from previous work that either requires access to the model's weights or trains a diffusion model from scratch.",
            "paper-title": "AVID: Adapting Video Diffusion Models to World Models",
            "image-path": "flux_paper_image/2410.12822_1729281839.png"
        },

        {
            "startTime": "14:50",
            "arxivId": "2410.13331",
            "arxivLink": "https://arxiv.org/abs/2410.13331",
            "title": "Discrete Optimization Gets a Temperature Check: Decoupled ST-GS Heats Things Up!",
            "institute": "Indian Institute of Technology Jodhpur, Google, National University of Singapore",
            "text": "This paper proposes a new approach called Decoupled ST-GS, which uses separate temperature parameters for the forward and backward passes of the Straight-Through Gumbel-Softmax (ST-GS) estimator. This differs from previous work that used a single temperature for both passes.",
            "paper-title": "Improving Discrete Optimisation Via Decoupled Straight-Through Gumbel-Softmax",
            "image-path": "flux_paper_image/2410.13331_1729280719.png"
        },

        {
            "startTime": "15:14",
            "arxivId": "2410.13722",
            "arxivLink": "https://arxiv.org/abs/2410.13722",
            "title": "LLMs: Poisoned at Birth, Still Acting Up After Therapy!",
            "institute": "Carnegie Mellon University, ETH Zurich, Meta...",
            "text": "This research investigates the persistence of pre-training poisoning attacks on large language models (LLMs) even after post-training alignment, which is a new area of exploration. Previous work focused on poisoning during fine-tuning or RLHF, but this paper examines the impact of poisoning during the initial pre-training phase.",
            "paper-title": "Persistent Pre-Training Poisoning of LLMs",
            "image-path": "flux_paper_image/2410.13722_1729280011.png"
        },

        {
            "startTime": "15:37",
            "arxivId": "2410.13691",
            "arxivLink": "https://arxiv.org/abs/2410.13691",
            "title": "Robots Gone Rogue: AI Jailbreaks Unleashed!",
            "institute": "University of Pennsylvania",
            "text": "This research explores the vulnerability of LLM-controlled robots to jailbreaking attacks, a concept previously studied in the context of text-based chatbots. The paper introduces ROBOPAIR, a novel algorithm specifically designed to elicit harmful physical actions from robots controlled by LLMs.",
            "paper-title": "Jailbreaking LLM-Controlled Robots",
            "image-path": "flux_paper_image/2410.13691_1729281367.png"
        },

        {
            "startTime": "16:03",
            "arxivId": "2410.13509",
            "arxivLink": "https://arxiv.org/abs/2410.13509",
            "title": "RAG-DDR: Giving LLMs a Data Diet to Stop Hallucinations",
            "institute": "Northeastern University, Tsinghua University, Carnegie Mellon University",
            "text": "This paper introduces a new method called Differentiable Data Rewards (DDR) for training Retrieval-Augmented Generation (RAG) systems. Unlike previous approaches that rely on supervised fine-tuning, DDR optimizes each module in the RAG system by aligning their data preferences. This means the system learns to work together more effectively, rather than just focusing on individual components.",
            "paper-title": "RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards",
            "image-path": "flux_paper_image/2410.13509_1729281162.png"
        },

        {
            "startTime": "16:27",
            "arxivId": "2410.13141",
            "arxivLink": "https://arxiv.org/abs/2410.13141",
            "title": "SciML Goes Decentralized: Training AI Models Without Sharing Your Data!",
            "institute": "Yale University",
            "text": "This research explores the integration of federated learning (FL) with scientific machine learning (SciML) to approximate functions and solve differential equations. It proposes two novel models: federated physics-informed neural networks (FedPINN) and federated deep operator networks (FedDeepONet). The study also introduces various data generation methods to control the degree of non-independent and identically distributed (non-iid) data and utilizes the 1-Wasserstein distance to quantify data heterogeneity.",
            "paper-title": "Federated scientific machine learning for approximating functions and solving differential equations with data heterogeneity",
            "image-path": "flux_paper_image/2410.13141_1729281016.png"
        },

        {
            "startTime": "16:44",
            "arxivId": "2410.13108",
            "arxivLink": "https://arxiv.org/abs/2410.13108",
            "title": "Algorithms Get Hooked: How Friction Makes Users More Engaged",
            "institute": "UC Berkeley, Stanford University",
            "text": "This research introduces a model for content selection that accounts for user disengagement, a factor often overlooked in traditional models. It deviates from previous work by considering content landscapes where revenue and user satisfaction are not directly aligned and by allowing users to temporarily disengage from the platform.",
            "paper-title": "Algorithmic Content Selection and the Impact of User Disengagement",
            "image-path": "flux_paper_image/2410.13108_1729280805.png"
        },

        {
            "startTime": "17:06",
            "arxivId": "2410.13816",
            "arxivLink": "https://arxiv.org/abs/2410.13816",
            "title": "Robot Policy Steering: Giving Generalists a Value-Boost!",
            "institute": "UC Berkeley, Carnegie Mellon University, Google DeepMind",
            "text": "This research introduces Value-Guided Policy Steering (V-GPS), a method that improves the performance of pre-trained generalist robotic policies by re-ranking their actions at deployment time based on a value function learned via offline RL. Unlike prior offline RL methods that use a value function on the training data, V-GPS directly improves the generalist policy on scenes and manipulation problems encountered at deployment time.",
            "paper-title": "Steering Your Generalists: Improving Robotic Foundation Models via Value Guidance",
            "image-path": "flux_paper_image/2410.13816_1729280461.png"
        },

        {
            "startTime": "17:30",
            "arxivId": "2410.12957",
            "arxivLink": "https://arxiv.org/abs/2410.12957",
            "title": "Music to Your Eyes: AI Makes Videos Sing with Perfect Rhythm!",
            "institute": "Zhejiang University, Harvard University",
            "text": "This research focuses on generating music that not only matches the overall theme and emotion of a video but also aligns with the specific visual dynamics and pacing, a challenge not fully addressed in previous work.",
            "paper-title": "MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic Synchronization",
            "image-path": "flux_paper_image/2410.12957_1729280974.png"
        },

        {
            "startTime": "17:50",
            "arxivId": "2410.13192",
            "arxivLink": "https://arxiv.org/abs/2410.13192",
            "title": "LLMs Get a New BFF: Self-Generated Documents Boost AI's Brainpower!",
            "institute": "Peking University",
            "text": "This research goes beyond simply using self-generated documents (SGDs) in retrieval-augmented generation (RAG) systems. It delves into the inherent properties of SGDs, classifying them based on their linguistic features and communicative functions.",
            "paper-title": "Evaluating Self-Generated Documents for Enhancing Retrieval-Augmented Generation with Large Language Models",
            "image-path": "flux_paper_image/2410.13192_1729282045.png"
        },

        {
            "startTime": "18:12",
            "arxivId": "2410.12949",
            "arxivLink": "https://arxiv.org/abs/2410.12949",
            "title": "Unlearning Facts: How to Make AI Forget What It Shouldn't Know",
            "institute": "University of Maryland, Google",
            "text": "This research explores the effectiveness of different localization methods for unlearning or editing factual associations in large language models. Unlike previous work that focuses on output-tracing techniques, this study investigates the impact of localizing edits to components associated with the \"fact lookup\" mechanism, which enriches the latent stream with information about the subject.",
            "paper-title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization",
            "image-path": "flux_paper_image/2410.12949_1729279887.png"
        },

        {
            "startTime": "18:34",
            "arxivId": "2410.13195",
            "arxivLink": "https://arxiv.org/abs/2410.13195",
            "title": "UniG: 3D Reconstruction Gets a Multi-View Makeover!",
            "institute": "Hong Kong University of Science and Technology, International Digital Economy Academy (IDEA), The Chinese University of Hong Kong Shenzhen...",
            "text": "This paper introduces UniG, a 3D reconstruction model that uses a unitary set of 3D Gaussians as queries in a deformable Transformer. This approach allows all input views to contribute to the same 3D representation, effectively addressing the view inconsistency issue and supporting an arbitrary number of input views.",
            "paper-title": "UniG: Modelling Unitary 3D Gaussians for View-consistent 3D Reconstruction",
            "image-path": "flux_paper_image/2410.13195_1729280171.png"
        },

        {
            "startTime": "18:56",
            "arxivId": "2410.13114",
            "arxivLink": "https://arxiv.org/abs/2410.13114",
            "title": "Sounding the Alarm: Auditing Audio Datasets for Bias and Toxicity",
            "institute": "CMU, Northwestern University, University of Washington...",
            "text": "This research focuses on the ethical implications of generative audio datasets, a topic that has received less attention than similar issues in text and image datasets. The authors conducted a comprehensive literature review of audio datasets and audited seven prominent ones for bias, toxicity, and copyright infringement.",
            "paper-title": "Sound Check: Auditing Audio Datasets",
            "image-path": "flux_paper_image/2410.13114_1729282023.png"
        },

        {
            "startTime": "19:14",
            "arxivId": "2410.13138",
            "arxivLink": "https://arxiv.org/abs/2410.13138",
            "title": "LLMs on Lockdown: New Data Defenses Stop AI from Sniffing Out Your Secrets",
            "institute": "CMU",
            "text": "This research introduces \"data defenses\" - a novel approach to directly empower data owners to block LLMs from performing inference on their data. Unlike previous work that focused on mitigating harms through policy, legal, or technical interventions, this paper proposes a method to automatically generate adversarial prompt injections that significantly reduce the ability of LLMs to accurately infer information from text.",
            "paper-title": "Data Defenses Against Large Language Models",
            "image-path": "flux_paper_image/2410.13138_1729279779.png"
        },

        {
            "startTime": "19:36",
            "arxivId": "2410.12857",
            "arxivLink": "https://arxiv.org/abs/2410.12857",
            "title": "LLMs Go Corporate: New Benchmarks for Evaluating AI in the Real World",
            "institute": "IBM",
            "text": "This research focuses on evaluating LLMs for enterprise applications, specifically in domains like finance, legal, climate, and cybersecurity. It differs from previous work by curating and developing domain-specific benchmarks and metrics, which are integrated into the widely adopted HELM evaluation framework.",
            "paper-title": "Enterprise Benchmarks for Large Language Model Evaluation",
            "image-path": "flux_paper_image/2410.12857_1729282864.png"
        },

        {
            "startTime": "19:57",
            "arxivId": "2410.12851",
            "arxivLink": "https://arxiv.org/abs/2410.12851",
            "title": "LLMs Have Vibes, and We Can Quantify Them!",
            "institute": "UC Berkeley",
            "text": "This research introduces VibeCheck, a system that automatically discovers and quantifies qualitative differences between large language models (LLMs) by identifying \"vibes\" that are well-defined, differentiating, and user-aligned. Unlike previous work that focuses on predefined concepts or correctness metrics, VibeCheck aims to capture the subjective aspects of LLM outputs that influence user preferences.",
            "paper-title": "VibeCheck: Discover and Quantify Qualitative Differences in Large Language Models",
            "image-path": "flux_paper_image/2410.12851_1729281027.png"
        },

        {
            "startTime": "20:16",
            "arxivId": "2410.12840",
            "arxivLink": "https://arxiv.org/abs/2410.12840",
            "title": "Contract Q&A: Prompt Chaining Makes Legal Text Easier to Digest",
            "institute": "Zuva Inc.",
            "text": "This research introduces a two-stage prompt chaining approach for contract question answering. Unlike previous work that relied on single-stage prompts, this method first generates a question-specific summary of the relevant legal text, which is then used to answer the question.",
            "paper-title": "Answering Questions in Stages: Prompt Chaining for Contract QA",
            "image-path": "flux_paper_image/2410.12840_1729281548.png"
        },

        {
            "startTime": "20:44",
            "arxivId": "2410.13798",
            "arxivLink": "https://arxiv.org/abs/2410.13798",
            "title": "Graph Tokenization Gets Quantized: A Transformer's Delight!",
            "institute": "Meta AI",
            "text": "This paper introduces a novel graph tokenizer called GQT (Graph Quantized Tokenizer) that decouples tokenizer training from Transformer training. Unlike previous approaches that rely on heuristics or co-training, GQT leverages multi-task graph self-supervised learning to learn robust and generalizable graph tokens.",
            "paper-title": "Learning Graph Quantized Tokenizers for Transformers",
            "image-path": "flux_paper_image/2410.13798_1729280797.png"
        },

        {
            "startTime": "21:03",
            "arxivId": "2410.13855",
            "arxivLink": "https://arxiv.org/abs/2410.13855",
            "title": "Imitation Learning Gets a Diffusion Makeover: Score-Matching for Smarter Bots!",
            "institute": "Cornell University, CMU, Harvard University",
            "text": "This paper proposes a new imitation learning framework called SMILING that uses diffusion models and score matching instead of adversarial training. Unlike previous methods that rely on discriminators to approximate f-divergences or IPMs, SMILING directly matches the score function of the learned policy to that of the expert policy.",
            "paper-title": "Diffusing States and Matching Scores: A New Framework for Imitation Learning",
            "image-path": "flux_paper_image/2410.13855_1729281337.png"
        },

        {
            "startTime": "21:28",
            "arxivId": "2410.13187",
            "arxivLink": "https://arxiv.org/abs/2410.13187",
            "title": "Tiny AI, Big Code: 7 Billion Parameters, 1.2 Trillion Tokens, and a Whole Lot of Code Completion Magic!",
            "institute": "Peking University",
            "text": "This research introduces a new training objective called Structured Fill-In-the-Middle (SFIM) that considers the syntax structures of code. This differs from previous work that mainly focused on predicting the next token or filling in missing code snippets without considering the underlying code structure.",
            "paper-title": "aiXcoder-7B: A Lightweight and Effective Large Language Model for Code Completion",
            "image-path": "flux_paper_image/2410.13187_1729279962.png"
        },

        {
            "startTime": "21:56",
            "arxivId": "2410.13201",
            "arxivLink": "https://arxiv.org/abs/2410.13201",
            "title": "Text Generation Gets a Meta Makeover: Diffusing Noise with Contextualized Scheduling",
            "institute": "University of Washington, Microsoft, National Taiwan University",
            "text": "This research introduces a novel approach to noise scheduling in sequence-to-sequence text diffusion models. Unlike previous methods that rely on fixed or handcrafted rules, this paper proposes a \"scheduler-exploiter\" framework that leverages Meta-Exploration to train a dedicated scheduler model for contextualized noise scheduling.",
            "paper-title": "Meta-DiffuB: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration",
            "image-path": "flux_paper_image/2410.13201_1729282160.png"
        },

        {
            "startTime": "22:25",
            "arxivId": "2410.13086",
            "arxivLink": "https://arxiv.org/abs/2410.13086",
            "title": "Reverse-Engineering the Reader: Can We Make Language Models More Human?",
            "institute": "ETH Zurich, University of Zurich",
            "text": "This research explores a novel approach to aligning language models with human psychometric data, specifically reading times, by directly optimizing the model's parameters to improve its predictive power. Unlike previous work that focuses on evaluating the psychometric fit of pretrained models, this study aims to fine-tune models to become better predictors of human reading behavior.",
            "paper-title": "Reverse-Engineering the Reader",
            "image-path": "flux_paper_image/2410.13086_1729280388.png"
        },

        {
            "startTime": "22:47",
            "arxivId": "2410.13106",
            "arxivLink": "https://arxiv.org/abs/2410.13106",
            "title": "Cliqueformer: A Neural Network That Learns to Design, Not Just Predict!",
            "institute": "UC Berkeley",
            "text": "This research introduces Cliqueformer, a neural network that learns the structure of a black-box function, enabling it to optimize designs more effectively than previous methods. Unlike prior work that focuses on discovering the structure of the function, Cliqueformer predefines the structure and learns representations that align with it.",
            "paper-title": "Cliqueformer: Model-Based Optimization with Structured Transformers",
            "image-path": "flux_paper_image/2410.13106_1729281972.png"
        },

        {
            "startTime": "23:14",
            "arxivId": "2410.13807",
            "arxivLink": "https://arxiv.org/abs/2410.13807",
            "title": "Super-Resolution Gets a Semantic Makeover: Diffusion Models Learn to See the Big Picture!",
            "institute": "Tsinghua University, vivo Mobile Communication Co. Ltd",
            "text": "This research introduces ConsisSR, a new method for image super-resolution that leverages pretrained text-to-image diffusion models. Unlike previous methods that focus solely on semantic consistency, ConsisSR incorporates both semantic and pixel-level consistency by integrating CLIP image embeddings and introducing Time-aware Latent Augmentation (TALA).",
            "paper-title": "ConsisSR: Delving Deep into Consistency in Diffusion-based Image Super-Resolution",
            "image-path": "flux_paper_image/2410.13807_1729279848.png"
        },

        {
            "startTime": "23:39",
            "arxivId": "2410.13849",
            "arxivLink": "https://arxiv.org/abs/2410.13849",
            "title": "Stop Clipping, Start Normalizing: A New Way to Tame Heavy-Tailed Gradients",
            "institute": "ETH Zurich",
            "text": "This research focuses on Normalized SGD (NSGD) for handling heavy-tailed gradient noise in non-convex optimization problems. Unlike previous work that relied on gradient clipping with increasing thresholds, this paper demonstrates that NSGD can achieve optimal sample complexity without clipping, even when problem parameters are unknown.",
            "paper-title": "From Gradient Clipping to Normalization for Heavy Tailed SGD",
            "image-path": "flux_paper_image/2410.13849_1729280569.png"
        },

        {
            "startTime": "23:58",
            "arxivId": "2410.13174",
            "arxivLink": "https://arxiv.org/abs/2410.13174",
            "title": "AI Drift Detection: Don't Let Your Medical Imaging Models Go Rogue!",
            "institute": "Microsoft, Harvard University",
            "text": "This research introduces MMC+, an enhanced framework for scalable drift monitoring in medical imaging AI, building upon the CheXstray framework. MMC+ incorporates uncertainty bounds, leverages MedImageInsight embeddings, and utilizes advanced metrics for more accurate and scalable monitoring.",
            "paper-title": "Scalable Drift Monitoring in Medical Imaging AI",
            "image-path": "flux_paper_image/2410.13174_1729281534.png"
        },

        {
            "startTime": "24:25",
            "arxivId": "2410.12870",
            "arxivLink": "https://arxiv.org/abs/2410.12870",
            "title": "LLMs Get a Grip on Planning: Process Mining Makes Plans More Efficient and Interpretable",
            "institute": "Technical University of Denmark, Microsoft",
            "text": "This research introduces a novel approach to skill learning in LLMs by integrating process mining techniques. Unlike previous work that relies solely on text-based planning, this method leverages process discovery to extract structured control flow models from flat action sequences, enabling more efficient and interpretable plan generation.",
            "paper-title": "Skill Learning Using Process Mining for Large Language Model Plan Generation",
            "image-path": "flux_paper_image/2410.12870_1729279824.png"
        },

        {
            "startTime": "24:53",
            "arxivId": "2410.13824",
            "arxivLink": "https://arxiv.org/abs/2410.13824",
            "title": "Webpage UIs: The Secret Sauce for Text-Rich Visual Understanding?",
            "institute": "Chinese University of Hong Kong, CMU",
            "text": "This research proposes using webpage UIs as a training resource for multimodal language models. Unlike previous work that extracts images and text from webpages, this study leverages the accessibility tree, a structured representation of a webpage's HTML and metadata, to generate multimodal instructions for training.",
            "paper-title": "Harnessing Webpage UIs for Text-Rich Visual Understanding",
            "image-path": "flux_paper_image/2410.13824_1729280955.png"
        },

        {
            "startTime": "25:17",
            "arxivId": "2410.13458",
            "arxivLink": "https://arxiv.org/abs/2410.13458",
            "title": "MedINST: A Mega Dataset to Train AI Doctors (and Make Them Super Smart)",
            "institute": "Eindhoven University of Technology, University of Liverpool, Yale University",
            "text": "This research introduces MEDINST, a meta-dataset of biomedical instructions, which is distinct from previous work because it focuses on providing a comprehensive collection of instructions for various biomedical NLP tasks, rather than just a single task or a limited set of tasks.",
            "paper-title": "MedINST: Meta Dataset of Biomedical Instructions",
            "image-path": "flux_paper_image/2410.13458_1729282498.png"
        },

        {
            "startTime": "25:43",
            "arxivId": "2410.12854",
            "arxivLink": "https://arxiv.org/abs/2410.12854",
            "title": "Tree-mendous Progress: LLMs Learn to Rank Preferences Like a Boss!",
            "institute": "Peking University",
            "text": "This research introduces Tree Preference Optimization (TPO), a new algorithm for aligning large language models (LLMs) with preference trees. Unlike previous methods that rely on sampling paired preferences, TPO directly learns from the entire preference tree, enabling it to model preferences with varying reward values.",
            "paper-title": "TPO: Aligning Large Language Models with Multi-branch&Multi-step Preference Trees",
            "image-path": "flux_paper_image/2410.12854_1729282764.png"
        },

        {
            "startTime": "26:07",
            "arxivId": "2410.13853",
            "arxivLink": "https://arxiv.org/abs/2410.13853",
            "title": "Active Learning Gets a Brain: AI Now Chooses the Best Data to Learn From!",
            "institute": "CMU, Clemson University",
            "text": "This research introduces AutoAL, a novel framework that automatically selects the best active learning strategy for a given task. Unlike previous methods that rely on pre-defined strategies or black-box optimization, AutoAL leverages a differentiable bi-level optimization framework to learn the optimal strategy from data.",
            "paper-title": "AutoAL: Automated Active Learning with Differentiable Query Strategy Search",
            "image-path": "flux_paper_image/2410.13853_1729282605.png"
        },

        {
            "startTime": "26:29",
            "arxivId": "2410.13864",
            "arxivLink": "https://arxiv.org/abs/2410.13864",
            "title": "UniDrive: Making Self-Driving Cars See the World, No Matter the Camera Angle!",
            "institute": "University of Michigan, University of California Berkeley",
            "text": "This research introduces UniDrive, a framework that addresses the challenge of generalizing vision-centric 3D perception models across different camera configurations. UniDrive uses a set of unified virtual cameras and a ground-aware projection method to transform images from different camera setups into a standardized virtual space. This allows the model to operate consistently across diverse physical camera setups.",
            "paper-title": "UniDrive: Towards Universal Driving Perception Across Camera Configurations",
            "image-path": "flux_paper_image/2410.13864_1729282623.png"
        },

        {
            "startTime": "26:53",
            "arxivId": "2410.13760",
            "arxivLink": "https://arxiv.org/abs/2410.13760",
            "title": "Eyelid Fold Consistency: A New Wrinkle in Facial Modeling!",
            "institute": "Microsoft",
            "text": "This research introduces a new definition of eyelid fold consistency, which is applied to reprocess existing head scan data and retrain a parametric face model. This approach differs from previous work by explicitly modeling eyelid folds in a unified topology, ensuring consistent representation across diverse eyelid shapes.",
            "paper-title": "Eyelid Fold Consistency in Facial Modeling",
            "image-path": "flux_paper_image/2410.13760_1729280631.png"
        },

        {
            "startTime": "27:09",
            "arxivId": "2410.12938",
            "arxivLink": "https://arxiv.org/abs/2410.12938",
            "title": "Weather Forecasting Gets a Graph Makeover: GNNs Bridge the Gap Between Global and Local Predictions",
            "institute": "MIT",
            "text": "This research introduces a multi-modal graph neural network (GNN) that combines global weather reanalysis data with local weather station observations to improve localized weather forecasting. Unlike previous work that relies solely on gridded data or local observations, this approach leverages the strengths of both data sources to achieve more accurate predictions.",
            "paper-title": "Multi-modal graph neural networks for localized off-grid weather forecasting",
            "image-path": "flux_paper_image/2410.12938_1729281588.png"
        },

        {
            "startTime": "27:32",
            "arxivId": "2410.13039",
            "arxivLink": "https://arxiv.org/abs/2410.13039",
            "title": "Pedestrian Prediction: A Skeleton Key to Safer Self-Driving Cars!",
            "institute": "Queen Mary University of London, University of Cambridge",
            "text": "This research proposes a new approach for pedestrian intent prediction (PIP) that uses skeleton-ization to compress pedestrian images, reducing data volume and computational complexity while maintaining accuracy. This differs from previous methods that often rely on computationally intensive models.",
            "paper-title": "A low complexity contextual stacked ensemble-learning approach for pedestrian intent prediction",
            "image-path": "flux_paper_image/2410.13039_1729281422.png"
        },

        {
            "startTime": "27:53",
            "arxivId": "2410.13850",
            "arxivLink": "https://arxiv.org/abs/2410.13850",
            "title": "Data Attribution for Diffusion Models: Who's the Real Artist?",
            "institute": "University of Cambridge",
            "text": "This paper proposes a scalable influence function framework for data attribution in diffusion models, using a K-FAC approximation of the generalized Gauss-Newton matrix (GGN) as a Hessian approximation. This approach differs from previous work by using a more theoretically sound GGN approximation and by leveraging K-FAC for efficient computation.",
            "paper-title": "Influence Functions for Scalable Data Attribution in Diffusion Models",
            "image-path": "flux_paper_image/2410.13850_1729280514.png"
        },

        {
            "startTime": "28:14",
            "arxivId": "2410.13862",
            "arxivLink": "https://arxiv.org/abs/2410.13862",
            "title": "DepthSplat: When Depth and Gaussian Splatting Fall in Love!",
            "institute": "ETH Zurich, University of T\u00fcbingen, Microsoft",
            "text": "This paper introduces DepthSplat, a novel approach that combines Gaussian splatting and depth estimation. Unlike previous methods that treat these tasks in isolation, DepthSplat leverages the strengths of both techniques to improve performance for both tasks. The key innovation lies in integrating pre-trained monocular depth features into the multi-view feature matching branch of the depth model, leading to more robust depth predictions, especially in challenging scenarios like occlusions and texture-less regions.",
            "paper-title": "DepthSplat: Connecting Gaussian Splatting and Depth",
            "image-path": "flux_paper_image/2410.13862_1729280940.png"
        },

        {
            "startTime": "28:34",
            "arxivId": "2410.12952",
            "arxivLink": "https://arxiv.org/abs/2410.12952",
            "title": "LLMs Learn to Plan: Multi-Turn Function Calling Gets a Boost!",
            "institute": "Peking University",
            "text": "This research focuses on teaching LLMs to perform multi-turn function calling, which involves planning and executing a series of function calls to solve complex tasks. Previous work primarily focused on single-turn function calling, where LLMs only need to select and use a single function at a time.",
            "paper-title": "Facilitating Multi-turn Function Calling for LLMs via Compositional Instruction Tuning",
            "image-path": "flux_paper_image/2410.12952_1729279842.png"
        },

        {
            "startTime": "28:53",
            "arxivId": "2410.13643",
            "arxivLink": "https://arxiv.org/abs/2410.13643",
            "title": "Fine-Tuning Diffusion Models: Making AI Designs More Natural and Rewarding!",
            "institute": "MIT, Genentech, UC Berkeley",
            "text": "This research focuses on fine-tuning discrete diffusion models to optimize specific objectives, unlike previous work that primarily focused on generating natural-like sequences. The paper introduces a novel algorithm, DRAKES, that enables direct backpropagation of rewards through entire trajectories generated by diffusion models.",
            "paper-title": "Fine-Tuning Discrete Diffusion Models via Reward Optimization with Applications to DNA and Protein Design",
            "image-path": "flux_paper_image/2410.13643_1729281957.png"
        },

        {
            "startTime": "29:21",
            "arxivId": "2410.12886",
            "arxivLink": "https://arxiv.org/abs/2410.12886",
            "title": "Stop Asking Dumb Questions! New AI Model Filters Out Irrelevant Info",
            "institute": "Munich Reinsurance Co-Canada",
            "text": "This research introduces AT-RAG, a Retrieval Augmented Generation (RAG) model that incorporates topic modeling to improve the efficiency and accuracy of multi-hop question answering. Unlike previous RAG models, AT-RAG uses a topic assignment module to filter out irrelevant information, focusing the retrieval process on contextually significant documents.",
            "paper-title": "AT-RAG: An Adaptive RAG Model Enhancing Query Efficiency with Topic Filtering and Iterative Reasoning",
            "image-path": "flux_paper_image/2410.12886_1729279663.png"
        },

        {
            "startTime": "29:44",
            "arxivId": "2410.13181",
            "arxivLink": "https://arxiv.org/abs/2410.13181",
            "title": "Big Brains, Small Budget: How LLMs Can Collaborate to Solve Complex Problems",
            "institute": "Peking University",
            "text": "This research proposes a novel framework called ADASWITCH, which allows smaller, locally deployed LLMs to collaborate with larger, cloud-based LLMs. This approach aims to improve the performance of smaller LLMs while reducing the computational cost associated with using large models.",
            "paper-title": "AdaSwitch: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning",
            "image-path": "flux_paper_image/2410.13181_1729279877.png"
        },

        {
            "startTime": "30:08",
            "arxivId": "2410.12948",
            "arxivLink": "https://arxiv.org/abs/2410.12948",
            "title": "Speech Models: Can They Hear Beyond the Words?",
            "institute": "CMU, Mohamed bin Zayed University of Artificial Intelligence",
            "text": "This research delves into the ability of speech foundation models to understand non-verbal cues, such as emotion and speaker intent, going beyond their typical focus on speech-to-text tasks. It analyzes the models' layer-wise representations to understand how they capture these cues.",
            "paper-title": "What Do Speech Foundation Models Not Learn About Speech?",
            "image-path": "flux_paper_image/2410.12948_1729282377.png"
        },

        {
            "startTime": "30:26",
            "arxivId": "2410.13067",
            "arxivLink": "https://arxiv.org/abs/2410.13067",
            "title": "Two-Timescale Learning: Constant Step Sizes Go the Distance!",
            "institute": "University of Wisconsin-Madison",
            "text": "This paper investigates the convergence behavior of two-timescale stochastic approximation (TTSA) algorithms using constant step sizes, a departure from previous work that focused on diminishing step sizes. The authors analyze the convergence in terms of the Wasserstein metric, providing explicit geometric and non-asymptotic convergence rates.",
            "paper-title": "Two-Timescale Linear Stochastic Approximation: Constant Stepsizes Go a Long Way",
            "image-path": "flux_paper_image/2410.13067_1729282192.png"
        },

        {
            "startTime": "30:46",
            "arxivId": "2410.12794",
            "arxivLink": "https://arxiv.org/abs/2410.12794",
            "title": "EMR Serving: A Tale of Two Servers and a Network's Dilemma",
            "institute": "University of Michigan, Rice University, Fudan University...",
            "text": "This research focuses on optimizing the serving of embedding-based recommendation (EMR) models by disaggregating embedding storage and neural network computation onto separate servers. Unlike previous work that primarily focused on resource provisioning, this paper delves into the networking challenges posed by this disaggregation, proposing techniques to leverage temporal and spatial locality in embedding lookups and optimize RDMA I/O engines.",
            "paper-title": "Disaggregating Embedding Recommendation Systems with FlexEMR",
            "image-path": "flux_paper_image/2410.12794_1729279714.png"
        },

        {
            "startTime": "31:11",
            "arxivId": "2410.13502",
            "arxivLink": "https://arxiv.org/abs/2410.13502",
            "title": "Math Word Problems: LLMs Get Stumped by Complex Proofs!",
            "institute": "ETH Zurich, Idiap Research Institute",
            "text": "This research introduces MathGAP, a framework for evaluating LLMs on math word problems with arbitrarily complex proofs. Unlike previous work, MathGAP generates problems with controlled proof structures, enabling systematic studies of out-of-distribution generalization.",
            "paper-title": "MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily Complex Proofs",
            "image-path": "flux_paper_image/2410.13502_1729280653.png"
        },

        {
            "startTime": "31:36",
            "arxivId": "2410.13300",
            "arxivLink": "https://arxiv.org/abs/2410.13300",
            "title": "Mode Collapse: When AI Gets Lost in the Multiverse",
            "institute": "CNRS, \u00c9cole Polytechnique, Institut Polytechnique de Paris...",
            "text": "This paper investigates the phenomenon of mode collapse in variational inference by analyzing the gradient flow dynamics of Gaussian mixture models. Unlike previous work, it focuses on the mechanisms driving mode collapse, identifying two key factors: mean alignment and vanishing weights.",
            "paper-title": "A theoretical perspective on mode collapse in variational inference",
            "image-path": "flux_paper_image/2410.13300_1729279872.png"
        },

        {
            "startTime": "32:02",
            "arxivId": "2410.13210",
            "arxivLink": "https://arxiv.org/abs/2410.13210",
            "title": "LLMs Hallucinate? FaithBench Knows the Truth!",
            "institute": "Vectara Inc., Iowa State University, University of Southern California...",
            "text": "This research introduces FaithBench, a new benchmark for evaluating hallucinations in summarization models. Unlike previous benchmarks, FaithBench includes a wider range of modern LLMs and incorporates human annotations to account for the subjective nature of hallucination perception.",
            "paper-title": "FaithBench: A Diverse Hallucination Benchmark for Summarization by Modern LLMs",
            "image-path": "flux_paper_image/2410.13210_1729280376.png"
        },

        {
            "startTime": "32:28",
            "arxivId": "2410.13671",
            "arxivLink": "https://arxiv.org/abs/2410.13671",
            "title": "Chatbots in India: Can AI Speak Our Language?",
            "institute": "Microsoft",
            "text": "This research evaluates 24 large language models (LLMs) on real-world healthcare queries in Indian English and four Indic languages. Unlike previous studies that relied on translated benchmarks, this study uses a dataset collected from actual users of a medical chatbot in India.",
            "paper-title": "HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World Multilingual Settings",
            "image-path": "flux_paper_image/2410.13671_1729282039.png"
        },

        {
            "startTime": "32:47",
            "arxivId": "2410.12832",
            "arxivLink": "https://arxiv.org/abs/2410.12832",
            "title": "LLMs as Judges: AI Feedback Gets a Reasoning Boost!",
            "institute": "SynthLabs, Stanford University",
            "text": "This paper proposes a new approach to training LLMs called Generative Reward Models (GenRM). Unlike traditional methods that rely on human feedback or explicit reward models, GenRM uses an LLM to generate its own reasoning traces, which are then used to improve its performance.",
            "paper-title": "Generative Reward Models",
            "image-path": "flux_paper_image/2410.12832_1729280396.png"
        },

        {
            "startTime": "33:16",
            "arxivId": "2410.13828",
            "arxivLink": "https://arxiv.org/abs/2410.13828",
            "title": "Language Models: When Good Intentions Go Bad (and It's All About the Gradient)",
            "institute": "Princeton University, University of Texas at Austin",
            "text": "This paper identifies a common pitfall in margin-based language model alignment methods, specifically the \"gradient entanglement\" effect. This effect occurs when the chosen and rejected responses are highly correlated, leading to synchronized changes in their probabilities, even when the desired outcome is to increase one and decrease the other. This is a new perspective on the limitations of margin-based methods, which have been widely used in RLHF.",
            "paper-title": "A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement",
            "image-path": "flux_paper_image/2410.13828_1729280055.png"
        },

        {
            "startTime": "33:44",
            "arxivId": "2410.12880",
            "arxivLink": "https://arxiv.org/abs/2410.12880",
            "title": "AI Goes Global: A Guide to Avoiding Cultural Faux Pas",
            "institute": "Indian Institute of Technology Kharagpur, Microsoft, Singapore University of Technology and Design",
            "text": "This research focuses on the cultural sensitivity of smaller language models, which are often trained on Western-centric data and struggle to navigate diverse cultural contexts. The paper introduces two datasets: one for evaluating cultural harm and another for mitigating it through fine-tuning.",
            "paper-title": "Navigating the Cultural Kaleidoscope: A Hitchhiker's Guide to Sensitivity in Large Language Models",
            "image-path": "flux_paper_image/2410.12880_1729280239.png"
        },

        {
            "startTime": "34:14",
            "arxivId": "2410.13025",
            "arxivLink": "https://arxiv.org/abs/2410.13025",
            "title": "LoRA-licious: Merging Skills for Smarter AI",
            "institute": "Microsoft Research, Princeton University, Harvard University",
            "text": "This paper explores a new way to combine different AI skills by merging their \"LoRA\" modules, which are like mini-brains that specialize in specific tasks. This is different from previous work that focused on merging entire models or training on mixed datasets.",
            "paper-title": "LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks",
            "image-path": "flux_paper_image/2410.13025_1729282617.png"
        },

        {
            "startTime": "34:34",
            "arxivId": "2410.13276",
            "arxivLink": "https://arxiv.org/abs/2410.13276",
            "title": "LLMs Get a Brain: Learning Sparse Attention for Speed Demons",
            "institute": "Microsoft",
            "text": "This paper proposes SeerAttention, a novel attention mechanism that learns attention sparsity instead of relying on predefined patterns or heuristics. It uses a learnable gate to identify significant blocks in the attention map, allowing for efficient block-sparse computation.",
            "paper-title": "SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs",
            "image-path": "flux_paper_image/2410.13276_1729281887.png"
        },

        {
            "startTime": "34:59",
            "arxivId": "2410.13002",
            "arxivLink": "https://arxiv.org/abs/2410.13002",
            "title": "\"Less is More: Teaching Robots to Navigate with a Single Text Command\"",
            "institute": "MIT",
            "text": "This research explores the minimal data requirements and architectural adaptations needed for robust text-instructed visual navigation in robots. Unlike previous work that relies on massive datasets and complex models, this paper proposes a minimalist approach using pre-trained Vision-Language Models (VLMs) as feature extractors and smaller policy networks.",
            "paper-title": "Flex: End-to-End Text-Instructed Visual Navigation with Foundation Models",
            "image-path": "flux_paper_image/2410.13002_1729280782.png"
        },

        {
            "startTime": "35:26",
            "arxivId": "2410.13675",
            "arxivLink": "https://arxiv.org/abs/2410.13675",
            "title": "Sign Language Anonymizer: Making Signers Invisible, But Not Their Signs!",
            "institute": "University of Zurich, sign.mt",
            "text": "This research focuses on transferring the appearance of a signer in a sign language video while preserving the sign content. Unlike previous work that anonymizes by concealing parts of the video or replacing the signer with an avatar, this method directly alters the appearance of the signer within the pose sequence.",
            "paper-title": "Pose-Based Sign Language Appearance Transfer",
            "image-path": "flux_paper_image/2410.13675_1729280648.png"
        },

        {
            "startTime": "35:48",
            "arxivId": "2410.13609",
            "arxivLink": "https://arxiv.org/abs/2410.13609",
            "title": "Model Selection on a Budget: How to Pick the Best AI Without Breaking the Bank",
            "institute": "ETH Zurich, Delft University of Technology",
            "text": "This research introduces MODEL SELECTOR, a framework for selecting the best pretrained classifier using a limited number of labeled examples. Unlike previous methods that often rely on assumptions about model architectures or require detailed knowledge of model internals, MODEL SELECTOR treats models as black boxes, requiring no additional training or information beyond hard predictions.",
            "paper-title": "All models are wrong, some are useful: Model Selection with Limited Labels",
            "image-path": "flux_paper_image/2410.13609_1729280638.png"
        },

        {
            "startTime": "36:07",
            "arxivId": "2410.13412",
            "arxivLink": "https://arxiv.org/abs/2410.13412",
            "title": "Robot Programming Gets a VR Makeover: AR Headset Makes Training a Breeze!",
            "institute": "Bogazici University, Adobe Research",
            "text": "This research introduces RAMPA, an AR-based system that uses a VR headset to collect and visualize robot trajectories, train machine learning models, and execute tasks in situ. This differs from previous work by integrating AR into the entire PfD process, allowing for real-time interaction and visualization.",
            "paper-title": "RAMPA: Robotic Augmented Reality for Machine Programming and Automation",
            "image-path": "flux_paper_image/2410.13412_1729280314.png"
        },

        {
            "startTime": "36:35",
            "arxivId": "2410.13708",
            "arxivLink": "https://arxiv.org/abs/2410.13708",
            "title": "Attention, Safety Heads! LLMs' Secret Weapon for Preventing Harm",
            "institute": "Alibaba, Tsinghua University, University of Science and Technology of China...",
            "text": "This research focuses on the role of multi-head attention mechanisms in LLM safety, a topic often overlooked in previous studies. It introduces a novel metric, Ships, to assess the individual heads' contributions to safety and proposes a heuristic algorithm, Sahara, to identify groups of heads that collectively impact safety.",
            "paper-title": "On the Role of Attention Heads in Large Language Model Safety",
            "image-path": "flux_paper_image/2410.13708_1729281682.png"
        },

        {
            "startTime": "37:01",
            "arxivId": "2410.13456",
            "arxivLink": "https://arxiv.org/abs/2410.13456",
            "title": "Swiss Courtroom Chatbot: New Dataset Helps AI Understand Legal Jargon in Three Languages!",
            "institute": "University of Bern, Bern University of Applied Sciences, Stanford University...",
            "text": "This research introduces a multilingual dataset of Swiss court rulings, including both the decisions and their summaries, to help train AI models for legal summarization. This is different from previous work that primarily focused on monolingual datasets, often in English.",
            "paper-title": "Unlocking Legal Knowledge: A Multilingual Dataset for Judicial Summarization in Switzerland",
            "image-path": "flux_paper_image/2410.13456_1729280033.png"
        },

        {
            "startTime": "37:32",
            "arxivId": "2410.13460",
            "arxivLink": "https://arxiv.org/abs/2410.13460",
            "title": "Legal Case Criticality: A Dataset That's Not Just Critical, It's Citation-Savvy!",
            "institute": "University of Bern, Bern University of Applied Sciences, Stanford University...",
            "text": "This research introduces a new dataset for evaluating the influence of legal cases, using a semi-automated labeling system instead of relying solely on manual annotations. This approach allows for a much larger dataset and a more nuanced evaluation of case importance.",
            "paper-title": "Breaking the Manual Annotation Bottleneck: Creating a Comprehensive Legal Case Criticality Dataset through Semi-Automated Labeling",
            "image-path": "flux_paper_image/2410.13460_1729282521.png"
        },

        {
            "startTime": "37:58",
            "arxivId": "2410.13638",
            "arxivLink": "https://arxiv.org/abs/2410.13638",
            "title": "Wearable Data: Scaling Up to 40 Million Hours of Health Insights!",
            "institute": "Google",
            "text": "This research explores the scaling properties of foundation models trained on wearable sensor data, focusing on the impact of compute, data size, and model parameters. Unlike previous work, it uses a masked input modeling approach and investigates the scaling laws for generative tasks like imputation, interpolation, and extrapolation.",
            "paper-title": "Scaling Wearable Foundation Models",
            "image-path": "flux_paper_image/2410.13638_1729281849.png"
        },

        {
            "startTime": "38:16",
            "arxivId": "2410.13548",
            "arxivLink": "https://arxiv.org/abs/2410.13548",
            "title": "Adaptive Adversaries: They're Just as Oblivious as You Think!",
            "institute": "Stanford",
            "text": "This paper proves that adaptive and oblivious statistical adversaries are equivalent, resolving a key open question from previous work. The authors show that any algorithm robust to an oblivious adversary can be made robust to its adaptive counterpart by simply taking a larger sample and using a subsampling filter.",
            "paper-title": "Adaptive and oblivious statistical adversaries are equivalent",
            "image-path": "flux_paper_image/2410.13548_1729281074.png"
        },

        {
            "startTime": "38:35",
            "arxivId": "2410.13490",
            "arxivLink": "https://arxiv.org/abs/2410.13490",
            "title": "Tired of Robots Learning Slowly? This New Trick Makes Them Super-Smart!",
            "institute": "Tsinghua University",
            "text": "This research introduces a novel method called Novelty-guided Sample Reuse (NSR) that optimizes the update frequency of each sample based on its novelty. Unlike traditional methods that update all samples equally, NSR prioritizes updates for infrequent, novel states, leading to more efficient learning.",
            "paper-title": "Novelty-based Sample Reuse for Continuous Robotics Control",
            "image-path": "flux_paper_image/2410.13490_1729280200.png"
        },

        {
            "startTime": "38:55",
            "arxivId": "2410.13224",
            "arxivLink": "https://arxiv.org/abs/2410.13224",
            "title": "ProofFlow: Teaching LLMs to Think Like Sherlock!",
            "institute": "UC San Diego, UC Santa Barbara, Montreal Institute for Learning Algorithms...",
            "text": "This research explores the use of Generative Flow Networks (GFlowNets) for fine-tuning LLMs to improve their reasoning abilities, specifically in the domain of formal theorem proving. Unlike traditional reinforcement learning methods, GFlowNets encourage exploration and diverse hypothesis generation, making them potentially more suitable for complex reasoning tasks.",
            "paper-title": "Proof Flow: Preliminary Study on Generative Flow Network Language Model Tuning for Formal Reasoning",
            "image-path": "flux_paper_image/2410.13224_1729282033.png"
        },

        {
            "startTime": "39:24",
            "arxivId": "2410.13639",
            "arxivLink": "https://arxiv.org/abs/2410.13639",
            "title": "OpenAI's o1 Model: Thinking Before Reasoning, One Step at a Time!",
            "institute": "University of Manchester, OpenO1 Team, 2077AI...",
            "text": "This research delves into the reasoning patterns of OpenAI's o1 model, comparing it to existing Test-time Compute methods. It analyzes the model's performance across various reasoning tasks, including math, code, and commonsense reasoning, and identifies six distinct reasoning patterns employed by o1.",
            "paper-title": "A Comparative Study on Reasoning Patterns of OpenAI's o1 Model",
            "image-path": "flux_paper_image/2410.13639_1729280829.png"
        },

        {
            "startTime": "39:44",
            "arxivId": "2410.13514",
            "arxivLink": "https://arxiv.org/abs/2410.13514",
            "title": "Simulating Traffic Chaos: How AI Learns to Recreate Real-World Driving Scenarios",
            "institute": "University of Oxford",
            "text": "This research proposes a novel method for generating dynamic driving scenarios in simulation using temporal scene graphs. Unlike previous approaches that rely on artificial agents or lack temporal information, this method leverages real-world data and a predefined ontology to create more realistic and diverse scenarios.",
            "paper-title": "CERES: Critical-Event Reconstruction via Temporal Scene Graph Completion",
            "image-path": "flux_paper_image/2410.13514_1729280338.png"
        },

        {
            "startTime": "40:08",
            "arxivId": "2410.13204",
            "arxivLink": "https://arxiv.org/abs/2410.13204",
            "title": "AI War Games: Can Language Models Handle the Pressure?",
            "institute": "University of Chicago, Northwestern University, Stanford University",
            "text": "This research measures the inconsistency of language models (LLMs) in a military crisis simulation, using a novel metric based on BERTScore to quantify semantic differences in free-form responses. Previous work focused on pre-defined actions, limiting the analysis of natural language decision-making.",
            "paper-title": "Measuring Free-Form Decision-Making Inconsistency of Language Models in Military Crisis Simulations",
            "image-path": "flux_paper_image/2410.13204_1729280270.png"
        },

        {
            "startTime": "40:31",
            "arxivId": "2410.13526",
            "arxivLink": "https://arxiv.org/abs/2410.13526",
            "title": "Radar Scene Generator: Fake It 'Til You Make It!",
            "institute": "RWTH Aachen University, Technische Universit\u00e4t Ilmenau, Technische Hochschule Ingolstadt",
            "text": "This research introduces a novel method for generating realistic radar point cloud scenes using a Generative Adversarial Network (GAN) based on the PointNet++ architecture. Unlike previous simulation-based approaches, this method aims to capture the complexities and variations of real-world radar data, including clutter and target fluctuations.",
            "paper-title": "Generative Adversarial Synthesis of Radar Point Cloud Scenes",
            "image-path": "flux_paper_image/2410.13526_1729280401.png"
        },

        {
            "startTime": "40:55",
            "arxivId": "2410.13768",
            "arxivLink": "https://arxiv.org/abs/2410.13768",
            "title": "AI Alloy Hunters: Graph Neural Networks Speed Up the Search for Super-Strong Metals",
            "institute": "MIT",
            "text": "This research introduces a multi-agent AI system that uses a graph neural network (GNN) to predict key properties of alloys, significantly reducing the reliance on computationally expensive simulations. This approach differs from previous work by integrating a GNN model into a multi-agent system, enabling faster exploration of the vast design space of multi-component alloys.",
            "paper-title": "Rapid and Automated Alloy Design with Graph Neural Network-Powered LLM-Driven Multi-Agent Systems",
            "image-path": "flux_paper_image/2410.13768_1729279695.png"
        },

        {
            "startTime": "41:16",
            "arxivId": "2410.13203",
            "arxivLink": "https://arxiv.org/abs/2410.13203",
            "title": "Tabular Data's New Dance: Feature Ordering Makes Deep Learning Models Groove!",
            "institute": "West Virginia University, University of South Carolina, Yale University",
            "text": "This research introduces TabSeq, a framework that uses feature ordering to improve deep learning models' performance on tabular data. Unlike previous work that often ignores feature arrangement, TabSeq systematically optimizes feature sequences by combining local and global ordering techniques.",
            "paper-title": "TabSeq: A Framework for Deep Learning on Tabular Data via Sequential Ordering",
            "image-path": "flux_paper_image/2410.13203_1729279861.png"
        },

        {
            "startTime": "41:40",
            "arxivId": "2410.12974",
            "arxivLink": "https://arxiv.org/abs/2410.12974",
            "title": "LLM Benchmarks Get a Makeover: Introducing BenchmarkCards!",
            "institute": "University of Notre Dame, IBM Research",
            "text": "This research introduces BenchmarkCards, a structured framework for documenting LLM benchmarks, focusing on the risks they target. This differs from previous work by specifically addressing the need for standardized documentation of LLM risk assessment, which is currently lacking.",
            "paper-title": "BenchmarkCards: Large Language Model and Risk Reporting",
            "image-path": "flux_paper_image/2410.12974_1729281129.png"
        },

        {
            "startTime": "42:03",
            "arxivId": "2410.12864",
            "arxivLink": "https://arxiv.org/abs/2410.12864",
            "title": "Big Brains, Big Biases: Do Bigger Language Models Mean More Prejudice?",
            "institute": "Enkrypt AI",
            "text": "This research goes beyond looking at explicit biases in LLMs and focuses on implicit biases, using a new method to measure them across a large set of models.",
            "paper-title": "Investigating Implicit Bias in Large Language Models: A Large-Scale Study of Over 50 LLMs",
            "image-path": "flux_paper_image/2410.12864_1729280842.png"
        },

        {
            "startTime": "42:26",
            "arxivId": "2410.12927",
            "arxivLink": "https://arxiv.org/abs/2410.12927",
            "title": "Deep Learning's Secret Sauce: Merging Models to Unlock Hidden Knowledge",
            "institute": "University of Chicago, Argonne National Laboratory, Colorado School of Mines",
            "text": "This research explores model merging techniques through the lens of loss landscape geometry, a novel approach that connects observations from interpretability, security, and loss landscape analysis to phenomena that govern neural network training.",
            "paper-title": "SoK: On Finding Common Ground in Loss Landscapes Using Deep Model Merging Techniques",
            "image-path": "flux_paper_image/2410.12927_1729280148.png"
        },

        {
            "startTime": "42:47",
            "arxivId": "2410.13799",
            "arxivLink": "https://arxiv.org/abs/2410.13799",
            "title": "Dark Matter's Hidden Glow: Machine Learning Uncovers Radiative Decays at the LHC",
            "institute": "Autonomous University of Madrid, Fermilab, International School for Advanced Studies...",
            "text": "This research explores the potential of machine learning to detect radiative decays of neutralinos, a type of weakly interacting massive particle (WIMP), into dark matter at the LHC. This approach differs from previous work by focusing on the soft photon signature produced in these decays, which is often overlooked in traditional searches.",
            "paper-title": "Machine-Learning Analysis of Radiative Decays to Dark Matter at the LHC",
            "image-path": "flux_paper_image/2410.13799_1729281032.png"
        },

        {
            "startTime": "43:09",
            "arxivId": "2410.13148",
            "arxivLink": "https://arxiv.org/abs/2410.13148",
            "title": "Neutrino Telescopes Get a Deep Learning Makeover: om2vec Makes Data Dance!",
            "institute": "Harvard University",
            "text": "This research introduces om2vec, a transformer-based variational autoencoder (VAE) that learns compact representations of neutrino telescope events. This approach differs from previous methods that relied on summary statistics or Gaussian mixture models.",
            "paper-title": "Learning Efficient Representations of Neutrino Telescope Events",
            "image-path": "flux_paper_image/2410.13148_1729281709.png"
        },

        {
            "startTime": "43:32",
            "arxivId": "2410.13779",
            "arxivLink": "https://arxiv.org/abs/2410.13779",
            "title": "Language Models Get Lost in the Path-Star Maze: A New Challenge for AI",
            "institute": "University of Toronto",
            "text": "This research investigates the limitations of language models (LMs) on a seemingly simple task called the \"path-star task.\" Unlike previous work, it focuses on the role of teacher-forcing and the causal constraint of decoder-only models in hindering the learning process.",
            "paper-title": "The Mystery of the Pathological Path-star Task for Language Models",
            "image-path": "flux_paper_image/2410.13779_1729281458.png"
        },

        {
            "startTime": "43:52",
            "arxivId": "2410.12812",
            "arxivLink": "https://arxiv.org/abs/2410.12812",
            "title": "RAG: Don't Just Search, Make Your Docs Answer Questions!",
            "institute": "IBM",
            "text": "This research focuses on optimizing the content itself for Retrieval-Augmented Generation (RAG) systems, rather than solely focusing on improving the retrieval or generation models.",
            "paper-title": "Optimizing and Evaluating Enterprise Retrieval-Augmented Generation (RAG): A Content Design Perspective",
            "image-path": "flux_paper_image/2410.12812_1729280077.png"
        },

        {
            "startTime": "44:13",
            "arxivId": "2410.12867",
            "arxivLink": "https://arxiv.org/abs/2410.12867",
            "title": "AI Gives Dysarthric Speech a Voice: LLMs Decode Slurred Words, Understand Emotions",
            "institute": "Pegasystems Worldwide Pvt Ltd, Capital Quants Solutions, Vasavi College of Engineering",
            "text": "This research uses advanced LLMs to not only correct dysarthric speech but also analyze the speaker's emotions, adding a new dimension to communication assistance. Unlike previous work, it focuses on both speech correction and emotion detection, aiming for a more inclusive communication experience.",
            "paper-title": "Empowering Dysarthric Speech: Leveraging Advanced LLMs for Accurate Speech Correction and Multimodal Emotion Analysis",
            "image-path": "flux_paper_image/2410.12867_1729280817.png"
        },

        {
            "startTime": "44:41",
            "arxivId": "2410.13047",
            "arxivLink": "https://arxiv.org/abs/2410.13047",
            "title": "LLMs: Confident but Clueless? New Method Uncovers Their Hidden Mistakes!",
            "institute": "University of Washington",
            "text": "This research introduces a novel uncertainty quantification (UQ) aggregation strategy for identifying misclassified LLM-labeled data in zero-shot CSS classification tasks. Unlike previous methods that rely on self-reporting or external training data, this approach leverages the confidence scores from multiple LLMs to pinpoint low-confidence annotations.",
            "paper-title": "LLM Confidence Evaluation Measures in Zero-Shot CSS Classification",
            "image-path": "flux_paper_image/2410.13047_1729281736.png"
        },

        {
            "startTime": "45:09",
            "arxivId": "2410.13286",
            "arxivLink": "https://arxiv.org/abs/2410.13286",
            "title": "Fairness in AI: It's Not Just About One Metric, It's About the Whole Landscape!",
            "institute": "University of Freiburg",
            "text": "This research proposes a new framework called ManyFairHPO that considers multiple fairness metrics simultaneously during model selection, unlike previous work that often focuses on a single fairness metric.",
            "paper-title": "A Human-in-the-Loop Fairness-Aware Model Selection Framework for Complex Fairness Objective Landscapes",
            "image-path": "flux_paper_image/2410.13286_1729281252.png"
        },

        {
            "startTime": "45:30",
            "arxivId": "2410.13247",
            "arxivLink": "https://arxiv.org/abs/2410.13247",
            "title": "Sentiment Analysis Gets a Collaborative AI Makeover: ChatGPT and Gemini Team Up to Predict Your Mood!",
            "institute": "Advanced Institute of Industrail Technology, Soochow University, Shanghai Jiaotong University...",
            "text": "This research introduces a collaborative AI framework for sentiment analysis, utilizing multiple AI systems to process complex multimodal data. Unlike previous work that focused on single AI models, this approach leverages the strengths of different AI systems to achieve more accurate and comprehensive results.",
            "paper-title": "Enhancing Sentiment Analysis with Collaborative AI: Architecture, Predictions, and Deployment Strategies",
            "image-path": "flux_paper_image/2410.13247_1729279750.png"
        },

        {
            "startTime": "45:55",
            "arxivId": "2410.13769",
            "arxivLink": "https://arxiv.org/abs/2410.13769",
            "title": "Team Up, Transformers! AI Learns to Build Winning Teams in Games",
            "institute": "CMU, United States Navy",
            "text": "This research proposes BERTeam, a novel algorithm that uses a transformer-based neural network to select the best team of players from a trained population. Unlike previous work that focused on evolutionary computation for team formation, BERTeam treats team selection as a token sequence generation process.",
            "paper-title": "Transformer Guided Coevolution: Improved Team Formation in Multiagent Adversarial Games",
            "image-path": "flux_paper_image/2410.13769_1729282142.png"
        },

        {
            "startTime": "46:22",
            "arxivId": "2410.13006",
            "arxivLink": "https://arxiv.org/abs/2410.13006",
            "title": "LLMs in a Chain Gang: Data Annotation Gets a Speed Boost!",
            "institute": "University of Washington, Army Cyber Institute, Carnegie Mellon University",
            "text": "This research introduces a novel LLM chain ensemble methodology for data annotation. Unlike previous work that focuses on prompt engineering or comparing individual LLMs, this approach treats LLMs as components within a broader system designed to enhance classification performance and reduce costs.",
            "paper-title": "LLM Chain Ensembles for Scalable and Accurate Data Annotation",
            "image-path": "flux_paper_image/2410.13006_1729279768.png"
        },

        {
            "startTime": "46:51",
            "arxivId": "2410.12837",
            "arxivLink": "https://arxiv.org/abs/2410.12837",
            "title": "RAG: The AI That Knows When to Google!",
            "institute": "Carnegie Mellon University, BIT Sindri",
            "text": "This research provides a comprehensive survey of Retrieval-Augmented Generation (RAG) models, tracing their evolution, highlighting key innovations, and exploring current challenges and future directions. It differs from previous work by offering a more in-depth analysis of the field, including a detailed review of recent advancements and a discussion of ethical considerations.",
            "paper-title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "image-path": "flux_paper_image/2410.12837_1729280963.png"
        }
    ],
    "stats": {
        "num_pick": 117,
        "num_total": 478,
    },
    "audio": "https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202410181339_audio.mp3"
}
