
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Fresh AI Paper Top Picks</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 80px;"></div>
            <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">
                Freshest
                Top Picks:
                <span class="highlightNumber" style="font-size: 28px;">52</span> out of <span
                    class="highlightNumber">264</span> arXiv AI
                papers
            </div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="line-height: 1.2;">just released on</div>
                <div id="data-default-date" data-date="2024-07-26"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 10px;">
            </div>
        </div>

<div class="tweet" id="tweet0">
 <div class="start-time-icon" title="Play from here">
  00:50
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17852" target="_blank">
    @arXiv 2407.17852
   </a>
   <span class="tweet-title">
    Zero-Shot Speech Recognition: Romanization Rules!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Monash University, Meta
   </span>
  </div>
  <div class="primary-text">
   This research proposes a novel zero-shot speech recognition approach that utilizes romanization instead of phonemes, eliminating the need for language-specific phonemizers.
  </div>
 </div>
</div>
<div class="tweet" id="tweet1">
 <div class="start-time-icon" title="Play from here">
  01:13
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17773" target="_blank">
    @arXiv 2407.17773
   </a>
   <span class="tweet-title">
    Kids Can Do It, Can AI? New Benchmark Tests Visual Reasoning in Large Models
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    University of California  Berkeley, Boston University, Google DeepMind
   </span>
  </div>
  <div class="primary-text">
   This research introduces a new benchmark called KiVA, which uses real-world objects and simple visual transformations to test analogical reasoning in large multimodal models. Unlike previous benchmarks that rely on abstract shapes and complex patterns, KiVA focuses on basic visual changes that even young children can understand.
  </div>
 </div>
</div>
<div class="tweet" id="tweet2">
 <div class="start-time-icon" title="Play from here">
  01:39
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17678" target="_blank">
    @arXiv 2407.17678
   </a>
   <span class="tweet-title">
    LLMs Get a Diet:  Sharding Attention for Faster Training and Serving
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Microsoft
   </span>
  </div>
  <div class="primary-text">
   This research proposes Sparsely-Sharded (S2) Attention, a new approach to training and serving LLMs. Unlike previous methods that focus on homogeneous sparsity patterns, S2-Attention assigns heterogeneous context partitions to different attention heads, allowing for greater sparsity and efficiency.
  </div>
 </div>
</div>
<div class="tweet" id="tweet3">
 <div class="start-time-icon" title="Play from here">
  02:03
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18112" target="_blank">
    @arXiv 2407.18112
   </a>
   <span class="tweet-title">
    Re-IDing the Occluded: Keypoint Prompts for Person Recognition
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    École Polytechnique Fédérale de Lausanne, Université Catholique de Louvain, Sportradar
   </span>
  </div>
  <div class="primary-text">
   This research introduces a novel approach to person re-identification (ReID) by incorporating keypoint prompts. Unlike previous methods that rely solely on bounding boxes, this method explicitly addresses the ambiguity caused by multiple individuals within a single bounding box.
  </div>
 </div>
</div>
<div class="tweet" id="tweet4">
 <div class="start-time-icon" title="Play from here">
  02:30
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18245" target="_blank">
    @arXiv 2407.18245
   </a>
   <span class="tweet-title">
    Synthetic Heads:  A Million Fake Faces for Real-World AI
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    University of Oxford, Ukrainian Catholic University, PiñataFarms AI
   </span>
  </div>
  <div class="primary-text">
   This research introduces a large-scale synthetic dataset of human heads, generated using diffusion models, and a novel model trained on this data that can simultaneously detect and reconstruct 3D head meshes from a single image. This approach differs from previous work by directly predicting 3D head models from images, rather than relying on separate detection and reconstruction steps.
  </div>
 </div>
</div>
<div class="tweet" id="tweet5">
 <div class="start-time-icon" title="Play from here">
  02:59
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18219" target="_blank">
    @arXiv 2407.18219
   </a>
   <span class="tweet-title">
    LLMs Learn to Self-Improve:  A Recursive Introspection Approach
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    CMU
   </span>
  </div>
  <div class="primary-text">
   This research introduces RISE, a novel approach for fine-tuning LLMs to enable them to improve their responses over multiple turns. Unlike previous work that focuses on prompting techniques or external feedback, RISE trains models to learn from their own mistakes and iteratively refine their answers.
  </div>
 </div>
</div>
<div class="tweet" id="tweet6">
 <div class="start-time-icon" title="Play from here">
  03:31
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18141" target="_blank">
    @arXiv 2407.18141
   </a>
   <span class="tweet-title">
    Tired of Yelling at Alexa? This Smart Ring Sees Your Home and Does Your Bidding!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    University of Washington
   </span>
  </div>
  <div class="primary-text">
   This research introduces IRIS, a wireless smart ring equipped with a camera that enables vision-based smart home interaction. Unlike previous work that relies on voice commands or requires modifications to existing devices, IRIS uses image recognition to identify and control smart home devices.
  </div>
 </div>
</div>
<div class="tweet" id="tweet7">
 <div class="start-time-icon" title="Play from here">
  03:55
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17817" target="_blank">
    @arXiv 2407.17817
   </a>
   <span class="tweet-title">
    LLMs: Memorizing More Than Just Words, They're Remembering the Vibe!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Stanford University
   </span>
  </div>
  <div class="primary-text">
   This research introduces a novel framework for studying verbatim memorization in LLMs by injecting specific sequences into the training data. This allows for a controlled setting to study the effects of language modeling quality on memorization, unlike previous observational studies.
  </div>
 </div>
</div>
<div class="tweet" id="tweet8">
 <div class="start-time-icon" title="Play from here">
  04:16
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17605" target="_blank">
    @arXiv 2407.17605
   </a>
   <span class="tweet-title">
    Speech to Text:  A Match Made in L2-Loss Heaven!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Google
   </span>
  </div>
  <div class="primary-text">
   This research introduces an "exporter" layer trained under L2-loss to align speech encoder embeddings with text model token embeddings. This approach differs from previous work by ensuring that the resulting cascade speech translation model performs no worse than the 1-best cascade baseline while allowing back-propagation gradient to flow from the text model into the speech recognition components.
  </div>
 </div>
</div>
<div class="tweet" id="tweet9">
 <div class="start-time-icon" title="Play from here">
  04:44
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18121" target="_blank">
    @arXiv 2407.18121
   </a>
   <span class="tweet-title">
    KV Cache:  Don't Throw It Away, Merge It!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Tsinghua University, University of Washington, Carnegie Mellon University...
   </span>
  </div>
  <div class="primary-text">
   This paper introduces ElasticCache, a novel approach to managing key-value (KV) caches in instruction-following vision-language models (LVLMs). Unlike previous methods that focus on cache eviction, ElasticCache employs a cache merging strategy, where less important vectors are merged with anchor points to preserve contextual information.
  </div>
 </div>
</div>
<div class="tweet" id="tweet10">
 <div class="start-time-icon" title="Play from here">
  05:09
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17792" target="_blank">
    @arXiv 2407.17792
   </a>
   <span class="tweet-title">
    Time Travel for Action Detection: New AI Sees the Future (and Past) of Videos!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    KAUST, 4Paradigm Inc., Moonshot AI...
   </span>
  </div>
  <div class="primary-text">
   This research introduces a novel approach to temporal action detection by incorporating temporal causality. Unlike previous methods that treat past and future information equally, this study proposes a hybrid causal block that leverages causal attention and causal Mamba to model the inherent causal relationships in action transitions.
  </div>
 </div>
</div>
<div class="tweet" id="tweet11">
 <div class="start-time-icon" title="Play from here">
  05:40
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18249" target="_blank">
    @arXiv 2407.18249
   </a>
   <span class="tweet-title">
    Action Recognition with a Pointy Twist: Trajectory-Aligned Tokens for Few-Shot Learning
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    University of Maryland, Meta
   </span>
  </div>
  <div class="primary-text">
   This research proposes a novel approach for few-shot action recognition by disentangling motion and appearance representations. It leverages point trajectories from tracking algorithms and self-supervised representation learning to create trajectory-aligned tokens (TATs), which capture both motion and appearance information. This approach significantly reduces the data requirements while retaining essential information.
  </div>
 </div>
</div>
<div class="tweet" id="tweet12">
 <div class="start-time-icon" title="Play from here">
  06:03
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17952" target="_blank">
    @arXiv 2407.17952
   </a>
   <span class="tweet-title">
    Depth Estimation:  Diffusion Models Get a Boost from Pre-Trained Experts!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    ETH Zürich, DisneyResearch|Studios
   </span>
  </div>
  <div class="primary-text">
   This paper proposes BetterDepth, a method that combines pre-trained zero-shot monocular depth estimation (MDE) models with diffusion-based refiners. This approach aims to leverage the strengths of both types of models, achieving robust performance with fine-grained details. Unlike previous diffusion-based MDE methods, BetterDepth does not require extensive training on real-world datasets, which often have noisy and incomplete depth labels.
  </div>
 </div>
</div>
<div class="tweet" id="tweet13">
 <div class="start-time-icon" title="Play from here">
  06:27
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17712" target="_blank">
    @arXiv 2407.17712
   </a>
   <span class="tweet-title">
    Predicting the Future: How Machine Learning Can Make Online Algorithms Smarter
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Google
   </span>
  </div>
  <div class="primary-text">
   This research explores using machine-learned predictions to improve the performance of online algorithms. Unlike previous work that focused on worst-case scenarios, this paper introduces the concepts of robustness and consistency, allowing algorithms to adapt to varying prediction accuracy.
  </div>
 </div>
</div>
<div class="tweet" id="tweet14">
 <div class="start-time-icon" title="Play from here">
  06:54
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18178" target="_blank">
    @arXiv 2407.18178
   </a>
   <span class="tweet-title">
    Piano-Playing Robot Learns From YouTube, Plays (Almost) Like a Pro!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    TU Munich, TU Darmstadt, UC Berkeley
   </span>
  </div>
  <div class="primary-text">
   This research differs from previous work by training a piano-playing robot using internet demonstrations, specifically YouTube videos, rather than relying solely on traditional reinforcement learning methods.
  </div>
 </div>
</div>
<div class="tweet" id="tweet15">
 <div class="start-time-icon" title="Play from here">
  07:14
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18184" target="_blank">
    @arXiv 2407.18184
   </a>
   <span class="tweet-title">
    Antibody Epitope Prediction:  A New Dataset and Model That's Not Just a Bunch of Hot Air!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    University College London
   </span>
  </div>
  <div class="primary-text">
   This research introduces a new dataset, AsEP, for antibody-specific epitope prediction, which is larger and more diverse than previous datasets. It also proposes a new model, WALLE, that combines protein language models and graph neural networks to improve epitope prediction accuracy.
  </div>
 </div>
</div>
<div class="tweet" id="tweet16">
 <div class="start-time-icon" title="Play from here">
  07:37
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17770" target="_blank">
    @arXiv 2407.17770
   </a>
   <span class="tweet-title">
    BOTEVAL:  Chatbots Get a Human Touch for Better Evaluation
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    University of Southern California, Microsoft
   </span>
  </div>
  <div class="primary-text">
   This research introduces BOTEVAL, a toolkit designed for evaluating NLP models in interactive tasks, where human evaluators directly interact with the models. This differs from previous approaches that focused on evaluating static outputs or completed conversations.
  </div>
 </div>
</div>
<div class="tweet" id="tweet17">
 <div class="start-time-icon" title="Play from here">
  08:00
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18247" target="_blank">
    @arXiv 2407.18247
   </a>
   <span class="tweet-title">
    Drag It, Don't Drag It Out: Region-Based Image Editing Speeds Up Diffusion Models
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    University of Hong Kong, University of Oxford
   </span>
  </div>
  <div class="primary-text">
   This paper proposes a region-based approach to image editing using diffusion models, unlike previous point-based methods. Instead of dragging individual points, users define handle and target regions, providing richer context for the model.
  </div>
 </div>
</div>
<div class="tweet" id="tweet18">
 <div class="start-time-icon" title="Play from here">
  08:19
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17956" target="_blank">
    @arXiv 2407.17956
   </a>
   <span class="tweet-title">
    Gigapixel Images? No Problem! SaccadeDet Sees It All, Fast!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Shanghai Jiao Tong University, Tsinghua University
   </span>
  </div>
  <div class="primary-text">
   This research introduces SaccadeDet, a novel dual-stage architecture for object detection in gigapixel images. Unlike previous methods that process the entire image, SaccadeDet strategically selects and processes only regions of interest, significantly reducing computational load and improving speed.
  </div>
 </div>
</div>
<div class="tweet" id="tweet19">
 <div class="start-time-icon" title="Play from here">
  08:43
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18145" target="_blank">
    @arXiv 2407.18145
   </a>
   <span class="tweet-title">
    Semantic Segmentation Goes Hyperbolic:  A Taxonomy-Aware Approach to Open-World Perception
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    University of Freiburg
   </span>
  </div>
  <div class="primary-text">
   This research proposes a new approach to class-incremental semantic segmentation that leverages hyperbolic space to represent class relationships. Unlike previous methods that focus on incremental learning from the background, this approach allows for new classes to originate from both the background and known classes, making it more realistic for real-world applications.
  </div>
 </div>
</div>
<div class="tweet" id="tweet20">
 <div class="start-time-icon" title="Play from here">
  09:16
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18046" target="_blank">
    @arXiv 2407.18046
   </a>
   <span class="tweet-title">
    Pixels Are So 2023: GaussianSR Blurs the Lines for Super-Resolution!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Tsinghua University
   </span>
  </div>
  <div class="primary-text">
   This paper introduces GaussianSR, a new method for arbitrary-scale super-resolution (ASSR) that uses 2D Gaussian Splatting to represent pixels as continuous fields instead of discrete points. This differs from previous INR-based ASSR methods that rely on discrete latent codes.
  </div>
 </div>
</div>
<div class="tweet" id="tweet21">
 <div class="start-time-icon" title="Play from here">
  09:38
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17933" target="_blank">
    @arXiv 2407.17933
   </a>
   <span class="tweet-title">
    Five Images, One Click:  New AI Tool Makes Medical Segmentation a Breeze!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    University College London, Wellcome/EPSRC Centre for Interventional and Surgical Sciences, Royal National Orthopaedic Hospital
   </span>
  </div>
  <div class="primary-text">
   This research proposes a novel approach to medical image segmentation using the Segment Anything Model (SAM). Unlike previous methods that require extensive training data or full segmentation labels, this technique leverages image registration to align a new image with a small set of reference images, each with only a few point prompts. This allows for accurate segmentation without the need for extensive manual annotation.
  </div>
 </div>
</div>
<div class="tweet" id="tweet22">
 <div class="start-time-icon" title="Play from here">
  10:06
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17854" target="_blank">
    @arXiv 2407.17854
   </a>
   <span class="tweet-title">
    Shapley Value: The Fair Way to Align Images and Text for Information Extraction
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Peking University
   </span>
  </div>
  <div class="primary-text">
   This research introduces a new paradigm for multimodal information extraction (MIE) called Image-Context-Text interaction. Instead of directly aligning images and text, it uses a large multimodal model (LMM) to generate a descriptive textual context that bridges the semantic and modality gaps between the two.
  </div>
 </div>
</div>
<div class="tweet" id="tweet23">
 <div class="start-time-icon" title="Play from here">
  10:28
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18207" target="_blank">
    @arXiv 2407.18207
   </a>
   <span class="tweet-title">
    Spherical Images:  FID's Got No View, OmniFID's the New Crew!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Technical University of Denmark, Google
   </span>
  </div>
  <div class="primary-text">
   This research introduces two new metrics, OmniFID and Discontinuity Score (DS), to evaluate the geometric fidelity of spherical images generated by AI models. Unlike the widely used Fréchet Inception Distance (FID), these metrics specifically account for the unique geometric constraints of spherical images, such as field-of-view and seam alignment.
  </div>
 </div>
</div>
<div class="tweet" id="tweet24">
 <div class="start-time-icon" title="Play from here">
  10:56
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17954" target="_blank">
    @arXiv 2407.17954
   </a>
   <span class="tweet-title">
    Stop Wasting Storage! New Research Shows How to Train AI Models on Compressed Images
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Granica
   </span>
  </div>
  <div class="primary-text">
   This research introduces a "bits-samples scaling law" that describes how the error rate of a machine learning model changes with both the number of training samples and the number of bits used to represent each sample. This differs from previous scaling laws that only considered the number of samples.
  </div>
 </div>
</div>
<div class="tweet" id="tweet25">
 <div class="start-time-icon" title="Play from here">
  11:19
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17616" target="_blank">
    @arXiv 2407.17616
   </a>
   <span class="tweet-title">
    Pre-trained in 1D, Ready to Rule in 2D: A Neural Operator's Low-Dimensional Trick
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Carnegie Mellon University
   </span>
  </div>
  <div class="primary-text">
   This research proposes a novel pretraining strategy for neural PDE solvers. Instead of relying on expensive high-dimensional PDE data, the authors pretrain the model on lower-dimensional (1D) PDEs, then transfer the learned weights to higher dimensions. This approach aims to reduce the cost of data collection and improve the accuracy of the model.
  </div>
 </div>
</div>
<div class="tweet" id="tweet26">
 <div class="start-time-icon" title="Play from here">
  11:43
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17477" target="_blank">
    @arXiv 2407.17477
   </a>
   <span class="tweet-title">
    AI Detects Bias in Doctor Talk:  Is Your Doc a Warm Fuzzy or a Cold Shoulder?
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    University of Washington, University of California San Diego
   </span>
  </div>
  <div class="primary-text">
   This research uses automated speech recognition (ASR) and natural language processing (NLP) to analyze the content of clinical conversations, going beyond just nonverbal cues to identify social signals that may indicate bias.
  </div>
 </div>
</div>
<div class="tweet" id="tweet27">
 <div class="start-time-icon" title="Play from here">
  12:08
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17624" target="_blank">
    @arXiv 2407.17624
   </a>
   <span class="tweet-title">
    LLMs Get Schooled: Traditional Methods Ace Credit Rating Forecasting
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    University of Oxford
   </span>
  </div>
  <div class="primary-text">
   This research investigates the performance of LLMs in forecasting corporate credit ratings, a task that has not been extensively explored before. It compares LLMs to traditional methods, specifically XGBoost, and finds that traditional methods are more effective when incorporating both textual and numerical data.
  </div>
 </div>
</div>
<div class="tweet" id="tweet28">
 <div class="start-time-icon" title="Play from here">
  12:29
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17879" target="_blank">
    @arXiv 2407.17879
   </a>
   <span class="tweet-title">
    FPGA-Accelerated Vision Transformers:  Hybrid-Grained Pipeline for Speed Demons!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Peking University
   </span>
  </div>
  <div class="primary-text">
   This research introduces a hybrid-grained pipeline architecture for accelerating Vision Transformers (ViTs) on FPGAs. Unlike previous approaches that rely on either coarse-grained or fine-grained pipelines, this method combines the benefits of both, resulting in a more efficient and balanced design.
  </div>
 </div>
</div>
<div class="tweet" id="tweet29">
 <div class="start-time-icon" title="Play from here">
  12:56
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17703" target="_blank">
    @arXiv 2407.17703
   </a>
   <span class="tweet-title">
    Traffic Forecasting Gets a Knowledge Graph Makeover:  Predicting Speed with Contextual Clues!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    ETH Zurich, University of Canterbury, University of Wisconsin-Madison
   </span>
  </div>
  <div class="primary-text">
   This research proposes a context-aware knowledge graph (CKG) framework for traffic speed forecasting, which integrates spatial and temporal context information into a graph neural network (GNN) model. This approach differs from previous work by explicitly modeling the relationships between traffic data and surrounding environmental factors.
  </div>
 </div>
</div>
<div class="tweet" id="tweet30">
 <div class="start-time-icon" title="Play from here">
  13:34
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17622" target="_blank">
    @arXiv 2407.17622
   </a>
   <span class="tweet-title">
    AI Minds:  Can Neural Networks Mimic Human Decision-Making?
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Singapore Management University, CMU, Rutgers University
   </span>
  </div>
  <div class="primary-text">
   This research proposes two new attention-based neural network models that personalize decision-making by incorporating individual memory, unlike previous work that often assumes a single model for all humans.
  </div>
 </div>
</div>
<div class="tweet" id="tweet31">
 <div class="start-time-icon" title="Play from here">
  13:56
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17492" target="_blank">
    @arXiv 2407.17492
   </a>
   <span class="tweet-title">
    Spectra-licious! New Dataset Makes Molecular Structure Elucidation a Breeze
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    IBM
   </span>
  </div>
  <div class="primary-text">
   This research introduces a multimodal spectroscopic dataset containing simulated spectra from various techniques for 790,000 molecules, enabling the development of AI models that can integrate information from multiple modalities, mimicking the approach used by human experts. This differs from previous work that primarily focused on single-modality tasks.
  </div>
 </div>
</div>
<div class="tweet" id="tweet32">
 <div class="start-time-icon" title="Play from here">
  14:18
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17695" target="_blank">
    @arXiv 2407.17695
   </a>
   <span class="tweet-title">
    LLMs Learn the Game:  How a Framework Makes AI Agents Smarter Than Humans in Crafter
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Université de Montréal, Mila, Microsoft Research
   </span>
  </div>
  <div class="primary-text">
   This research introduces DiVE, a framework that helps large language models (LLMs) learn the dynamics of a game environment from a small number of demonstrations. Unlike previous methods that rely on extensive training data, DiVE focuses on discovering, verifying, and evolving knowledge about the game world, enabling LLMs to make better decisions.
  </div>
 </div>
</div>
<div class="tweet" id="tweet33">
 <div class="start-time-icon" title="Play from here">
  14:39
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17686" target="_blank">
    @arXiv 2407.17686
   </a>
   <span class="tweet-title">
    Transformers: Deep Learning Doesn't Need to Be Deep!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    UC Berkeley
   </span>
  </div>
  <div class="primary-text">
   This research challenges the common belief that transformers need a large number of layers to learn complex patterns in data. It shows that a transformer with just three layers can effectively learn kth-order Markov processes, which are models that capture dependencies between consecutive elements in a sequence. This finding contrasts with previous studies that suggested a linear scaling of the number of layers with the order of the Markov process.
  </div>
 </div>
</div>
<div class="tweet" id="tweet34">
 <div class="start-time-icon" title="Play from here">
  15:02
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17998" target="_blank">
    @arXiv 2407.17998
   </a>
   <span class="tweet-title">
    Deep Learning Debugging:  A Visual Guide to Untangling the Knots
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    ETH Zurich, University of Konstanz
   </span>
  </div>
  <div class="primary-text">
   This research proposes a structured framework for debugging deep learning models, going beyond traditional metrics like accuracy and loss. It emphasizes the importance of analyzing data across multiple levels of abstraction, from the model's architecture to individual neurons, and provides a system called iNNspector to facilitate this process.
  </div>
 </div>
</div>
<div class="tweet" id="tweet35">
 <div class="start-time-icon" title="Play from here">
  15:30
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18110" target="_blank">
    @arXiv 2407.18110
   </a>
   <span class="tweet-title">
    Mapping Madness: AI Tunes Up ASIC Design with a Library of Tricks
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    University of Maryland, MIT
   </span>
  </div>
  <div class="primary-text">
   This research proposes MapTune, a framework that uses reinforcement learning to optimize the selection of cells from a technology library during ASIC technology mapping. Unlike previous work that focuses on end-to-end tool parameter space exploration, MapTune specifically targets the cell selection process, aiming to reduce the search space and improve mapping quality.
  </div>
 </div>
</div>
<div class="tweet" id="tweet36">
 <div class="start-time-icon" title="Play from here">
  15:56
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18074" target="_blank">
    @arXiv 2407.18074
   </a>
   <span class="tweet-title">
    AI Agents Get Paid: Contracts for Smarter Bots
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Google, Technion – Israel Institute of Technology, Harvard University
   </span>
  </div>
  <div class="primary-text">
   This research explores the use of contracts in reinforcement learning settings where the agent's goals are misaligned with the principal's. It differs from previous work by focusing on sequential contracting problems and developing a meta-algorithm that provably converges to a subgame-perfect equilibrium.
  </div>
 </div>
</div>
<div class="tweet" id="tweet37">
 <div class="start-time-icon" title="Play from here">
  16:30
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18058" target="_blank">
    @arXiv 2407.18058
   </a>
   <span class="tweet-title">
    Music AI Can Hear, But Can It Read?
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Queen Mary University of London, Spotify
   </span>
  </div>
  <div class="primary-text">
   This research delves into the limitations of two-tower multimodal systems for instrument recognition, specifically focusing on the text encoder's ability to accurately interpret musical descriptions. It evaluates the systems' performance using different prompts and analyzes the semantic meaningfulness of the text embeddings.
  </div>
 </div>
</div>
<div class="tweet" id="tweet38">
 <div class="start-time-icon" title="Play from here">
  16:54
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17642" target="_blank">
    @arXiv 2407.17642
   </a>
   <span class="tweet-title">
    Traffic Accidents?  Let's Hypergraph Our Way to Safety!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    University College London
   </span>
  </div>
  <div class="primary-text">
   This research introduces a new model called SMA-Hyper, which uses adaptive hypergraphs and graphs to capture complex relationships between urban regions, unlike previous models that rely on predefined structures.
  </div>
 </div>
</div>
<div class="tweet" id="tweet39">
 <div class="start-time-icon" title="Play from here">
  17:21
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18251" target="_blank">
    @arXiv 2407.18251
   </a>
   <span class="tweet-title">
    Tiny Tweaks, Big Trouble: How a Few Pixels Can Break Multimodal Models
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    École Polytechnique Fédérale de Lausanne, armasuisse
   </span>
  </div>
  <div class="primary-text">
   This research focuses on the impact of sparse and contiguous pixel perturbations on the robustness of multimodal models. Unlike previous work that focused on whole-image perturbations, this study investigates the vulnerability of these models to attacks targeting a limited number of pixels, exploring different spatial distributions of those pixels.
  </div>
 </div>
</div>
<div class="tweet" id="tweet40">
 <div class="start-time-icon" title="Play from here">
  17:50
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17491" target="_blank">
    @arXiv 2407.17491
   </a>
   <span class="tweet-title">
    Black-Box Vision:  How to Teach a Model Without Peeking!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    University of Wisconsin-Madison, CMU
   </span>
  </div>
  <div class="primary-text">
   This research introduces a new method called "Black-Box Visual Prompting" (BlackVIP) that allows users to adapt pre-trained vision models without needing access to the model's internal parameters or a large memory capacity. This differs from previous methods that require full parameter access and significant memory for gradient calculations.
  </div>
 </div>
</div>
<div class="tweet" id="tweet41">
 <div class="start-time-icon" title="Play from here">
  18:19
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18227" target="_blank">
    @arXiv 2407.18227
   </a>
   <span class="tweet-title">
    AI Doctor's New Trick:  Mixing Images and Records for Better Diagnoses!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    University of Oxford
   </span>
  </div>
  <div class="primary-text">
   This research introduces AutoPrognosis-M, an automated machine learning framework that combines structured clinical data (like patient records) with medical images to create more accurate diagnostic and prognostic models. Unlike previous approaches that typically rely on a single data type, AutoPrognosis-M leverages the power of multiple modalities, mirroring how clinicians make decisions in real-world settings.
  </div>
 </div>
</div>
<div class="tweet" id="tweet42">
 <div class="start-time-icon" title="Play from here">
  18:52
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17777" target="_blank">
    @arXiv 2407.17777
   </a>
   <span class="tweet-title">
    Babel:  Six-Sensing Superpower for Your Devices!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Microsoft, University of Wisconsin-Madison, Hong Kong University of Science and Technology
   </span>
  </div>
  <div class="primary-text">
   This research introduces Babel, a framework that aligns multiple sensing modalities using partially paired data. Unlike previous work that requires fully paired data, Babel leverages existing single-modal networks and a novel expandable architecture to achieve multi-modal alignment.
  </div>
 </div>
</div>
<div class="tweet" id="tweet43">
 <div class="start-time-icon" title="Play from here">
  19:23
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17716" target="_blank">
    @arXiv 2407.17716
   </a>
   <span class="tweet-title">
    Speech Recognition:  When Noise Gets in the Way, Text to the Rescue!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    University of Texas at Dallas, Boston University, Harvard University...
   </span>
  </div>
  <div class="primary-text">
   This research explores using text descriptions of the environment to improve speech emotion recognition (SER) systems' performance in noisy conditions. Unlike previous work that focused on data augmentation or feature enhancement, this study leverages the semantic information from text prompts to guide the SER model's adaptation to different noisy environments.
  </div>
 </div>
</div>
<div class="tweet" id="tweet44">
 <div class="start-time-icon" title="Play from here">
  19:48
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17505" target="_blank">
    @arXiv 2407.17505
   </a>
   <span class="tweet-title">
    Voice of Health: Can Your Speech Reveal Your Secrets?
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    University of Freiburg
   </span>
  </div>
  <div class="primary-text">
   This research paper provides a comprehensive overview of vocal biomarkers, focusing on their potential for detecting various health conditions and estimating measurements from other sensory modalities. It distinguishes itself from previous work by offering a broader definition of vocal biomarkers, encompassing systems that utilize human vocalizations to detect a condition or estimate measurements from other sensors.
  </div>
 </div>
</div>
<div class="tweet" id="tweet45">
 <div class="start-time-icon" title="Play from here">
  20:14
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18243" target="_blank">
    @arXiv 2407.18243
   </a>
   <span class="tweet-title">
    Blind Photographers, Private Pics: A Dataset for Privacy-Aware AI
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    University of Colorado, University of Illinois, University of Washington
   </span>
  </div>
  <div class="primary-text">
   This research introduces BIV-Priv-Seg, the first localization dataset specifically designed for images taken by people with visual impairments that shows private content. This dataset is unique because it includes images lacking target objects, objects with greater variability in size, and a higher prevalence of objects with text.
  </div>
 </div>
</div>
<div class="tweet" id="tweet46">
 <div class="start-time-icon" title="Play from here">
  20:49
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18044" target="_blank">
    @arXiv 2407.18044
   </a>
   <span class="tweet-title">
    Query-Based RAG:  A New Way to Ask Questions and Get Answers Right!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Verily
   </span>
  </div>
  <div class="primary-text">
   This research introduces a novel approach called Query-Based Retrieval Augmented Generation (QB-RAG) that pre-computes a database of potential queries from a content base using LLMs. This differs from previous methods that rely on generating hypothetical documents or candidate answers.
  </div>
 </div>
</div>
<div class="tweet" id="tweet47">
 <div class="start-time-icon" title="Play from here">
  21:17
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17688" target="_blank">
    @arXiv 2407.17688
   </a>
   <span class="tweet-title">
    LLMs: Politically Biased or Just Bad at Stance?
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Carnegie Mellon University, Army Cyber Institute, Singapore University of Technology and Design
   </span>
  </div>
  <div class="primary-text">
   This research investigates the influence of political biases on Large Language Models (LLMs) performance in stance classification tasks. Unlike previous work that focused on political biases in general responses, this study specifically examines how these biases affect the accuracy of stance detection.
  </div>
 </div>
</div>
<div class="tweet" id="tweet48">
 <div class="start-time-icon" title="Play from here">
  21:44
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17654" target="_blank">
    @arXiv 2407.17654
   </a>
   <span class="tweet-title">
    Predicting Vehicle Faults:  A Deep Dive into the Army's Data-Driven Maintenance Revolution
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Duke University, Stanford University
   </span>
  </div>
  <div class="primary-text">
   This research proposes a hybrid generative model that combines DeepAR and STAM to predict vehicle faults, incorporating real-world factors like vehicle age and location. This approach differs from previous work by generating future sensor data using STAM, addressing the need for future covariates in DeepAR.
  </div>
 </div>
</div>
<div class="tweet" id="tweet49">
 <div class="start-time-icon" title="Play from here">
  22:08
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.17835" target="_blank">
    @arXiv 2407.17835
   </a>
   <span class="tweet-title">
    IsUMap:  Unfolding Data Like a Swiss Roll, One Metric at a Time!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Max Planck Society
   </span>
  </div>
  <div class="primary-text">
   This paper introduces IsUMap, a manifold learning technique that combines aspects of UMAP and Isomap with Vietoris-Rips filtrations.  Unlike previous methods, IsUMap addresses limitations in existing methods by accommodating non-uniform data distributions and intricate local geometries.
  </div>
 </div>
</div>
<div class="tweet" id="tweet50">
 <div class="start-time-icon" title="Play from here">
  22:28
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18213" target="_blank">
    @arXiv 2407.18213
   </a>
   <span class="tweet-title">
    Big Models, Big Problems?  Scaling Up Language Models Doesn't Always Mean Scaling Up Robustness!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    FAR AI, Mila, Universit´e de Montr´eal
   </span>
  </div>
  <div class="primary-text">
   This research investigates the relationship between model size and adversarial robustness in language models. Unlike previous work that focused on computer vision, this study examines how scaling affects the ability of language models to withstand attacks specifically designed to exploit their vulnerabilities.
  </div>
 </div>
</div>
<div class="tweet" id="tweet51">
 <div class="start-time-icon" title="Play from here">
  23:01
 </div>
 <div class="tweet-content">
  <div class="tweet-header">
   <a class="arxiv-id" href="https://arxiv.org/abs/2407.18147" target="_blank">
    @arXiv 2407.18147
   </a>
   <span class="tweet-title">
    Multilingual News Bias: A Shared Task to Unmask Propaganda in Five Languages!
   </span>
  </div>
  <div class="institute-line">
   <img alt="Institute Icon" class="institute-icon" src="assets/buttonInstitute.svg"/>
   <span class="institute-text">
    Northwestern University, Birzeit University, NYU...
   </span>
  </div>
  <div class="primary-text">
   This research focuses on developing annotation guidelines for identifying bias and propaganda in news articles related to the Israel War on Gaza, using a multilingual corpus of Facebook posts. This approach differs from previous work by emphasizing the collaborative development of guidelines and the inclusion of multiple languages.
  </div>
 </div>
</div>


        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Listen and learn ^.^</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202407261714_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>    
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/privacy.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading privacy.html:', error));
    </script>
</body>
</html>