
daily_data = {
    "date": "2024-09-05",
    "tweets": [
            {
                "startTime": "00:57",
                "arxivId": "2409.02482",
                "arxivLink": "https://arxiv.org/abs/2409.02482",
                "title": "Fuzzy Objects, Sharp Rendering: Multi-Layer Meshes for Real-Time View Synthesis",
                "institute": "University of T\u00fcbingen, Meta Reality Labs",
                "text": "This paper introduces a novel representation called \"k-SDF\" for real-time view synthesis of fuzzy objects. Unlike previous methods that rely on dense sampling or sorting, k-SDF uses multiple semi-transparent mesh layers, each represented as a signed distance field (SDF), to efficiently render fuzzy geometries.",
                "paper-title": "Volumetric Surfaces: Representing Fuzzy Geometries with Multiple Meshes",
                "image-path": ""
            },

            {
                "startTime": "01:20",
                "arxivId": "2409.02529",
                "arxivLink": "https://arxiv.org/abs/2409.02529",
                "title": "Blurry Images? Not Anymore! New Autoencoder Uses Diffusion to Sharpen Up",
                "institute": "Google, xAI",
                "text": "This paper proposes a novel autoencoder architecture that leverages a diffusion-based loss function for image reconstruction. Unlike previous approaches that rely on adversarial or perceptual losses, this method utilizes the theoretical underpinnings of diffusion models to achieve better reconstruction quality.",
                "paper-title": "Sample what you cant compress",
                "image-path": ""
            },

            {
                "startTime": "01:38",
                "arxivId": "2409.02778",
                "arxivLink": "https://arxiv.org/abs/2409.02778",
                "title": "Stop the Negative Transfer! New Model Makes Multi-Output Gaussian Processes Smarter",
                "institute": "Peking University, University of Iowa",
                "text": "This research introduces a regularized multi-output Gaussian convolution process (MGCP) model that addresses two key challenges in transfer learning: negative transfer and input domain inconsistency. Unlike previous work, this model uses a special convolution process structure to select informative sources and mitigate negative transfer, while simultaneously adapting inconsistent input domains through a marginalization and expansion (DAME) technique.",
                "paper-title": "Regularized Multi-Output Gaussian Convolution Process With Domain Adaptation",
                "image-path": ""
            },

            {
                "startTime": "02:02",
                "arxivId": "2409.02910",
                "arxivLink": "https://arxiv.org/abs/2409.02910",
                "title": "Stop the Presses! Action Recognition Gets a Super-Image Makeover!",
                "institute": "IBM",
                "text": "This research proposes a novel approach for semi-supervised action recognition in videos using a 2D image transformer. Unlike previous methods that rely on 3D video transformers, this approach utilizes superimages, which are constructed by rearranging video frames into a grid format. This allows for efficient processing and reduces computational complexity.",
                "paper-title": "SITAR: Semi-supervised Image Transformer for Action Recognition",
                "image-path": ""
            },

            {
                "startTime": "02:33",
                "arxivId": "2409.02604",
                "arxivLink": "https://arxiv.org/abs/2409.02604",
                "title": "LLMs: The New Sherlock Holmes of Missing Causal Variables?",
                "institute": "CISPA Helmholtz Center for Information Security, Microsoft",
                "text": "This research introduces a novel task of using LLMs to hypothesize missing variables in a partially known causal graph, a step that typically requires human expertise. It differs from previous work that focused on using LLMs for causal reasoning after data collection and analysis.",
                "paper-title": "Hypothesizing Missing Causal Variables with LLMs",
                "image-path": ""
            },

            {
                "startTime": "02:58",
                "arxivId": "2409.02828",
                "arxivLink": "https://arxiv.org/abs/2409.02828",
                "title": "Facial Expressions: A Chain of Thought for AI",
                "institute": "University of Chinese Academy of Sciences, Tsinghua University, Pengcheng Laboratory...",
                "text": "This research proposes a novel method called ExpLLM, which uses large language models to generate a chain of thought (CoT) for facial expression recognition. Unlike previous approaches that rely on facial action units (AUs) alone, ExpLLM analyzes the interactions and relationships between AUs to provide a more comprehensive understanding of facial expressions.",
                "paper-title": "ExpLLM: Towards Chain of Thought for Facial Expression Recognition",
                "image-path": ""
            },

            {
                "startTime": "03:31",
                "arxivId": "2409.02877",
                "arxivLink": "https://arxiv.org/abs/2409.02877",
                "title": "LLMs: Not Just Big Brains, But Brick-Built!",
                "institute": "Tsinghua University",
                "text": "This paper proposes a modular approach to building LLMs, breaking them down into functional units called \"bricks.\" This differs from previous work that focuses on training monolithic models.",
                "paper-title": "Configurable Foundation Models: Building LLMs from a Modular Perspective",
                "image-path": ""
            },

            {
                "startTime": "03:55",
                "arxivId": "2409.02228",
                "arxivLink": "https://arxiv.org/abs/2409.02228",
                "title": "Can AI Forget? It's Tricky, and Sometimes Hilarious!",
                "institute": "MIT",
                "text": "This research investigates the generalization of forgetting in language models, focusing on how the ability to forget a specific task affects the model's performance on other, similar tasks. Unlike previous work that primarily focused on removing specific facts or knowledge, this study examines the broader implications of forgetting on a model's overall capabilities.",
                "paper-title": "Unforgettable Generalization in Language Models",
                "image-path": ""
            },

            {
                "startTime": "04:16",
                "arxivId": "2409.02347",
                "arxivLink": "https://arxiv.org/abs/2409.02347",
                "title": "Weight-Ensembling: A Greedier Approach to Soup Up Your Models!",
                "institute": "Harvard University, Microsoft Research",
                "text": "This research introduces two novel weight-ensembling algorithms, \"greedier\" and \"ranked,\" which explore the relationship between functional diversity and performance dynamics in weight-ensembles. Unlike previous work that primarily focused on a \"greedy\" approach, this study investigates the impact of different diversity measures on the selection mechanism of these algorithms.",
                "paper-title": "Understanding the Role of Functional Diversity in Weight-Ensembling with Ingredient Selection and Multidimensional Scaling",
                "image-path": ""
            },

            {
                "startTime": "04:40",
                "arxivId": "2409.02384",
                "arxivLink": "https://arxiv.org/abs/2409.02384",
                "title": "Speech Tokenizers: A Benchmark for Sorting the Wheat from the Chaff!",
                "institute": "Google",
                "text": "This research introduces STAB, a benchmark for evaluating speech tokenizers, which is more efficient than evaluating them on downstream tasks.",
                "paper-title": "STAB: Speech Tokenizer Assessment Benchmark",
                "image-path": ""
            },

            {
                "startTime": "04:55",
                "arxivId": "2409.02813",
                "arxivLink": "https://arxiv.org/abs/2409.02813",
                "title": "MMMU-Pro: AI's New Vision Test - Can It Really \"See\" and \"Read\"?",
                "institute": "CMU",
                "text": "This research introduces MMMU-Pro, a more robust version of the MMMU benchmark, which aims to evaluate multimodal AI models' true understanding and reasoning capabilities by filtering out questions answerable by text-only models, augmenting candidate options, and introducing a vision-only input setting.",
                "paper-title": "MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark",
                "image-path": ""
            },

            {
                "startTime": "05:22",
                "arxivId": "2409.02730",
                "arxivLink": "https://arxiv.org/abs/2409.02730",
                "title": "Molecular Modeling Gets a Matrix Makeover: New Method Makes Quantum Chemistry Calculations Faster",
                "institute": "Google DeepMind, T U Berlin, Berlin Institute for the Foundation of Learning and Data...",
                "text": "This research proposes a new method for representing molecular structures using matrix multiplication instead of Clebsch-Gordan operations, which are commonly used in equivariant machine learning models. This approach significantly reduces computational complexity, making it more efficient for learning molecular quantum properties.",
                "paper-title": "Complete and Efficient Covariants for 3D Point Configurations with Application to Learning Molecular Quantum Properties",
                "image-path": ""
            },

            {
                "startTime": "05:42",
                "arxivId": "2409.02709",
                "arxivLink": "https://arxiv.org/abs/2409.02709",
                "title": "Brain's Got Probabilities: A New Code for Thinking Under Uncertainty",
                "institute": "University of Rochester, Duke University, NYU...",
                "text": "This paper proposes a unified language for comparing three prominent models of probabilistic computation in the brain: Probabilistic Population Codes (PPCs), Distributed Distributional Codes (DDCs), and Neural Sampling Codes (NSCs). It also reviews key empirical data previously taken as evidence for each model and describes how it may or may not be explainable by alternative proposals.",
                "paper-title": "How does the brain compute with probabilities?",
                "image-path": ""
            },

            {
                "startTime": "06:19",
                "arxivId": "2409.02224",
                "arxivLink": "https://arxiv.org/abs/2409.02224",
                "title": "Feeling the Pressure: New Dataset Tracks Hand Poses and Pressure in VR",
                "institute": "ETH Z\u00fcrich, Microsoft",
                "text": "This research introduces EgoPressure, a dataset that captures hand pressure and pose from an egocentric perspective, unlike previous datasets that focused on static views.",
                "paper-title": "EgoPressure: A Dataset for Hand Pressure and Pose Estimation in Egocentric Vision",
                "image-path": ""
            },

            {
                "startTime": "06:40",
                "arxivId": "2409.02313",
                "arxivLink": "https://arxiv.org/abs/2409.02313",
                "title": "Remembering the Past: How Memory Makes PDEs More Predictable",
                "institute": "CMU",
                "text": "This research explores the impact of memory in neural networks for solving time-dependent partial differential equations (PDEs). Unlike previous work that treats PDEs as Markovian systems, this study investigates the benefits of explicitly incorporating past states into the model.",
                "paper-title": "On the Benefits of Memory for Modeling Time-Dependent PDEs",
                "image-path": ""
            },

            {
                "startTime": "06:59",
                "arxivId": "2409.02795",
                "arxivLink": "https://arxiv.org/abs/2409.02795",
                "title": "LLM Alignment: A Unified View of Preference Learning for Large Language Models",
                "institute": "Peking University, Alibaba",
                "text": "This research offers a unified framework for understanding preference learning in LLMs, breaking down existing strategies into four components: model, data, feedback, and algorithm. This approach helps to connect different methods and identify potential synergies.",
                "paper-title": "Towards a Unified View of Preference Learning for Large Language Models: A Survey",
                "image-path": ""
            },

            {
                "startTime": "07:20",
                "arxivId": "2409.02438",
                "arxivLink": "https://arxiv.org/abs/2409.02438",
                "title": "Cross-Modal Knowledge Distillation: When the Teacher's Side Hustle Matters More",
                "institute": "Peking University, IEEE",
                "text": "This research introduces the Non-target Divergence Hypothesis (NTDH), which posits that the effectiveness of cross-modal knowledge distillation is determined by the distribution divergence of non-target classes between modalities. This differs from previous work that focused on modality alignment or feature fusion strategies.",
                "paper-title": "Non-target Divergence Hypothesis: Toward Understanding Domain Gaps in Cross-Modal Knowledge Distillation",
                "image-path": ""
            },

            {
                "startTime": "07:48",
                "arxivId": "2409.02908",
                "arxivLink": "https://arxiv.org/abs/2409.02908",
                "title": "Masked Diffusion Models: Time-Agnostic, Masked, and Secretly Faster",
                "institute": "Tsinghua University, Nvidia",
                "text": "This research delves into the inner workings of masked diffusion models (MDMs), revealing that both their training and sampling processes are essentially time-agnostic, meaning they don't rely on the continuous time variable that's a hallmark of traditional diffusion models. This finding connects MDMs to masked models, demonstrating their equivalence and highlighting the potential for simpler, more efficient training and sampling methods.",
                "paper-title": "Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling",
                "image-path": ""
            },

            {
                "startTime": "08:21",
                "arxivId": "2409.02817",
                "arxivLink": "https://arxiv.org/abs/2409.02817",
                "title": "Secure ML Accelerators: Obsidian's State-Space Exploration for Faster Inference",
                "institute": "University of Texas at Austin, ARM Holdings",
                "text": "This research introduces Obsidian, a framework that combines analytical and cycle-accurate models to optimize the mapping of machine learning kernels to secure ML accelerators. This approach addresses the limitations of previous methods, which either relied on fast but inaccurate analytical models or slow but precise cycle-accurate models.",
                "paper-title": "Obsidian: Cooperative State-Space Exploration for Performant Inference on Secure ML Accelerators",
                "image-path": ""
            },

            {
                "startTime": "08:48",
                "arxivId": "2409.02851",
                "arxivLink": "https://arxiv.org/abs/2409.02851",
                "title": "Single Image, 3D Human: Video Diffusion Makes It Happen!",
                "institute": "Sun Yat-sen University, CMU",
                "text": "This research proposes a novel method for generating 3D human models from a single image using video diffusion models. Unlike previous methods that often struggle with inconsistent views and artifacts, this approach leverages a video diffusion model to generate temporally consistent views, resulting in more realistic and accurate 3D human reconstructions.",
                "paper-title": "Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video Diffusion Models",
                "image-path": ""
            },

            {
                "startTime": "09:21",
                "arxivId": "2409.02389",
                "arxivLink": "https://arxiv.org/abs/2409.02389",
                "title": "AI Gets Its Bearings: New Dataset Helps Robots Understand 3D Worlds",
                "institute": "Peking University",
                "text": "This research introduces a new dataset called MSQA, which is designed to help AI agents understand and reason about 3D scenes. Unlike previous datasets, MSQA uses a multi-modal approach, combining text, images, and point clouds to provide a more comprehensive understanding of the situation.",
                "paper-title": "Multi-modal Situated Reasoning in 3D Scenes",
                "image-path": ""
            },

            {
                "startTime": "09:46",
                "arxivId": "2409.02451",
                "arxivLink": "https://arxiv.org/abs/2409.02451",
                "title": "Speech Synthesis Gets a Speed Boost: DDSP Makes Articulatory Vocoders Faster and Smaller!",
                "institute": "UC Berkeley",
                "text": "This research integrates Differentiable Digital Signal Processing (DDSP) into articulatory synthesis, a technique that uses physical movements of the vocal tract to generate speech. This approach differs from previous methods by leveraging the efficiency of DDSP to create a more parameter-efficient and faster model.",
                "paper-title": "Fast, High-Quality and Parameter-Efficient Articulatory Synthesis using Differentiable DSP",
                "image-path": ""
            },

            {
                "startTime": "10:09",
                "arxivId": "2409.02581",
                "arxivLink": "https://arxiv.org/abs/2409.02581",
                "title": "Sparse Views, Big Pose: How a Random Cuboid Cracked Monocular Object Pose Estimation",
                "institute": "Chinese Academy of Sciences, University of Birmingham, NVIDIA",
                "text": "This research introduces SGPose, a framework that estimates object pose from sparse views using Gaussian-based methods. Unlike previous work that relies on CAD models or Structure-from-Motion (SfM) pipelines, SGPose starts with a random cuboid initialization and learns geometric-aware depth to guide object reconstruction.",
                "paper-title": "Object Gaussian for Monocular 6D Pose Estimation from Sparse Views",
                "image-path": ""
            },

            {
                "startTime": "10:34",
                "arxivId": "2409.02489",
                "arxivLink": "https://arxiv.org/abs/2409.02489",
                "title": "Brainwaves to the Rescue: New AI Uses Your Thoughts to Extract Speech!",
                "institute": "University of Bremen, National University of Singapore, The Chinese University of Hong Kong",
                "text": "This research uses EEG signals, a measure of brain activity, as the sole reference cue to guide a speaker extraction model. Unlike previous work that relied on pre-recorded speech or visual cues, this approach leverages the brain's response to the attended speech in real-time.",
                "paper-title": "NeuroSpex: Neuro-Guided Speaker Extraction with Cross-Modal Attention",
                "image-path": ""
            },

            {
                "startTime": "10:59",
                "arxivId": "2409.02428",
                "arxivLink": "https://arxiv.org/abs/2409.02428",
                "title": "LLMs: The New Reward Function Whisperers!",
                "institute": "University of Oxford, New Jersey Institute of Technology",
                "text": "This research explores using large language models (LLMs) to design reward functions for multi-objective reinforcement learning tasks. Unlike previous work that focuses on black-box optimization, this paper proposes a white-box approach, separating reward code design and weight assignment into distinct stages.",
                "paper-title": "Large Language Models as Efficient Reward Function Searchers for Custom-Environment Multi-Objective Reinforcement Learning",
                "image-path": ""
            },

            {
                "startTime": "11:24",
                "arxivId": "2409.02239",
                "arxivLink": "https://arxiv.org/abs/2409.02239",
                "title": "Speech Recognition Gets a Time-Traveling Upgrade: New Research Uses Temporal Order to Boost Accuracy!",
                "institute": "National Institute of Information and Communications Technology, Academia Sinica",
                "text": "This research introduces a new method for cross-modal knowledge transfer in automatic speech recognition (ASR) that preserves the temporal order of acoustic and linguistic features. Unlike previous methods that treat these features as unordered sets, this approach explicitly incorporates temporal information during feature alignment, leading to improved ASR performance.",
                "paper-title": "Temporal Order Preserved Optimal Transport-based Cross-modal Knowledge Transfer Learning for ASR",
                "image-path": ""
            },

            {
                "startTime": "11:54",
                "arxivId": "2409.02919",
                "arxivLink": "https://arxiv.org/abs/2409.02919",
                "title": "Tired of blurry, repetitive AI art? HiPrompt's got your high-res fix!",
                "institute": "Hong Kong University of Science and Technology, Nanyang Technological University, Tsinghua University...",
                "text": "This research introduces HiPrompt, a method for generating higher-resolution images without fine-tuning the model. It uses hierarchical prompts, which provide both global and local guidance for the image generation process, addressing the issue of object repetition and structural artifacts that plague other methods.",
                "paper-title": "HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts",
                "image-path": ""
            },

            {
                "startTime": "12:32",
                "arxivId": "2409.02751",
                "arxivLink": "https://arxiv.org/abs/2409.02751",
                "title": "Pre-training vs. Self-training: A Head-to-Head Showdown in NLP!",
                "institute": "Peking University",
                "text": "This research compares pre-training and self-training in semi-supervised learning (SSL) using a consistent foundational setting of language models. It examines all feasible training paradigms combining these two methods, unlike previous studies that focused on specific combinations or used different settings.",
                "paper-title": "A Comparative Study of Pre-training and Self-training",
                "image-path": ""
            },

            {
                "startTime": "12:55",
                "arxivId": "2409.02897",
                "arxivLink": "https://arxiv.org/abs/2409.02897",
                "title": "LLMs Get Citation-Savvy: Fine-Grained Footnotes for Long-Form Answers",
                "institute": "Tsinghua University, Zhipu AI",
                "text": "This research focuses on enabling long-context LLMs to generate responses with sentence-level citations, a feature missing in previous work. The paper introduces a novel pipeline, CoF, to automatically construct a large-scale dataset for training LLMs to generate these fine-grained citations.",
                "paper-title": "LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA",
                "image-path": ""
            },

            {
                "startTime": "13:23",
                "arxivId": "2409.02415",
                "arxivLink": "https://arxiv.org/abs/2409.02415",
                "title": "SD Maps: Giving Autonomous Cars a Sense of Direction (and a Cheaper One!)",
                "institute": "Tsinghua University, Beihang University, University of Wisconsin-Madison",
                "text": "This research focuses on using Standard Definition Maps (SD Maps) as prior information for local map construction in autonomous driving, a departure from the traditional reliance on high-definition maps (HD Maps).",
                "paper-title": "Local map Construction Methods with SD map: A Novel Survey",
                "image-path": ""
            },

            {
                "startTime": "13:44",
                "arxivId": "2409.02392",
                "arxivLink": "https://arxiv.org/abs/2409.02392",
                "title": "Math Agents Get a Preference Upgrade: Direct Preference Learning for Tool-Integrated Reasoning",
                "institute": "University of Illinois, Google",
                "text": "This research introduces a multi-turn direct preference learning framework for training LLMs to solve mathematical problems using external tools. Unlike previous work that focuses on synthetic data generation and supervised fine-tuning, this approach directly learns from human feedback on the quality of reasoning trajectories.",
                "paper-title": "Building Math Agents with Multi-Turn Iterative Preference Learning",
                "image-path": ""
            },

            {
                "startTime": "14:23",
                "arxivId": "2409.02416",
                "arxivLink": "https://arxiv.org/abs/2409.02416",
                "title": "Move Over, Wasserstein! A New Distance Metric That's Translation-Invariant",
                "institute": "George Washington University, UC Los Angeles, Princeton University",
                "text": "This paper introduces a new family of distances called \"relative-translation invariant Wasserstein distances\" (RWp). Unlike the classical Wasserstein distance, RWp is invariant to translations of the probability distributions, making it more robust to distribution shifts.",
                "paper-title": "Relative-Translation Invariant Wasserstein Distance",
                "image-path": ""
            },

            {
                "startTime": "14:47",
                "arxivId": "2409.02580",
                "arxivLink": "https://arxiv.org/abs/2409.02580",
                "title": "Group Decisions: When Everyone's Happy (Except Maybe the Pizza)",
                "institute": "University of Hong Kong, Hong Kong Polytechnic University, CMU",
                "text": "This research proposes a new group recommendation method called AlignGroup, which focuses on both group consensus and individual preferences of group members. Unlike previous methods that primarily rely on aggregating individual preferences or capturing group consensus through multi-view information, AlignGroup utilizes a hypergraph neural network to learn both intra- and inter-group relationships, effectively capturing the group decision-making process.",
                "paper-title": "AlignGroup: Learning and Aligning Group Consensus with Member Preferences for Group Recommendation",
                "image-path": ""
            },

            {
                "startTime": "15:05",
                "arxivId": "2409.02327",
                "arxivLink": "https://arxiv.org/abs/2409.02327",
                "title": "Factor Models Get a Predictive Makeover: Supervised Latent Variables for Better Brain Stimulation",
                "institute": "Stanford University, Duke University, Emory University",
                "text": "This research introduces a novel inference algorithm called generative principal component regression (gPCR) that incorporates predictive information into generative models. Unlike previous methods like supervised variational autoencoders (SVAEs), gPCR ensures that the latent variables are relevant to the outcome of interest, even when the outcome is a low-variance signal.",
                "paper-title": "Generative Principal Component Regression via Variational Inference",
                "image-path": ""
            },

            {
                "startTime": "15:32",
                "arxivId": "2409.02141",
                "arxivLink": "https://arxiv.org/abs/2409.02141",
                "title": "Tool Retrieval: From Descriptions to Usage, LLMs Get Smarter!",
                "institute": "UC Berkeley, POSCO HOLDINGS, LBNL",
                "text": "This research proposes a novel two-stage tool retrieval method that leverages usage-driven tool embeddings instead of relying solely on tool descriptions. This approach addresses the semantic gap between tool descriptions and user queries, leading to more accurate tool retrieval.",
                "paper-title": "Efficient and Scalable Estimation of Tool Representations in Vector Space",
                "image-path": ""
            },

            {
                "startTime": "15:55",
                "arxivId": "2409.02343",
                "arxivLink": "https://arxiv.org/abs/2409.02343",
                "title": "Embeddings Get a Nudge: Fine-Tuning Without the Fuss!",
                "institute": "UC Berkeley",
                "text": "This paper introduces NUDGE, a non-parametric approach to fine-tuning embeddings for retrieval tasks. Unlike previous methods that fine-tune the entire model or train adaptors, NUDGE directly modifies the embeddings of data records, making it more efficient and accurate.",
                "paper-title": "NUDGE: Lightweight Non-Parametric Fine-Tuning of Embeddings for Retrieval",
                "image-path": ""
            },

            {
                "startTime": "16:23",
                "arxivId": "2409.02368",
                "arxivLink": "https://arxiv.org/abs/2409.02368",
                "title": "Salient Object Detection: It's Not Just One Mask, It's a Party!",
                "institute": "University at Buffalo, Microsoft",
                "text": "This research introduces \"pluralistic salient object detection\" (PSOD), which acknowledges the inherent ambiguity in defining salient objects in real-world images. Unlike traditional methods that produce a single segmentation mask, PSOD generates multiple plausible masks, reflecting the diverse interpretations possible.",
                "paper-title": "Pluralistic Salient Object Detection",
                "image-path": ""
            },

            {
                "startTime": "17:01",
                "arxivId": "2409.02549",
                "arxivLink": "https://arxiv.org/abs/2409.02549",
                "title": "Perimeter Hunting: A Game of Congestion!",
                "institute": "Ben-Gurion University Of The Negev",
                "text": "This research proposes a sequential decision-making model for perimeter identification, framing it as a game between an agent and the environment. Unlike previous methods that rely on static or dynamic analysis, this approach uses real-time congestion heatmaps and a reinforcement learning algorithm to find optimal perimeters.",
                "paper-title": "A Sequential Decision-Making Model for Perimeter Identification",
                "image-path": ""
            },

            {
                "startTime": "17:23",
                "arxivId": "2409.02148",
                "arxivLink": "https://arxiv.org/abs/2409.02148",
                "title": "AI to the Rescue: Foundation Models Power Up the Grid!",
                "institute": "IBM Research Europe, IBM Research",
                "text": "This research proposes using Foundation Models (FMs) trained on power flow dynamics to accelerate grid analysis and optimization, a novel approach compared to previous AI applications in the field.",
                "paper-title": "Optimal Power Grid Operations with Foundation Models",
                "image-path": ""
            },

            {
                "startTime": "17:45",
                "arxivId": "2409.02135",
                "arxivLink": "https://arxiv.org/abs/2409.02135",
                "title": "Quantum Annealing Gets a Gradient Boost: New Solver Finds Optimal Solutions Faster!",
                "institute": "Fujitsu Limited, University of Tokyo",
                "text": "This research proposes a new approach to combinatorial optimization that integrates gradient-based updates through continuous relaxation, combined with Quasi-Quantum Annealing (QQA). This differs from previous work by leveraging gradient information and introducing an extended Boltzmann distribution with a communication term between parallel runs.",
                "paper-title": "Optimization by Parallel Quasi-Quantum Annealing with Gradient-Based Sampling",
                "image-path": ""
            },

            {
                "startTime": "18:16",
                "arxivId": "2409.02644",
                "arxivLink": "https://arxiv.org/abs/2409.02644",
                "title": "Conformal Prediction: A New Way to Predict Biological Systems' Behavior, Without the Bayesian Blues!",
                "institute": "Spanish National Research Council, Harvard University",
                "text": "This research proposes two novel algorithms for conformal prediction in dynamic biological systems, offering an alternative to traditional Bayesian methods. The algorithms are designed to optimize statistical efficiency, especially when dealing with limited data, and provide non-asymptotic guarantees for prediction regions.",
                "paper-title": "Conformal Prediction in Dynamic Biological Systems",
                "image-path": ""
            },

            {
                "startTime": "18:46",
                "arxivId": "2409.02747",
                "arxivLink": "https://arxiv.org/abs/2409.02747",
                "title": "Learning from the Past: A New Trick for Making Robots Smarter in Complex Worlds",
                "institute": "Pompeu Fabra University, Sapienza University of Rome, University of Oxford...",
                "text": "This research introduces a new way to learn from past experiences in complex environments where the current situation depends on the entire history of events. Previous methods relied on a metric called L-infinity distinguishability, which can be inefficient in certain scenarios. This paper proposes a new metric based on formal languages, which can be more efficient in these cases.",
                "paper-title": "Tractable Offline Learning of Regular Decision Processes",
                "image-path": ""
            },

            {
                "startTime": "19:08",
                "arxivId": "2409.02346",
                "arxivLink": "https://arxiv.org/abs/2409.02346",
                "title": "Federated Learning Gets a LoRA-ly Boost: Alternating Minimization for Robust Fine-Tuning",
                "institute": "University of Toronto",
                "text": "This research introduces RoLoRA, a federated fine-tuning framework that uses alternating minimization for LoRA. Unlike previous methods that directly aggregate LoRA modules, RoLoRA alternates between updating the down-projection and up-projection matrices, leading to greater robustness against decreasing fine-tuning parameters and increasing data heterogeneity.",
                "paper-title": "Robust Federated Finetuning of Foundation Models via Alternating Minimization of LoRA",
                "image-path": ""
            },

            {
                "startTime": "19:32",
                "arxivId": "2409.02596",
                "arxivLink": "https://arxiv.org/abs/2409.02596",
                "title": "Attention, Please! Linear Complexity Attention Substitutes for Speech SSL Models",
                "institute": "Avignon Universit\u00e9, Samsung AI Center Cambridge, Univervist\u00b4e Grenoble Alpes...",
                "text": "This research investigates the performance of linear complexity attention substitutes in a self-supervised learning (SSL) setting for speech tasks, a novel application compared to previous work that focused on supervised tasks.",
                "paper-title": "An Analysis of Linear Complexity Attention Substitutes with BEST-RQ",
                "image-path": ""
            },

            {
                "startTime": "20:03",
                "arxivId": "2409.02864",
                "arxivLink": "https://arxiv.org/abs/2409.02864",
                "title": "Bioinformatics Bot: A Digital Lab Assistant That's Actually Helpful",
                "institute": "University of Michigan",
                "text": "This research introduces BRAD, a digital assistant that integrates a suite of tools for bioinformatics tasks, including code execution, online search, and question-and-answering. Unlike previous work, BRAD utilizes a multi-agent system of LLMs to handle complex workflows and distribute tasks across agents.",
                "paper-title": "Bioinformatics Retrieval Augmentation Data (BRAD) Digital Assistant",
                "image-path": ""
            },

            {
                "startTime": "20:21",
                "arxivId": "2409.02430",
                "arxivLink": "https://arxiv.org/abs/2409.02430",
                "title": "Deep Receivers on a Diet: How Poisoning Attacks Can Make Them Choke on Data",
                "institute": "Nanyang Technological University",
                "text": "This research focuses on poisoning attacks against online deep receivers, which are used in wireless communication systems to adapt to dynamic channels. Unlike previous work that primarily focused on evasion attacks, this paper explores how malicious users can corrupt the training data used by these receivers, leading to performance degradation.",
                "paper-title": "Transfer-based Adversarial Poisoning Attacks for Online (MIMO-)Deep Receviers",
                "image-path": ""
            },

            {
                "startTime": "20:53",
                "arxivId": "2409.02449",
                "arxivLink": "https://arxiv.org/abs/2409.02449",
                "title": "Whisper's Big Mistake: How Normalization Is Messing Up Multilingual Speech Recognition",
                "institute": "Digital University Kerala",
                "text": "This research focuses on the unintended consequences of text normalization routines used in evaluating multilingual Automatic Speech Recognition (ASR) models, particularly for Indic languages. Unlike previous work, it highlights how these routines, while aiming to standardize outputs, can actually distort the linguistic structure of languages like Hindi, Tamil, and Malayalam, leading to inaccurate performance metrics.",
                "paper-title": "What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations",
                "image-path": ""
            },

            {
                "startTime": "21:19",
                "arxivId": "2409.02231",
                "arxivLink": "https://arxiv.org/abs/2409.02231",
                "title": "Llama-ing for Drugs: AI Learns to Design Molecules with a Smile",
                "institute": "UC Berkeley",
                "text": "This research explores using a pre-trained Large Language Model (LLM) called Llama to generate drug-like molecules. Unlike previous work that focused on training specialized chemical language models (CLMs) on SMILES strings, this study demonstrates that fine-tuning an LLM on chemical data can achieve comparable or even better performance.",
                "paper-title": "SmileyLlama: Modifying Large Language Models for Directed Chemical Space Exploration",
                "image-path": ""
            },

            {
                "startTime": "21:41",
                "arxivId": "2409.02866",
                "arxivLink": "https://arxiv.org/abs/2409.02866",
                "title": "Crack Detection: When Transformers Meet CNNs, Infrastructure Wins!",
                "institute": "University College London",
                "text": "This research introduces Hybrid-Segmentor, a crack segmentation model that combines CNN and Transformer architectures to improve accuracy and generalization capabilities. Unlike previous models that rely solely on one architecture, this hybrid approach leverages the strengths of both CNNs and Transformers for more robust crack detection.",
                "paper-title": "Hybrid-Segmentor: A Hybrid Approach to Automated Fine-Grained Crack Segmentation in Civil Infrastructure",
                "image-path": ""
            },

            {
                "startTime": "22:02",
                "arxivId": "2409.02150",
                "arxivLink": "https://arxiv.org/abs/2409.02150",
                "title": "Asteroid Apocalypse? Not So Fast! AI to the Rescue!",
                "institute": "National Central University, California Institute of Technology, NASA",
                "text": "This research compares the performance of various machine learning and deep learning models for classifying hazardous asteroids, using two different datasets. It highlights the effectiveness of Random Forest and CatBoost algorithms in achieving high accuracy.",
                "paper-title": "Hazardous Asteroids Classification",
                "image-path": ""
            },

            {
                "startTime": "22:24",
                "arxivId": "2409.02154",
                "arxivLink": "https://arxiv.org/abs/2409.02154",
                "title": "N-body Simulations Get a Speed Boost: Machine Learning Meets Physics!",
                "institute": "Institut d\u2019Astrophysique de Paris",
                "text": "This research introduces a new framework called COmoving Computer Acceleration (COCA) that combines machine learning with N-body simulations. Unlike previous approaches that directly emulate simulation outputs, COCA emulates a frame of reference within which the simulation is run, allowing for correction of emulation errors.",
                "paper-title": "COmoving Computer Acceleration (COCA): $N$-body simulations in an emulated frame of reference",
                "image-path": ""
            },

            {
                "startTime": "22:48",
                "arxivId": "2409.02691",
                "arxivLink": "https://arxiv.org/abs/2409.02691",
                "title": "LLMs: The New BFFs of Visual Analytics?",
                "institute": "City University of London",
                "text": "This research explores the integration of large language models (LLMs) into visual analytics (VA) systems, focusing on the opportunities and challenges associated with leveraging these powerful language models to enhance mixed-initiative VA systems. It goes beyond previous work by examining how LLMs can be used to support the entire VA pipeline, including data management, language interaction, visualisation generation, and language generation.",
                "paper-title": "LLM-Assisted Visual Analytics: Opportunities and Challenges",
                "image-path": ""
            },

            {
                "startTime": "23:09",
                "arxivId": "2409.02136",
                "arxivLink": "https://arxiv.org/abs/2409.02136",
                "title": "LLMs vs. CMLs: Who Wins the COVID-19 Mortality Prediction Game?",
                "institute": "Shahid Beheshti University of Medical Sciences, Isfahan University of Medical Sciences, Ontario Tech University...",
                "text": "This research compares the performance of large language models (LLMs) and classical machine learning (CML) models in predicting COVID-19 mortality using a high-dimensional tabular dataset. Unlike previous studies that focused on low-dimensional datasets and limited sample sizes, this study utilizes a large dataset with 9,134 patients and 81 features, allowing for a more robust comparison.",
                "paper-title": "Large Language Models versus Classical Machine Learning: Performance in COVID-19 Mortality Prediction Using High-Dimensional Tabular Data",
                "image-path": ""
            },

            {
                "startTime": "23:33",
                "arxivId": "2409.02391",
                "arxivLink": "https://arxiv.org/abs/2409.02391",
                "title": "AI Translators: Faster, Better, and More Money?",
                "institute": "Yale University",
                "text": "This paper goes beyond simply showing that LLMs can improve translation productivity. It investigates how the size of the LLM, measured by training compute, directly impacts translator performance and earnings.",
                "paper-title": "Scaling Laws for Economic Productivity: Experimental Evidence in LLM-Assisted Translation",
                "image-path": ""
            }
    ],
    "stats": {
        "num_pick": 54,
        "num_total": 251,
    },
    "audio": "https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202409052044_audio.mp3"
}