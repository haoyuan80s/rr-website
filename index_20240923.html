
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY</div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">Fresh Picks: 
                    <span class="highlightNumber" style="font-size: 28px;">58</span> out of <span
                    class="highlightNumber">262</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-09-23"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>


        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">01:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13430" target="_blank">@arXiv 2409.13430</a>
                    <span class="tweet-title">Occupancy Prediction Gets a Time Machine: New Research Uses Temporal Fusion to See Through Walls!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces CVT-Occ, a novel approach that leverages temporal fusion through the geometric correspondence of voxels over time to improve the accuracy of 3D occupancy predictions. Unlike previous methods that primarily rely on warping and concatenating features, CVT-Occ explicitly utilizes parallax cues from historical observations to refine the depth of 3D voxels.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">01:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13426" target="_blank">@arXiv 2409.13426</a>
                    <span class="tweet-title">Smart Glasses, Dumb Body? New AI Makes Your Virtual Self Move!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tübingen, Max Planck Institute for Informatics, Stanford University...</span>
                </div>
                <div class="primary-text">
                    This research introduces HMD2, a system that generates full-body motion from a single head-mounted device, unlike previous methods that rely on multiple sensors or offline processing. HMD2 utilizes a multi-modal conditional motion diffusion model, incorporating a time-series backbone for temporal coherence and autoregressive in-painting for online inference with minimal latency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">02:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13038" target="_blank">@arXiv 2409.13038</a>
                    <span class="tweet-title">Radiology Reports:  Ontology-Normalized Evaluation for AI-Generated Head CTs</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, NYU Langone Health, Columbia University...</span>
                </div>
                <div class="primary-text">
                    This research introduces HeadCT-ONE, a metric for evaluating AI-generated head CT reports that uses ontology normalization to address the variability in radiological language. This approach differs from previous work by incorporating a standardized framework for medical terminology, enabling fine-grained categorization of concepts and allowing for customizable weighting of different entities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">02:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13222" target="_blank">@arXiv 2409.13222</a>
                    <span class="tweet-title">Watermarking 3D Scenes:  A Gaussian Splatting Surprise!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Korea University, Google Research, University of Alberta</span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel watermarking method specifically designed for 3D Gaussian splatting, a popular technique for representing 3D scenes. Unlike previous methods that embed watermarks after rendering, this approach integrates the watermark directly into the 3D model during the training process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">02:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13175" target="_blank">@arXiv 2409.13175</a>
                    <span class="tweet-title">Cache Allocation:  A Reinforcement Learning Framework for Recommender Systems That's Actually Smart!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Kuaishou Technology, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research tackles the challenge of allocating real-time and cached recommendations in large-scale recommender systems by considering the value-strategy dependency and the streaming allocation problem. Unlike previous work, it proposes a two-stage framework that uses reinforcement learning to estimate the value of different cache choices and then allocates them in a streaming manner.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">03:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13523" target="_blank">@arXiv 2409.13523</a>
                    <span class="tweet-title">EMMeTT:  Training  a  Speech-to-Text  Model  That  Doesn't  Forget  How  to  Read!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nvidia</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel training framework called EMMeTT for multimodal machine translation, specifically focusing on joint training of speech and text translation. The key innovation lies in its data sampling strategies and 2D bucketing scheme, which aim to improve training efficiency and prevent catastrophic forgetting of text translation capabilities when extending to speech.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">03:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13065" target="_blank">@arXiv 2409.13065</a>
                    <span class="tweet-title">Multi-Agent Vulcan:  When Robots Team Up to Find Treasure!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research extends previous work on single-agent adaptive sampling to a multi-agent setting, addressing challenges like redundant observations and limited communication.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">04:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13652" target="_blank">@arXiv 2409.13652</a>
                    <span class="tweet-title">Pruning Transformers:  Outlier-Aware Compression for a Speedier Brain!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research introduces OATS, a method for compressing large transformer models without retraining. Unlike previous methods, OATS leverages the second moment of input embeddings to decompose model weights into sparse and low-rank matrices.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">04:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13689" target="_blank">@arXiv 2409.13689</a>
                    <span class="tweet-title">Audio-Visual Alignment:  When Your Ears and Eyes Finally Agree!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tampere University, University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research introduces V-AURA, an autoregressive model that generates audio aligned with video, unlike previous diffusion and RFM-based models. V-AURA uses a high-framerate visual feature extractor and a cross-modal audio-visual feature fusion strategy to achieve precise temporal alignment.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">04:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13551" target="_blank">@arXiv 2409.13551</a>
                    <span class="tweet-title">Data Wrangling:  From Notebook Chaos to Code Nirvana!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Chinese University of Hong Kong, Sun Yat-sen University, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research focuses on generating data wrangling code within the context of computational notebooks, incorporating code, textual, and data context. Unlike previous work that primarily relies on textual or input-output examples, this study emphasizes the importance of integrating all three modalities for more accurate code generation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">05:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13213" target="_blank">@arXiv 2409.13213</a>
                    <span class="tweet-title">Malware Classification:  A Few-Shot Learning Recipe for a Safer Internet</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Vanderbilt University, Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel data augmentation technique for malware classification that leverages domain knowledge to generate synthetic malware samples. Unlike previous work that relies on image-based augmentation, this approach focuses on manipulating static features of malware binaries in a more semantically meaningful way.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">05:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13507" target="_blank">@arXiv 2409.13507</a>
                    <span class="tweet-title">Singing Sketches: How Your Voice Can Paint Soundscapes</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research explores "non-phonorealistic" sound representation, focusing on how people use vocal imitation to communicate the essence of sounds rather than replicating them perfectly. Unlike previous work on realistic sound synthesis, this study investigates the cognitive processes behind auditory abstraction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">06:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12964" target="_blank">@arXiv 2409.12964</a>
                    <span class="tweet-title">OpenRANet:  Deep Learning Gets a Power Boost with Convex Optimization!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">City University of Hong Kong, Nanyang Technological University, Nanjing University of Aeronautics and Astronautics...</span>
                </div>
                <div class="primary-text">
                    This research proposes OpenRANet, a deep learning model that integrates convex optimization layers to address the nonconvex problem of joint subcarrier and power allocation in Open RAN systems. This approach differs from previous work by combining machine learning techniques with iterative optimization algorithms, leading to improved constraint adherence, solution accuracy, and computational efficiency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">06:35</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12993" target="_blank">@arXiv 2409.12993</a>
                    <span class="tweet-title">Verilog Code Generation:  Fixing Tiny Mistakes for Big Gains!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nvidia</span>
                </div>
                <div class="primary-text">
                    This research focuses on improving the quality of synthetic data used to train LLMs for Verilog code generation. It introduces two novel techniques: generating correct-by-construction data for non-textual representations and creating a targeted code repair dataset to address common "minor" errors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">06:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13669" target="_blank">@arXiv 2409.13669</a>
                    <span class="tweet-title">Brain Waves: The Secret to Super-Smart Memories?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, Western University, Salk Institute for Biological Studies...</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new "spacetime" perspective on neural computation, arguing that structured spatiotemporal dynamics, like traveling waves, are not just noise but a fundamental mechanism for encoding symmetries and improving generalization and working memory. This differs from previous work that focused primarily on single-neuron selectivity and static representations.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">07:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13156" target="_blank">@arXiv 2409.13156</a>
                    <span class="tweet-title">Reward Models:  Stop Hacking, Start Learning!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces a causal framework for reward model training, which aims to disentangle contextual preference signals from irrelevant artifacts. Unlike previous work that focuses on specific artifacts like length, this approach tackles a broader range of potential biases by augmenting the training data with counterfactual examples.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">07:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13082" target="_blank">@arXiv 2409.13082</a>
                    <span class="tweet-title">Rust's Proof-Generating Pal: AutoVerus Makes Verification a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Illinois, Columbia University, UC Irvine...</span>
                </div>
                <div class="primary-text">
                    This research focuses on automatically generating correctness proofs for Rust code using a large language model (LLM) specifically for the Verus verification tool. Unlike previous work that focuses on proof-oriented languages like LEAN or F*, this paper tackles the unique challenges of Verus, which directly works on Rust code and has a limited amount of training data available.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">08:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13609" target="_blank">@arXiv 2409.13609</a>
                    <span class="tweet-title">Tuning Up Vision: A New Trick for Understanding Images with Words</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, National University of Defense Technology, Hefei University of Technology...</span>
                </div>
                <div class="primary-text">
                    This paper proposes a new framework called MaPPER for referring expression comprehension (REC). Unlike previous methods that fine-tune the entire model, MaPPER uses a parameter-efficient approach, focusing on updating only a small subset of parameters. This approach is more efficient and less computationally expensive.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">08:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13464" target="_blank">@arXiv 2409.13464</a>
                    <span class="tweet-title">Compressed Images? No Problem! New AI Makes Salient Object Detection Robust</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University</span>
                </div>
                <div class="primary-text">
                    This research focuses on salient object detection (SOD) for compressed images, a less explored area. It proposes a hybrid prior learning strategy to improve the robustness of SOD models by mimicking feature learning from clean images.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">08:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13241" target="_blank">@arXiv 2409.13241</a>
                    <span class="tweet-title">Deep Ritz Method:  Cracking the Code of Strain Localization</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cuenca, California Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This research explores the Deep Ritz Method (DRM) for modeling strain localization in solids as a sharp discontinuity in the displacement field. Unlike previous approaches that rely on regularizing gradients or introducing new fields, this method directly incorporates discontinuities into the kinematics using a customized activation function within an Artificial Neural Network (ANN).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">09:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13347" target="_blank">@arXiv 2409.13347</a>
                    <span class="tweet-title">Virtual Whiteboard, Real Hands: New Tech Tracks Your Fingers for Remote Collaboration!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces a real-time method for tracking 3D hand poses from capacitive touchscreen images, unlike previous methods that either require bulky cameras or only track static hand poses.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">09:41</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13171" target="_blank">@arXiv 2409.13171</a>
                    <span class="tweet-title">Low-Res to High-Res:  AI Makes 3D Printing See Clearly</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Carnegie Mellon University, Sandia National Laboratories</span>
                </div>
                <div class="primary-text">
                    This research uses a generative diffusion model to upscale low-resolution images of a 3D printing process to high-resolution, enabling more detailed defect detection. This approach differs from previous work by leveraging a latent space for efficiency and incorporating a pre-trained segmentation model for 3D reconstruction.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">10:03</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13346" target="_blank">@arXiv 2409.13346</a>
                    <span class="tweet-title">Imagine Yourself:  Personalized Images Without the Tuning Blues!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta</span>
                </div>
                <div class="primary-text">
                    This research proposes a tuning-free approach to personalized image generation, unlike previous methods that require individual model adjustments for each user.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">10:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13582" target="_blank">@arXiv 2409.13582</a>
                    <span class="tweet-title">Speech Dysfluency Detection:  Tokenizing the Stutters and Slips</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel token-based approach to speech dysfluency detection, contrasting with the prevalent time-based methods. Instead of focusing on the temporal location of dysfluencies, this method tokenizes dysfluencies and models the detection problem as an automatic speech recognition (ASR) task.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">10:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13074" target="_blank">@arXiv 2409.13074</a>
                    <span class="tweet-title">Diffusion Guidance: Not What You Think, But Still Kinda Cool</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Duke University, MIT, Harvard University...</span>
                </div>
                <div class="primary-text">
                    This paper rigorously proves that diffusion guidance, a popular technique for controlling diffusion models, doesn't actually sample from the intended distribution. It provides a fine-grained analysis of the dynamics of guidance in two simple settings: mixtures of compactly supported distributions and mixtures of Gaussians.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">11:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13167" target="_blank">@arXiv 2409.13167</a>
                    <span class="tweet-title">E-Noses Get a Multi-Source Makeover:  Unsupervised Learning for Drift-Proof Gas Detection</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nanyang Technological University, Tongji University</span>
                </div>
                <div class="primary-text">
                    This research introduces an unsupervised attention-based multi-source domain adaptation framework (AMDS-PFFA) for gas identification with drift compensation in electronic nose systems. Unlike previous work that focuses on domain-invariant features, AMDS-PFFA leverages both shared and private features from multiple source domains to improve accuracy and mitigate drift in the target domain.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">11:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13138" target="_blank">@arXiv 2409.13138</a>
                    <span class="tweet-title">HLS Design Space Exploration:  A Tale of Two Designs!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Nvidia, UC Los Angeles</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to design space exploration (DSE) in high-level synthesis (HLS) that leverages comparative learning. Unlike traditional methods that focus on predicting performance for individual designs, this approach learns to compare designs directly, focusing on the most informative differences between them.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">12:11</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13153" target="_blank">@arXiv 2409.13153</a>
                    <span class="tweet-title">Neuro-Symbolic AI:  It's Not Just About Brains, It's About Brawn Too!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Georgia Institute of Technology, University of California  Berkeley, IBM Research</span>
                </div>
                <div class="primary-text">
                    This research delves into the computational demands of neuro-symbolic AI, going beyond just algorithm performance and exploring the hardware implications. It systematically categorizes neuro-symbolic AI algorithms and then analyzes their performance on various platforms, including CPUs, GPUs, and edge SoCs.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">12:33</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13244" target="_blank">@arXiv 2409.13244</a>
                    <span class="tweet-title">Robots with Foresight: Navigating Crowds with Precognition!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Hong Kong University of Science and Technology</span>
                </div>
                <div class="primary-text">
                    This research introduces a new framework for social navigation that incorporates trajectory prediction, allowing robots to anticipate human movements and avoid collisions more effectively. Unlike previous methods that rely on current human positions, this approach explicitly predicts future trajectories, enabling proactive collision avoidance and social compliance.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">12:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13221" target="_blank">@arXiv 2409.13221</a>
                    <span class="tweet-title">RLHFuse:  Fusing LLMs for Faster Training, No More Bubbles!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Peking University, StepFun</span>
                </div>
                <div class="primary-text">
                    This research proposes a new approach to RLHF training by breaking down tasks into smaller subtasks, enabling inter- and intra-stage fusion. This differs from previous work that focused on optimizing individual tasks without considering their internal structure.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">13:20</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13321" target="_blank">@arXiv 2409.13321</a>
                    <span class="tweet-title">Tiny AI, Big Results: New Model Makes Chest X-Ray Reports a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University College London, University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research introduces a new training method called Re3 Training, which simulates the cognitive development of radiologists. It also proposes a data synthesis method called RADEX, which generates a high-quality and diverse training corpus for chest X-ray report automation. This approach differs from previous work by focusing on training a smaller, more efficient model that can be used in resource-constrained environments.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">13:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13317" target="_blank">@arXiv 2409.13317</a>
                    <span class="tweet-title">Japanese LLMs Get a Medical Checkup: JMedBench Benchmark Unveiled!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo, National Institute of Informatics</span>
                </div>
                <div class="primary-text">
                    This research introduces JMedBench, a new benchmark specifically designed for evaluating Japanese biomedical large language models (LLMs). Unlike previous benchmarks that primarily focus on English or general domains, JMedBench includes a diverse set of Japanese biomedical datasets across five tasks, including multi-choice question-answering, named entity recognition, machine translation, document classification, and semantic text similarity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">14:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13166" target="_blank">@arXiv 2409.13166</a>
                    <span class="tweet-title">Modular Satellites:  Building Brains and Bodies for Space!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University, Beijing Institute of Technology</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel gradient-based approach to simultaneously optimize both the morphology and control of modular satellites, unlike previous work that relied on evolutionary algorithms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">14:30</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13262" target="_blank">@arXiv 2409.13262</a>
                    <span class="tweet-title">Pinyin Power: How Chinese Speech Recognition Got a Pronunciation Boost</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Huawei</span>
                </div>
                <div class="primary-text">
                    This research introduces Pinyin-enhanced Generative Error Correction (PY-GEC), which uses Pinyin, the phonetic representation of Mandarin Chinese, as supplementary information to improve Chinese Automatic Speech Recognition (ASR) error correction. This approach differs from previous work by exclusively utilizing the one-best hypothesis instead of the N-best hypotheses, employing pseudo ASR errors for model training rather than real ASR errors, and incorporating multitask training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">14:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12979" target="_blank">@arXiv 2409.12979</a>
                    <span class="tweet-title">Goodbye Shots, Hello Guidelines: A New Way to Prompt LLMs</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Fudan University, Microsoft Azure AI, Monash University</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel multi-agent framework called FGT to automatically learn task-specific guidelines from data, instead of relying on human-crafted examples or optimization trajectories.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">15:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13532" target="_blank">@arXiv 2409.13532</a>
                    <span class="tweet-title">MRI Magic:  Synthesizing Brain Scans with a Physics-Informed Twist!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Technical University of Munich, Munich Center for Machine Learning, Stanford University</span>
                </div>
                <div class="primary-text">
                    This research introduces a physics-informed generative model for multimodal brain MRI synthesis. Unlike previous models that are limited to fixed sets of modalities, this approach can generate a variable number of modalities, even those not present in the original dataset.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">15:32</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13626" target="_blank">@arXiv 2409.13626</a>
                    <span class="tweet-title">Brain Tumor Segmentation Gets a GSConv-ECA Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">George Washington University, University of California  Berkeley</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to brain tumor segmentation by incorporating the GSConv module and ECA attention mechanism into the U-Net architecture. This combination aims to improve feature extraction and utilization, leading to more accurate segmentation results.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">15:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13095" target="_blank">@arXiv 2409.13095</a>
                    <span class="tweet-title">Kids Say the Darndest Things: AI Learns to Understand Them on the Fly!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Southern California, Columbia University, Sara Technology Inc.</span>
                </div>
                <div class="primary-text">
                    This research focuses on adapting pre-trained speech recognition models to individual children's speech at test time, rather than requiring extensive training data for each child. This approach uses unsupervised test-time adaptation (TTA) methods, which allows the model to learn from the child's speech during the recognition process without needing labeled data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">16:14</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.12965" target="_blank">@arXiv 2409.12965</a>
                    <span class="tweet-title">Training AI with Light:  A New Way to Build Super-Smart Brains!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">École Normale Supérieure</span>
                </div>
                <div class="primary-text">
                    This research explores training large-scale neural networks using direct feedback alignment (DFA) implemented on an optical processing unit (OPU). Unlike traditional backpropagation, DFA uses a random projection of the error signal to update the network's parameters, enabling parallel processing. This approach is particularly well-suited for optical hardware, which excels at performing large-scale random matrix multiplications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">16:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13598" target="_blank">@arXiv 2409.13598</a>
                    <span class="tweet-title">PrithviWxC: The AI That's Got the Weather Forecast Down to a Science (and a Few Billion Parameters)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM, National Aeronautics and Space Administration</span>
                </div>
                <div class="primary-text">
                    This research introduces PrithviWxC, a foundation model for weather and climate applications, trained on a vast dataset of atmospheric variables. Unlike previous work that focused on task-specific models, PrithviWxC is designed to be adaptable to a range of downstream tasks, including forecasting, downscaling, and parameterization.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">16:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13312" target="_blank">@arXiv 2409.13312</a>
                    <span class="tweet-title">Text Classification Gets a Graph Makeover: Prototypes Meet Attention!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Drexel University, Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces GAProtoNet, a novel text classification model that uses multi-head graph attention to learn relationships between input text and prototype vectors. This approach differs from previous work by actively training prototype vectors through edge construction within a graph, rather than relying solely on heuristic distance metrics.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">17:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13053" target="_blank">@arXiv 2409.13053</a>
                    <span class="tweet-title">F1 Score for Time Series Anomaly Detection:  A Fairer Fight!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research proposes a new point adjustment protocol called "Balanced Point Adjustment" (BA) for evaluating time-series anomaly detectors. Unlike previous methods, BA penalizes false positives and balances adjustments made for true positives, leading to a more accurate and fair evaluation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">17:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13210" target="_blank">@arXiv 2409.13210</a>
                    <span class="tweet-title">Recommender Systems:  Are You Really in Control?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU, University of Texas at Austin</span>
                </div>
                <div class="primary-text">
                    This research proposes a causal framework for auditing recommender systems, focusing on user agency. It introduces two new classes of metrics: reachability and stability, which measure a user's ability to influence their own and other users' recommendations. This differs from previous work by considering the multi-step dynamics of the recommendation process.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">18:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13155" target="_blank">@arXiv 2409.13155</a>
                    <span class="tweet-title">Local Adam:  Adaptive Optimization Gets a Speed Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley, MIT</span>
                </div>
                <div class="primary-text">
                    This paper proves that distributed adaptive optimization algorithms with local updates can outperform their minibatch counterparts in terms of communication efficiency, particularly in convex and weakly convex settings. This is a novel finding, as previous research has not fully explored the theoretical benefits of local updates within adaptive methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">18:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13035" target="_blank">@arXiv 2409.13035</a>
                    <span class="tweet-title">Prompt Compression:  Reinforcement Learning Makes LLMs Think Lean and Mean!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel task-aware prompt compression method using reinforcement learning (RL). Unlike previous methods that rely on task-agnostic approaches or expensive fine-tuning, TACO-RL leverages task-specific reward signals to guide the compression process, resulting in a more efficient and effective model.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">19:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13474" target="_blank">@arXiv 2409.13474</a>
                    <span class="tweet-title">LLMs Forget Facts, But Not Their Sense of Humor: New Unlearning Method Keeps AI Chatty and Consistent</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Massachusetts, Microsoft</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel unlearning method called Alternate Preference Optimization (AltPO) that combines negative feedback with in-domain positive feedback to improve the effectiveness and stability of unlearning in large language models (LLMs). Unlike previous methods that rely solely on negative feedback, AltPO generates multiple plausible alternative answers to forgotten information, guiding the model towards more coherent and consistent responses.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">19:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13133" target="_blank">@arXiv 2409.13133</a>
                    <span class="tweet-title">Privacy-Preserving Machine Learning: Sharing Secrets Without Sharing Data</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Florida International University, University of Michigan</span>
                </div>
                <div class="primary-text">
                    This research introduces a new privacy mechanism called CorBin-FL, which uses correlated binary stochastic quantization to achieve differential privacy in federated learning. This approach differs from previous work by utilizing shared common randomness among clients to coordinate their updates without compromising individual privacy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet47">
            <div class="start-time-icon" title="Play from here">19:59</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13682" target="_blank">@arXiv 2409.13682</a>
                    <span class="tweet-title">Robots with Memories:  How AI is Giving Robots Long-Term Recall!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Southern California, Nvidia</span>
                </div>
                <div class="primary-text">
                    This research introduces ReMEmbR, a system that uses a retrieval-based LLM-agent to query a long-horizon memory of a robot's experiences. This differs from previous work that focused on short-term memory or fixed-size context windows.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet48">
            <div class="start-time-icon" title="Play from here">20:23</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13655" target="_blank">@arXiv 2409.13655</a>
                    <span class="tweet-title">Ads Auction Tuning:  A Mix-Master Approach to Finding the Sweet Spot</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel approach to tuning ads auctions using a mixture distribution as the proposal distribution. Unlike previous methods that rely on single Gaussian distributions, this approach dynamically adjusts both the mixture distribution parameters and their mixing rates at each iteration, enhancing search diversity and efficiency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet49">
            <div class="start-time-icon" title="Play from here">20:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13108" target="_blank">@arXiv 2409.13108</a>
                    <span class="tweet-title">RL's Regretful Heart: When AI Gets Lost in the Pixels</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University, Nokia Bell Labs</span>
                </div>
                <div class="primary-text">
                    This paper introduces the concepts of "recognition regret" and "decision regret" to disentangle the sources of error in image-based reinforcement learning (RL) agents. This approach allows researchers to pinpoint whether the agent's problem lies in extracting relevant features from images or in making decisions based on those features.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet50">
            <div class="start-time-icon" title="Play from here">21:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13589" target="_blank">@arXiv 2409.13589</a>
                    <span class="tweet-title">MRI's Secret Sauce:  Unlocking the Power of Frequency Domain Data!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research integrates k-space features, representing the frequency domain data in MRI scans, into a CNN model. This approach differs from previous work by directly incorporating frequency domain information, enhancing model interpretability and diagnostic accuracy.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet51">
            <div class="start-time-icon" title="Play from here">21:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13471" target="_blank">@arXiv 2409.13471</a>
                    <span class="tweet-title">Brain's Got a New Trick: Learning by Mimicking!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Caltech</span>
                </div>
                <div class="primary-text">
                    This study proposes a new model of stimulus substitution in the cortex, using two-compartment neurons and mixed stimulus representations, which allows for learning a wide range of associations without task-specific parameter fine-tuning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet52">
            <div class="start-time-icon" title="Play from here">21:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13096" target="_blank">@arXiv 2409.13096</a>
                    <span class="tweet-title">Decision Trees: The Key to Cracking Code-Breaking?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford</span>
                </div>
                <div class="primary-text">
                    This paper establishes a novel connection between the problem of learning decision trees and the parameterized Nearest Codeword Problem (k-NCP). It shows that any improvement in the runtime of the fastest known algorithm for learning decision trees would lead to exponentially better approximation algorithms for k-NCP. This connection is unique as previous research on these two problems has proceeded independently.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet53">
            <div class="start-time-icon" title="Play from here">22:18</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13688" target="_blank">@arXiv 2409.13688</a>
                    <span class="tweet-title">Microplastics:  A Deep Dive into a New Dataset for Detecting Tiny Plastic Troublemakers</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Toronto Metropolitan University</span>
                </div>
                <div class="primary-text">
                    This research introduces a new, open-source dataset called MiNa, specifically designed for training deep learning algorithms to detect and classify microplastics and nanoplastics. Unlike previous datasets, MiNa focuses on categorizing particles by polymer type rather than just their physical shape.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet54">
            <div class="start-time-icon" title="Play from here">22:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13530" target="_blank">@arXiv 2409.13530</a>
                    <span class="tweet-title">Time Series Models Get a Memory Boost:  Infini-Channel Mixer Makes Long Data a Breeze!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel compressive memory mechanism called Infini-Channel Mixer (ICM) to enable encoder-only Transformers to effectively model multivariate time series. Unlike previous approaches that either downsample long time series or treat channels independently, ICM allows the model to handle long and multivariate data by aggregating information from all channels into a compressive memory matrix.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet55">
            <div class="start-time-icon" title="Play from here">23:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13654" target="_blank">@arXiv 2409.13654</a>
                    <span class="tweet-title">Neural Network's Long-Term Memory: A Kalman Filter for Better Predictions</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Maryland</span>
                </div>
                <div class="primary-text">
                    This research introduces a "neural filter" that combines neural network predictions with measurements from the physical system to improve long-term prediction accuracy. This approach differs from previous work by directly addressing the issue of error accumulation in neural network models of dynamic systems.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet56">
            <div class="start-time-icon" title="Play from here">23:24</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13371" target="_blank">@arXiv 2409.13371</a>
                    <span class="tweet-title">Prostate Segmentation Gets a Semi-Supervised Makeover: MCICSAM to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Northeastern University, China Medical University, University of Chicago...</span>
                </div>
                <div class="primary-text">
                    This research introduces MCICSAM, a semi-supervised learning model that leverages the Segment Anything Model (SAM) for prostate segmentation. Unlike previous approaches, MCICSAM incorporates Monte Carlo uncertainty analysis into interpolation consistency training, enhancing the model's ability to learn from unlabeled data.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet57">
            <div class="start-time-icon" title="Play from here">23:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.13385" target="_blank">@arXiv 2409.13385</a>
                    <span class="tweet-title">LLMs on a Diet:  How to Shrink Knowledge Without Losing the Brains!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research focuses on contextual compression techniques for Large Language Models (LLMs), specifically exploring methods to reduce the size of the context while preserving its meaning and relevance. This is different from previous work that primarily focused on increasing the context window size or using more efficient attention mechanisms.
                </div>
            </div>
        </div>

        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202409231517_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>    
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>
</html>