
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ribbit Ribbit - Discover research the fun way!</title>
    <meta name="description"
        content="Discover top ArXiv papers in AI and machine learning daily with our fresh picks. Browse easily through tweet-sized summaries or listen on-the-go. Make learning engaging and accessible anytime, anywhere.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Amatic+SC:wght@400;700&family=Dosis:wght@200..800&family=Roboto:wght@300..700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
    <link rel="icon" href="assets/frogFavicon.png" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>
</head>

<body>
    <div id="headerId" class="header"></div>
    <div class="container">
        <div id="topblock">
            <div style="height: 150px;"></div>
            <div class="page-title">DISCOVER RESEARCH THE FUN WAY</div>
            <div style="display: flex; flex-direction: column; justify-content: flex-start; align-items: flex-start;">
                <div class="institute-text" style="padding-top: 3px; line-height: 1.2;">Fresh Picks: 
                    <span class="highlightNumber" style="font-size: 28px;">47</span> out of <span
                    class="highlightNumber">294</span> AI papers
                </div>
                <div class="institute-text" style="line-height: 1.2;">that were just released in arXiv on</div>
                <div id="data-default-date" data-date="2024-09-30"></div>
                <input type="text" id="selectedDate" style="text-decoration: underline;" />
            </div>
            <div style=" height: 80px;">
            </div>
        </div>


        <div class="tweet" id="tweet0">
            <div class="start-time-icon" title="Play from here">01:00</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18314" target="_blank">@arXiv 2409.18314</a>
                    <span class="tweet-title">Model Merging:  A Recipe for Generalization, But Don't Overcook It!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto</span>
                </div>
                <div class="primary-text">
                    This research focuses on evaluating the ability of model merging to achieve compositional generalization, a more realistic goal than simply improving performance on tasks the constituent models were trained on. It benchmarks different merging methods across image classification, image generation, and natural language processing, providing a comprehensive and unified evaluation setup.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet1">
            <div class="start-time-icon" title="Play from here">01:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18428" target="_blank">@arXiv 2409.18428</a>
                    <span class="tweet-title">Multilingual Speech Recognition:  A Simple Trick for Big Gains!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Meta, CMU</span>
                </div>
                <div class="primary-text">
                    This research proposes a novel N-best re-ranking approach for multilingual Automatic Speech Recognition (ASR) systems. Unlike previous work that focuses on improving the spoken language identification (SLID) model itself, this paper leverages external features like language models and text-based language identification models to re-rank the ASR outputs, effectively mitigating the impact of SLID errors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet2">
            <div class="start-time-icon" title="Play from here">01:49</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18209" target="_blank">@arXiv 2409.18209</a>
                    <span class="tweet-title">Unnormalized Distributions: A Unified View Through the Lens of NCE</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This paper provides a unified perspective on various methods for learning unnormalized distributions, which have been independently proposed and studied in separate research communities, through the lens of Noise-Contrastive Estimation (NCE).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet3">
            <div class="start-time-icon" title="Play from here">02:09</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18475" target="_blank">@arXiv 2409.18475</a>
                    <span class="tweet-title">Data Analysis Gets a Makeover: AI Makes It Easy Peasy!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Microsoft</span>
                </div>
                <div class="primary-text">
                    This research explores how generative AI can be used to enhance the entire data analysis workflow, going beyond just code generation and focusing on user-centered design principles. It differs from previous work by examining how AI can assist with tasks like data discovery, hypothesis exploration, and report generation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet4">
            <div class="start-time-icon" title="Play from here">02:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18216" target="_blank">@arXiv 2409.18216</a>
                    <span class="tweet-title">Instruction Following:  A Multimodal, Multi-Turn Chat Challenge for LLMs</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Google</span>
                </div>
                <div class="primary-text">
                    This research introduces a new benchmark, MMMT-IF, for evaluating instruction following capabilities of large language models (LLMs) in a multimodal, multi-turn dialogue setting. Unlike previous work that focuses on single-turn or text-based evaluations, MMMT-IF incorporates images and multiple instructions dispersed across a conversation, making it more challenging and realistic.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet5">
            <div class="start-time-icon" title="Play from here">02:51</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18582" target="_blank">@arXiv 2409.18582</a>
                    <span class="tweet-title">Protein Design: A Game of Amino Acid Chess!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google, Max Planck Society, ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research introduces GAMEOPT, a novel game-theoretical approach to combinatorial Bayesian optimization. Unlike previous methods that struggle with large, unstructured search spaces, GAMEOPT defines a cooperative game between the variables, allowing for efficient computation of game equilibria as candidate evaluation points.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet6">
            <div class="start-time-icon" title="Play from here">03:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18313" target="_blank">@arXiv 2409.18313</a>
                    <span class="tweet-title">Robots Get Memories: Embodied-RAG Gives Bots a Brain Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces Embodied-RAG, a framework that builds a hierarchical memory system for embodied agents, allowing them to store and retrieve experiences at different levels of detail. This differs from previous work by addressing the challenges of applying retrieval-augmented generation (RAG) to the embodied domain, which involves multimodal data, correlated experiences, and varying levels of perception.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet7">
            <div class="start-time-icon" title="Play from here">03:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18390" target="_blank">@arXiv 2409.18390</a>
                    <span class="tweet-title">From Speech to Stuff: Robots Build Your Words!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research combines 3D generative AI with discrete robotic assembly, using voxels as building blocks, to create physical objects directly from speech input. This differs from previous work that primarily focused on 3D printing or CNC machining of smaller objects.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet8">
            <div class="start-time-icon" title="Play from here">03:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18653" target="_blank">@arXiv 2409.18653</a>
                    <span class="tweet-title">SAM2:  The Camouflage-Busting Vision Model</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich, University of Zurich, University of Bologna</span>
                </div>
                <div class="primary-text">
                    This research explores the application of the Segment Anything Model 2 (SAM2) for video camouflaged object segmentation (VCOS), a task that involves identifying objects that blend seamlessly into their surroundings in videos. Unlike previous work that focused on static images, this study investigates SAM2's performance in dynamic scenarios, where both objects and backgrounds can change over time.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet9">
            <div class="start-time-icon" title="Play from here">04:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18431" target="_blank">@arXiv 2409.18431</a>
                    <span class="tweet-title">Search3D:  Giving 3D Scenes a  Brain  for  Open-Vocabulary  Queries!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">ETH Zurich</span>
                </div>
                <div class="primary-text">
                    This research introduces Search3D, a hierarchical open-vocabulary 3D segmentation method that goes beyond object-level queries to understand finer-grained scene entities like object parts and attributes. Unlike previous methods that focus on object-centric representations, Search3D builds a hierarchical scene graph representation, enabling more flexible and granular queries.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet10">
            <div class="start-time-icon" title="Play from here">04:39</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18876" target="_blank">@arXiv 2409.18876</a>
                    <span class="tweet-title">Face Recognition's New Trick:  Generating Faces That Look *Almost* Like You!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Queen Mary University of London, University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research proposes a new method for generating synthetic faces that are more discriminative for training face recognition models. Unlike previous approaches that focused on generating diverse styles, this method focuses on generating faces with varying levels of similarity to the original image, creating "semi-hard" samples that are more effective for training.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet11">
            <div class="start-time-icon" title="Play from here">05:04</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18614" target="_blank">@arXiv 2409.18614</a>
                    <span class="tweet-title">Meta-Surfaces:  The New Convolutional Kernel Kings!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel frequency domain training method for creating large, arbitrary analog convolution kernels using metasurfaces. This approach differs from previous work by enabling the generation of kernels with both positive and negative weights, which is crucial for complex machine vision tasks.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet12">
            <div class="start-time-icon" title="Play from here">05:25</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18326" target="_blank">@arXiv 2409.18326</a>
                    <span class="tweet-title">Tired of Manually Measuring Melt Tracks? This AI Does It for You!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Massachusetts Institute of Technology, University of Illinois at Urbana-Champaign</span>
                </div>
                <div class="primary-text">
                    This research introduces a U-Net neural network specifically designed to automatically segment and analyze microscopy images of laser powder bed fusion melt tracks, a task previously done manually.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet13">
            <div class="start-time-icon" title="Play from here">05:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18203" target="_blank">@arXiv 2409.18203</a>
                    <span class="tweet-title">Mapping Out AI's Wildest Behavior: A New Tool for Taming Language Models</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Stanford University, Apple Inc., CMU</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel approach to designing AI policies for large language models (LLMs) by drawing inspiration from mapmaking. Unlike previous methods that focus on defining principles or using past cases, this approach emphasizes explicitly representing policy coverage over an unbounded space of model behaviors.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet14">
            <div class="start-time-icon" title="Play from here">06:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18462" target="_blank">@arXiv 2409.18462</a>
                    <span class="tweet-title">Brain Waves to Blood Flow: A New Way to Translate Brain Signals!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Yale University, Mila - Quebec AI Institute, Université de Montr´eal...</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel framework called SAMBA, which translates between electrophysiological signals (like EEG) and hemodynamic signals (like fMRI) by learning a unified latent space. Unlike previous work that focused on enhancing fMRI signals with EEG, SAMBA aims to bridge the spatial and temporal resolution gaps between these modalities.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet15">
            <div class="start-time-icon" title="Play from here">06:29</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18330" target="_blank">@arXiv 2409.18330</a>
                    <span class="tweet-title">AI Goes Blind: New Benchmark Tests Robots' Vision in Distracting Worlds</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset, DMC-VB, specifically designed to evaluate how well offline reinforcement learning agents can handle visual distractions. Unlike previous datasets, DMC-VB includes a variety of tasks, distractors, and data quality levels, making it a more comprehensive benchmark for testing the robustness of visual representation learning methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet16">
            <div class="start-time-icon" title="Play from here">06:52</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18333" target="_blank">@arXiv 2409.18333</a>
                    <span class="tweet-title">Similarity Measures:  A Framework for Keeping It Real (and Consistent)</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">MIT</span>
                </div>
                <div class="primary-text">
                    This research tackles the growing problem of inconsistent naming and implementation conventions for similarity measures, which are used to compare artificial and biological systems. The authors propose a framework for standardizing these measures, creating a repository that benchmarks and validates them.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet17">
            <div class="start-time-icon" title="Play from here">07:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18303" target="_blank">@arXiv 2409.18303</a>
                    <span class="tweet-title">Deep Learning Speeds Up Brain Scans by 600 Times!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Massachusetts General Hospital, Harvard Medical School, Medical University of Vienna...</span>
                </div>
                <div class="primary-text">
                    This research introduces a deep learning model called Deep-ER for reconstructing Magnetic Resonance Spectroscopic Imaging (MRSI) data acquired using the ECCENTRIC pulse sequence. Unlike previous work, Deep-ER operates on non-Cartesian k-space data and reconstructs each time point independently, making it more flexible and generalizable.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet18">
            <div class="start-time-icon" title="Play from here">07:36</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18901" target="_blank">@arXiv 2409.18901</a>
                    <span class="tweet-title">Visual Object Tracking Gets a Prompting Makeover: CLIP to the Rescue!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National Yang Ming Chiao Tung University, Academia Sinica</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel visual prompting mechanism for generic object tracking. Unlike previous methods that rely on predefined language descriptors, this approach leverages the zero-shot capability of CLIP to automatically generate and refine visual prompts, enabling the tracker to adapt to new targets and handle unseen objects.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet19">
            <div class="start-time-icon" title="Play from here">08:02</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18290" target="_blank">@arXiv 2409.18290</a>
                    <span class="tweet-title">AI Doctor's In-Basket: Can GPT-4 Handle Prostate Cancer Queries?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Mayo Clinic</span>
                </div>
                <div class="primary-text">
                    This research focuses on evaluating a specialized Large Language Model (LLM) called RadOnc-GPT, trained specifically for prostate cancer, to generate responses to patient inquiries within an electronic health record (EHR) messaging system. Unlike previous studies that applied LLMs across various disease sites, this study focuses on a specific disease domain, allowing for more accurate and relevant responses.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet20">
            <div class="start-time-icon" title="Play from here">08:19</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18339" target="_blank">@arXiv 2409.18339</a>
                    <span class="tweet-title">LLMs Feeling Ambiguous: Can AI Understand Our Mixed Emotions?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Melbourne, MIT, University of New South Wales</span>
                </div>
                <div class="primary-text">
                    This research explores the ability of Large Language Models (LLMs) to recognize ambiguous emotions, a concept often overlooked in previous studies. Unlike prior work focusing on single emotion labels, this paper investigates how LLMs can understand and predict the complex, multi-faceted nature of human emotions.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet21">
            <div class="start-time-icon" title="Play from here">08:43</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18319" target="_blank">@arXiv 2409.18319</a>
                    <span class="tweet-title">LLMs Get Structured:  Lung Cancer Reports Go From Free-Text to Fancy JSON!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Rensselaer Polytechnic Institute, Wake Forest University, Massachusetts General Hospital...</span>
                </div>
                <div class="primary-text">
                    This research focuses on using large language models (LLMs) to automatically create structured radiology reports for lung cancer screening, a task that has been challenging due to formatting errors and content hallucinations. The authors propose a dynamic template-constrained decoding method to address these issues, which consistently improves the performance of open-source LLMs on cross-institutional datasets.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet22">
            <div class="start-time-icon" title="Play from here">09:08</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18961" target="_blank">@arXiv 2409.18961</a>
                    <span class="tweet-title">ProMerge:  Unsupervised Instance Segmentation Gets a Speed Boost!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This paper proposes ProMerge, a new method for unsupervised instance segmentation that uses self-supervised visual features to generate initial groupings of patches and then merges them iteratively. This approach differs from previous methods that rely on computationally intensive normalized-cut algorithms.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet23">
            <div class="start-time-icon" title="Play from here">09:27</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18827" target="_blank">@arXiv 2409.18827</a>
                    <span class="tweet-title">RL Hyperparameter Tuning:  A Benchmark That's Actually Fast!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Leibniz University Hannover, RWTH Aachen University, University of Freiburg...</span>
                </div>
                <div class="primary-text">
                    This research introduces ARLBench, a benchmark for hyperparameter optimization (HPO) in reinforcement learning (RL) that focuses on efficiency and flexibility. Unlike previous benchmarks, ARLBench allows for large configuration spaces and supports dynamic hyperparameter schedules, making it more representative of real-world HPO applications.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet24">
            <div class="start-time-icon" title="Play from here">09:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18680" target="_blank">@arXiv 2409.18680</a>
                    <span class="tweet-title">AI Gets Two Ears:  New Model Listens to Multiple Audios at Once!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">National University of Singapore, A*STAR, Polytechnic University of Madrid...</span>
                </div>
                <div class="primary-text">
                    This research introduces the first multi-audio benchmark (MAE) for evaluating audio large language models (ALLMs).  Previous work primarily focused on single-audio tasks, while MAE assesses the ability of ALLMs to process multiple audio streams simultaneously.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet25">
            <div class="start-time-icon" title="Play from here">10:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18401" target="_blank">@arXiv 2409.18401</a>
                    <span class="tweet-title">Texturing 3D Models:  A New Way to Make Textures Look Real!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Zhejiang University, Tencent IEG, University College London</span>
                </div>
                <div class="primary-text">
                    This research introduces a novel local attention mechanism that leverages 3D priors to improve the consistency and quality of textures generated for 3D models. Unlike previous methods that rely on global attention or independent view generation, this approach focuses on enhancing local details while preserving cross-view consistency.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet26">
            <div class="start-time-icon" title="Play from here">10:45</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18486" target="_blank">@arXiv 2409.18486</a>
                    <span class="tweet-title">OpenAI's o1: Can This AI Actually Think?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Alberta, University of Georgia, Northwestern Polytechnical University...</span>
                </div>
                <div class="primary-text">
                    This research evaluates OpenAI's o1 model on a diverse set of complex reasoning tasks, going beyond standard benchmarks. The study focuses on o1's ability to handle multi-step reasoning problems across various domains, including coding, mathematics, science, and medicine. This approach differs from previous work by emphasizing the model's reasoning capabilities rather than solely focusing on its ability to generate human-like text.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet27">
            <div class="start-time-icon" title="Play from here">11:13</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18842" target="_blank">@arXiv 2409.18842</a>
                    <span class="tweet-title">Overfitting's New Trick: It's Not Always Bad, But It Depends!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This paper explores how the bias-variance tradeoff, a fundamental concept in statistics, changes when we move from evaluating models on the same data they were trained on (in-sample) to evaluating them on new, unseen data (out-of-sample).
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet28">
            <div class="start-time-icon" title="Play from here">11:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18201" target="_blank">@arXiv 2409.18201</a>
                    <span class="tweet-title">Loop-Diffusion: A Protein Loop Whisperer Learns to Score and Design!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington</span>
                </div>
                <div class="primary-text">
                    This research introduces Loop-Diffusion, a diffusion model trained on a dataset of protein loops to learn an energy function that can be used to predict the effects of mutations on protein function. This approach differs from previous work by focusing specifically on loops, which are often responsible for protein function, and by leveraging a large dataset of general protein loops to learn a more generalizable energy function.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet29">
            <div class="start-time-icon" title="Play from here">11:54</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18448" target="_blank">@arXiv 2409.18448</a>
                    <span class="tweet-title">Taming the Wild Model Drift: A Multi-Timescale Gradient Correction for Hierarchical Federated Learning</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Purdue University, Yonsei University, IBM</span>
                </div>
                <div class="primary-text">
                    This paper proposes a novel multi-timescale gradient correction (MTGC) method for hierarchical federated learning (HFL). Unlike previous HFL algorithms, MTGC introduces distinct control variables to correct model drift at multiple levels of the hierarchy, addressing the challenge of multi-timescale model drift caused by data heterogeneity.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet30">
            <div class="start-time-icon" title="Play from here">12:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18324" target="_blank">@arXiv 2409.18324</a>
                    <span class="tweet-title">GPU Power: It's Not Just About the Hardware, It's About the Data!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Washington, Microsoft Azure Research</span>
                </div>
                <div class="primary-text">
                    This research explores the impact of input data patterns on GPU power consumption during GEMM operations, a core component of machine learning workloads. Unlike previous work that focused on hardware optimizations, this study investigates how varying input data values, placement, and sparsity can significantly affect power usage.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet31">
            <div class="start-time-icon" title="Play from here">12:44</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18223" target="_blank">@arXiv 2409.18223</a>
                    <span class="tweet-title">Light Field Microscopy Gets a Neural Makeover:  Sharper Images, Faster Than Ever!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Tsinghua University</span>
                </div>
                <div class="primary-text">
                    This research introduces PNR, a method for high-resolution light field microscopy (LFM) reconstruction that utilizes an unsupervised and explicit feature representation approach. Unlike previous methods, PNR incorporates a frequency-based training loss, enabling better recovery of high-frequency details.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet32">
            <div class="start-time-icon" title="Play from here">13:05</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18924" target="_blank">@arXiv 2409.18924</a>
                    <span class="tweet-title">AI Patient:  EHRs  Meet  LLMs  for  a  Medical  Chatbot  That  Actually  Knows  Its  Stuff!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Harvard University</span>
                </div>
                <div class="primary-text">
                    This research introduces AIPatient, a simulated patient system that uses a knowledge graph built from Electronic Health Records (EHRs) and a multi-agent workflow powered by Large Language Models (LLMs) to generate realistic and accurate patient responses to medical questions. This approach differs from previous work by incorporating a more comprehensive and diverse patient knowledgebase, along with a robust reasoning framework to minimize hallucinations and ensure consistency in responses.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet33">
            <div class="start-time-icon" title="Play from here">13:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18402" target="_blank">@arXiv 2409.18402</a>
                    <span class="tweet-title">Embeddings for Inference:  A New Way to Squeeze Data and Find Parameters!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">The University of Chicago</span>
                </div>
                <div class="primary-text">
                    This paper introduces Embed and Emulate (E&E), a new method for simulation-based inference (SBI) that uses contrastive learning to efficiently handle high-dimensional data. Unlike previous methods that directly model the likelihood or posterior, E&E learns a low-dimensional latent embedding of the data and a corresponding emulator in the latent space.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet34">
            <div class="start-time-icon" title="Play from here">14:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18158" target="_blank">@arXiv 2409.18158</a>
                    <span class="tweet-title">Transformer Point Processes: Ditch the Thinning Algorithm, Get Faster Predictions!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Cambridge</span>
                </div>
                <div class="primary-text">
                    This research proposes a new framework for modeling marked point processes that decomposes the likelihood function into separate distributions for event types and inter-event times. This approach eliminates the need for the computationally intensive thinning algorithm used in previous Transformer-based methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet35">
            <div class="start-time-icon" title="Play from here">14:22</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18819" target="_blank">@arXiv 2409.18819</a>
                    <span class="tweet-title">Swiss Nurses Get a Talking AI: Can It Understand Their Dialects?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Bern University of Applied Sciences, Bern Academy of the Arts</span>
                </div>
                <div class="primary-text">
                    This research focuses on the challenges of using transcription models in home care nursing in Switzerland, specifically addressing the need for models to handle local dialects and foreign accents in German. This is different from previous work that primarily focuses on English language transcription.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet36">
            <div class="start-time-icon" title="Play from here">14:37</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18382" target="_blank">@arXiv 2409.18382</a>
                    <span class="tweet-title">Robots Learning to Run? LLMs Are the New Coaches!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">UC Berkeley</span>
                </div>
                <div class="primary-text">
                    This research proposes CurricuLLM, a system that uses large language models (LLMs) to automatically design training curricula for robots. Unlike previous methods that rely on human experts or predefined tasks, CurricuLLM leverages LLMs' ability to understand and translate natural language into executable code to generate a sequence of subtasks that progressively increase in difficulty.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet37">
            <div class="start-time-icon" title="Play from here">14:57</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18359" target="_blank">@arXiv 2409.18359</a>
                    <span class="tweet-title">AI Makes Fluid Dynamics Flow Faster and More Accurately</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Jua.ai, California Institute of Technology, ETH Zurich...</span>
                </div>
                <div class="primary-text">
                    This research introduces a generative AI algorithm, GenCFD, based on conditional score-based diffusion models, for computing statistical solutions of fluid flow equations. Unlike previous work that focuses on deterministic models, GenCFD leverages the inherent instability of turbulent fluid flows to achieve accurate statistical computation.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet38">
            <div class="start-time-icon" title="Play from here">15:31</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18804" target="_blank">@arXiv 2409.18804</a>
                    <span class="tweet-title">Diffusion Models: Learning Data on Manifolds Without Getting Lost in Dimensions</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Oxford</span>
                </div>
                <div class="primary-text">
                    This research explores the convergence of diffusion models under the manifold hypothesis, proving that they achieve rates independent of the ambient dimension in terms of learning the score function. This is a significant departure from previous work, which showed a strong dependence on the ambient dimension.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet39">
            <div class="start-time-icon" title="Play from here">15:55</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18417" target="_blank">@arXiv 2409.18417</a>
                    <span class="tweet-title">RLHF Just Got an Auction Makeover:  Bidding for Better Bots!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">State Key Laboratory of General Artificial Intelligence  BIGAI, Allianz Global Investors Japan Co.  Ltd.</span>
                </div>
                <div class="primary-text">
                    This paper introduces a novel auction-based mechanism for collecting preference data in RLHF, addressing the cost-efficiency concerns of dataset construction. Unlike previous methods, it incentivizes LLM agents to provide truthful bids, leading to a more efficient allocation of resources.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet40">
            <div class="start-time-icon" title="Play from here">16:16</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18761" target="_blank">@arXiv 2409.18761</a>
                    <span class="tweet-title">Galaxies Aligned: Deep Learning Predicts Cosmic Web's Orientation</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">CMU</span>
                </div>
                <div class="primary-text">
                    This research uses a deep generative model trained on the IllustrisTNG-100 simulation to sample 3D galaxy shapes and orientations, accurately reproducing intrinsic alignments. This approach differs from previous work by employing E(3) equivariant Graph Neural Networks, which explicitly respect the Euclidean symmetries of the universe.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet41">
            <div class="start-time-icon" title="Play from here">16:42</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18164" target="_blank">@arXiv 2409.18164</a>
                    <span class="tweet-title">Data Prep Kit:  LLM's New BFF for Taming Wild Data</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">IBM</span>
                </div>
                <div class="primary-text">
                    This research introduces a new open-source toolkit called Data Prep Kit (DPK) specifically designed for preparing data for Large Language Model (LLM) development. Unlike previous projects that focus on data preparation for specific tasks like pretraining, DPK aims to be more versatile, supporting a wider range of LLM applications, including fine-tuning, RAG, and instruction tuning.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet42">
            <div class="start-time-icon" title="Play from here">17:06</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18885" target="_blank">@arXiv 2409.18885</a>
                    <span class="tweet-title">Extreme Weather Forecasting:  A Dataset So Hot, It's 3km Resolution!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Manchester, Hunan University, Microsoft Research...</span>
                </div>
                <div class="primary-text">
                    This research introduces a new dataset, HR-Extreme, specifically designed for evaluating extreme weather forecasting models. Unlike previous datasets, HR-Extreme uses high-resolution data from the High-Resolution Rapid Refresh (HRRR) system, providing a more detailed and accurate representation of extreme weather events.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet43">
            <div class="start-time-icon" title="Play from here">17:38</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18170" target="_blank">@arXiv 2409.18170</a>
                    <span class="tweet-title">AI Doctors: Can We Trust Them to Summarize Our Medical Records?</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Wisconsin  Madison, University of Colorado  Aurora, Epic Systems...</span>
                </div>
                <div class="primary-text">
                    This research focuses on evaluating the performance of Large Language Models (LLMs) for summarizing medical text, specifically addressing the challenges of ensuring accuracy and reliability in a high-stakes environment like healthcare. It proposes a new approach using LLMs themselves as evaluators, a departure from traditional human or automated methods.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet44">
            <div class="start-time-icon" title="Play from here">18:01</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18459" target="_blank">@arXiv 2409.18459</a>
                    <span class="tweet-title">Recipe for Success: AI Cooks Up Japanese Cuisine with Multimodal Magic</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Tokyo</span>
                </div>
                <div class="primary-text">
                    This research focuses on generating Japanese recipes from food images using Multimodal Large Language Models (MLLMs). Unlike previous work that primarily used English recipes, this study utilizes a Japanese recipe dataset and evaluates the generated recipes based on Japanese food culture.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet45">
            <div class="start-time-icon" title="Play from here">18:21</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18239" target="_blank">@arXiv 2409.18239</a>
                    <span class="tweet-title">Speech Enhancement on Hearables:  Sub-Millisecond Latency,  No More Lag!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">Google DeepMind, Google Research, Google Platforms & Devices</span>
                </div>
                <div class="primary-text">
                    This research focuses on achieving sub-millisecond latency for real-time speech enhancement, a feat previously unexplored in the field.  The paper introduces a Deep FIR filtering method that enables sample-by-sample processing, significantly reducing algorithmic latency compared to traditional techniques.
                </div>
            </div>
        </div>
        <div class="tweet" id="tweet46">
            <div class="start-time-icon" title="Play from here">18:40</div>
            <div class="tweet-content">
                <div class="tweet-header">
                    <a class="arxiv-id" href="https://arxiv.org/abs/2409.18472" target="_blank">@arXiv 2409.18472</a>
                    <span class="tweet-title">URIEL+:  Giving Languages a Voice, Even the Quiet Ones!</span>
                </div>
                <div class="institute-line">
                    <img class="institute-icon" src="assets/buttonInstitute.svg" alt="Institute Icon">
                    <span class="institute-text">University of Toronto, Ghent University, Ontario Tech University</span>
                </div>
                <div class="primary-text">
                    This research expands the URIEL knowledge base by integrating five new databases, increasing the number of languages with typological data from 2724 to 4366. It also introduces new imputation algorithms and confidence scores to improve the reliability of distance calculations.
                </div>
            </div>
        </div>

        <div
            style="padding: 50px 0; text-align: center; margin: 5px 0; font-weight: 300; font-size: 10px; color: #000000;">
            Opening music
            from <a href="https://ikson.com" target="_blank" style="color: black; text-decoration: none;">TELL
                YOUR STORY by ikson</a></div>
        <div style="height: 50px;"></div>
    </div>

    <footer class="player-footer">
        <div class="player-detail-hidden-on-small-screen">
            <div id="audio-title" class="tweet-title-small">Hit play and learn on the go!</div>
            <div style="height: 2px;"></div>
            <div id="audio-institutes" class="institute-text-small"></div>
        </div>
        <div class="control-panel">
            <div class="control-horizontal">
                <div id="playSpeedButton" title="Adjust speed">
                    <div id="selectedSpeed">1&times;</div>
                    <div class="speedMenuItems">
                        <div data-value="0.6">Speed 0.6&times;</div>
                        <div data-value="0.8">Speed 0.8&times;</div>
                        <div data-value="1" class="selected">Speed 1&times;</div>
                        <div data-value="1.2">Speed 1.2&times;</div>
                        <div data-value="1.4">Speed 1.4&times;</div>
                        <div data-value="1.6">Speed 1.6&times;</div>
                    </div>
                    <select id="speedOptionsHidden">
                        <option value="0.6">0.6&times;</option>
                        <option value="0.8">0.8&times;</option>
                        <option value="1" selected>1&times;</option>
                        <option value="1.2">1.2&times;</option>
                        <option value="1.4">1.4&times;</option>
                        <option value="1.6">1.6&times;</option>
                    </select>
                </div>
                <div id="playLastButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(false)" title="Play last" disabled>
                        <img src="assets/buttonLastEpisode.svg" alt="Last" style="width: 12px;">
                    </button>
                </div>
                <button class="controllerButton" onclick="scrollToCurrentTweet()" title="Scoll to the current episode">
                    <img id="locateImage" src="assets/buttonTarget.svg" alt="Locate">
                </button>
                <button class="controllerButton" onclick="togglePlayPause()" title="Play/Pause">
                    <img id="playPauseImage" src="assets/buttonPlay.svg" alt="Play">
                </button>
                <div id="playNextButton" style="position: relative;">
                    <div class="footerButtonMask"></div>
                    <button class="controllerButton" onclick="playAdjacentTweet(true)" title="Play next" disabled>
                        <img src="assets/buttonNextEpisode.svg" alt="Next" style="width: 12px;">
                    </button>
                </div>
            </div>
            <div class="control-horizontal">
                <div id="progressTime">0:00</div>
                <div id="progressContainer" onclick="seek(event)">
                    <div id="progressBar"></div>
                    <div id="progressCircle" draggable="true"></div>
                </div>
                <audio id="audioPlayer"
                    src="https://d2irtorupa9e8g.cloudfront.net/daily_podcast/202409301846_audio.mp3"></audio>
                <div id="progressTime"><span id="audioDuration">Loading...</span></div>
            </div>
        </div>
    </footer>
    <div id="privacyModal" class="modal"></div>
    <script src="script.js"></script>
    <script src="calendar.js"></script>    
    <script>
        fetch('https://ribbitribbit.co/header.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('headerId').innerHTML = data;
            })
            .catch(error => console.error('Error loading header.html:', error));
        fetch('https://ribbitribbit.co/terms.html')
            .then(response => response.text()) // Convert the response to text
            .then(data => {
                document.getElementById('privacyModal').innerHTML = data;
            })
            .catch(error => console.error('Error loading terms.html:', error));
    </script>
</body>
</html>